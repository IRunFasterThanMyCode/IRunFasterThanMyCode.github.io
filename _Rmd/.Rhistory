geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
ggplot(URC, aes(x = comments_count, y = likes_count)) +
geom_point(shape = 19, alpha = 0.2, size = 5) +
geom_smooth(method = "lm", se = TRUE) +
geom_point(URC %>% top_n(5, comments_count), aes(x = comments_count, y = likes_count)) +
labs(x = "Number of Comments", y = "Number of Likes") +
theme(axis.text   = element_text(size = 16),
axis.title  = element_text(size = 20))
URC %>% top_n(5, comments_count)
ggplot(URC, aes(x = comments_count, y = likes_count)) +
geom_point(shape = 19, alpha = 0.2, size = 5) +
geom_smooth(method = "lm", se = TRUE) +
geom_point(data = URC %>% top_n(5, comments_count), aes(x = comments_count, y = likes_count)) +
labs(x = "Number of Comments", y = "Number of Likes") +
theme(axis.text   = element_text(size = 16),
axis.title  = element_text(size = 20))
ggplot(URC, aes(x = comments_count, y = likes_count)) +
geom_point(shape = 19, alpha = 0.2, size = 5) +
geom_smooth(method = "lm", se = TRUE) +
geom_point(data = URC %>% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = "red", size = 5)) +
labs(x = "Number of Comments", y = "Number of Likes") +
theme(axis.text   = element_text(size = 16),
axis.title  = element_text(size = 20))
ggplot(URC, aes(x = comments_count, y = likes_count)) +
geom_point(shape = 19, alpha = 0.2, size = 5) +
geom_smooth(method = "lm", se = TRUE) +
geom_point(data = URC %>% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = "red", size = 5)) +
geom_point(data = URC %>% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = "blue", size = 5)) +
labs(x = "Number of Comments", y = "Number of Likes") +
theme(axis.text   = element_text(size = 16),
axis.title  = element_text(size = 20))
cor(URC$comments_count, URC$likes_count, method = "spearman")
sprintf("%.2f", cor(URC$comments_count, URC$likes_count, method = "spearman"))
library("tm")
library("wordcloud")
library("SnowballC")
popular <- URC                        %>%
top_n(100, likes_count)   %>%
arrange(desc(likes_count)) %>%
transmute(message = iconv(message, "latin1", "ASCII", sub = ""))
cloud_dat <- Corpus(VectorSource(popular$message))                   ## Create Corpus
cloud_dat <- tm_map(cloud_dat, content_transformer(tolower))         ## Convert to lower case
cloud_dat <- tm_map(cloud_dat, removeWords, stopwords("english"))    ## Remove common English words
cloud_dat <- tm_map(cloud_dat, stemDocument)                        ## Condense words down to common stems
wordcloud(cloud_dat, max.words = 1000, min.freq = 30, random.order = FALSE, colors = brewer.pal(9, "RdBu")[-c(4:6)])
popular <- URC                        %>%
top_n(100, comments_count)   %>%
arrange(desc(comments_count)) %>%
transmute(message = iconv(message, "latin1", "ASCII", sub = ""))
cloud_dat <- Corpus(VectorSource(popular$message))                   ## Create Corpus
cloud_dat <- tm_map(cloud_dat, content_transformer(tolower))         ## Convert to lower case
cloud_dat <- tm_map(cloud_dat, stemDocument)                        ## Condense words down to common stems
wordcloud(cloud_dat, max.words = 1000, min.freq = 30, random.order = FALSE, colors = brewer.pal(9, "RdBu")[-c(4:6)])
wordcloud(cloud_dat, max.words = 1000, min.freq = 10, random.order = FALSE, colors = brewer.pal(9, "RdBu")[-c(4:6)])
ggplot(URC, aes(x = comments_count, y = likes_count)) +
geom_point(shape = 19, alpha = 0.2, size = 5) +
geom_smooth(method = "lm", se = TRUE) +
geom_point(data = URC %>% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = "red", size = 5)) +
geom_point(data = URC %>% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = "blue", size = 5)) +
labs(x = "Number of Comments", y = "Number of Likes") +
theme(axis.text   = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
library(text2vec)
library(text2vec)
train_tokens <- train$message                      %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
posts50 <- URC                 %>%
group_by(from_name) %>%
filter(n() >= 50)   %>%
select(from_name, message)
set.seed(0)
ids   <- sample(1:nrow(posts50), 4000)
dim(posts50)
dim(URC)
load("~/Dropbox/Blog/IRunFasterThanMyCode/files/URC_posts.Rdat")
like_comment_share <- URC                                                 %>%
gather(count_type, count, likes_count:shares_count) %>%
select(count_type, count)
head(like_comment_share)
all(like_comment_share$count == as.integer(like_comment_share$count))
all(like_comment_share$count >= 0)
ggplot(like_comment_share, aes(x = log10(count+1), col = count_type, fill = count_type)) +
geom_density(alpha = 0.1) +
labs(x = "Count (log10)", y = "Density") +
theme(axis.text    = element_text(size = 16),
axis.title   = element_text(size = 20),
legend.text  = element_text(size = 18),
legend.title = element_text(size = 24))
top_contributors <- URC              %>%
count(from_name) %>%
top_n(50, n)     %>%
arrange(desc(n))
ggplot(top_contributors,
aes(x = factor(from_name, levels = top_contributors$from_name),
y = n,
fill = n)) +
geom_bar(stat = "identity") +
scale_fill_gradient(low="blue", high="red") +
labs(x = "", y = "Number of Posts") +
theme(axis.title  = element_text(size = 18),
axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 14),
legend.position = "none")
jamesadams <- URC                                                        %>%
filter(from_name == "James Adams")                         %>%
mutate(created_time = as.POSIXct(created_time))            %>%
count(created_time)
jamesadams_xts <- xts(jamesadams$n, order.by = jamesadams$created_time)
jamesadams_month <- apply.monthly(jamesadams_xts, FUN = sum)
plot(jamesadams_month, ylab = "Number of Posts", main = "", cex.lab = 1.7, cex.axis = 1.4)
dim(URC)
tmp <- URC
URC      <- URC %>%
mutate(dow = factor(weekdays(created_time), labels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))
URC      <- URC %>%
mutate(dow = factor(weekdays(as.POSIXct(created_time)), labels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))
dim(URC)
post_day <- URC %>%
count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) +
geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
post_day <- URC %>%
top_n(1000, likes_count)
post_day <- URC                      %>%
top_n(1000, likes_count) %>%
count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) +
geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
post_day <- URC                      %>%
top_n(1000, comments_count) %>%
count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) +
geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
post_day <- URC                      %>%
top_n(1000, likes_count) %>%
count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) +
geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
post_day <- URC                      %>%
top_n(1000, likes_count) %>%
count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) +
geom_bar(stat = "identity") +
labs(x = "", y = "Number of Posts") +
theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
axis.text.y = element_text(size = 16),
axis.title  = element_text(size = 20),
legend.position = "none")
dim(URC)
posts50 <- URC                 %>%
group_by(from_name) %>%
filter(n() >= 50)   %>%
select(from_name, message)
set.seed(0)
ids   <- sample(1:nrow(posts50), 4000)
train <- posts50[ids,]
test  <- posts50[-ids,]
dim(posts50)
library(text2vec)
train_tokens <- train$message                      %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_train <- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train
vocab <- create_vocabulary(it_train)
vocab
vocab <- create_vocabulary(it_train)
vocab      <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
train_dtm  <- create_dtm(it_train, vectorizer)
dim(train_dtm)
library("randomForest")
install.packages("randomForest")
library("randomForest")
head(train_dtm)
class(train_dtm)
names(train_dtm)
slotNames(train_dtm)
train_dtm
head(as.matrix(train_dtm))
as.matrix(train_dtm)[1:10, 1:10]
?randomForest
head(names(train_dtm))
head(rownames(train_dtm))
rf_model <- randomForest(x = train_dtm, y = rownames(train_dtm))
rf_model <- randomForest(x = as.matrix(train_dtm), y = rownames(train_dtm))
rf_model <- randomForest(x = as.matrix(train_dtm), y = as.factor(rownames(train_dtm)))
test_tokens <- test$message                       %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_test     <- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    <- create_dtm(it_test, vectorizer)
?save
save(rf_model, file = "~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model.Rdat")
test_tokens <- test$message                       %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_test     <- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    <- create_dtm(it_test, vectorizer)
test_predict <- predict(rf_model, as.matrix(test_dtm), type = "response")
table(test_predict == rownames(test_dtm))
472/761
472/(761+472
)
?normalize
train_dtm_norm <- normalize(train_dtm, "11")
train_dtm_norm <- normalize(train_dtm, "l1")
tfidf           <- TfIdf$new()
train_dtm_tfidf <- fit_transform(train_dtm, tfidf)
vocab <- create_vocabulary(it_train, ngram = c(1L, 2L, 3L))
vocab <- create_vocabulary(it_train, ngram = c(1L, 3L))
vocab <- vocab %>%
prune_vocabulary(term_count_min = 10)
vectorizer <- vocab_vectorizer(vocab)
dtm_train  <- create_dtm(it_train, vectorizer)
tfidf           <- TfIdf$new()
train_dtm_tfidf <- fit_transform(train_dtm, tfidf)
test_dtm_tfidf  <- create_dtm(it_test, vectorizer) %>%
transform(tfidf)
train_dtm_tfidf <- fit_transform(train_dtm, tfidf)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE, eval = TRUE, results = "hold", fig.height = 10, fig.width = 10, dpi = 300)
options(scipen = 999)
library("randomForest")
train_dtm
?randomForest
rf_model <- randomForest(x = as.matrix(train_dtm_tfidf),
y = as.factor(rownames(train_dtm_tfidf),
ntree = 1000)
save(rf_model, "~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model_tfidf.Rdat")
rf_model <- randomForest(x = as.matrix(train_dtm_tfidf),
y = as.factor(rownames(train_dtm_tfidf)),
ntree = 1000)
rf_model <- randomForest(x = as.matrix(train_dtm_tfidf),
y = as.factor(rownames(train_dtm_tfidf)),
ntree = 1000)
class(rf_model)
rf_model_tfidf <- rf_model
save(rf_model_tfidf, "~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model_tfidf.Rdat")
save(rf_model_tfidf, "~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model_tfidf.Rdat")
save(rf_model_tfidf, file = "~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model_tfidf.Rdat")
load("~/Dropbox/Blog/IRunFasterThanMyCode/files/rf_model.Rdat")
dtm_test       <- create_dtm(it_test, vectorizer)
test_dtm_norm  <- normalize(test_dtm, "l1")
test_dtm_tfidf <- fit_transform(test_dtm, tfidf)
test_predict <- predict(rf_model_tfidf, as.matrix(test_dtm_tfidf), type = "response")
table(test_predict == rownames(test_dtm))
483/(483+750)
table(test_predict == rownames(test_dtm))[2]/sum(table(test_predict == rownames(test_dtm)))
round(100*table(test_predict == rownames(test_dtm))[2]/sum(table(test_predict == rownames(test_dtm))), 1)
library("glmnet")
install.packages("glmnet")
library("glmnet")
glm_train <- cv.glmnet(x = train_dtm_tfidf, y = train$from_name,
family = 'multinomial',
alpha = 1,
type.measure = "deviance",
nfolds = 5,
thresh = 1e-3,
maxit = 1e3)
glm_train
test_predict <- predict(glm_train, as.matrix(test_dtm_tfidf), type = "response")
table(test_predict == rownames(test_dtm))
test_predict
class(test_predict)
apply(test_predict, MAR = 2, which.max)
rownames(test_predict)[apply(test_predict, MAR = 2, which.max)]
table(rownames(test_predict)[apply(test_predict, MAR = 2, which.max)] == rownames(test_dtm))
length(test_dtm)
nrow(test_dtm)
rownames(test_predict)
dim(test_predict)
length(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)])
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
glm_model_tfidf <- glm_train
rm(glm_train)
save(glm_model_tfidf, "~/Dropbox/Blog/IRunFasterThanMyCode/files/glm_model_tfidf.Rdat")
save(glm_model_tfidf, file = "~/Dropbox/Blog/IRunFasterThanMyCode/files/glm_model_tfidf.Rdat")
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
481/(481+752)
barplot(test_predict)
barplot(as.matrix(test_predict))
as.matrix(test_predict)
head(test_predict)
test_predict <- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = "response")
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
barplot(as.matrix(test_predict))
head(test_predict)
test_predict
class(test_predict)
class(test_predict[[1]])
classas.data.frame(test_predict)
class(as.data.frame(test_predict))
head(as.data.frame(test_predict))
barplot(as.data.frame(test_predict))
barplot(as.matrix(as.data.frame(test_predict)))
483/(483+750)
?cv.glmnet
summary(glm_model_tfidf)
glm_model_tfidf
plot(glm_model_tfidf)
glm_model_tfidf <- cv.glmnet(x = train_dtm_tfidf, y = train$from_name,
family = 'multinomial',
alpha = 1,
type.measure = "class",
nfolds = 10,
thresh = 1e-3,
maxit = 1e3)
plot(glm_model_tfidf)
glm_model_tfidf
test_predict <- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = "class")
test_predict
class(test_predict)
head(test_predict)
dim(test_predict)
head(rownames(test_dtm))
table(test_predict[[1]] == rownames(test_dtm))
test_predict <- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = "response")
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
glm_model_tfidf <- cv.glmnet(x = train_dtm_tfidf, y = train$from_name,
family = 'multinomial',
alpha = 1,
type.measure = "class",
nfolds = 10,
thresh = 1e-3,
maxit = 1e5)
test_predict <- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = "response")
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
load("~/Dropbox/Blog/IRunFasterThanMyCode/files/glm_model_tfidf.Rdat")
test_predict <- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = "response")
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
481/(752+481)
head(URC)
length(grep(URC$message))
length(grep("suunto", URC$message, ignore.case = TRUE))
length(grep("sunto", URC$message, ignore.case = TRUE))
length(grep("garmin", URC$message, ignore.case = TRUE))
length(agrep("suunto", URC$message, ignore.case = TRUE))
length(agrep("garmin", URC$message, ignore.case = TRUE))
length(agrep("garmin&suunto", URC$message, ignore.case = TRUE))
suunto <- agrep("suunto", URC$message, ignore.case = TRUE)
garmin <- agrep("garmin", URC$message, ignore.case = TRUE)
length(suunto)/length(URC$message)
100*length(suunto)/length(URC$message)
100*length(garmin)/length(URC$message)
100*length(intersect(garmin, suunto))/length(URC$message)
length(intersect(suunto, garmin))
URC$comment[intersect(suunto, garmin)]
URC$message[intersect(suunto, garmin)]
nrow(URC)
length(suunto)
length(garmin)
pie(c(setdiff(suunto, garmin), setdiff(garmin, suunto), intersect(suunto, garmin)))
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))))
?pie
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c("Suunto", "Garmin", "Both"))
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c("Suunto", "Garmin", "Both"), cex = 2)
install.packages("sentiment")
install.packages("Rsentiment")
install.packages("RSentiment")
library("RSentiment")
?calculate_sentiment
calculate_sentiment(head(URC$message))
calculate_sentiment(URC$message[1])
test <- calculate_sentiment(URC$message[1])
test
test <- calculate_sentiment(URC$message[suunto])
URC$message[suunto]
?calculate_sentiment
calculate_sentiment(c("This is good","This is bad"))
calculate_sentiment(c("This is good","This is not bad"))
length(URC$message[suunto])
test <- calculate_sentiment(URC$message[suunto][1:10])
suunto_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
garmin_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
test <- calculate_sentiment(suunto_msg)
suunto_sentiment <- calculate_sentiment(suunto_msg[1:10])
suunto_msg[1:10]
suunto_sentiment <- calculate_sentiment(suunto_msg[10])
suunto_sentiment <- calculate_sentiment(suunto_msg[1:2])
suunto_sentiment <- calculate_sentiment(suunto_msg[1])
suunto_sentiment <- calculate_sentiment(suunto_msg[2])
suunto_sentiment <- calculate_sentiment(suunto_msg[1:2])
suunto_sentiment <- calculate_sentiment(as.character(suunto_msg[1:2])
garmin_sentiment <- calculate_sentiment(garmin_msg)
suunto_sentiment <- calculate_sentiment(as.character(suunto_msg[1:2]))
class(suunto_msg[1:2])
as.vector(suunto_msg[1:2])
suunto_sentiment <- calculate_sentiment(as.vector(suunto_msg[1:2]))
install.packages(qdap)
install.packages("qdap")
library("qdap")
suunto_sentiment <- polarity(as.vector(suunto_msg[1:2]))
?sentSplit
suunto_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Get rid of upper case
gsub("[[:punct:]]", "")
gsub("[[:punct:]]", "", suunto_msg[1:2])
gsub("[[:punct:]]", "", suunto_msg[1]
)
suunto_msg[1]
suunto_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
garmin_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
gsub("[[:punct:]]", "", suunto_msg[1])
gsub("[[:punct:]]", "", suunto_msg[1:5])
suunto_sentiment <- polarity(gsub("[[:punct:]]", "", suunto_msg[1:5]))
suunto_sentiment
suunto_sentiment <- calculate_sentiment(gsub("[[:punct:]]", "", suunto_msg[1:5]))
suunto_sentiment
class(suunto_sentiment)
names(suunto_sentiment)
suunto_sentiment$all
suunto_sentiment$group
suunto_sentiment$all$polarity
suunto_sentiment <- polarity(gsub("[[:punct:]]", "", suunto_msg))
garmin_sentiment <- polarity(gsub("[[:punct:]]", "", garmin_msg))
data.frame(Watch     = c(rep("Suunto", length(suunto)),
rep("Garmin", length(garmin))),
Sentiment = c(suunto_sentiment$all$polarity,
garmin_sentiment$all$polarity))
data.frame(Watch     = c(rep("Suunto", length(suunto)),
rep("Garmin", length(garmin))),
Sentiment = c(suunto_sentiment$all$polarity,
garmin_sentiment$all$polarity))
length(c(suunto_sentiment$all$polarity,
garmin_sentiment$all$polarity))
length(c(rep("Suunto", length(suunto)),
rep("Garmin", length(garmin))))
length(suunto)
length(suunto_msg)
length(suunto_sentiment)
length(suunto_sentiment$all$polarity)
sent_dat <- data.frame(Watch     = c(rep("Suunto", length(suunto)),
rep("Garmin", length(garmin))),
Sentiment = c(suunto_sentiment$all$polarity,
garmin_sentiment$all$polarity))
suunto_msg <- URC$message[suunto]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
garmin_msg <- URC$message[garmin]                %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                                # Get rid of upper case
suunto_sentiment <- polarity(gsub("[[:punct:]]", "", suunto_msg))
garmin_sentiment <- polarity(gsub("[[:punct:]]", "", garmin_msg))
sent_dat <- data.frame(Watch     = c(rep("Suunto", length(suunto)),
rep("Garmin", length(garmin))),
Sentiment = c(suunto_sentiment$all$polarity,
garmin_sentiment$all$polarity))
head(sent_dat)
ggplot(sent_dat, aes(x = Sentiment, col = Watch, fill = Watch)) +
geom_density(alpha = 0.1) +
labs(x = "Sentiment Polarity", y = "Density") +
theme(axis.text    = element_text(size = 16),
axis.title   = element_text(size = 20),
legend.text  = element_text(size = 18),
legend.title = element_text(size = 24))
length(intersect(suunto, garmin))
77/24,836
77/24836
100*77/24836
ultraposts <- URC                                                        %>%
filter(str_detect(message, "ultra"))                       %>%
mutate(created_time = as.POSIXct(created_time))            %>%
count(created_time)
ultraposts_xts <- xts(ultraposts$n, order.by = ultraposts$created_time)
ultraposts_month <- apply.monthly(ultraposts_xts, FUN = sum)
plot(ultraposts_month, ylab = "Number of Ultra Posts", main = "")
plot(ultraposts_month, ylab = "Number of Ultra Posts", main = "")
rmd2md()
getwd()
source("../rmd2md.R")
rmd2md()
rmd2md()
