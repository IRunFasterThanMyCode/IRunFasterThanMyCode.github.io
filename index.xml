<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IRunFasterThanMyCode</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>IRunFasterThanMyCode</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017 Sam Robson</copyright><lastBuildDate>Fri, 25 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/irunfasterthanmycode.jpg</url>
      <title>IRunFasterThanMyCode</title>
      <link>/</link>
    </image>
    
    <item>
      <title>How To Use Python</title>
      <link>/resources/pythontutorial/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/resources/pythontutorial/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-python&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Installing Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basics-of-python&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Basics of Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Getting started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loading-new-packages&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Loading new packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#white-space&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; White-space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#object-oriented-programming&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Object-oriented programming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-types&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.5&lt;/span&gt; Data Types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#function&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.6&lt;/span&gt; Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#printing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#storing-multiple-values&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Storing Multiple Values&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lists-and-tuples&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Lists and Tuples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dictionary&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Dictionary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numpy&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; NumPy&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#arrays&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; Arrays&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.3&lt;/span&gt; Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#indexing-and-slicing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.4&lt;/span&gt; Indexing and slicing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-multiplication&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.5&lt;/span&gt; Matrix multiplication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.6&lt;/span&gt; Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#structured-array&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.7&lt;/span&gt; Structured Array&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pandas&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Pandas&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; Data Frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#indexing-and-slicing-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Indexing and Slicing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-filtering&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Data Filtering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-sorting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; Data Sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combining-dataframes&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; Combining DataFrames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-and-writing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; Reading and Writing Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-sequences&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Control Sequences&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#if-else&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.1&lt;/span&gt; IF ELSE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.2&lt;/span&gt; FOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#while&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.3&lt;/span&gt; WHILE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loop-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.4&lt;/span&gt; Loop Control&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#writing-functions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Writing Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Statistics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-statistics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.1&lt;/span&gt; Basic Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.2&lt;/span&gt; Variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.3&lt;/span&gt; Correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.4&lt;/span&gt; Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plotting-with-python&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Plotting With Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.1&lt;/span&gt; Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.2&lt;/span&gt; Quantile-Quantile Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pie-chart&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.3&lt;/span&gt; Pie Chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bar-plot&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.4&lt;/span&gt; Bar Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#line-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.5&lt;/span&gt; Line Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.6&lt;/span&gt; Scatterplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.7&lt;/span&gt; Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seaborn-package&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.8&lt;/span&gt; Seaborn package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.9&lt;/span&gt; Subplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-figures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.10&lt;/span&gt; Saving Figures&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Example Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-3&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.2&lt;/span&gt; Load Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculate-fold-change&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.3&lt;/span&gt; Calculate Fold Change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.4&lt;/span&gt; Compare Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;As a bioinformatics researcher, I primarily use the statistical programming language R for my analyses, mainly due to the huge number of resources available in the &lt;a href=&#34;https://bioconductor.org&#34;&gt;Bioconductor&lt;/a&gt; libraries. I have previously written a &lt;a href=&#34;/resources/rtutorial/index.html&#34;&gt;Tutorial for R&lt;/a&gt;, so thought that I would do something similar for the data science programming language &lt;a href=&#34;https://www.python.org&#34;&gt;Python&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Python is an interpreted, high-level language that supports both object-oriented and structured programming. Both Python and R are widely used in the data science world, with both having very similar functionality, but specific pros and cons making the “correct” choice for which one to use being somewhat dependent on the task at hand. Python is typically considered to be more memory efficient than R, and has some amazingly powerful libraries for data science such as scikit-learn and &lt;a href=&#34;https://www.tensorflow.org&#34;&gt;TensorFlow&lt;/a&gt;, whilst R has amazing libraries for bioinformatics and superb data visualization through the &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;ggplot2&lt;/a&gt; package and the other &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;TidyVerse&lt;/a&gt; packages from Hadley Wickham.&lt;/p&gt;
&lt;p&gt;However, R has some amazingly memory-efficient implementations (including an interface to TensorFlow and other machine learning tools), and Python has a huge bioinformatics suite with the &lt;a href=&#34;https://biopython.org&#34;&gt;biopython&lt;/a&gt; suite and excellent plotting packages such as &lt;a href=&#34;https://seaborn.pydata.org&#34;&gt;seaborn&lt;/a&gt;. So ultimately, both are excellent, and knowing either can be a fantastic tool in your data science toolbox.&lt;/p&gt;
&lt;p&gt;This is an introduction to Python that should allow new users to understand how Python works. It will follow a similar flow to the R introduction, so the two can be compared to understand the differences between the two languages.&lt;/p&gt;
&lt;p&gt;As with R and other languages, comments can be added using the &lt;code&gt;#&lt;/code&gt; comment character. Everything to the right of this character is ignored by Python. This allows you to add words and descriptions to your code which are ignored entirely by the interpreter, but can be used for people looking at the source code to help document what a particular code chunk does. You can NEVER have too many comments!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Installing Python&lt;/h1&gt;
&lt;p&gt;Python can be downloaded and installed from the &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; website. It is available for all major operating systems, and installation is quite straightforward. The current version is 3.8.0, but updates are released regularly. Additional packages can be installed by using the &lt;code&gt;pip&lt;/code&gt; Python package manager as follows:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pip install newpackage&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to use larger package managers such as the &lt;a href=&#34;https://anaconda.org&#34;&gt;Anaconda&lt;/a&gt; data science platform. This is an open-source distribution containing over 1,500 Python and R packages that ensures all dependencies are maintained and any required packages are present. &lt;code&gt;conda&lt;/code&gt; will allow you to generate multiple environments to work in depending on requirements.&lt;/p&gt;
&lt;p&gt;Unlike R, there is no specific Integrated Development Environment, but there are multiple different programs that can be sconsidered such as &lt;a href=&#34;https://www.spyder-ide.org&#34;&gt;Spyder&lt;/a&gt; and &lt;a href=&#34;https://www.jetbrains.com/pycharm&#34;&gt;PyCharm&lt;/a&gt;. There is also the &lt;a href=&#34;https://jupyter.org&#34;&gt;Jupyter&lt;/a&gt; notebook program, which allows interactive computing supporting many different programming languages. This is similar to R markdown (which is used to generate the content of this blog), in that it merges markdown formatted text with code chunks. This can be used to create auto-generated reports by including code output embedded into the text.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics-of-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basics of Python&lt;/h1&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Getting started&lt;/h2&gt;
&lt;p&gt;If using a command line interface, Python can be run simply by typing the name as follows:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open up an interactive Python environment, which can be closed by pressing &lt;code&gt;Ctrl+D&lt;/code&gt;. This is a command line version allowing you to see the results of the commands that you enter as you run them. You can also generate Python scripts, which typically have the suffix &lt;code&gt;.py&lt;/code&gt; and can be run by calling Python on them:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;python myfile.py&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command line is shown by the &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; character. Simply type your command here and press return to see the results. If your command is not complete, then you will see a syntax error:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print (&amp;quot;Hello World!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is possible to store data values in objects in Python using the &lt;code&gt;=&lt;/code&gt; assignment command. Since these objects can have their value reassigned, they are known as variables. Unlike some other languages, variables do not need to be declared ahead of their use, and their type is declared automatically when you assign them based on their value. Basic data types include integers (a whole number), floating point numbers (fractions), and character strings:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;int = 3
float = 4.678
str = &amp;quot;hello&amp;quot;

print (int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print (float)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4.678&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print (str)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hello&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Variables should ideally be named with some descriptor that gives some idea of what it represents. It must start with a letter or underscore, and can only contain alpha-numeric characters and underscores. It is worth bearing in mind that the names are case-sensitive, so &lt;code&gt;myvar&lt;/code&gt;, &lt;code&gt;MyVar&lt;/code&gt; and &lt;code&gt;MYVAR&lt;/code&gt; would be three different variables.&lt;/p&gt;
&lt;p&gt;There are lots of different variable naming conventions to choose from (e.g. see &lt;a href=&#34;https://en.wikipedia.org/wiki/Naming_convention_(programming)&#34;&gt;here&lt;/a&gt;), but once you have chosen one try and stick to it throughout.&lt;/p&gt;
&lt;p&gt;Simple arithmetic can be performed using the standard arithmetic operators (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;), as well as the exponent operator (&lt;code&gt;**&lt;/code&gt;). There is a level of precedence to these functions – the exponent will be calculated first, followed by multiplication and division, followed by plus and minus. For this reason, you must be careful that your arithmetic is doing what you expect it to do. You can get around this by encapsulating subsets of the sum in parentheses, which will be calculated from the inside out:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;1 + 2 * 3   # 2 times 3 is 6, then add 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(1 + 2) * 3 # 1 plus 2 is 3, then times 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;1 + (2 * 3) # 2 times 3 is 6, then add 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;2**4+1      # 2 to the power of 4 is 16, then add 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The remainder left over following division can be returned by using the “modulo” operator (&lt;code&gt;%%&lt;/code&gt;), and can be important for instance when testing for even numbers:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;6%2 # 6 is divisible by 2 exactly three times&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;6%4 # 6 is divisible by 4 one time with a remainder of 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use other variables in these assignments:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 1
y = x
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;z = x + y 
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-new-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Loading new packages&lt;/h2&gt;
&lt;p&gt;Additional packages can be loaded into Python by using the &lt;code&gt;import&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import newpackage&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can name the imported package something different to save having to type the full name every time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import newpackage as new&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use a method (e.g. &lt;code&gt;foo()&lt;/code&gt;) from an imported package, we use the dot notation as follows. If we have used a shorter name for the package, we can use this name to save typing the full name each time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;new.foo()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes, we may want to only load a package subset which we can do as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from newpackage import subpackage&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All functions available from this package are now available for use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;white-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; White-space&lt;/h2&gt;
&lt;p&gt;One big difference with using Python compared with many other languages is that Python uses white-space to delineate code blocks rather than using curly brackets (&lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt;). So indentations must align correctly to ensure that the interpreter is able to determine which code chunks are in the same block. An example of a code chunk is an &lt;code&gt;ifelse&lt;/code&gt; chunk, which will be described later. In R, using curly braces, this would look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 10
y &amp;lt;- 20
if (x &amp;gt;= y) {
  if (x &amp;gt; y) {
    cat(&amp;quot;x is bigger than y&amp;quot;)
  } else {
    cat(&amp;quot;x is equal to y&amp;quot;)
  }
} else {
  cat(&amp;quot;x is smaller than y&amp;quot;)
} &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## x is smaller than y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In Python, this is done by ensuring the indentation level of the parent and child statements is kept consistent:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 10
y = 20
if x &amp;gt;= y:
  if x &amp;gt; y:
    print (&amp;quot;x is bigger than y&amp;quot;)
  else:
    print (&amp;quot;x is equal to y&amp;quot;)
else:
  print (&amp;quot;x is smaller than y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## x is smaller than y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ensures that the code looks more clean and elegant, but it does mean that it is possible to run into issues if you do not ensure that the indentation levels are correct. For instance, let’s see what happens if we change the indentation levels of the print functions:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 10
y = 20
if x &amp;gt;= y:
  if x &amp;gt; y:
  print (&amp;quot;x is bigger than y&amp;quot;)
  else:
  print (&amp;quot;x is equal to y&amp;quot;)
else:
  print (&amp;quot;x is smaller than y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;object-oriented-programming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Object-oriented programming&lt;/h2&gt;
&lt;p&gt;Python supports an object-oriented programming (OOP) paradigm. The idea here is that each object that you use in Python is an instance of some well-defined class of data that has a set of attributes that can be accessed and methods that can be applied to it. New data classes can be specified which inherit the attributes and methods from other data types, making OOP a very modular programming approach. All of this information can be hidden away from the user, and only certain attributes and methods are made available whilst the rest are kept internal and private.&lt;/p&gt;
&lt;p&gt;An example of this is to consider a new class &lt;code&gt;Car&lt;/code&gt;. There are many different cars out there, but each specific car is an instance of this new class &lt;code&gt;Car&lt;/code&gt;. When creating the new class, we can assign it certain attributes, such as &lt;code&gt;model&lt;/code&gt;, &lt;code&gt;make&lt;/code&gt;, &lt;code&gt;colour&lt;/code&gt;, &lt;code&gt;horsepower&lt;/code&gt;, &lt;code&gt;engine size&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;We can create such a new class as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class Car:
  def __init__(self):
    pass&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The class now exists, but there are no objects of this class. The class has an initializer function (the &lt;code&gt;__init(self)__&lt;/code&gt; function) that will create a new object of class &lt;code&gt;Car&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mycar = Car()
print(mycar)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;__main__.Car object at 0x12444c9d0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;mycar&lt;/code&gt; is an object of class &lt;code&gt;Car&lt;/code&gt;, but does not have any attributes associated with it. So let’s add a few:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class Car:
  def __init__(self, make, model):
    self.make  = make
    self.model = model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can instantiate a new object of class &lt;code&gt;Car&lt;/code&gt; with a specific make and model. The attributes are accessed using dot notation as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mycar = Car(&amp;quot;Honda&amp;quot;, &amp;quot;Civic&amp;quot;)
print(mycar.make)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Honda&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(mycar.model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Civic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We may also want to include some methods that can be applied to our object, such as &lt;code&gt;drive&lt;/code&gt;, where we can input a new destination to drive to. So let’s add a &lt;code&gt;drive()&lt;/code&gt; method to the &lt;code&gt;Car&lt;/code&gt; class:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;class Car:
  def __init__(self, make, model):
    self.make  = make
    self.model = model
    
  def drive(self, where):
    print (&amp;quot;Driving to &amp;quot; + where)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s drive home in our car:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mycar = Car(&amp;quot;Honda&amp;quot;, &amp;quot;Civic&amp;quot;)
mycar.drive(&amp;quot;home&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Driving to home&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will describe some of these features in more detail later, but the dot notation for accessing object attributes and methods will feature in the following sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.5&lt;/span&gt; Data Types&lt;/h2&gt;
&lt;p&gt;There are many classes available in Python, as well as the ability to generate new classes as described above. The most common are “numeric” (which you can do numerical calculations on – integers or floating point numbers), character strings (can contain letters, numbers, symbols etc., but cannot run numerical calculations), and boolean (TRUE or FALSE). The speech marks character &lt;code&gt;&amp;quot;&lt;/code&gt; is used to show that the class of y is “character”. You can also use the apostrophe &lt;code&gt;&#39;&lt;/code&gt;. You can check the class of a variable by using the &lt;code&gt;type()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 12345
type(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;int&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;y = 12345.0
type(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;float&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;z = &amp;quot;12345&amp;quot;
type(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;str&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Addition is a well-defined operation on numerical objects, but is not defined on character class objects. Attempting to use a function which has not been defined for the object in question will throw an error:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x + 1 # x is numeric, so addition is well defined
y + 1 # y is a character, so addition is not defined - produces an error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other important data class is “boolean”, which is simply a binary &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt; value. There are certain operators that are used to compare two variables. The obvious ones are “is less than” (&lt;code&gt;&amp;lt;&lt;/code&gt;), “is greater than” (&lt;code&gt;&amp;gt;&lt;/code&gt;), “is equal to”&amp;quot; (&lt;code&gt;==&lt;/code&gt;). You can also combine these to see “is less than or equal to” (&lt;code&gt;&amp;lt;=&lt;/code&gt;) or “is greater than or equal to” (&lt;code&gt;&amp;gt;=&lt;/code&gt;). If the statement is true, then it will return the output “True”. Otherwise it will return “False”:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 2
y = 3

x &amp;lt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## True&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x &amp;gt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## False&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also combine these logical tests to ask complex questions by using the &lt;code&gt;and&lt;/code&gt; (&lt;code&gt;&amp;amp;&lt;/code&gt;) or the &lt;code&gt;or&lt;/code&gt; (&lt;code&gt;|&lt;/code&gt;) operators. You can also negate the output of a logical test by using the &lt;code&gt;not&lt;/code&gt; (&lt;code&gt;!&lt;/code&gt;) operator. This lets you test for very specific events in your data. Again, I recommend using parentheses to break up your tests to ensure that the tests occur in the order which you expect:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 3
y = 7
z = 6

(x &amp;lt;= 3 &amp;amp; y &amp;gt;= 8) &amp;amp; z == 6  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## False&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(x &amp;lt;= 3 &amp;amp; y &amp;gt;= 8) | z == 6 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## True&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;(x != y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## True&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.6&lt;/span&gt; Function&lt;/h2&gt;
&lt;p&gt;A function (also known as a method, subroutine, or procedure) is a named block of code that takes in one or more values, does something to them, and returns a result. A simple example is the &lt;code&gt;sum()&lt;/code&gt; function, which takes in two or more values in the form of a list, and returns the sum of all of the values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sum([1,2,3]) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the &lt;code&gt;sum()&lt;/code&gt; function takes only one variable (in this case a numeric list). Sometimes functions take more than one variable (also known as “arguments”). These are named values that must be specified for the function to run. The &lt;code&gt;sum()&lt;/code&gt; function takes two named arguments – &lt;code&gt;iterable&lt;/code&gt; (the list, tuple, etc whose items you wish to add together) and &lt;code&gt;start&lt;/code&gt; (an optional value to start the sum from). By default, the value of &lt;code&gt;start&lt;/code&gt; is 0, but you may want to add the sum to another value (for instance if you are calculating a cummulative sum):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sum([1,2,3], 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not name arguments, they will be taken and assigned to the arguments in the order in which they are input. Any arguments not submitted will use their default value (0 for the case of &lt;code&gt;start&lt;/code&gt; here). We can however name the arguments and set the value using the &lt;code&gt;=&lt;/code&gt; sign.&lt;/p&gt;
&lt;p&gt;To see the documentation for a specific function, use the &lt;code&gt;help()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;help(sum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Help on built-in function sum in module builtins:
## 
## sum(iterable, start=0, /)
##     Return the sum of a &amp;#39;start&amp;#39; value (default: 0) plus an iterable of numbers
##     
##     When the iterable is empty, return the start value.
##     This function is intended specifically for use with numeric values and may
##     reject non-numeric types.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;printing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;print()&lt;/code&gt; can be used to print whatever is stored in the variable the function is called on. Print is what is known as an “overloaded”&amp;quot; function, which means that there are many functions named &lt;code&gt;print()&lt;/code&gt;, each written to deal with an object of a different class. The correct one is used based on the object that you supply. So calling &lt;code&gt;print()&lt;/code&gt; on a numeric variable will print the value stored in the variable. Calling it on a list prints all of the values stored in the list:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 1
print (x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;y = [1,2,3,4,5]
print (y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 4, 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you notice, using &lt;code&gt;print()&lt;/code&gt; is the default when you just call the variable itself:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 4, 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are supplying a single argument to &lt;code&gt;print()&lt;/code&gt;. Multiple different string values can be concatenated using the &lt;code&gt;+&lt;/code&gt; character, and this concatenated string can then be printed:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = &amp;quot;hello&amp;quot;
y = &amp;quot;world&amp;quot;
print(x + y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## helloworld&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that the &lt;code&gt;+&lt;/code&gt; character will join the strings directly, so there is no space. However, &lt;code&gt;print()&lt;/code&gt; is also able to take any number of different arguments, and will print them one after the other with a space in between:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = &amp;quot;hello&amp;quot;
y = &amp;quot;world&amp;quot;
print(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hello world&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The concatenate command is only able to concatanate strings, so could not deal with numerical values:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 100
y = &amp;quot;bottles of beer on the wall&amp;quot;
print(x + y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is possible to caste a numeric value to a string using the &lt;code&gt;str()&lt;/code&gt; function, but using the multi-parameter use of the &lt;code&gt;print()&lt;/code&gt; method will automatically take care of this:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = 100
y = &amp;quot;bottles of beer on the wall&amp;quot;
print(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 100 bottles of beer on the wall&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;\t&lt;/code&gt; is a special printing characters that you can use to print a tab character. Another similar special character that you may need to use is &lt;code&gt;\n&lt;/code&gt; which prints a new line:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = &amp;quot;hello&amp;quot;
y = &amp;quot;world&amp;quot;
print(x + &amp;quot;\t&amp;quot; + y + &amp;#39;\n&amp;#39; + &amp;quot;next line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hello    world
## next line&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also other characters, such as &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&amp;quot;&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;\&lt;/code&gt;, which may require “escaping” with a backslash to avoid being interpreted in a different context. For instance, if you have a string containing an apostrophe within a string defined using apostrophes, the string will be interpreted as terminating earlier, and the code will not do what you expect:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;#39;It&amp;#39;s very annoying when this happens...&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the string submitted to &lt;code&gt;print()&lt;/code&gt; is actual “It” rather than the intended “It’s very annoying when this happens…”. The function will not know what to do about the remainder of the string, so an error will occur. However, by escaping the apostrophe, the string will be interpreted correctly:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;#39;It\&amp;#39;s easily fixed though!&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another alternative is to use double apostrophes as the delimiter, which will avoid the single apostrophe being misinterpreted:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;It&amp;#39;s easily fixed though!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative approach to using the concatanate operator that is able to cope with multiple different classes is to use %-formatting:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%10s\t%5s&amp;quot; % (&amp;quot;Hello&amp;quot;, &amp;quot;World&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Hello   World&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%10s\t%5s&amp;quot; % (&amp;quot;Helloooooo&amp;quot;, &amp;quot;World&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Helloooooo   World&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first string shows how we want the inputs to be formatted, followed by a &lt;code&gt;%&lt;/code&gt; operator and a list of the inputs. Within the formatting string, placeholders of the form &lt;code&gt;%10s&lt;/code&gt; are replaced by the given inputs, with the first being replaced by the first argument in the list, and so on (so the number of additional arguments after the &lt;code&gt;%&lt;/code&gt; operator must match the number of placeholders). The number in the placeholder defines the width to allocate for printing that argument (positive is right aligned, negative is left aligned), decimal numbers in the placeholder define precision of floating point numbers, and the letter defines the type of argument to print (e.g. &lt;code&gt;s&lt;/code&gt; for string, &lt;code&gt;i&lt;/code&gt; for integer, &lt;code&gt;f&lt;/code&gt; for fixed point decimal, &lt;code&gt;e&lt;/code&gt; for exponential decimal). Here are some examples:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%20s&amp;quot;  % (&amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%-20s&amp;quot; % (&amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%10i&amp;quot;  % (12345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      12345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%10f&amp;quot;  % (12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  12.345000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;%10e&amp;quot;  % (12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1.234500e+01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;f-strings offer the same control on incorprating multiple variables into a string, but in a slightly more readable manner. In f-strings, the string is preceeded by the &lt;code&gt;f&lt;/code&gt; operator, and the variables are incorprated into the string by name within curly braces:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;forename = &amp;quot;Sam&amp;quot;
surname = &amp;quot;Robson&amp;quot;
job = &amp;quot;Bioinformatician&amp;quot;
f&amp;quot;My name is {forename} {surname} and I am a {job}&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;My name is Sam Robson and I am a Bioinformatician&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The contents of the curly braces are evaluated at runtime, so can contain any valid expression:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;f&amp;quot;2 times 3 is {2*3}&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;2 times 3 is 6&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For readibility, these can be split over multiple lines, provided that each new line begins with the &lt;code&gt;f&lt;/code&gt; operator:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;forename = &amp;quot;Sam&amp;quot;
surname = &amp;quot;Robson&amp;quot;
job = &amp;quot;Bioinformatician&amp;quot;
description = (f&amp;quot;My name is {forename} {surname}. &amp;quot;
               f&amp;quot;I am a {job}&amp;quot;)
description&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;My name is Sam Robson. I am a Bioinformatician&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;storing-multiple-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Storing Multiple Values&lt;/h1&gt;
&lt;div id=&#34;lists-and-tuples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Lists and Tuples&lt;/h2&gt;
&lt;p&gt;In Python, vector-like lists of values are known as &lt;code&gt;lists&lt;/code&gt;. They can be declared simply by using square bracket notation as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist = [1,2,3,4,5]
mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 4, 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lists can store multiple variables of any data type, and are a very useful way of storing linked data together:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist = [1,2,3,4,5,&amp;quot;once&amp;quot;,&amp;quot;I&amp;quot;,&amp;quot;caught&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;fish&amp;quot;,&amp;quot;alive&amp;quot;,True]
mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 2, 3, 4, 5, &amp;#39;once&amp;#39;, &amp;#39;I&amp;#39;, &amp;#39;caught&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;fish&amp;#39;, &amp;#39;alive&amp;#39;, True]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use the &lt;code&gt;range()&lt;/code&gt; function to create a sequence of integer values from a starting value to an end value. However, note that the end value is &lt;em&gt;not&lt;/em&gt; included:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;myrange = range(0,5)
for i in myrange:
  print(i)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0
## 1
## 2
## 3
## 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I will discuss the &lt;code&gt;for&lt;/code&gt; loop in a later section. You can also add a third parameter to define the step between the sequential values. So to take every &lt;em&gt;other&lt;/em&gt; value from 0 to 10, you would add a step value of 2:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;myrange = range(0,11,2)
for i in myrange:
  print(i)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0
## 2
## 4
## 6
## 8
## 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access the individual elements of the list by using square brackets (&lt;code&gt;[&lt;/code&gt;) to index the array. The elements in the vector are numbered from 0 upwards, so to take the first and last values we do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;once&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, an error is returned if you try to take an element that does not exist. The subset can be as long as you like, as long as it’s not longer than the full set:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [2, 3, 4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This process is known as “slicing”. It is also possible to tell Python how many values we want to increment by as we slice. So to take every &lt;em&gt;other&lt;/em&gt; value in the above slice, we would do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[1:4:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [2, 4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can also be used to reverse the order, by adding a minus sign to this final value. Here we do not specifically tell Python which values to use as the first and last value (which will take &lt;em&gt;all&lt;/em&gt; values), and increment by 1 again (which is implicit when no third value is included) using a minus sign to get the values in the reverse order:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[::-1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [True, &amp;#39;alive&amp;#39;, &amp;#39;fish&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;caught&amp;#39;, &amp;#39;I&amp;#39;, &amp;#39;once&amp;#39;, 5, 4, 3, 2, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the &lt;code&gt;:&lt;/code&gt; in the brackets simply means to take all of the numbers from the first up to (but not including) the last. So &lt;code&gt;[0:2]&lt;/code&gt; will return the first 2 elements of the vector.&lt;/p&gt;
&lt;p&gt;Using negative values for the start or end values will start the counting from the end of the array. So the index -2 will be the position 2 from the end of the array. Thus to get the final 2 values from a list we can do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[-2:]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;alive&amp;#39;, True]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use the square bracket notation to reassign elements of the list:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist2 = [1,2,3,4,5]
mylist2[0] = &amp;quot;Hello World!&amp;quot;
mylist2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;Hello World!&amp;#39;, 2, 3, 4, 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;code&gt;tuple&lt;/code&gt; is very similar to a list, with two main differences. The first is that we use normal parentheses &lt;code&gt;(&lt;/code&gt; rather than square brackets, and the second is that we cannot change the elements of the tuple. The tuple isordered and &lt;em&gt;immutable&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist2 = (1,2,3,4,5)
mylist2[0] = &amp;quot;Hello World!&amp;quot;
mylist2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can completely reassign the entire variable if you want to, but once the elements of the tuple have been set they cannot be changed.&lt;/p&gt;
&lt;p&gt;To drop elements from a list, you use the &lt;code&gt;pop()&lt;/code&gt; method. So to remove the value &lt;code&gt;2&lt;/code&gt; from the array we would do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist = [1,2,3,4,5]
mylist.pop(1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 3, 4, 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To add extra elements to a list, you can use the &lt;code&gt;append()&lt;/code&gt; method. So to add the value 2 back in we would do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist.append(2)
mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 3, 4, 5, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that &lt;code&gt;append()&lt;/code&gt; will add the value to the very end of the list To add an element to a specified position, you can use the &lt;code&gt;insert()&lt;/code&gt; method, specifying the specific position to which you want to add the value in the list:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist.insert(1, &amp;quot;Hello World!&amp;quot;)
mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, &amp;#39;Hello World!&amp;#39;, 3, 4, 5, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the list can take values of different types. You can even create lists of lists, which can be accessed using multiple square brackets:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist = [1,2,3]
mylist.append([&amp;quot;hello&amp;quot;, &amp;quot;world&amp;quot;])
mylist&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1, 2, 3, [&amp;#39;hello&amp;#39;, &amp;#39;world&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mylist[3][0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;hello&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we select the first value from the array that makes up the fourth element of the array after appending.&lt;/p&gt;
&lt;p&gt;We can also sort data in a list simply using the &lt;code&gt;sort()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mynum = [3, 94637, 67, 45, 23, 100, 45]
mynum.sort()
print(mynum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [3, 23, 45, 45, 67, 100, 94637]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works on both numeric and string data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mystr = [&amp;#39;o&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;u&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;i&amp;#39;]
mystr.sort()
print(mystr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;a&amp;#39;, &amp;#39;e&amp;#39;, &amp;#39;i&amp;#39;, &amp;#39;o&amp;#39;, &amp;#39;u&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default sorting is from lowest to highest. By including the &lt;code&gt;reverse = True&lt;/code&gt; argument, we can get the list sorted in a reverse order:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mynum = [3, 94637, 67, 45, 23, 100, 45]
mynum.sort(reverse = True)
print(mynum)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [94637, 100, 67, 45, 45, 23, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dictionary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Dictionary&lt;/h2&gt;
&lt;p&gt;A dictionary in Python is a named list, containing a series of key-value pairs, and is declared surrounded by curly braces. Each item in the dictionary is separated by commas and contains a key and a value separated by ‘:’ characters. Each key must be unique, whilst the values to which they refer may not be.&lt;/p&gt;
&lt;p&gt;Dictionaries have the benefit compared to lists that you can access values based on the key values, using standard square bracket notation with the key name. This means that you do not need to know the index number for the value that you wish to access:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict = {&amp;#39;forename&amp;#39;:&amp;#39;Sam&amp;#39;, &amp;#39;surname&amp;#39;:&amp;#39;Robson&amp;#39;, &amp;#39;job&amp;#39;:&amp;#39;Bioinformatician&amp;#39;}
print(mydict[&amp;quot;forename&amp;quot;], mydict[&amp;quot;surname&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sam Robson&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use the square bracket notation to update the entry in the dictionary for the specified item:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict[&amp;quot;forename&amp;quot;] = &amp;quot;Samuel&amp;quot;
print(mydict[&amp;quot;forename&amp;quot;], mydict[&amp;quot;surname&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Samuel Robson&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And new items can be added by using a key that does not exist in the dictionary already:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict[&amp;quot;skills&amp;quot;] = &amp;quot;Python&amp;quot;
print(mydict[&amp;quot;skills&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Python&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;del&lt;/code&gt; statement can be used to remove specific entries in the dictionary:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;del mydict[&amp;quot;skills&amp;quot;]
print(mydict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {&amp;#39;forename&amp;#39;: &amp;#39;Samuel&amp;#39;, &amp;#39;surname&amp;#39;: &amp;#39;Robson&amp;#39;, &amp;#39;job&amp;#39;: &amp;#39;Bioinformatician&amp;#39;}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And all items can be cleared by calling the &lt;code&gt;clear()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict.clear()
print(mydict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The length of a dictionary, specifically the number of key-value pairs, can be shown by using the &lt;code&gt;len()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict = {&amp;#39;forename&amp;#39;:&amp;#39;Sam&amp;#39;, &amp;#39;surname&amp;#39;:&amp;#39;Robson&amp;#39;, &amp;#39;job&amp;#39;:&amp;#39;Bioinformatician&amp;#39;}
len(mydict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also pull out all of the key-value pairs in the dictionary using the &lt;code&gt;items()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mydict = {&amp;#39;forename&amp;#39;:&amp;#39;Sam&amp;#39;, &amp;#39;surname&amp;#39;:&amp;#39;Robson&amp;#39;, &amp;#39;job&amp;#39;:&amp;#39;Bioinformatician&amp;#39;}
mydict.items()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dict_items([(&amp;#39;forename&amp;#39;, &amp;#39;Sam&amp;#39;), (&amp;#39;surname&amp;#39;, &amp;#39;Robson&amp;#39;), (&amp;#39;job&amp;#39;, &amp;#39;Bioinformatician&amp;#39;)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;numpy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; NumPy&lt;/h1&gt;
&lt;div id=&#34;introduction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Many calculations in data science require a field of mathematics called “linear algebra”, which is all about multiplication of data stored in vectors and matrices. &lt;a href=&#34;https://numpy.org&#34;&gt;NumPy&lt;/a&gt; is one of the most commonly used packages for scientific computing and provides very powerful tools for linear algebra using code from fast languages such as C/C++ and fortran. It allows you to generate arrays of values and offers vectorised calculations for linear algebra that offer huge improvements in performance. For instance, to sum two vectors of equal size (1 dimensional arrays), the result will be a vector where the i&lt;sup&gt;th&lt;/sup&gt; entry is the sum of the i&lt;sup&gt;th&lt;/sup&gt; entries from the input vectors.&lt;/p&gt;
&lt;p&gt;NumPy is imported by using the &lt;code&gt;import&lt;/code&gt; declaration, and access functions from the package directly using the package name. We can define a shortened name for the package so that we do not have to type &lt;code&gt;numpy&lt;/code&gt; every time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;arrays&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Arrays&lt;/h2&gt;
&lt;p&gt;We can convert Python lists into NumPy arrays and use these to access the powerful additional methods associated with these data types:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = np.array([2,3,2,4,5])
y = np.array([4,1,1,2,3])

x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([6, 4, 3, 6, 8])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x*y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([ 8,  3,  2,  8, 15])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is not much difference between the normal Python lists that we saw previously and the np arrays shown here, but the functionality is significantly improved. One function is the &lt;code&gt;np.arange()&lt;/code&gt; method, which allows us to generate a list of values between a start and end point incremented by a specific value. Unlike the &lt;code&gt;range()&lt;/code&gt; function seen previously, the step value does not need to be an integer:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.arange(0,10,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([0, 2, 4, 6, 8])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.arange(0,100,10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.arange(1,2,0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The three arguments are the starting value, the final value (note that this will &lt;em&gt;not&lt;/em&gt; be included in the final list), and the step size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Matrices&lt;/h2&gt;
&lt;p&gt;As well as 1D arrays, we can look at 2D arrays, or &lt;code&gt;matrices&lt;/code&gt;. For example, this may be how you are used to seeing data in a spreadsheet context, with each row representing a different sample, and each row representing some measurement for that sample. An array is a special case of a matrix with only 1 row. We can use the &lt;code&gt;np.reshape()&lt;/code&gt; method to reshape an array into a different shape by giving the number of rows and columns of the output that we want to see:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.arange(1,10,1).reshape(3,3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[1, 2, 3],
##        [4, 5, 6],
##        [7, 8, 9]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that a matrix uses double square brackets &lt;code&gt;[[&lt;/code&gt; rather than the single square brackets used for 1-dimensional arrays. We can check the shape of the matrix by using the &lt;code&gt;shape()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;myarray = np.arange(1,10,1)
myarray.shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (9,)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = myarray.reshape(3,3)
mymatrix.shape&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (3, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The shape is given by a tuple where the first value gives the number of rows and the second gives the number of columns. Before reshaping, &lt;code&gt;myarray&lt;/code&gt; is a 1D array. After reshaping, &lt;code&gt;mymatrix&lt;/code&gt; becomes a 2D array, or matrix.&lt;/p&gt;
&lt;p&gt;You can also generate a matrix as an array of arrays using double square baracket notation. Note that when doing this the length of each array must match to create a matrix:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[1,2,3],[4,5,6]])
print(mymatrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1 2 3]
##  [4 5 6]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;indexing-and-slicing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.4&lt;/span&gt; Indexing and slicing&lt;/h2&gt;
&lt;p&gt;Indexing of 1D arrays works similarly to that of lists, by using the index of the value that you want to access in square brackets. Again, remember that indexing begins at 0, not 1, so to get the second value in the array you would use the index 1:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([1,2,3,4,5,6])
print(mymatrix[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The syntax for slicing is also the same as for lists, using &lt;code&gt;:&lt;/code&gt; to define the range:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([1,2,3,4,5,6])
print(mymatrix[2:4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [3 4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(mymatrix[::2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1 3 5]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 2D matrix indexing, we need to specify two values - one for the index of the rows and another for the index of the columns:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[1,2,3],[4,5,6]])
print(mymatrix[0,0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Slicing can be a little more complex, but you can use the same approach of specifying the row and column slices separated by a comma as above. So to take the first two columns we would do the following to take all of the rows (&lt;code&gt;:&lt;/code&gt;), but only the first two columns (&lt;code&gt;0:2&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[1,2,3],[4,5,6]])
print(mymatrix[:,0:2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1 2]
##  [4 5]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-multiplication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.5&lt;/span&gt; Matrix multiplication&lt;/h2&gt;
&lt;p&gt;Multiplication of matrices is probably too much to go into in too much detail here, but the &lt;code&gt;dot product&lt;/code&gt; of two matrices can only be performed if the number of &lt;em&gt;columns&lt;/em&gt; of the first matrix matches the number of &lt;em&gt;rows&lt;/em&gt; of the second. The dot product can be calculated for two such matrices using the &lt;code&gt;np.dot()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;A = np.array([1,2,3]).reshape(1,3)
B = np.arange(1,10,1).reshape(3,3)
np.dot(A,B)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[30, 36, 42]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Matrix multiplication does not always follow the commutative law of multiplication, since it is not always the case that &lt;code&gt;A.B&lt;/code&gt; is the same as &lt;code&gt;B.A&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.dot(A,B)
np.dot(B,A)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The identity matrix is an example where matrix multiplication &lt;em&gt;is&lt;/em&gt; commutative, since any matrix multiplied by the identity matrix (of the correct dimensions) gives itself. To give an identity matrix, a square matrix with 1s on the diagonals and 0s everywhere else, you can use the &lt;code&gt;np.eye()&lt;/code&gt; function with the dimension specified:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;C = np.eye(3)
print(C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1. 0. 0.]
##  [0. 1. 0.]
##  [0. 0. 1.]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.dot(B,C)
np.dot(C,B)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a number of other useful functions available that are very useful in the field of linear algebra and matrix multiplication. For example, to generate a matrix of 0s, you can use the &lt;code&gt;np.zeros()&lt;/code&gt; function, giving a 2D tuple as input:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.zeros((2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[0., 0., 0.],
##        [0., 0., 0.]])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.6&lt;/span&gt; Sorting&lt;/h2&gt;
&lt;p&gt;NumPy has a powerful sorting method, which can act much like the list &lt;code&gt;sort()&lt;/code&gt; function seen previously:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;myarray = np.array([3, 94637, 67, 45, 23, 100, 45])
myarray.sort()
print(myarray)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [    3    23    45    45    67   100 94637]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are also able to use the &lt;code&gt;argsort()&lt;/code&gt; method to get the indexing of the input array, rather than the sorted array itself:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;myarray = np.array([3, 94637, 67, 45, 23, 100, 45])
myarray.argsort()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([0, 4, 3, 6, 2, 5, 1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This sort function is more powerful than the standard Python sort function for lists as it is able to sort both 1D arrays and 2D (and indeed multi-dimensional in general) matrices, and can use multiple different sorting algorithms. For matrices, the default is to sort the values on the final axis, which in the case of the case of a 2D matrix will sort the values within each row individually:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[6,3,3],[1,2,4]])
mymatrix.sort()
print(mymatrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[3 3 6]
##  [1 2 4]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we would instead prefer to sort on the columns, we can specify the &lt;code&gt;axis&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[6,3,3],[1,2,4]])
mymatrix.sort(axis=0)
print(mymatrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1 2 3]
##  [6 3 4]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also specify which algorithm we want to use for sorting (e.g. &lt;code&gt;quicksort&lt;/code&gt;, &lt;code&gt;mergesort&lt;/code&gt; or &lt;code&gt;heapsort&lt;/code&gt;), which can have an impact on the speed and efficiency:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mymatrix = np.array([[6,3,3],[1,2,4]])
mymatrix.sort(kind = &amp;#39;mergesort&amp;#39;)
print(mymatrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[3 3 6]
##  [1 2 4]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;structured-array&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.7&lt;/span&gt; Structured Array&lt;/h2&gt;
&lt;p&gt;NumPy arrays are homogenous, and contain a single class of data. Structured arrays on the other hand are arrays of structures, each of a different class. For this, we need to define ahead of time the class of data to be included by using a &lt;code&gt;dtype&lt;/code&gt; parameter. This is a list of tupels, where each tupel is a pair giving the name of the structure and the type of data that it will hold:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dtype = [(&amp;#39;Name&amp;#39;, (np.str_, 10)), (&amp;#39;Age&amp;#39;, np.int32), (&amp;#39;Score&amp;#39;, np.float64)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we are defining 3 types of fields within the structure:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;Name&lt;/code&gt; – A string with a maximum of 10 characters&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Age&lt;/code&gt; – A 32 bit integer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Score&lt;/code&gt; – A 64 bit floating point value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then we can create a structured array where every element is a tuple containing exactly 3 values corresponding to the types as described in &lt;code&gt;dtype&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mystruct = np.array([(&amp;#39;John&amp;#39;, 37, 99.3), (&amp;#39;Paul&amp;#39;, 33, 92.6), (&amp;#39;Ringo&amp;#39;, 40, 97.5), (&amp;#39;George&amp;#39;, 35, 92.6)], dtype = dtype)
print(mystruct)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [(&amp;#39;John&amp;#39;, 37, 99.3) (&amp;#39;Paul&amp;#39;, 33, 92.6) (&amp;#39;Ringo&amp;#39;, 40, 97.5)
##  (&amp;#39;George&amp;#39;, 35, 92.6)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This structured array is a table-like structure that can make it easier to access and modify elements of the data. For instance, the sort function on this data type can take the parameter &lt;code&gt;order&lt;/code&gt; to specify which field to use for sorting. So to sort by the &lt;code&gt;Score&lt;/code&gt; field, we would do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mystruct.sort(order=&amp;quot;Score&amp;quot;)
print(mystruct)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [(&amp;#39;George&amp;#39;, 35, 92.6) (&amp;#39;Paul&amp;#39;, 33, 92.6) (&amp;#39;Ringo&amp;#39;, 40, 97.5)
##  (&amp;#39;John&amp;#39;, 37, 99.3)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can order on multiple fields to break ties in the &lt;code&gt;Score&lt;/code&gt; field by organising also on the &lt;code&gt;Age&lt;/code&gt; field:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;mystruct.sort(order=[&amp;quot;Score&amp;quot;,&amp;quot;Age&amp;quot;])
print(mystruct)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [(&amp;#39;Paul&amp;#39;, 33, 92.6) (&amp;#39;George&amp;#39;, 35, 92.6) (&amp;#39;Ringo&amp;#39;, 40, 97.5)
##  (&amp;#39;John&amp;#39;, 37, 99.3)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pandas&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Pandas&lt;/h1&gt;
&lt;div id=&#34;introduction-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pandas.pydata.org&#34;&gt;Pandas&lt;/a&gt; is another commonly used library in Python that introduces a number of incredibly powerful tools for data wrangling from Wes McKinney. Pandas can load data from a number of sources, including comma separated or tab-delimited text files (CSV or TSV files respectively) and creates aPython object in the sort of structure that one would expect from a spreadsheet type software such as Excel. This &lt;code&gt;data frame&lt;/code&gt; shows many similarities with the &lt;code&gt;data.frame&lt;/code&gt; class in R. This is a high-level data structure, offering more control in order to access specific data than working with dictionaries and structured arrays.&lt;/p&gt;
&lt;p&gt;To import the package, we use the &lt;code&gt;import&lt;/code&gt; command, and as with NumPy we can give it a local name to avoid having to write the whole name every time:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; Data Frames&lt;/h2&gt;
&lt;p&gt;Data frames can be generated by converting lists, dictionaries or NumPy arrays by using the &lt;code&gt;pd.DataFrame()&lt;/code&gt; function. Unlike the structured array, we do not need to specify the data types ahead of time. DataFrames can be generated from a list of lists by specifying the column names as follows:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;array = [[&amp;#39;John&amp;#39;, 37, 99.3], [&amp;#39;Paul&amp;#39;, 33, 92.6], [&amp;#39;Ringo&amp;#39;, 40, 97.5], [&amp;#39;George&amp;#39;, 35, 92.6]]
df = pd.DataFrame(array, columns = [&amp;#39;Name&amp;#39;, &amp;#39;Age&amp;#39;, &amp;#39;Score&amp;#39;])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## 0    John   37   99.3
## 1    Paul   33   92.6
## 2   Ringo   40   97.5
## 3  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you can use the dict syntax, wherby each column of the DataFrame is a different key-value pair in the dictionary, and the value is a list of entries of the same type:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dict = {&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;, &amp;#39;Ringo&amp;#39;, &amp;#39;George&amp;#39;], &amp;#39;Age&amp;#39;:[37,33,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,92.6]}
df = pd.DataFrame(dict)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## 0    John   37   99.3
## 1    Paul   33   92.6
## 2   Ringo   40   97.5
## 3  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Pandas DataFrame class offers a large number of commonly used statistical functions which can generate summary statistics for the columns in the dataFrame. So to get the mean value of the numerical columns we would use &lt;code&gt;df.mean()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.mean()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Age      36.25
## Score    95.50
## dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for the standard deviation we would use df.std()`:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.std()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Age      2.986079
## Score    3.428313
## dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;indexing-and-slicing-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Indexing and Slicing&lt;/h2&gt;
&lt;p&gt;Selecting data from a DataFrame is significantly easier than trying to do so from lower-level Python classes. Getting data from specific columns is simply done by using the name of the column in square brackets:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;Name&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0      John
## 1      Paul
## 2     Ringo
## 3    George
## Name: Name, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Multiple columns can be specified as a list:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[[&amp;#39;Age&amp;#39;, &amp;#39;Score&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Age  Score
## 0   37   99.3
## 1   33   92.6
## 2   40   97.5
## 3   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Selecting data in this way will return a new DataFrame. Rows of a DataFrame are labelled by an index, which can be accessed via the &lt;code&gt;index&lt;/code&gt; attribute:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.index&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## RangeIndex(start=0, stop=4, step=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the index is a range of indices starting from 0, and these are immutable. However, these can be converted to anything that you like provided that they are unique:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.index = [&amp;#39;a&amp;#39;,&amp;#39;b&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;d&amp;#39;]
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## a    John   37   99.3
## b    Paul   33   92.6
## c   Ringo   40   97.5
## d  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to specify an index when you generate the DataFrame:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = pd.DataFrame(dict, index = [&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## a    John   37   99.3
## b    Paul   33   92.6
## c   Ringo   40   97.5
## d  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These indexes can then be used select rows of interest. This can be done either by the index as we have done previously with lists, or by the index name. So to get the second row, we could either use the index number (2) or the index name (‘b’). To index by location, we use the &lt;code&gt;iloc&lt;/code&gt; attribute:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.iloc[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Name     Paul
## Age        33
## Score    92.6
## Name: b, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or to index by the row index names we use te &lt;code&gt;loc&lt;/code&gt; attribute:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.loc[&amp;#39;b&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Name     Paul
## Age        33
## Score    92.6
## Name: b, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can of course slice multiple rows and multiple columns to select any combination of the DataFrame that we desire:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.loc[[&amp;#39;b&amp;#39;,&amp;#39;d&amp;#39;],[&amp;#39;Name&amp;#39;,&amp;#39;Age&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age
## b    Paul   33
## d  George   35&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-filtering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Data Filtering&lt;/h1&gt;
&lt;p&gt;DataFrames can be filtered by using a conditional index to the rows or columns. So for instance, if we wanted to select only the people who scored greater than 95% in their test, we might run the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[df[&amp;#39;Score&amp;#39;] &amp;gt; 95]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Name  Age  Score
## a   John   37   99.3
## c  Ringo   40   97.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conditional statements can include any logical operators such as &lt;code&gt;and&lt;/code&gt; and &lt;code&gt;or&lt;/code&gt;, provided that they result in a list of booleans.&lt;/p&gt;
&lt;div id=&#34;data-sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; Data Sorting&lt;/h2&gt;
&lt;p&gt;Sorting of the data can be done by using the &lt;code&gt;sort_values&lt;/code&gt; method. This is similar to the NumPy sort, but will sort the whole DataFrame using the sorted index of the column (or index) of interest. There are a number of important parameters to consider:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;by&lt;/code&gt; – The name or index of the column (or row) to use for sorting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;axis&lt;/code&gt; – Whether you want to sort over the indexes (0) or the columns (1)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ascending&lt;/code&gt; – Whether to sort in ascending (True) or descending order (False)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt; – The sorting algorithm to use&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So to sort the DataFrame in descending order on the &lt;code&gt;Score&lt;/code&gt; field, we would do the following:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df_srt = df.sort_values(by = [&amp;#39;Score&amp;#39;], ascending = False)
df_srt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## a    John   37   99.3
## c   Ringo   40   97.5
## b    Paul   33   92.6
## d  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, there is another parameter, &lt;code&gt;inplace&lt;/code&gt;. If set to True, calling this method will replace the original object with its sorted output, rather than you having to explicitly assign this:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.sort_values(by = [&amp;#39;Score&amp;#39;], ascending = False, inplace = True)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## a    John   37   99.3
## c   Ringo   40   97.5
## b    Paul   33   92.6
## d  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to use the function &lt;code&gt;sort_index()&lt;/code&gt; to sort on the index of the DataFrame rather than the values within the DataFrame itself. This can be used, for instance, to return a sorted DataFrame back to its original state. By sorting the DataFrame above, we have created DataFrame with a non-consecutive index. By sorting on the index, we will return the DataFrame to its original state before we sorted it:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.sort_index(inplace = True)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## a    John   37   99.3
## b    Paul   33   92.6
## c   Ringo   40   97.5
## d  George   35   92.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; Data Cleaning&lt;/h2&gt;
&lt;p&gt;One of the first steps for any data analysis is to ensure that the data set that you are starting with is clean and does not contain any missing values. Let’s create a DataFrame with some missing values using the &lt;code&gt;np.nan&lt;/code&gt; value:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dict = {&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, np.nan, &amp;#39;Ringo&amp;#39;, &amp;#39;George&amp;#39;], &amp;#39;Age&amp;#39;:[37,np.nan,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,np.nan]}
df = pd.DataFrame(dict)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name   Age  Score
## 0    John  37.0   99.3
## 1     NaN   NaN   92.6
## 2   Ringo  40.0   97.5
## 3  George  35.0    NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Missing values can be identified using the &lt;code&gt;isnull()&lt;/code&gt; method, which produces an array of True or False values depending on whether the data are missing or not:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.isnull()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Name    Age  Score
## 0  False  False  False
## 1   True   True  False
## 2  False  False  False
## 3  False  False   True&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since True is treated as 1, whilst False is treated as 0 in numerical calculations, we can count the number of missing values by simply summing the values in this array:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.isnull().sum()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Name     1
## Age      1
## Score    1
## dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have 1 value missing in each column. There are different ways of dealing with missing values, and it largely depends on the context. Sometimes, we may want to simply drop the data if it is no longer useful. For instance, the second row gives us only the score but we cannot link this to the name nor the age. Similarly, we may not be able to use the data for individuals where we do not have a score. The &lt;code&gt;dropna()&lt;/code&gt; method will remove all rows that contain an NA value:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.dropna()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Name   Age  Score
## 0   John  37.0   99.3
## 2  Ringo  40.0   97.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to instead remove all &lt;em&gt;columns&lt;/em&gt; that contain an NA value, we can use the &lt;code&gt;axis&lt;/code&gt; parameter:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.dropna(axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Empty DataFrame
## Columns: []
## Index: [0, 1, 2, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since every column contains a missing value, we end up with an empty DataFrame. Other times, it may make sense to replace missing values with another value. The following will replace every missing value with the value &lt;code&gt;Missing&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.fillna(&amp;#39;Missing&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Name      Age    Score
## 0     John       37     99.3
## 1  Missing  Missing     92.6
## 2    Ringo       40     97.5
## 3   George       35  Missing&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, this is not a whole lot better than the missing values for the numeric data. For numeric values, we can attempt to impute a value that is suitable by using the other data in the data set. Sometimes, simply using the mean of all of the other values in the column may be suitable:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.fillna(df.mean())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name        Age      Score
## 0    John  37.000000  99.300000
## 1     NaN  37.333333  92.600000
## 2   Ringo  40.000000  97.500000
## 3  George  35.000000  96.466667&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-dataframes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; Combining DataFrames&lt;/h2&gt;
&lt;p&gt;Very often, it is necessary to combine data from multiple sources. For instance, we may want to scrape data from multiple different websites, and then combine them into a single DataFrame for analysis. If we have two DataFrames containing the same columns, we can append the rows of one to the other using the &lt;code&gt;append()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df1 = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;, &amp;#39;Ringo&amp;#39;, &amp;#39;George&amp;#39;], &amp;#39;Age&amp;#39;:[37,33,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,92.6]})
df2 = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;Yoko&amp;#39;, &amp;#39;Brian&amp;#39;], &amp;#39;Age&amp;#39;:[32,54], &amp;#39;Score&amp;#39;:[91.6,97.0]})
df1.append(df2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score
## 0    John   37   99.3
## 1    Paul   33   92.6
## 2   Ringo   40   97.5
## 3  George   35   92.6
## 0    Yoko   32   91.6
## 1   Brian   54   97.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice here that the row indexes are appended as is, and should be regenerated to make this dataframe more usable.&lt;/p&gt;
&lt;p&gt;To add additional columns when the number of rows are identical, we use the &lt;code&gt;concat&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df1 = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;, &amp;#39;Ringo&amp;#39;, &amp;#39;George&amp;#39;], &amp;#39;Age&amp;#39;:[37,33,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,92.6]})
df2 = pd.DataFrame({&amp;#39;Gender&amp;#39;:[&amp;#39;Male&amp;#39;, &amp;#39;Male&amp;#39;, &amp;#39;Male&amp;#39;, &amp;#39;Male&amp;#39;]})
pd.concat([df1,df2], axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score Gender
## 0    John   37   99.3   Male
## 1    Paul   33   92.6   Male
## 2   Ringo   40   97.5   Male
## 3  George   35   92.6   Male&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A slightly safer way to combine two DataFrames is to merge them using &lt;code&gt;join()&lt;/code&gt;, where you can combine two DataFrames that share Columns. This might happen where you have two different sets of data for the same set of samples:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df1 = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;, &amp;#39;Ringo&amp;#39;, &amp;#39;George&amp;#39;], &amp;#39;Age&amp;#39;:[37,33,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,92.6]})
df2 = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;], &amp;#39;Gender&amp;#39;:[&amp;#39;Male&amp;#39;, &amp;#39;Male&amp;#39;]})
pd.merge(df1, df2, on = &amp;#39;Name&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Name  Age  Score Gender
## 0  John   37   99.3   Male
## 1  Paul   33   92.6   Male&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, this uses an &lt;code&gt;inner&lt;/code&gt; join, such that outputs are only given where values for &lt;code&gt;Name&lt;/code&gt; are found for both &lt;code&gt;df1&lt;/code&gt; and &lt;code&gt;df2&lt;/code&gt;. However, if we want to create a DataFrame with all values in it, but missing values where necessary, we can use an &lt;code&gt;outer&lt;/code&gt; join:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pd.merge(df1, df2, on = &amp;#39;Name&amp;#39;, how = &amp;#39;outer&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name  Age  Score Gender
## 0    John   37   99.3   Male
## 1    Paul   33   92.6   Male
## 2   Ringo   40   97.5    NaN
## 3  George   35   92.6    NaN&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-and-writing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; Reading and Writing Data&lt;/h2&gt;
&lt;p&gt;As well as generating DataFrames from arrays created in Python, we can directly read data from a variety of sources using Pandas. Examples include comma-separated value (CSV) files and tab-separated value (TSV) files, where rows are separated by new lines and columns are separated by a delimiter such as a comma or tab-character. In addition, Pandas can read directly from SQL databases, web sites, or Excel spreadsheets. There are some very basic example files available from &lt;a href=&#34;/files/RTutorial/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;read_csv&lt;/code&gt; is one of the most commonly used functions and will load in data from a CSV or TSV file. By default, it uses new lines (&lt;code&gt;\n&lt;/code&gt;) to delimit rows and commas (&lt;code&gt;,&lt;/code&gt;) to delimit columns. The &lt;code&gt;sep&lt;/code&gt; parameter can be used to set a different delimiter, for instance if the data are tab-delimited:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pd.read_csv(&amp;quot;sample_annotation.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   SampleName Treatment  Replicate CellType
## 0    sample1   Control          1     HeLa
## 1    sample2   Control          2     HeLa
## 2    sample3   Control          3     HeLa
## 3    sample4      Drug          1     HeLa
## 4    sample5      Drug          2     HeLa
## 5    sample6      Drug          3     HeLa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are lots of additional arguments to the &lt;code&gt;read_csv()&lt;/code&gt; function; &lt;code&gt;header&lt;/code&gt; allows you to specify the row that should be used to name the columns of the data frame, &lt;code&gt;index_col&lt;/code&gt; allows you to specify the column that should be used as the row index, &lt;code&gt;sep&lt;/code&gt; gives the delimiter between column entries (e.g. &lt;code&gt;\t&lt;/code&gt; for tab-delimited files, or &lt;code&gt;,&lt;/code&gt; for comma-separated files), ‘usecols’ allows you to specify which subset of columns to load, &lt;code&gt;dtype&lt;/code&gt; allows you to specify which data types each column represent (as seen previously for structured arrays), &lt;code&gt;skiprows&lt;/code&gt; allows you to skip the given numbber of rows at the start of the file, &lt;code&gt;skipfooter&lt;/code&gt; does the same but skipping from the bottom of the file, &lt;code&gt;nrows&lt;/code&gt; gives the number of rows to read in, &lt;code&gt;na_values&lt;/code&gt; allows you to specify which values to count as being ‘NaN’. There are many more, but here is an example of using some of these different parameters:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;pd.read_csv(&amp;quot;sample_annotation.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;, nrows = 4, index_col = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Treatment  Replicate CellType
## SampleName                              
## sample1      Control          1     HeLa
## sample2      Control          2     HeLa
## sample3      Control          3     HeLa
## sample4         Drug          1     HeLa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the unique &lt;code&gt;SampleName&lt;/code&gt; field has been used as the index, which may be more useful than simply numbering them as it is more descriptive.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;control-sequences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Control Sequences&lt;/h1&gt;
&lt;p&gt;One of the most useful things to be able to do with computers is to repeat the same command multiple times without having to do it by hand each time. For this, control sequences can be used to give you close control over the progress of your program.&lt;/p&gt;
&lt;div id=&#34;if-else&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.1&lt;/span&gt; IF ELSE&lt;/h2&gt;
&lt;p&gt;The first control sequence to look at is the “if else” command, which acts as a switch to run one of a selection of possible commands given a switch that you specify. For instance, you may want to do something different depending on whether a value is odd or even:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;val = 3
if val%2 == 0:   # If it is even (exactly divisible by 2)
  print(&amp;quot;Value is even&amp;quot;)
else:             # Otherwise it must be odd
  print(&amp;quot;Value is odd&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value is odd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the expression in the parentheses following “if” is evaluated, and if it evaluates to True then the block of code contained within the following curly braces is evaluated. Otherwise, the block of code following the “else” statement is evaluated. You can add additional tests by using the &lt;code&gt;elif&lt;/code&gt; statement:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;my_val = 27
if my_val%2 == 0:
  print(&amp;quot;Value is divisible by 2\n&amp;quot;)
elif my_val%3 == 0:
  print(&amp;quot;Value is divisible by 3\n&amp;quot;)
else:
  print(&amp;quot;Value is not divisible by 2 or 3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value is divisible by 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each switch is followed by a block of code in the same indentation level, and the conditional statements are evaluated until one evaluates to True, at which point the relevant output is produced. If none of them evaluate to True, then the default code block following &lt;code&gt;else&lt;/code&gt; is evaluated instead. If no &lt;code&gt;else&lt;/code&gt; block is present, then the default is to just do nothing. These blocks can be as complicated as you like, and you can have &lt;code&gt;elif&lt;/code&gt; statements within the blocks to create a hierarchical structure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.2&lt;/span&gt; FOR&lt;/h2&gt;
&lt;p&gt;Another control structure is the &lt;code&gt;for&lt;/code&gt; loop, which will conduct the code in the block multiple times for a variety of values that you specify at the start. For instance, here is a simple countdown script:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i in range(10,0,-1):
  print(i)
  if i == 1:
    print(&amp;quot;Blastoff!&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10
## 9
## 8
## 7
## 6
## 5
## 4
## 3
## 2
## 1
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the index &lt;code&gt;i&lt;/code&gt; is taken from the set of numbers (10, 9, …, 1), starting at the first value 10, and each time prints out the number followed by a newline. Then an &lt;code&gt;if&lt;/code&gt; statement checks to see if we have reached the final number, which we have not. It therefore returns to the start of the block, updates the number to the second value 9, and repeats. It does this until there are no more values to use.&lt;/p&gt;
&lt;p&gt;As a small aside, this is slightly inefficient. Evaluation of the &lt;code&gt;if&lt;/code&gt; statement is conducted every single time the loop is traversed (10 times in this example). It will only ever be True at the end of the loop, so we could always take this out of the loop and evaluate the final printout after the loop is finished and save ourselves 10 calculations. Whilst the difference here is negligible, thinking of things like this may save you time in the future:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i in range(10,0,-1):
  print(i)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10
## 9
## 8
## 7
## 6
## 5
## 4
## 3
## 2
## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Blastoff!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;while&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.3&lt;/span&gt; WHILE&lt;/h2&gt;
&lt;p&gt;The final main control structure is the &lt;code&gt;while&lt;/code&gt; loop. This is similar to the &lt;code&gt;for&lt;/code&gt; loop, and will continue to evaluate the code chunk as long as the specified expression evaluates to True:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;i = 10
while i &amp;gt; 0:
  print(i)
  i -= 1 ## Equavalent to i = i-1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10
## 9
## 8
## 7
## 6
## 5
## 4
## 3
## 2
## 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Blastoff!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This does exactly the same as the &lt;code&gt;for&lt;/code&gt; loop above. In general, either can be used for a given purpose, but there are times when one would be more “elegant” than the other. For instance, here the for loop is better as you do not need to manually subtract 1 from the index each time.&lt;/p&gt;
&lt;p&gt;However, if you did not know how many iterations were required before finding what you are looking for (for instance searching through a number of files), a &lt;code&gt;while&lt;/code&gt; loop may be more suitable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HOWEVER&lt;/strong&gt;: Be aware that it is possible to get caught up in an “infinite loop”. This happens if the conditional statement never evaluates to False. For instance, if we forget to decrement the index, &lt;code&gt;i&lt;/code&gt; will always be 10 and will therefore never be less than 0. This loop will therefore run forever:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;i = 10
while i &amp;gt; 0:
  print(i)
print(&amp;quot;Blastoff!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loop-control&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.4&lt;/span&gt; Loop Control&lt;/h2&gt;
&lt;p&gt;You can leave control loops early by using flow control constructs. &lt;code&gt;continue&lt;/code&gt; skips out of the current loop and moves onto the next in the sequence. In the following case, it will restart the loop when &lt;code&gt;i&lt;/code&gt; is 5 before printing:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i in range(1,11):
  if i == 5:
    continue
  print(i) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4
## 6
## 7
## 8
## 9
## 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Finished loop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;break&lt;/code&gt; will leave the code chunk entirely, and in this case will move onto the final print function as soon as it is identified that &lt;code&gt;i&lt;/code&gt; is equal to 5:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;for i in range(1,11):
  if i == 5:
    break
  print(i) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Finished loop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Writing Functions&lt;/h1&gt;
&lt;p&gt;There are many functions available in Python, and chances are if you want to do something somebody has already written the function to do it. It is best to not re-invent the wheel if possible (or at least it is more efficient – sometimes it is good to reinvent the wheel to understand how it works), but very often you will want to create your own functions to save replicating code.&lt;/p&gt;
&lt;p&gt;A function takes in one or more variables, does something with them, and returns something (e.g. a value or a plot). For instance, calculating the mean of a number of values is simply a case of adding them together and dividing by the number of values. Let’s write a function to do this and check that it matches the &lt;code&gt;mean()&lt;/code&gt; function from NumPy:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def my_mean (x): # Here, x is an array of numbers
  nvals = len(x)
  valsum = sum(x)
  return valsum/nvals

my_vals = [3,5,6,3,4,3,4,7]
my_mean(my_vals) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, as with the loops earlier, the function is contained within a block of code with the same indentation level. A numeric vector is given to the function, the mean is calculated, and this value is returned to the user using the &lt;code&gt;return()&lt;/code&gt; function. This value can be captured into a variable of your choosing in the same way as with any function. As we see here, the value returned by this user-defined function is identical to that from the NumPy pacakge.&lt;/p&gt;
&lt;p&gt;You can also add further arguments to the function call. If you want an argument to have a default value, you can specify this in the function declaration. This is the value that will be used if no argument value is specified. Any arguments that do not have a default value must be specified when calling the function, or an error will be thrown:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def foo(x, arg):
  print(x, arg)
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try and add a default value for &lt;code&gt;arg&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def foo(x, arg = &amp;quot;World!&amp;quot;):
  print(x, arg)
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello World!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a good point to mention an idea known as “scope”. After running the following function, have a look at the value &lt;code&gt;valsum&lt;/code&gt; calculated within the function:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def my_mean(x): # Here, x is a numeric vector 
  nvals = len(x)
  valsum = sum(x)
  return valsum/nvals
my_vals = [3,5,6,3,4,3,4,7]
my_mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(valsum) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &amp;#39;valsum&amp;#39; is not defined
## 
## Detailed traceback: 
##   File &amp;quot;&amp;lt;string&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what went wrong? When we try to print the variable, Python is unable to find the object &lt;code&gt;valsum&lt;/code&gt;. So where is it? The “scope” of an object is the environment where it can be found. Up until now, we have been using what are known as “global variables”. That is we have created all of our objects within the “global environment”, which is the top level where Python searches for objects. These objects are available at all times.&lt;/p&gt;
&lt;p&gt;However, when we call a function, a new environment, or “scope”, is created, and all variables created within the function become “local variables” that can only be accessed from within the function itself. As soon as we leave the scope of the function, these local variables are deleted. If you think about it, this makes sense – otherwise, every time we called a function, memory would fill up with a whole load of temporary objects.&lt;/p&gt;
&lt;p&gt;So, the function itself is completely self-contained. A copy of the input variable is stored in a new local variable (called &lt;code&gt;x&lt;/code&gt; in this case), something is done to this object (possibly creating additional objects along the way), something is returned, and then all of these objects in the scope of the function are removed, and we move back into the global environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Statistics&lt;/h1&gt;
&lt;div id=&#34;basic-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.1&lt;/span&gt; Basic Statistics&lt;/h2&gt;
&lt;p&gt;Python is a commonly used programming language for data science, so statistical computation is incredibly important. The NumPy package provides a large number of in-built functions for caluclating a wide range of common statistics. The following example creates two vectors of 100 random values sampled from a normal distribution with mean 0 and standard deviation 1, then calculates various basic summary statistics:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = np.sort(np.random.normal(loc = 0, scale = 1, size = (100)))
np.min(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -2.4901017513978916&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.max(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2.0342720925979605&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.mean(x)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -0.005346002854634584&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.median(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.05515553362632984&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum and maximum values are the smallest and largest values respectively. The mean is what most people would think of when you asked for the average, and is calculated by summing the values and dividing by the total number of values. The median is another way of looking at the average, and is essentially the middle value (&lt;code&gt;50^th^&lt;/code&gt; percentile). Other percentiles can be calculated, which can give you an idea of where the majority of your data lie:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.quantile(x, q = 0.25) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -0.5658977276720896&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.quantile(x, q = 0.75) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.5276885798940764&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.quantile(x, q = np.arange(0, 1, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([-2.49010175, -1.18479618, -0.77638742, -0.45972912, -0.15753098,
##         0.05515553,  0.23199671,  0.46014186,  0.65004843,  1.31168354])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;stats.describe()&lt;/code&gt; function from the &lt;code&gt;stats&lt;/code&gt; &lt;code&gt;scipy&lt;/code&gt; package will calculate many of these basic statistics for you:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import scipy
from scipy import stats
scipy.stats.describe(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DescribeResult(nobs=100, minmax=(-2.4901017513978916, 2.0342720925979605), mean=-0.005346002854634584, variance=0.9294613607091599, skewness=-0.2268484990299433, kurtosis=-0.11476026245922721)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean and variance show you the average value and the spread about this average value for these data. Skewness and kurtosis tell us how different from a standard normal distribution these data are, with skewness representing asymmmetry and kurtosis representing the presence of a significant tail in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.2&lt;/span&gt; Variation&lt;/h2&gt;
&lt;p&gt;Variance is the average of the squared distances of each individual value from their mean, and is a measure of how spread out the data are from the average. The standard deviation is simply the square root of this value &lt;span class=&#34;math inline&#34;&gt;\(var(x) = sd(x)^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.std(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.9592532236599824&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.var(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.9201667471020682&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The covariance is a measure of how much two sets of data vary together:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;y = np.sort(np.random.normal(loc = 0, scale = 1, size = (100)))
np.var(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.6278146179612955&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.cov(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[0.92946136, 0.76000158],
##        [0.76000158, 0.63415618]])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.3&lt;/span&gt; Correlation&lt;/h2&gt;
&lt;p&gt;The covariance is related to the correlation between two data sets, which is a number between -1 and 1 indicating the level of dependance between the two variables. A value of 1 indicates perfect correlation, so that as one value increases so does the other. A value of -1 indicates perfect anti-correlation, so that as one value increases the other decreases. A value of 0 indicates that the two values change independently of one another:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;np.cov(x, y)/(np.std(x) * np.std(y)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([[1.22287583, 0.99992061],
##        [0.99992061, 0.83434804]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This value is known as the Pearson correlation, which can be calculated using the &lt;code&gt;scipy.stats&lt;/code&gt; function &lt;code&gt;pearsonr()&lt;/code&gt;. The first value gives the Pearson correlation coefficient between the two arrays, and the second gives a p-value which gives some idea of the significance of the correlation (more on this later):&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scipy.stats.pearsonr(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (0.9899214012931843, 5.23717802380351e-85)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative method for calculating the correlation between two sets of values is to use the Spearman correlation, which is essentially the same as the Pearson correlation but is calculated on the &lt;em&gt;ranks&lt;/em&gt; of the data rather than the values themselves. In this way, each value increases by only one unit at a time in a monotonic fashion, meaning that the correlation score is more robust to the presence of outliers:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scipy.stats.spearmanr(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## SpearmanrResult(correlation=0.9999999999999999, pvalue=0.0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So these values are pretty highly dependent on one another – not surprising considering that they are both drawn randomly from the same distribution (notice that we sorted them such that the values were increasing for both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.4&lt;/span&gt; Linear Models&lt;/h2&gt;
&lt;p&gt;We can calculate the line of best fit between the two vectors by using linear regression, which searches for the best model &lt;span class=&#34;math inline&#34;&gt;\(y = a + bx\)&lt;/span&gt; that minimises the squared distances between the line (estimated values) and the observed data points. We can implement this using the &lt;code&gt;OLS&lt;/code&gt; ordinary least squares function from package from &lt;code&gt;statsmodels&lt;/code&gt;. We will use &lt;code&gt;y&lt;/code&gt; as the response variable, and &lt;code&gt;x&lt;/code&gt; as the explanatory variable:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import statsmodels.api as sm
model = sm.OLS(y,x)
fit = model.fit()
fit.summary2()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;statsmodels.iolib.summary2.Summary&amp;#39;&amp;gt;
## &amp;quot;&amp;quot;&amp;quot;
##                         Results: Ordinary least squares
## ===============================================================================
## Model:                  OLS              Adj. R-squared (uncentered): 0.970    
## Dependent Variable:     y                AIC:                         -110.8315
## Date:                   2019-11-07 12:42 BIC:                         -108.2263
## No. Observations:       100              Log-Likelihood:              56.416   
## Df Model:               1                F-statistic:                 3211.    
## Df Residuals:           99               Prob (F-statistic):          2.88e-77 
## R-squared (uncentered): 0.970            Scale:                       0.019137 
## -------------------------------------------------------------------------------------
##            Coef.        Std.Err.          t          P&amp;gt;|t|        [0.025       0.975]
## -------------------------------------------------------------------------------------
## x1         0.8172         0.0144       56.6686       0.0000       0.7886       0.8458
## -------------------------------------------------------------------------------
## Omnibus:                  40.164            Durbin-Watson:               0.167 
## Prob(Omnibus):            0.000             Jarque-Bera (JB):            78.945
## Skew:                     1.647             Prob(JB):                    0.000 
## Kurtosis:                 5.846             Condition No.:               1     
## ===============================================================================
## 
## &amp;quot;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explaining this output is beyond the scope of this short tutorial, but the coefficient estimates give us the values for the slope in the linear model, and tells us by how many units y increases as x increases by 1. The R-squared value is the proportion of variance in the data that is explained by the model, with a number closer to 1 indicating a better model fit.&lt;/p&gt;
&lt;p&gt;The p-value tells us how significant these estimates are, both for the model as a whole from the F-statistic, and from the individual variables from the t-statistics. In statistical terms, we are testing the null hypothesis that the coefficient is actually equal to zero (i.e. there is not an association between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;). The p-value gives the probability of detecting a coefficient at least as large as the one that we calculated in our model given that the null hypothesis is actually true. If this probability is low enough, we can safely reject the null hypothesis and say that this variable is statistically significant. Often a value of 0.05 (5%) is used as the cutoff for rejection of the null hypothesis.&lt;/p&gt;
&lt;p&gt;Hypothesis testing is a large part of statistics. The t-test is a commonly used test for comparing the means of two sets of numeric data. In simple terms we are looking to see if they are significantly different (e.g. does the expression of a particular gene change significantly following treatment with a drug). In statistical terms, we are testing to see if the change that we see in the means is greater than we would expect by chance alone.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scipy.stats.ttest_ind(x, y) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ttest_indResult(statistic=-0.6452839600923539, pvalue=0.5194902137636705)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are drawn from the same distribution, the test shows there is no evidence that there is a difference between the mean. Let’s try again with a different data set, drawn from a different distribution with mean 10:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;z = np.sort(np.random.normal(loc = 10, scale = 1, size = (100)))
scipy.stats.ttest_ind(x, z) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Ttest_indResult(statistic=-71.79781491100934, pvalue=1.0011045801868256e-143)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the p-value is much less than 0.05, so we can reject the null hypothesis and make the claim that the mean of &lt;code&gt;z&lt;/code&gt; is significantly different from that of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-with-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Plotting With Python&lt;/h1&gt;
&lt;p&gt;Similarly to R, Python is able to generate publication quality figures simply and easily. One of the most common packages for plotting in Python is the &lt;a href=&#34;https://matplotlib.org&#34;&gt;&lt;code&gt;matplotlib&lt;/code&gt;&lt;/a&gt; package, in particular using the &lt;code&gt;pyplot&lt;/code&gt; module:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import matplotlib.pyplot as plt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pyplot&lt;/code&gt; module gives a Matlab-like interface for plotting figures in Python. The general layout is to call a plot function from &lt;code&gt;pyplot&lt;/code&gt; such as &lt;code&gt;plot()&lt;/code&gt;, &lt;code&gt;hist()&lt;/code&gt;, &lt;code&gt;scatter()&lt;/code&gt;, etc to initiate the plot, add various additional features (such as labels and legends), and finally use &lt;code&gt;show()&lt;/code&gt; to generate the image from the combined components.&lt;/p&gt;
&lt;p&gt;There are a huge number of features that can be added to any plot, including the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Label the x-axis and y-axis with &lt;code&gt;xlabel()&lt;/code&gt; and &lt;code&gt;ylabel()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add a title to the plot with &lt;code&gt;title()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add a legend with &lt;code&gt;legend()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change the range of the x-axis and y-axis by using &lt;code&gt;xlim()&lt;/code&gt; and &lt;code&gt;ylim()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Define the position of the ticks in the x and y-axes with &lt;code&gt;xticks()&lt;/code&gt; and &lt;code&gt;yticks()&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are also some common arguments in &lt;code&gt;pyplot&lt;/code&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;color&lt;/code&gt; for specifying the color&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha&lt;/code&gt; to define the opacity of the colors (0 is entirely see through, 1 is solid)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linestyle&lt;/code&gt; to define whether the line should be solid (&lt;code&gt;-&lt;/code&gt;), dashed (&lt;code&gt;--&lt;/code&gt;), dot-dashed (&lt;code&gt;-.&lt;/code&gt;, etc)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linewidth&lt;/code&gt; to define the width of the line&lt;/li&gt;
&lt;li&gt;&lt;code&gt;marker&lt;/code&gt; to define the type of point to use (&lt;code&gt;.&lt;/code&gt; for points, &lt;code&gt;o&lt;/code&gt; for circles, &lt;code&gt;s&lt;/code&gt; for square, &lt;code&gt;v&lt;/code&gt; for downward triangle, &lt;code&gt;^&lt;/code&gt; for upward triangle, &lt;code&gt;$...$&lt;/code&gt; to render strings in mathtext style)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;markersize&lt;/code&gt; to define the size of the points to use&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Whilst this is not an in depth tutorial for plotting in Python, here are a selection of commonly used plots, and how to generate them. We will use two randomly generated independent normally distributed arrays as an example:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;x = np.random.normal(loc = 0, scale = 1, size = (100))
y = np.random.normal(loc = 0, scale = 1, size = (100))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.1&lt;/span&gt; Histograms&lt;/h2&gt;
&lt;p&gt;A histogram is used to examine the distribution of 1-dimensional data, by counting up the number of values that fall into discrete bins. The size of the bins (or the number of bins) can be specified by using the &lt;code&gt;bins&lt;/code&gt; argument. The counts for each bin are returned in the form of an array for further analysis:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;h = plt.hist(x, bins = 20, range = (-3,3), color = &amp;quot;red&amp;quot;)
plt.title(&amp;quot;This is a histogram&amp;quot;)
plt.xlabel(&amp;quot;x&amp;quot;)
plt.ylabel(&amp;quot;count&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.2&lt;/span&gt; Quantile-Quantile Plots&lt;/h2&gt;
&lt;p&gt;After looking at the histogram, we may think that our data follows some specific distribution (e.g. normal distribution). Quantile-quantile plots can be used to see if two data sets are drawn from the same distribution. To do this, it plots the quantiles of each data set against one another. That is, it plots the 0&lt;sup&gt;th&lt;/sup&gt; percentile of data set A (the minimum value) against the 0th percentile of data set B, the 50&lt;sup&gt;th&lt;/sup&gt; percentiles (the medians) against each other, the 100&lt;sup&gt;th&lt;/sup&gt; percentiles (the maximum values) against each other, etc. Simply, it sorts both data sets, and makes them both the same length by estimating any missing values, then plots a scatterplot of the sorted data. If the two data sets are drawn from the same distribution, this plot should follow the &lt;span class=&#34;math inline&#34;&gt;\(x = y\)&lt;/span&gt; identity line at all but the most extreme point.&lt;/p&gt;
&lt;p&gt;We can use the &lt;code&gt;api&lt;/code&gt; module from &lt;code&gt;statsmodels&lt;/code&gt; to plot a QQ plot to see if our randomly generated array was indeed drawn from a normal distribution, by comparing &lt;code&gt;x&lt;/code&gt; with a standard normal distribution (&lt;code&gt;scipy.stats.distributions.norm&lt;/code&gt;). A 45 degree line is added to show the theoretical identity between the two distributions. The closer the points lie to this line, the closer the two distributions are:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import statsmodels.api as sm
sm.qqplot(x, line = &amp;#39;45&amp;#39;, dist = scipy.stats.distributions.norm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/QQplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, these data seem to approximate a normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pie-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.3&lt;/span&gt; Pie Chart&lt;/h2&gt;
&lt;p&gt;Now let’s say that we have a data set that shows the number of called peaks from a ChIPseq data set that fall into distinct genomic features (exons, introns, promoters and intergenic regions). One way to look at how the peaks fall would be to look at a pie graph, which shows proportions in a data set represented by slices of a circle:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;peaknums  = [1400,900,200,150]
peaknames = [&amp;quot;exons&amp;quot;, &amp;quot;intron&amp;quot;, &amp;quot;promoter&amp;quot;, &amp;quot;intergenic&amp;quot;]
p = plt.pie(peaknums, labels = peaknames)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/pie1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the majority of the peaks fall into exons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.4&lt;/span&gt; Bar Plot&lt;/h2&gt;
&lt;p&gt;However, pie charts are typically discouraged by statisticians, because your eyes can often misjudge estimates of the area taken up by each feature. A better way of looking at data such as this would be in the form of a barplot:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;b = plt.bar(peaknames, peaknums)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/barplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.5&lt;/span&gt; Line Plots&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;pyplot&lt;/code&gt; function &lt;code&gt;plot()&lt;/code&gt; is the standard plotting method for 2-dimensional data, and is used to plot paired data &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; against one another, using markers and/or lines. Plots such as these are useful for looking at correlation between two data sets. By default, the &lt;code&gt;plot()&lt;/code&gt; function will plot a line plot:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(x,y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/lineplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that this looks very messy. This is due to the fact that the line starts with the first pair of values &lt;code&gt;($x_1$, $x_2$)&lt;/code&gt;, then plots a line to the second pair of values &lt;code&gt;($x_1$, $x_2$)&lt;/code&gt;, and so on. Since &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are completely random, there is no order from lowest to highest for either, and so we end up with this messy image. For a line plot, we need to sort the data so that the x-axis data run from left to right. Since both data sets need to remain paired, we only sort one data set, and use this ordering to rearrange both data sets using the &lt;code&gt;argsort()&lt;/code&gt; method:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(x[x.argsort()],y[x.argsort()])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/lineplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use a number of the arguments described above to modify this plot, including adding axis labels, a title, and some markers for each of the points:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(x[x.argsort()],y[x.argsort()], marker = &amp;#39;o&amp;#39;, color = &amp;#39;red&amp;#39;, linestyle = &amp;#39;--&amp;#39;)
plt.title(&amp;quot;This is y vs. x&amp;quot;)
plt.xlabel(&amp;quot;This is x&amp;quot;)
plt.ylabel(&amp;quot;This is y&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/lineplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.6&lt;/span&gt; Scatterplots&lt;/h2&gt;
&lt;p&gt;Sometimes we may only want to look at the markers and not plot a line between them, for instance to save us worrying about ordering as we had to do above. This allows us to take a general look over the relationship between two numeric data sets. For instance, for every student in a class, we may have scores from tests taken at the start and at the end of the year, and we want to compare them against one another to see how they compare. We can use the same function, and simply set the &lt;code&gt;linestyle&lt;/code&gt; to &lt;code&gt;&#39;&#39;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(x,y, color = &amp;quot;red&amp;quot;, marker = &amp;#39;o&amp;#39;, markersize = 10, linestyle = &amp;#39;&amp;#39;)
plt.title(&amp;quot;This is y vs. x&amp;quot;)
plt.xlabel(&amp;quot;This is x&amp;quot;)
plt.ylabel(&amp;quot;This is y&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/scatterplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we have coloured all of our points a single colour by using the &lt;code&gt;color = &amp;quot;red&amp;quot;&lt;/code&gt; argument. However, we may want to assign colours to each point separately by supplying an array of colours that is the same length as &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. This means that we can set colours based on the data themselves, e.g. to colour male and female samples differently to one another, or to color points that exceed some threshold of interest. To do this, we can use the &lt;code&gt;scatter()&lt;/code&gt; function, which can take a list of colors using the &lt;code&gt;c&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;## Create an array containing &amp;quot;black&amp;quot; for every element in x
mycols = np.repeat(&amp;quot;black&amp;quot;, len(x))

## Change the color based on the values in x and y
mycols[(x &amp;gt;  0.5) &amp;amp; (y &amp;gt;  0.5)] = &amp;quot;red&amp;quot;
mycols[(x &amp;lt; -0.5) &amp;amp; (y &amp;lt; -0.5)] = &amp;quot;blue&amp;quot;

## Plot the scatter plot
plt.scatter(x, y, color = mycols, marker = &amp;#39;o&amp;#39;)
plt.title(&amp;quot;This is y vs. x&amp;quot;)
plt.xlabel(&amp;quot;This is x&amp;quot;)
plt.ylabel(&amp;quot;This is y&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/scatterplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.7&lt;/span&gt; Boxplots&lt;/h2&gt;
&lt;p&gt;To compare the distribution of multiple numeric data sets, we can use boxplots. A boxplot shows the overal distribution by plotting a box bounded by the first and third quartiles, with the median highlighted. This shows where the majority of the data lie. Additional values are plotted as whiskers coming out from the main box. Multiple boxes can be plotted next to one another allowing you to compare similarlities or differences between them. This can be used for instance to compare distributions of two different data sets, or to compare a numeric value in a data set after separating out multiple classes (e.g. comparing different age groups). The plot produces a &lt;code&gt;dict&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;bp = plt.boxplot([x,y])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/boxplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a number of arguments that can be supplied, including &lt;code&gt;labels&lt;/code&gt; which allows you to specify the labels on the x-axis, &lt;code&gt;notch&lt;/code&gt; which will add a notch into the boxplot to represent the confident interval, &lt;code&gt;whis&lt;/code&gt; to determine the extent of the whiskers (any values not included within the whiskers are plotted as outliers), and &lt;code&gt;widths&lt;/code&gt; to change the widths of the boxes:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;bp = plt.boxplot([x,y], labels = [&amp;#39;x&amp;#39;, &amp;#39;y&amp;#39;], notch = 1, whis = 1, widths = [0.1, 1])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/boxplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pandas also has a &lt;code&gt;boxplot()&lt;/code&gt; function which can take the data in the form of a DataFrame, which is useful for instance if you want to compare the distribution of expression values over all genes for a number of different samples. It will do a pretty good job of pulling out only the numeric columns by default, or you can specify which columns you would like to plot:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = pd.DataFrame({&amp;#39;Name&amp;#39;:[&amp;#39;John&amp;#39;, &amp;#39;Paul&amp;#39;, &amp;#39;Jean&amp;#39;, &amp;#39;Paula&amp;#39;], &amp;#39;Age&amp;#39;:[37,33,40,35], &amp;#39;Score&amp;#39;:[99.3,92.6,97.5,92.6], &amp;#39;Gender&amp;#39;:[&amp;#39;Male&amp;#39;, &amp;#39;Male&amp;#39;, &amp;#39;Female&amp;#39;, &amp;#39;Female&amp;#39;]})
df.boxplot(column = [&amp;#39;Age&amp;#39;, &amp;#39;Score&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/boxplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seaborn-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.8&lt;/span&gt; Seaborn package&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://seaborn.pydata.org/generated/seaborn.boxplot.html&#34;&gt;Seaborn&lt;/a&gt; package is an incredibly powerful package for plotting using DataFrames, and is quite similar to the `&lt;code&gt;ggplots2&lt;/code&gt; package in R. It makes defining the components of a plot much simpler in cases when data are stored correctly in DataFrame objects. It is beyond the scope of this tutorial to go into this in too much detail, but the idea is to use the names of the variables in the DataFrame to define various elements of the plot. For instance, in the following plot, we split data on the x-axis by their Gender, and then for each subset we plot the score:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import seaborn as sb
sb.boxplot(x = &amp;#39;Gender&amp;#39;, y = &amp;#39;Score&amp;#39;, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/unnamed-chunk-145-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Seaborn will use some defaults to make this figure look nice, such as filling in the boxes automatically, labelling the axes, etc. However, a lot of additional parameters can be set to change these. If you are interested, I recommend spending some time looking over this package to see the number of options available to you.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.9&lt;/span&gt; Subplots&lt;/h2&gt;
&lt;p&gt;By default, the graphics device will plot a single figure only. The &lt;code&gt;subplots()&lt;/code&gt; function can be used to create multiple subplots within a single plot. By default, the method will create a single figure and a single axis, but by including the number of rows and columns the axis can contain a list of multiple axes for plotting. Each element of the list can then be used to build up the plot as described above:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, subplt = plt.subplots(2, 1)
subplt[0].plot(x,y)
subplt[1].plot(x,-y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/subplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To plot them side-by side instead, this is as simple as changing the arguments to &lt;code&gt;subplots()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, subplt = plt.subplots(1,2)
subplt[0].plot(x,y)
subplt[1].plot(x,-y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/subplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For multiple rows and columns, the axis output is a 2-dimensional NumPy array so each plot can be accessed and modified using square brackets notation:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, subplt = plt.subplots(2,2)
subplt[0,0].plot(x,y)
subplt[0,0].set_title(&amp;quot;Plot1&amp;quot;)
subplt[0,1].plot(x,-y)
subplt[0,1].set_title(&amp;quot;Plot2&amp;quot;)
subplt[1,0].plot(-x,y)
subplt[1,0].set_title(&amp;quot;Plot3&amp;quot;)
subplt[1,1].plot(-x,-y)
subplt[1,1].set_title(&amp;quot;Plot4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/subplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also create specific objects for each of the plots directly when calling &lt;code&gt;subplots()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2,2)
plt1.plot(x,y, color = &amp;quot;red&amp;quot;)
plt1.set_title(&amp;quot;Plot1&amp;quot;)
plt2.plot(x,-y, color = &amp;quot;blue&amp;quot;)
plt2.set_title(&amp;quot;Plot2&amp;quot;)
plt3.plot(-x,y, color = &amp;quot;orange&amp;quot;)
plt3.set_title(&amp;quot;Plot3&amp;quot;)
plt4.plot(-x,-y, color = &amp;quot;green&amp;quot;)
plt4.set_title(&amp;quot;Plot4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/subplot4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, each plot will have its axis limits calculated independently, but it is possible to use the &lt;code&gt;sharex&lt;/code&gt; and &lt;code&gt;sharey&lt;/code&gt; arguments to share the axis limits across all plots:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fig, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2,2, sharex = True, sharey = True)
plt1.plot(x,2*y, color = &amp;quot;red&amp;quot;)
plt1.set_title(&amp;quot;Plot1&amp;quot;)
plt2.plot(x,-y, color = &amp;quot;blue&amp;quot;)
plt2.set_title(&amp;quot;Plot2&amp;quot;)
plt3.plot(-3*x,2*y, color = &amp;quot;orange&amp;quot;)
plt3.set_title(&amp;quot;Plot3&amp;quot;)
plt4.plot(-x,-4*y, color = &amp;quot;green&amp;quot;)
plt4.set_title(&amp;quot;Plot4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/subplot5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-figures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.10&lt;/span&gt; Saving Figures&lt;/h2&gt;
&lt;p&gt;By default, figures are generated in a separate window by using the &lt;code&gt;show()&lt;/code&gt; function. However, you can save the figure to an external file by using the &lt;code&gt;savefig()&lt;/code&gt; function. The output format can be defined simply by adding a suffix to the file – &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, etc.:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(x,y)
plt.savefig(&amp;quot;test.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To change the size of the image, you can specify the size of the figure (in inches) before generating the plot using the &lt;code&gt;figsize&lt;/code&gt; argument in the &lt;code&gt;figure()&lt;/code&gt; method. To define the resolution of the output figure, you can set the &lt;code&gt;dpi&lt;/code&gt; argument for &lt;code&gt;savefig()&lt;/code&gt; to change the dots per inch value. Also, by default the margins for output figures can be quite large, so you can use the `bbox_inches’ argument to change this:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.figure(figsize=(10,10))
plt.plot(x,y)
plt.savefig(&amp;quot;test.png&amp;quot;, bbox_inches=&amp;#39;tight&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Example Analysis&lt;/h1&gt;
&lt;div id=&#34;introduction-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;This is just a simple example analysis to give you an idea of the sort of things that we can do with Python. Suppose that we have two experiments, each looking at the effects on gene expression of using a particular drug (“Drug A” and “Drug B”). For each experiment we have two samples; one showing the gene expression when treated with the drug, and the other showing the gene expression when treated with some control agent. Obviously in a real experiment, we would have many replicates, but here we have &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;. We want to do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For each drug, we want to get the fold change for each gene&lt;/li&gt;
&lt;li&gt;For each drug, we want to identify the genes that are significantly changed when using the drug&lt;/li&gt;
&lt;li&gt;We want to compare the results for Drug A with those from Drug B to find genes that are affected similarly by both drugs&lt;/li&gt;
&lt;li&gt;We want to plot the correlation between the fold change values for the two drugs to see how similar they are&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, we will need four files. These files are in a tab-delimited text format. They are tables of values where each row is separated by a new line, and each column is separated by a tab character (&lt;code&gt;\t&lt;/code&gt;). These files can be created by and read into Excel for ease of use. To avoid errors when reading in files from text, it is good practice to ensure that there are no missing cells in your data. Instead try to get into the habit of using some “missing”&amp;quot; character (e.g. &lt;code&gt;NA&lt;/code&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;File Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_control.txt&#34;&gt;experiment1_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for ctrl in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_drug.txt&#34;&gt;experiment1_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_control.txt&#34;&gt;experiment2_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for control in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_drug.txt&#34;&gt;experiment2_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.2&lt;/span&gt; Load Data&lt;/h2&gt;
&lt;p&gt;First let’s load in the data:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;expt1_ctrl = pd.read_csv(&amp;quot;experiment1_control.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;)
expt1_drug = pd.read_csv(&amp;quot;experiment1_drug.txt&amp;quot;,    sep = &amp;quot;\t&amp;quot;)
expt2_ctrl = pd.read_csv(&amp;quot;experiment2_control.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;)
expt2_drug = pd.read_csv(&amp;quot;experiment2_drug.txt&amp;quot;,    sep = &amp;quot;\t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of these files contains two columns; the gene name and some value that represents the expression level for that gene (assume that these values have been calculated after pre-processing, normalisation, etc.).&lt;/p&gt;
&lt;p&gt;In all of these cases, the list of gene names is identical, and in the same order which means that we could compare row 1 from the control-treated file with row 2 from the drug-treated file to get all of the comparisons. However, in a real data set you will not know for sure that the gene names match so I recommend merging the files together into a single data frame to ensure that all analyses are conducted on a gene by gene basis on the correct values.&lt;/p&gt;
&lt;p&gt;We therefore create a single DataFrame for both experiments using the &lt;code&gt;merge()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;expt1 = pd.merge(expt1_ctrl, expt1_drug, on = &amp;quot;GeneName&amp;quot;, suffixes = (&amp;quot;_Control&amp;quot;, &amp;quot;_Drug&amp;quot;))
expt2 = pd.merge(expt2_ctrl, expt2_drug, on = &amp;quot;GeneName&amp;quot;, suffixes = (&amp;quot;_Control&amp;quot;, &amp;quot;_Drug&amp;quot;))
expt1.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GeneName  Expression_Control  Expression_Drug
## 0    gene1                  64               48
## 1    gene2                  50               62
## 2    gene3                  74               73
## 3    gene4                  31               34
## 4    gene5                  63               66&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;expt2.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GeneName  Expression_Control  Expression_Drug
## 0    gene1                  55               60
## 1    gene2                  51               51
## 2    gene3                  77               74
## 3    gene4                  23               22
## 4    gene5                  72              101&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-fold-change&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.3&lt;/span&gt; Calculate Fold Change&lt;/h2&gt;
&lt;p&gt;Now we calculate the fold change for each gene by dividing the drug-treated expression by the control expression. To avoid divide by zero errors, we can set a minimum expression value. This will also ensure that we are only looking at expression changes between significant expression values. Since we want to do the same thing to both the experiment 1 and the experiment 2 data sets, it makes sense to write a single function to use for both. We will use the &lt;code&gt;apply()&lt;/code&gt; function from Pandas to calculate over every row in the DataFrame:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def get_fold_change(x, min_expression = 10):
  ctrl_val = x[&amp;quot;Expression_Control&amp;quot;]
  drug_val = x[&amp;quot;Expression_Drug&amp;quot;]
  if ctrl_val &amp;lt; min_expression:
    ctrl_val = min_expression
  if drug_val &amp;lt; min_expression:
    drug_val = min_expression  
  return drug_val/ctrl_val
expt1[&amp;#39;FoldChange&amp;#39;] = expt1.apply(get_fold_change, axis = 1)
expt2[&amp;#39;FoldChange&amp;#39;] = expt2.apply(get_fold_change, axis = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.4&lt;/span&gt; Compare Data&lt;/h2&gt;
&lt;p&gt;Now let’s find the genes that are upregulated and downregulated in each experiment. Due to the lack of replicates, we do not have any estimate for the variance of these genes, so we cannot use any hypothesis testing methods such as a t-test. Instead, we will have to make do with using a threshold on the fold change:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fold_change_threshold = 1.5
expt1_up   = expt1[expt1[&amp;#39;FoldChange&amp;#39;] &amp;gt;= fold_change_threshold][&amp;#39;GeneName&amp;#39;]
expt1_down = expt1[expt1[&amp;#39;FoldChange&amp;#39;] &amp;lt;= 1/fold_change_threshold][&amp;#39;GeneName&amp;#39;]
expt2_up   = expt2[expt2[&amp;#39;FoldChange&amp;#39;] &amp;gt;= fold_change_threshold][&amp;#39;GeneName&amp;#39;]
expt2_down = expt2[expt2[&amp;#39;FoldChange&amp;#39;] &amp;gt;= 1/fold_change_threshold][&amp;#39;GeneName&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s see which genes are changing:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Upregulated in Experiment 1:&amp;quot;,   expt1_up.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1: gene8, gene12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Downregulated in Experiment 1:&amp;quot;, expt1_down.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1: gene32, gene46&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Upregulated in Experiment 2:&amp;quot;,   expt2_up.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 2: gene8, gene18, gene50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Downregulated in Experiment 2:&amp;quot;, expt2_down.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 2: gene1, gene2, gene3, gene4, gene5, gene6, gene7, gene8, gene9, gene10, gene11, gene12, gene13, gene14, gene15, gene16, gene17, gene18, gene19, gene20, gene21, gene23, gene24, gene25, gene26, gene27, gene28, gene29, gene30, gene31, gene32, gene33, gene34, gene35, gene36, gene37, gene38, gene39, gene40, gene41, gene42, gene44, gene45, gene46, gene47, gene48, gene49, gene50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we now have the genes that change when each of the drugs is used. But now we want to compare the two drugs together. First, let’s see if there are any genes similarly affected by both drugs. We can do this using the &lt;code&gt;intersect1d()&lt;/code&gt; function from NumPy which gives the intersect of two arrays:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;common_up   = pd.Series(np.intersect1d(expt1_up, expt2_up))
common_down = pd.Series(np.intersect1d(expt1_down, expt2_down))
print(&amp;quot;Upregulated in Experiment 1 and Experiment 2:&amp;quot;,   common_up.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1 and Experiment 2: gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Downregulated in Experiment 1 and Experiment 2:&amp;quot;, common_down.str.cat(sep = &amp;quot;, &amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1 and Experiment 2: gene32, gene46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see that only one gene is similarly affected by both drugs (“gene8”). Now let’s plot a figure to see how the fold change differs between the two drugs:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;fc = pd.merge(expt1[[&amp;#39;GeneName&amp;#39;, &amp;#39;FoldChange&amp;#39;]], expt2[[&amp;#39;GeneName&amp;#39;, &amp;#39;FoldChange&amp;#39;]], on = &amp;quot;GeneName&amp;quot;, suffixes = [&amp;quot;_Experiment1&amp;quot;, &amp;quot;_Experiment2&amp;quot;])
plt.scatter(np.log2(fc[&amp;quot;FoldChange_Experiment1&amp;quot;]), 
            np.log2(fc[&amp;#39;FoldChange_Experiment2&amp;#39;]),
            marker = &amp;quot;o&amp;quot;, color = &amp;quot;red&amp;quot;)
plt.xlim(-2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (-2, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.ylim(-2,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (-2, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.title(&amp;quot;Experiment1 vs Experiment2&amp;quot;)
plt.xlabel(&amp;quot;log2(Experiment1 Fold Change)&amp;quot;)
plt.ylabel(&amp;quot;log2(Experiment2 Fold Change)&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/PythonTutorial/index_files/figure-html/foldchange-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the effect on the gene expression is actually quite different for the two drugs. We can also see this by looking at the correlation between the two experiments:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;scipy.stats.pearsonr(fc[&amp;quot;FoldChange_Experiment1&amp;quot;], fc[&amp;quot;FoldChange_Experiment2&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (0.0838161376824968, 0.5627922240659949)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can therefore conclude that the effect of the drug on gene expression is quite different between the two experiments.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning using TensorFlow Through the Keras API in RStudio</title>
      <link>/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensorflow&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-database&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-transformation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequential-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropout-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-initial-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compile-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-the-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Machine learning and artificial intelligence is a hot topic in the tech world, but the expression “machine learning” can describe anything from fitting a straight line through some data, to a machine able to think, learn and react to the world in highly sophisticated ways (e.g. self-driving cars if you want to be positive about AI, or SkyNet from Terminator if you want to be a naysayer). Whilst common machine learning techniques like support vector machines and k-Nearest Neighbour algorithms can be used to solve a huge number of problems, deep learning algorithms like neural networks are required to create these highly sophisticated models.&lt;/p&gt;
&lt;p&gt;In this blog, I will explore the use of some commonly used tools for generating neural networks within the R programming language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/h1&gt;
&lt;p&gt;TensorFlow is one of the most powerful tools for deep learning, and in particular is widely used for training neural networks to classify various aspects of images. It is a freely distributed open-source library in python (but mainly written in C++) originally created by Google, but has become the cornerstone of many deep learning models currently out there&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Tensor&lt;/em&gt; is a multi-dimensional array, and the TensorFlow libraries represent a highly efficient pipeline for the myriad linear algebra calculations required to generate new tensors through the layers of the network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;keras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://keras.io&#34;&gt;Keras API&lt;/a&gt; is a high-level user-friendly neural network API (application programming interface) designed for accessing deep neural networks. One of the benefits is that it is able to run on GPUs as well as CPUs, which have been shown to work better for training neural networks since they are able more efficient at running the huge number of simple calculations required for model training (for example convolutions of image data).&lt;/p&gt;
&lt;p&gt;Keras can be used an interface to TensorFlow for training deep multi-level networks for use in deep learning applications. Both are developed in python, but here I am going to use the RStudio interface to run a few simple deep learning models to trial the process ahead of a more in-depth application. R and python are somewhat at war in the data science community, with (in my opinion) R being better for more general data analysis and visualisation (for instance, whilst the python &lt;code&gt;seaborn&lt;/code&gt; package produces beautiful images, the &lt;code&gt;ggplot2&lt;/code&gt; package is far more elaborate). However, with the Keras and TensorFlow packages (and the generally higher memory impact of using R), python is typically far more suited for deep learning applications.&lt;/p&gt;
&lt;p&gt;However, the ability to access the Keras API through RStudio, and the amazing power of using RStudio to develop workflows, will make this a perfect “one stop shop” for data science needs. Much of this work is developed from the &lt;a href=&#34;https://tensorflow.rstudio.com&#34;&gt;RStudio Keras and TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the &lt;code&gt;reticulate&lt;/code&gt; package to pipe python commands through R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;reticulate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then install and load the &lt;code&gt;keras&lt;/code&gt; package. When we load it using the &lt;code&gt;install_keras()&lt;/code&gt; function, we can define different backend engines and choose to use GPUs rather than CPUs, but for this example I will simply use the default TensorFlow backend on my laptop CPU:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/keras&amp;quot;)
library(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mnist-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/h1&gt;
&lt;p&gt;So let’s have a little play by looking at a standard machine learning approach, looking at the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST&lt;/a&gt; dataset. This is the Modified National Institute of Standards and Technology database, and contains a large amount of images of handwritten digits that is used to train models for handwriting recognition. Ultimately, the same models can be used for a huge number of classification tasks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mnist_dat &amp;lt;- dataset_mnist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset contains a training set of 60,000 images, and a test set of 10,000 images. Each image is pre-normalised such that each digit is a grayscale image that fits into a 28x28 pixel bounding box.Each image is also supplied with a label that tells us what the digit should really be. This dataset is commonly used as a kind of benchmark for new models, with people vying to build the model with the lowest error rate possible:&lt;/p&gt;
&lt;p&gt;So let’s define our data sets. We will require two main data sets; a &lt;strong&gt;training&lt;/strong&gt; set where we show the model the images and tell it what it should recognise, and a &lt;strong&gt;test&lt;/strong&gt; dataset where we can predict the result and check against the ground level label. For each data set, we will create a dataset &lt;code&gt;x&lt;/code&gt; containing all of the images, and a dataset &lt;code&gt;y&lt;/code&gt; containing the labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;x&amp;quot;]]
training_dat_y_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;y&amp;quot;]]
test_dat_x_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;x&amp;quot;]]
test_dat_y_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;y&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the images is essentially a 2D array, with 28 rows and 28 columns, with each cell representing the greyscale value of the pixel. So the &lt;code&gt;_dat_x&lt;/code&gt; data sets are 3D arrays. So accessing specific elements from these arrays in R is similar to accessing rows and columns using the &lt;code&gt;[x,y]&lt;/code&gt; style axis, but we need to specify a third element &lt;code&gt;z&lt;/code&gt; for the specific array that we want to access – &lt;code&gt;[z,x,y]&lt;/code&gt;. So lets take a look at an exampl of the input data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in 1:18) { 
  plot_dat &amp;lt;- training_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = training_dat_y_raw[i],
        cex  = 4, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/index_files/figure-html/MNIST_plot_entries-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/h1&gt;
&lt;p&gt;We can easily reduce this 3D data by essentially taking each 28x28 matrix and collapsing the 784 values down into a 1D vector. Then we can make one big 2D matrix containing all of the data. Ordinarily, this could be done by reassigning the dimensions of the array, but by using the &lt;code&gt;array_reshape()&lt;/code&gt; function, the data is adjusted to meet the requirements for Keras:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- array_reshape(training_dat_x_raw, c(nrow(training_dat_x_raw), 784))
test_dat_x     &amp;lt;- array_reshape(test_dat_x_raw,     c(nrow(test_dat_x_raw), 784))
dim(training_dat_x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000   784&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The values in these arrays are greyscale values, representing 256 integer values between 0 (black) and 255 (white). It will be useful for downstream analyses to rescale these values to real values in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- training_dat_x/255
test_dat_x     &amp;lt;- test_dat_x/255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The R-specific way to deal with categorical data would be to encode the values in the &lt;code&gt;y&lt;/code&gt; datasets to a factor with 10 levels (“0”, “1”, “2”, etc). However, Keras requires the data to be in a slightly different format, so we use the &lt;code&gt;to_categorical()&lt;/code&gt; function instead. This will encode the value in a new matrix with 10 columns and &lt;code&gt;n&lt;/code&gt; rows, such that every row contains exactly one &lt;code&gt;1&lt;/code&gt; (representing the label) and nine &lt;code&gt;0s&lt;/code&gt;. This is known as an identity matrix. Keras uses a lot of linear algebra, and this encoding makes these calculations much simpler:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_y &amp;lt;- to_categorical(training_dat_y_raw, 10)
test_dat_y     &amp;lt;- to_categorical(test_dat_y_raw, 10)
dim(training_dat_y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000    10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequential-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/h1&gt;
&lt;p&gt;A standard deep learning neural network model can be thought of as a number of sequential layers, with each layer representing a different abstraction of the data. For instance, consider a model looking at facial recognition from image data. The first layer might represent edges of different aspects of the image. The next layer might be designed to pick out nose shape. The next might pick out hair. The next might determine the orientation of the face. etc. Then by adding more and more layers, we can develop models able to classify samples based on a wide range of different features.&lt;/p&gt;
&lt;p&gt;Of course, there is a danger in statistics of &lt;em&gt;over-fitting&lt;/em&gt; data, which is when we create a model so specific for the training data that it becomes practically worthless. By definition, adding more variables into a model will always improve the fit, but at the cost of its applicability to other data sets. In models such as linear models, we look for parsimony – a model should be as complicated as it needs to be &lt;em&gt;and no more complicated&lt;/em&gt;. The old phrase is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you hear hooves, think horse not zebra&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, deep learning sequential models such as these are robust to these problems, since model training can back-propagate, allowing us to incorporate far more levels than would be possible with other machine learning techniques.&lt;/p&gt;
&lt;p&gt;The general steps involved in using Keras for deep learning are to first build your model, compile it to configure the parameters that will be used to develop the “best” model, train it using your training data, then test it on an additional data set to see how it copes.&lt;/p&gt;
&lt;p&gt;So let’s build a simple sequential neural network model object using &lt;code&gt;keras_model_sequential()&lt;/code&gt;, then add a series of additional layers that we hope will accurately identify our different categories. Adding sequential layers uses similar syntax to the tidyverse libraries such as &lt;code&gt;dplyr&lt;/code&gt;, by using the pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;%
  layer_dense(units = 28, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dense-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/h1&gt;
&lt;p&gt;The first layer is a densely connected neural network layer, which takes a set of nD input tensors (in this case 1D input tensors), and generate a weights matrix by breaking the tensor into subsets and using this to learn the weights by doing some linear algebra (vector and matrix multiplication). The output from the &lt;code&gt;dense&lt;/code&gt; layer is then generated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[output = activation(dot(input, kernel) + bias)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the weights kernel is generated and multiplied (dot product) with the input. If requested, a bias is also calculated and added to account for any systematic bias identified in the data. An &lt;code&gt;activation&lt;/code&gt; function is then used to generate the final tensor to go on to the following layer (in this case we have specified this is a separate layer).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activation-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/h1&gt;
&lt;p&gt;An activation function can often be necessary to ensure the back-propogation and gradient descent algorithms work. By default, no activation is used. However, this is a linear identity function, which is very limited. A common activation function is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectified Linear Unit&lt;/a&gt; (&lt;code&gt;ReLU&lt;/code&gt;), which is linear for positive values, but zero for negative values. This is usually a good starting point as it is very simple and fast. Another option is the &lt;code&gt;[softmax](https://en.wikipedia.org/wiki/Softmax_function)&lt;/code&gt; function, which transforms each input logit (the pre-activated values) by taking the exponential and normalizing by the sum of exponentials over all inputs so that the sum is exactly 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma(y_i) = \frac{e^{y_i}}{\sum^{K}_{j=1}e^{y_j}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is commonly used for multinomial logistic regression, where a different softmax function is applied for each class with a different probability incorporated, since it is able to transform input numbers into probabilities. The use of exponentials ensures that there are no negative values, no matter how negative the input logit. So the &lt;code&gt;softmax&lt;/code&gt; function outputs a probability distribution for potential outcomes in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dropout-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/h1&gt;
&lt;p&gt;Finally, we specify a dropout layer, which essentially drops a proportion of the nodes in the neural network to prevent over-fitting. In this case we have connections in the network between all of the tensor subsets generated. However, many of them are more useful in the model than others, so here we deactivate the 40% least useful nodes. Of course, this will reduce the training performance, but will prevent the issue of over-fitting making the model more generalised and applicable to other data sets. Model fitting is all about tweaking parameters and layers to get the most effective model, and this is one way in which we can improve the effectiveness of the model at predicting unseen data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;define-initial-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/h1&gt;
&lt;p&gt;So let’s generate a more thorough model of 4 dense layers, gradually filtering down to a final output of 10 probabilities using the &lt;code&gt;softmax&lt;/code&gt; activation function – the probabilities for the 10 digits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model &amp;lt;- keras_model_sequential()
MNIST_model %&amp;gt;%
  layer_dense(units = 256, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4) %&amp;gt;%
  layer_dense(units = 128) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units = 56) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.2) %&amp;gt;%
  layer_dense(units = 10) %&amp;gt;%
  layer_activation(activation = &amp;quot;softmax&amp;quot;)
summary(MNIST_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential_1&amp;quot;
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 256)                   200960      
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 256)                   0           
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 256)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 128)                   32896       
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 128)                   0           
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 128)                   0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 56)                    7224        
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 56)                    0           
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 56)                    0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 10)                    570         
## ___________________________________________________________________________
## activation_4 (Activation)        (None, 10)                    0           
## ===========================================================================
## Total params: 241,650
## Trainable params: 241,650
## Non-trainable params: 0
## ___________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see the change in shape of the tensors throughout the model, and the number of trainable parameters at each layer level. These are fully connected layers, so every neuron (values in the tensors) is connected to every other neuron. So the number of parameters (or connections) is given by multiplying the number of values in the input layer by the number in the previous layer plus one. So in total we have nearly a quarter of a million parameters to estimate here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compile-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/h1&gt;
&lt;p&gt;So next we can compile this model to tell it which methods we want to use to estimate these parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model %&amp;gt;% 
  compile(
    loss = &amp;quot;categorical_crossentropy&amp;quot;,
    optimizer = optimizer_rmsprop(),
    metrics = c(&amp;quot;accuracy&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss function here is the method that will be used to optimise the parameters by comparing the predicted value with that of the actual value. Categorical crossentropy is commonly used in classification models when the output is a probability. It increases logarithmically as the predicted value diverges from the true value.&lt;/p&gt;
&lt;p&gt;The optimizer is used to ensure that the algorithm converges in training. We are trying to minimise the loss function, so &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt; can be used to optimse by iteratively recalculating the weights and bias until the minima is found. There is a danger of getting stuck at a local minima value, so sometimes it may be necessary to tune the parameters to avoid this. In this case, we are using RMSProp optimizer, which is similar to Gradient Descent but attempts to avoid this by adding oscillations to the descent.&lt;/p&gt;
&lt;p&gt;Finally, we can specify which metrics we wish to output inorder to evaluate the model during training. Here we look at the accuracy to determine how often our model gives the correct prediction in the trained data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/h1&gt;
&lt;p&gt;So now let’s train our model with our training data to estimate the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_output &amp;lt;- MNIST_model %&amp;gt;% 
  fit(
    training_dat_x, training_dat_y,
    batch_size = 128,
    epochs = 30,
    verbose = 1,
    validation_split = 0.2
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are going to run the model using our reformated training data above. The &lt;code&gt;epoch&lt;/code&gt; argument determines the number of iterations used to optimize the model parameters. In each epoch, we will use &lt;code&gt;batch_size&lt;/code&gt; samples per epoch for the gradient update.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;validation_split&lt;/code&gt; argument is used for running cross-validation in order to evaluate the quality of the model. A portion of the training data is kept aside, and is used to validate the current model parameters and calculate the accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/h1&gt;
&lt;p&gt;Let’s take a look at how the accuracy and the loss (caluclated as categorical cross-entropy) change as the model training progresses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(training_output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/index_files/figure-html/MNIST_training_plot-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the loss is high and the accuracy low at the start of the training, but they quickly improve within the first 10 epochs. After this, they begin to plateau, resulting in a loss of 0.07 and accuracy of 98.39%.&lt;/p&gt;
&lt;p&gt;This is pretty good, so let’s see how it works with the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_output &amp;lt;- MNIST_model %&amp;gt;% evaluate(test_dat_x, test_dat_y)
test_output&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $loss
## [1] 0.1082727
## 
## $acc
## [1] 0.9817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So 98.17% of the 10,000 test cases were predicted accurately. So this means that 183 were wrong. Let’s take a look at some of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted &amp;lt;- MNIST_model %&amp;gt;% predict_classes(test_dat_x)
which_wrong &amp;lt;- which(predicted != test_dat_y_raw)
par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in which_wrong[1:18]) {
  plot_dat &amp;lt;- test_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = paste(&amp;quot;Predict =&amp;quot;, predicted[i], &amp;quot;\nReal =&amp;quot;, test_dat_y_raw[i]),
        cex  = 2, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/index_files/figure-html/MNIST_wrong-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we can see why the algorithm struggled with some of these, such as predicting 6s as 0s, and numbers that are slanted or squished. However, this obviously shows a lack of generalisation in the model, which is not brilliant for dealing with hand written numbers.&lt;/p&gt;
&lt;p&gt;Obviously this is a fairly basic example of a neural network model, and the sorts of models being used in technology like self-driving cars contain far more layers than this. Model tuning is essential to compare and contrast models to identify the optimum model.&lt;/p&gt;
&lt;!-- # Model Comparison --&gt;
&lt;!-- Models can be compared in various different ways, but one example is to use the TensorBoard, a visualisation tool from TensorFlow that shows dynamic graphs of the Keras training and test metrics. To compare multiple models, we can record the data, and then visualise it on TensorBoard.  --&gt;
&lt;!-- So let&#39;s try to compare a 1-layer model with a 4-layer model. We use the ```callback_tensorboard()``` function to save the data to add to TensorBoard.  --&gt;
&lt;!-- First let&#39;s run a 1-layer model: --&gt;
&lt;!-- ``` {r model_comparison_1layer} --&gt;
&lt;!-- model1 &lt;- keras_model_sequential() --&gt;
&lt;!-- model1 %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model1&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So we can see that this is much worse than our first model, with only an 82.5% accuracy. Let&#39;s now try a 10-layer model: --&gt;
&lt;!-- ``` {r model_comparison_10layer} --&gt;
&lt;!-- model10 &lt;- keras_model_sequential() --&gt;
&lt;!-- model10 %&gt;% --&gt;
&lt;!--   layer_dense(units = 500, input_shape = c(784)) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 450) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 400) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 350) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 300) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 250) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 200) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 150) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 100) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.1) %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model10&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So this model is better than the 1-layer model, but I have essentially added layers with fairly random parameters. The result is a model that is more complicated, but less accurate than the initial model that we generated. But we can compare these directly using Tensorboard: --&gt;
&lt;!-- ``` {r tensorboard} --&gt;
&lt;!-- tensorboard(c(&#34;model1&#34;, &#34;model10&#34;)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This allows us to explore differences betwen multiple models, and can be used to interactively identify the optimal model for our needs. --&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/h1&gt;
&lt;p&gt;According to Wikipedia, one of the best results for the MNIST database used a hierarchical system of convolutional neural networks and managed to get an error rate of 0.23%. Here I have an error rate of 1.83%, so I clearly have a way to go! Often in classification algorithms, using standard machine learning algorithms will get you pretty far with pretty good error rates. However, to tune the models further to get error rates down to these sorts of levels, more complex models are required. Neural networks can be used to push the error rates down further. Getting the right answer 96% of the time is pretty good, but if you’re relying on that classification to tell whether there is a pedestrian stood in front of a self-driving car, it is incredibly important to ensure that this error rate is as close to 0 as possible.&lt;/p&gt;
&lt;p&gt;However, this has been a very useful attempt at incorparting the powerful interface of Keras and the workflow of TensorFlow in R. Being able to incorporate powerful deep learning networks in R is incredobly useful, and will allow for incorporation with pre-existing pipelines already developed for bioinformatics analyses utilising the powerful pacakges available from &lt;a href=&#34;https://bioconductor.org&#34;&gt;Bioconductor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deep learning algorithms currently have a huge number of applications, from self-driving cars to facial recognition, and are being incorporated into technology in many industries. Development of deep learning algorithms and Big Data processing approaches will provide significant technological advancements. I am currently working on some potentially interesting applications, and hope to further my expertise in this area by working more with the R Keras API interface.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] keras_2.2.5.0   reticulate_1.13
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.5  xfun_0.8          remotes_2.1.0    
##  [4] reshape2_1.4.3    purrr_0.3.3       lattice_0.20-38  
##  [7] colorspace_1.4-1  generics_0.0.2    testthat_2.1.1   
## [10] htmltools_0.3.6   usethis_1.5.1     yaml_2.2.0       
## [13] base64enc_0.1-3   rlang_0.4.0       pkgbuild_1.0.3   
## [16] pillar_1.4.2      glue_1.3.1        withr_2.1.2      
## [19] sessioninfo_1.1.1 plyr_1.8.4        tensorflow_2.0.0 
## [22] stringr_1.4.0     munsell_0.5.0     blogdown_0.16    
## [25] gtable_0.3.0      devtools_2.1.0    codetools_0.2-16 
## [28] memoise_1.1.0     evaluate_0.14     labeling_0.3     
## [31] knitr_1.23        callr_3.3.0       ps_1.3.0         
## [34] tfruns_1.4        curl_3.3          Rcpp_1.0.2       
## [37] backports_1.1.4   scales_1.0.0      desc_1.2.0       
## [40] pkgload_1.0.2     jsonlite_1.6      fs_1.3.1         
## [43] ggplot2_3.2.0     digest_0.6.20     stringi_1.4.3    
## [46] dplyr_0.8.3       bookdown_0.12     processx_3.4.1   
## [49] grid_3.6.1        rprojroot_1.3-2   cli_1.1.0        
## [52] tools_3.6.1       magrittr_1.5      tibble_2.1.3     
## [55] lazyeval_0.2.2    pkgconfig_2.0.2   crayon_1.3.4     
## [58] whisker_0.4       zeallot_0.1.0     Matrix_1.2-17    
## [61] prettyunits_1.0.2 assertthat_0.2.1  rmarkdown_1.14   
## [64] R6_2.4.0          compiler_3.6.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Pokémon Recomendation Machine</title>
      <link>/post/2019-07-15-building-a-pokemon-recommendation-machine/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-07-15-building-a-pokemon-recommendation-machine/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#download-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Download data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data Cleaning&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numeric-range&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Numeric Range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#change-variable-class&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Change Variable Class&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploratory-analyses&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Exploratory Analyses&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#attack&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Attack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defence&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Defence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensuring-we-use-accurate-data-classes-throughout&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Ensuring we use accurate data classes throughout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Normalization&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#min-max-normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; Min-Max Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#z-score-normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; Z-Score Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-normalization-method&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.3&lt;/span&gt; Choosing a Normalization Method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#looking-for-patterns&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Looking for Patterns&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-of-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Correlation of Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#height-vs-weight&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; Height vs Weight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-between-pokemon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Correlation between Pokémon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.4&lt;/span&gt; Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-based-recommendation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.5&lt;/span&gt; Correlation-Based Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-legendary-pokemon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Predicting Legendary Pokémon&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; Support-Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbour&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; k-Nearest Neighbour&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;When I saw the &lt;a href=&#34;https://www.kaggle.com/rounakbanik/pokemon&#34;&gt;Complete Pokémon Dataset on Kaggle&lt;/a&gt;, I just had to download it and have a look! When I was younger, I was a big fan of Pokémon and used to play it regularly and watch the TV show (to this day I can recite much of the original &lt;a href=&#34;https://www.youtube.com/watch?v=xMk8wuw7nek&#34;&gt;Pokémon Rap&lt;/a&gt;). More recently, my daughter has become a fan, and watches the show incessently (although it beats watching Peppa Pig…). So I am going to have a look over these data and see what they can show us about these pocket monsters.&lt;/p&gt;
&lt;p&gt;This is a fairly comprehensive analysis of the data, and will include introductions to a number of different data science techniques. I may further develop these into posts of their own in the future, so will only skim over most of them here. I hope that this post shows a fairly complete example of the types of analyses that it is possible to do with data such as these for prediction and recommendation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;download-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Download data&lt;/h1&gt;
&lt;p&gt;The data about each of the Pokémon can be downloaded directly from Kaggle &lt;a href=&#34;https://www.kaggle.com/rounakbanik/pokemon/download&#34;&gt;here&lt;/a&gt;, and I have also downloaded some images for each of the Pokémon (at least for Generations 1 to 6) from Kaggle &lt;a href=&#34;https://www.kaggle.com/kvpratama/pokemon-images-dataset/download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main data are in the form of a compressed comma separated values (CSV) file. After unzipping the file, we are left with a plain text file where every row is a separate entry, and the various columns of the data set are separated by commas. So let’s load the data in using the &lt;code&gt;read.csv&lt;/code&gt; function, which will generate a &lt;code&gt;data.frame&lt;/code&gt; object, and take a look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat &amp;lt;- read.csv(&amp;quot;pokemon.csv&amp;quot;)
rownames(pokedat) &amp;lt;- pokedat[[&amp;quot;name&amp;quot;]]
dim(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 801  41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have information for 41 variables for 801 unique Pokémon. Each Pokémon has a unique number assigned to it in the so called “Pokédex”, which is common between this data set and the list of Pokémon images. This makes it easy to link the two. Back in my day, there were only 151 Pokémon to keep track of, but many years later they have added more and more with each “Generation”. Pokémon Generation 8 is the most recent and has only recently been released, so we can see which generations are present in this data set by using the &lt;code&gt;table&lt;/code&gt; function, which will show us the number of entries with each value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(pokedat[[&amp;quot;generation&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   5   6   7 
## 151 100 135 107 156  72  80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see the 151 Generation 1 Pokémon, but also additional Pokémon from up to Generation 7. We can get a fairly broad overview of the data set by using the &lt;code&gt;str&lt;/code&gt; function to gain an overview of what is contained within each of the columns of this &lt;code&gt;data.frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    801 obs. of  41 variables:
##  $ abilities        : Factor w/ 482 levels &amp;quot;[&amp;#39;Adaptability&amp;#39;, &amp;#39;Download&amp;#39;, &amp;#39;Analytic&amp;#39;]&amp;quot;,..: 244 244 244 22 22 22 453 453 453 348 ...
##  $ against_bug      : num  1 1 1 0.5 0.5 0.25 1 1 1 1 ...
##  $ against_dark     : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_dragon   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_electric : num  0.5 0.5 0.5 1 1 2 2 2 2 1 ...
##  $ against_fairy    : num  0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 ...
##  $ against_fight    : num  0.5 0.5 0.5 1 1 0.5 1 1 1 0.5 ...
##  $ against_fire     : num  2 2 2 0.5 0.5 0.5 0.5 0.5 0.5 2 ...
##  $ against_flying   : num  2 2 2 1 1 1 1 1 1 2 ...
##  $ against_ghost    : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_grass    : num  0.25 0.25 0.25 0.5 0.5 0.25 2 2 2 0.5 ...
##  $ against_ground   : num  1 1 1 2 2 0 1 1 1 0.5 ...
##  $ against_ice      : num  2 2 2 0.5 0.5 1 0.5 0.5 0.5 1 ...
##  $ against_normal   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_poison   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_psychic  : num  2 2 2 1 1 1 1 1 1 1 ...
##  $ against_rock     : num  1 1 1 2 2 4 1 1 1 2 ...
##  $ against_steel    : num  1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 1 ...
##  $ against_water    : num  0.5 0.5 0.5 2 2 2 0.5 0.5 0.5 1 ...
##  $ attack           : int  49 62 100 52 64 104 48 63 103 30 ...
##  $ base_egg_steps   : int  5120 5120 5120 5120 5120 5120 5120 5120 5120 3840 ...
##  $ base_happiness   : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ base_total       : int  318 405 625 309 405 634 314 405 630 195 ...
##  $ capture_rate     : Factor w/ 34 levels &amp;quot;100&amp;quot;,&amp;quot;120&amp;quot;,&amp;quot;125&amp;quot;,..: 26 26 26 26 26 26 26 26 26 21 ...
##  $ classfication    : Factor w/ 588 levels &amp;quot;Abundance Pokémon&amp;quot;,..: 449 449 449 299 187 187 531 546 457 585 ...
##  $ defense          : int  49 63 123 43 58 78 65 80 120 35 ...
##  $ experience_growth: int  1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1000000 ...
##  $ height_m         : num  0.7 1 2 0.6 1.1 1.7 0.5 1 1.6 0.3 ...
##  $ hp               : int  45 60 80 39 58 78 44 59 79 45 ...
##  $ japanese_name    : Factor w/ 801 levels &amp;quot;Abagouraアバゴーラ&amp;quot;,..: 200 201 199 288 417 416 794 334 336 80 ...
##  $ name             : Factor w/ 801 levels &amp;quot;Abomasnow&amp;quot;,&amp;quot;Abra&amp;quot;,..: 73 321 745 95 96 93 656 764 56 88 ...
##  $ percentage_male  : num  88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 50 ...
##  $ pokedex_number   : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ sp_attack        : int  65 80 122 60 80 159 50 65 135 20 ...
##  $ sp_defense       : int  65 80 120 50 65 115 64 80 115 20 ...
##  $ speed            : int  45 60 80 65 80 100 43 58 78 45 ...
##  $ type1            : Factor w/ 18 levels &amp;quot;bug&amp;quot;,&amp;quot;dark&amp;quot;,&amp;quot;dragon&amp;quot;,..: 10 10 10 7 7 7 18 18 18 1 ...
##  $ type2            : Factor w/ 19 levels &amp;quot;&amp;quot;,&amp;quot;bug&amp;quot;,&amp;quot;dark&amp;quot;,..: 15 15 15 1 1 9 1 1 1 1 ...
##  $ weight_kg        : num  6.9 13 100 8.5 19 90.5 9 22.5 85.5 2.9 ...
##  $ generation       : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ is_legendary     : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we gave a lot of different information contained in this data set, with the vast majority being represented by numerical values. As well as the name by which we likely know them, we have the original Japanese name, as well as their fighting statistics such as &lt;code&gt;hp&lt;/code&gt; (Health Points), &lt;code&gt;speed&lt;/code&gt;, &lt;code&gt;attack&lt;/code&gt; and &lt;code&gt;defense&lt;/code&gt;. We also get their specific abilities, which are given in a listed format within square brackets (e.g. three abilities – &lt;code&gt;[&#39;Adaptability&#39;, &#39;Download&#39;, &#39;Analytic&#39;]&lt;/code&gt; – for &lt;code&gt;Abomasnow&lt;/code&gt;). There are also various other things that we will explore now in the following sections. So let’s explore these data to see how they look, and to check for any inconsistencies that need to be corrected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Cleaning&lt;/h1&gt;
&lt;p&gt;The first step in any data analysis is to check the consistency of the data to ensure that there are no missing values (and if there are, decide the best thing to do with them), to make sure that the data are consistent and fit within expected bounds, and to generally make sure that these data make sense. These are data that somebody else has generated, so it is best not to assume that they are perfect. If there are any issues, this will propogate to our downstream analyses.&lt;/p&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Missing Data&lt;/h2&gt;
&lt;p&gt;First of all, let’s take a look to see which of the entries for each of the columns is missing (i.e. is coded as &lt;code&gt;NA&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;has_na &amp;lt;- apply(pokedat, MAR = 2, FUN = function(x) which(is.na(x))) 
has_na[sapply(has_na, length) &amp;gt; 0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $height_m
##   Rattata  Raticate    Raichu Sandshrew Sandslash    Vulpix Ninetales 
##        19        20        26        27        28        37        38 
##   Diglett   Dugtrio    Meowth   Persian   Geodude  Graveler     Golem 
##        50        51        52        53        74        75        76 
##    Grimer       Muk Exeggutor   Marowak     Hoopa  Lycanroc 
##        88        89       103       105       720       745 
## 
## $percentage_male
##  Magnemite   Magneton    Voltorb  Electrode     Staryu    Starmie 
##         81         82        100        101        120        121 
##      Ditto    Porygon   Articuno     Zapdos    Moltres     Mewtwo 
##        132        137        144        145        146        150 
##        Mew      Unown   Porygon2     Raikou      Entei    Suicune 
##        151        201        233        243        244        245 
##      Lugia      Ho-Oh     Celebi   Shedinja   Lunatone    Solrock 
##        249        250        251        292        337        338 
##     Baltoy    Claydol     Beldum     Metang  Metagross   Regirock 
##        343        344        374        375        376        377 
##     Regice  Registeel     Kyogre    Groudon   Rayquaza    Jirachi 
##        378        379        382        383        384        385 
##     Deoxys    Bronzor   Bronzong  Magnezone  Porygon-Z      Rotom 
##        386        436        437        462        474        479 
##       Uxie    Mesprit      Azelf     Dialga     Palkia  Regigigas 
##        480        481        482        483        484        486 
##   Giratina     Phione    Manaphy    Darkrai    Shaymin     Arceus 
##        487        489        490        491        492        493 
##    Victini      Klink      Klang  Klinklang  Cryogonal     Golett 
##        494        599        600        601        615        622 
##     Golurk   Cobalion  Terrakion   Virizion   Reshiram     Zekrom 
##        623        638        639        640        643        644 
##     Kyurem     Keldeo   Meloetta   Genesect    Carbink    Xerneas 
##        646        647        648        649        703        716 
##    Yveltal    Zygarde    Diancie      Hoopa  Volcanion Type: Null 
##        717        718        719        720        721        772 
##   Silvally     Minior   Dhelmise  Tapu Koko  Tapu Lele  Tapu Bulu 
##        773        774        781        785        786        787 
##  Tapu Fini     Cosmog    Cosmoem   Solgaleo     Lunala   Nihilego 
##        788        789        790        791        792        793 
##   Buzzwole  Pheromosa  Xurkitree Celesteela    Kartana   Guzzlord 
##        794        795        796        797        798        799 
##   Necrozma   Magearna 
##        800        801 
## 
## $weight_kg
##   Rattata  Raticate    Raichu Sandshrew Sandslash    Vulpix Ninetales 
##        19        20        26        27        28        37        38 
##   Diglett   Dugtrio    Meowth   Persian   Geodude  Graveler     Golem 
##        50        51        52        53        74        75        76 
##    Grimer       Muk Exeggutor   Marowak     Hoopa  Lycanroc 
##        88        89       103       105       720       745&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, this seems to be a fairly complete data set, with only three of the variables showing any NA data. We can see that there are 20 Pokémon with no height nor weight data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(pokedat, is.na(height_m) | is.na(weight_kg))[, c(&amp;quot;name&amp;quot;, &amp;quot;height_m&amp;quot;, &amp;quot;weight_kg&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                name height_m weight_kg
## Rattata     Rattata       NA        NA
## Raticate   Raticate       NA        NA
## Raichu       Raichu       NA        NA
## Sandshrew Sandshrew       NA        NA
## Sandslash Sandslash       NA        NA
## Vulpix       Vulpix       NA        NA
## Ninetales Ninetales       NA        NA
## Diglett     Diglett       NA        NA
## Dugtrio     Dugtrio       NA        NA
## Meowth       Meowth       NA        NA
## Persian     Persian       NA        NA
## Geodude     Geodude       NA        NA
## Graveler   Graveler       NA        NA
## Golem         Golem       NA        NA
## Grimer       Grimer       NA        NA
## Muk             Muk       NA        NA
## Exeggutor Exeggutor       NA        NA
## Marowak     Marowak       NA        NA
## Hoopa         Hoopa       NA        NA
## Lycanroc   Lycanroc       NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many of these are Pokémon that I know from Generation 1, and in fact seem to be sets of evolutions. For instance, Rattata evolves into Raticate:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/19.png&#34; alt=&#34;Rattata&#34; /&gt;
&lt;img src=&#34;pokemon/20.png&#34; alt=&#34;Raticate&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sandshrew evolves into Sandslash:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/27.png&#34; alt=&#34;Sandshrew&#34; /&gt;
&lt;img src=&#34;pokemon/28.png&#34; alt=&#34;Sandslash&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And Vulpix evolves into Ninetales:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/37.png&#34; alt=&#34;Vulpix&#34; /&gt;
&lt;img src=&#34;pokemon/38.png&#34; alt=&#34;Ninetales&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are also a couple of other none-Generation 1 Pokémon, including Lycanroc which is one of my daughter’s favourited from Pokémon Sun and Moon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/lycanroc.png&#34; alt=&#34;Lycanroc&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Lycanroc&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;However, there is no obvious reason why values are missing. There are methods that can be used to account for missing data. One possible approach is to &lt;em&gt;impute&lt;/em&gt; the data – that is, we use the rest of the data to give us a rough idea of what we should see for these missing values. An example of this is to simply use the mean of the non-missing values for the missing variable. However, in this case, we can actually find these missing values by visiting an online &lt;a href=&#34;https://pokemondb.net/pokedex&#34;&gt;Pokedex&lt;/a&gt;, so let’s correct these, ensuring that we match the units for weight (kg) and height (m):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missing_height &amp;lt;- list(Rattata   = c(height_m = 0.3, weight_kg =   3.5), 
                       Raticate  = c(height_m = 0.7, weight_kg =  18.5),
                       Raichu    = c(height_m = 0.8, weight_kg =  30.0),
                       Sandshrew = c(height_m = 0.6, weight_kg =  12.0),
                       Sandslash = c(height_m = 1.0, weight_kg =  29.5),
                       Vulpix    = c(height_m = 0.6, weight_kg =   9.9),
                       Ninetales = c(height_m = 1.1, weight_kg =  19.9),
                       Diglett   = c(height_m = 0.2, weight_kg =   0.8),
                       Dugtrio   = c(height_m = 0.7, weight_kg =  33.3),
                       Meowth    = c(height_m = 0.4, weight_kg =   4.2),
                       Persian   = c(height_m = 1.0, weight_kg =  32.0),
                       Geodude   = c(height_m = 0.4, weight_kg =  20.0),
                       Graveler  = c(height_m = 0.3, weight_kg = 105.0),
                       Golem     = c(height_m = 1.4, weight_kg = 300.0),
                       Grimer    = c(height_m = 0.9, weight_kg =  30.0),
                       Muk       = c(height_m = 1.2, weight_kg =  30.0),
                       Exeggutor = c(height_m = 2.0, weight_kg = 120.0),
                       Marowak   = c(height_m = 1.0, weight_kg =  45.0),
                       Hoopa     = c(height_m = 0.5, weight_kg =   9.0),
                       Lycanroc  = c(height_m = 0.8, weight_kg =  25.0))
missing_height &amp;lt;- t(rbind.data.frame(missing_height))
pokedat[match(rownames(missing_height), pokedat[[&amp;quot;name&amp;quot;]]), c(&amp;quot;height_m&amp;quot;, &amp;quot;weight_kg&amp;quot;)] &amp;lt;- missing_height&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also 98 Pokémon with a missing &lt;code&gt;percentage_male&lt;/code&gt; value. This value gives the proportion of the Pokémon out in the world that you might come across in the game that are male as a percentage. These seem to be spread throughout the entire list of Pokémon across all Generations, with no clear reason as to why they have missing values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(subset(pokedat, is.na(percentage_male)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            abilities against_bug
## Magnemite      [&amp;#39;Magnet Pull&amp;#39;, &amp;#39;Sturdy&amp;#39;, &amp;#39;Analytic&amp;#39;]         0.5
## Magneton       [&amp;#39;Magnet Pull&amp;#39;, &amp;#39;Sturdy&amp;#39;, &amp;#39;Analytic&amp;#39;]         0.5
## Voltorb        [&amp;#39;Soundproof&amp;#39;, &amp;#39;Static&amp;#39;, &amp;#39;Aftermath&amp;#39;]         1.0
## Electrode      [&amp;#39;Soundproof&amp;#39;, &amp;#39;Static&amp;#39;, &amp;#39;Aftermath&amp;#39;]         1.0
## Staryu    [&amp;#39;Illuminate&amp;#39;, &amp;#39;Natural Cure&amp;#39;, &amp;#39;Analytic&amp;#39;]         1.0
## Starmie   [&amp;#39;Illuminate&amp;#39;, &amp;#39;Natural Cure&amp;#39;, &amp;#39;Analytic&amp;#39;]         2.0
##           against_dark against_dragon against_electric against_fairy
## Magnemite            1            0.5              0.5           0.5
## Magneton             1            0.5              0.5           0.5
## Voltorb              1            1.0              0.5           1.0
## Electrode            1            1.0              0.5           1.0
## Staryu               1            1.0              2.0           1.0
## Starmie              2            1.0              2.0           1.0
##           against_fight against_fire against_flying against_ghost
## Magnemite           2.0          2.0           0.25             1
## Magneton            2.0          2.0           0.25             1
## Voltorb             1.0          1.0           0.50             1
## Electrode           1.0          1.0           0.50             1
## Staryu              1.0          0.5           1.00             1
## Starmie             0.5          0.5           1.00             2
##           against_grass against_ground against_ice against_normal
## Magnemite           0.5              4         0.5            0.5
## Magneton            0.5              4         0.5            0.5
## Voltorb             1.0              2         1.0            1.0
## Electrode           1.0              2         1.0            1.0
## Staryu              2.0              1         0.5            1.0
## Starmie             2.0              1         0.5            1.0
##           against_poison against_psychic against_rock against_steel
## Magnemite              0             0.5          0.5          0.25
## Magneton               0             0.5          0.5          0.25
## Voltorb                1             1.0          1.0          0.50
## Electrode              1             1.0          1.0          0.50
## Staryu                 1             1.0          1.0          0.50
## Starmie                1             0.5          1.0          0.50
##           against_water attack base_egg_steps base_happiness base_total
## Magnemite           1.0     35           5120             70        325
## Magneton            1.0     60           5120             70        465
## Voltorb             1.0     30           5120             70        330
## Electrode           1.0     50           5120             70        490
## Staryu              0.5     45           5120             70        340
## Starmie             0.5     75           5120             70        520
##           capture_rate      classfication defense experience_growth
## Magnemite          190     Magnet Pokémon      70           1000000
## Magneton            60     Magnet Pokémon      95           1000000
## Voltorb            190       Ball Pokémon      50           1000000
## Electrode           60       Ball Pokémon      70           1000000
## Staryu             225  Starshape Pokémon      55           1250000
## Starmie             60 Mysterious Pokémon      85           1250000
##           height_m hp        japanese_name      name percentage_male
## Magnemite      0.3 25           Coilコイル Magnemite              NA
## Magneton       1.0 50   Rarecoilレアコイル  Magneton              NA
## Voltorb        0.5 40 Biriridamaビリリダマ   Voltorb              NA
## Electrode      1.2 60   Marumineマルマイン Electrode              NA
## Staryu         0.8 30  Hitodemanヒトデマン    Staryu              NA
## Starmie        1.1 60    Starmieスターミー   Starmie              NA
##           pokedex_number sp_attack sp_defense speed    type1   type2
## Magnemite             81        95         55    45 electric   steel
## Magneton              82       120         70    70 electric   steel
## Voltorb              100        55         55   100 electric        
## Electrode            101        80         80   150 electric        
## Staryu               120        70         55    85    water        
## Starmie              121       100         85   115    water psychic
##           weight_kg generation is_legendary
## Magnemite       6.0          1            0
## Magneton       60.0          1            0
## Voltorb        10.4          1            0
## Electrode      66.6          1            0
## Staryu         34.5          1            0
## Starmie        80.0          1            0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, by looking at a few of these in the Pokedex, it would appear that these are generally genderless Pokémon, which would explain the missing values. A sensible value to use in these cases would therefore be 0.5, representing an equal spit of male and female:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[is.na(pokedat[[&amp;quot;percentage_male&amp;quot;]]), &amp;quot;percentage_male&amp;quot;] &amp;lt;- 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also worth noting that there are also missing values for the type2 Pokémon. These were not picked up as NA values, because they are encoded as a blank entry &amp;quot;“. We can convert this to a more descriptive factor such as”none&amp;quot;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;type2&amp;quot;]])[levels(pokedat[[&amp;quot;type2&amp;quot;]]) == &amp;quot;none&amp;quot;] &amp;lt;- &amp;quot;none&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;numeric-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Numeric Range&lt;/h2&gt;
&lt;p&gt;The vast majority of these variables are numeric in nature, so it is worth checking the range of these values to ensure that they are within typical ranges that we might expect. For instance, we would not expect negative values, zero values, or values greater than some sensible limit for things like height, weight, etc. So let’s take an overall look at these data ranges by using the &lt;code&gt;summary()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                      abilities    against_bug    
##  [&amp;#39;Levitate&amp;#39;]                             : 29   Min.   :0.2500  
##  [&amp;#39;Beast Boost&amp;#39;]                          :  7   1st Qu.:0.5000  
##  [&amp;#39;Shed Skin&amp;#39;]                            :  5   Median :1.0000  
##  [&amp;#39;Clear Body&amp;#39;, &amp;#39;Light Metal&amp;#39;]            :  4   Mean   :0.9963  
##  [&amp;#39;Justified&amp;#39;]                            :  4   3rd Qu.:1.0000  
##  [&amp;#39;Keen Eye&amp;#39;, &amp;#39;Tangled Feet&amp;#39;, &amp;#39;Big Pecks&amp;#39;]:  4   Max.   :4.0000  
##  (Other)                                  :748                   
##   against_dark   against_dragon   against_electric against_fairy  
##  Min.   :0.250   Min.   :0.0000   Min.   :0.000    Min.   :0.250  
##  1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.500    1st Qu.:1.000  
##  Median :1.000   Median :1.0000   Median :1.000    Median :1.000  
##  Mean   :1.057   Mean   :0.9688   Mean   :1.074    Mean   :1.069  
##  3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.000    3rd Qu.:1.000  
##  Max.   :4.000   Max.   :2.0000   Max.   :4.000    Max.   :4.000  
##                                                                   
##  against_fight    against_fire   against_flying  against_ghost  
##  Min.   :0.000   Min.   :0.250   Min.   :0.250   Min.   :0.000  
##  1st Qu.:0.500   1st Qu.:0.500   1st Qu.:1.000   1st Qu.:1.000  
##  Median :1.000   Median :1.000   Median :1.000   Median :1.000  
##  Mean   :1.066   Mean   :1.135   Mean   :1.193   Mean   :0.985  
##  3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:1.000   3rd Qu.:1.000  
##  Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  
##                                                                 
##  against_grass   against_ground   against_ice    against_normal 
##  Min.   :0.250   Min.   :0.000   Min.   :0.250   Min.   :0.000  
##  1st Qu.:0.500   1st Qu.:1.000   1st Qu.:0.500   1st Qu.:1.000  
##  Median :1.000   Median :1.000   Median :1.000   Median :1.000  
##  Mean   :1.034   Mean   :1.098   Mean   :1.208   Mean   :0.887  
##  3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:1.000  
##  Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :1.000  
##                                                                 
##  against_poison   against_psychic  against_rock  against_steel   
##  Min.   :0.0000   Min.   :0.000   Min.   :0.25   Min.   :0.2500  
##  1st Qu.:0.5000   1st Qu.:1.000   1st Qu.:1.00   1st Qu.:0.5000  
##  Median :1.0000   Median :1.000   Median :1.00   Median :1.0000  
##  Mean   :0.9753   Mean   :1.005   Mean   :1.25   Mean   :0.9835  
##  3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:2.00   3rd Qu.:1.0000  
##  Max.   :4.0000   Max.   :4.000   Max.   :4.00   Max.   :4.0000  
##                                                                  
##  against_water       attack       base_egg_steps  base_happiness  
##  Min.   :0.250   Min.   :  5.00   Min.   : 1280   Min.   :  0.00  
##  1st Qu.:0.500   1st Qu.: 55.00   1st Qu.: 5120   1st Qu.: 70.00  
##  Median :1.000   Median : 75.00   Median : 5120   Median : 70.00  
##  Mean   :1.058   Mean   : 77.86   Mean   : 7191   Mean   : 65.36  
##  3rd Qu.:1.000   3rd Qu.:100.00   3rd Qu.: 6400   3rd Qu.: 70.00  
##  Max.   :4.000   Max.   :185.00   Max.   :30720   Max.   :140.00  
##                                                                   
##    base_total     capture_rate          classfication    defense      
##  Min.   :180.0   45     :250   Dragon Pokémon  :  8   Min.   :  5.00  
##  1st Qu.:320.0   190    : 75   Mouse Pokémon   :  6   1st Qu.: 50.00  
##  Median :435.0   255    : 69   Mushroom Pokémon:  6   Median : 70.00  
##  Mean   :428.4   75     : 61   Balloon Pokémon :  5   Mean   : 73.01  
##  3rd Qu.:505.0   3      : 58   Fairy Pokémon   :  5   3rd Qu.: 90.00  
##  Max.   :780.0   120    : 55   Flame Pokémon   :  5   Max.   :230.00  
##                  (Other):233   (Other)         :766                   
##  experience_growth    height_m            hp        
##  Min.   : 600000   Min.   : 0.100   Min.   :  1.00  
##  1st Qu.:1000000   1st Qu.: 0.600   1st Qu.: 50.00  
##  Median :1000000   Median : 1.000   Median : 65.00  
##  Mean   :1054996   Mean   : 1.155   Mean   : 68.96  
##  3rd Qu.:1059860   3rd Qu.: 1.500   3rd Qu.: 80.00  
##  Max.   :1640000   Max.   :14.500   Max.   :255.00  
##                                                     
##              japanese_name         name     percentage_male 
##  Abagouraアバゴーラ :  1   Abomasnow :  1   Min.   :  0.00  
##  Absolアブソル      :  1   Abra      :  1   1st Qu.: 50.00  
##  Abulyアブリー      :  1   Absol     :  1   Median : 50.00  
##  Aburibbonアブリボン:  1   Accelgor  :  1   Mean   : 48.47  
##  Achamoアチャモ     :  1   Aegislash :  1   3rd Qu.: 50.00  
##  Agehuntアゲハント  :  1   Aerodactyl:  1   Max.   :100.00  
##  (Other)            :795   (Other)   :795                   
##  pokedex_number   sp_attack        sp_defense         speed       
##  Min.   :  1    Min.   : 10.00   Min.   : 20.00   Min.   :  5.00  
##  1st Qu.:201    1st Qu.: 45.00   1st Qu.: 50.00   1st Qu.: 45.00  
##  Median :401    Median : 65.00   Median : 66.00   Median : 65.00  
##  Mean   :401    Mean   : 71.31   Mean   : 70.91   Mean   : 66.33  
##  3rd Qu.:601    3rd Qu.: 91.00   3rd Qu.: 90.00   3rd Qu.: 85.00  
##  Max.   :801    Max.   :194.00   Max.   :230.00   Max.   :180.00  
##                                                                   
##      type1         type2       weight_kg        generation  
##  water  :114          :384   Min.   :  0.10   Min.   :1.00  
##  normal :105   flying : 95   1st Qu.:  9.00   1st Qu.:2.00  
##  grass  : 78   ground : 34   Median : 27.30   Median :4.00  
##  bug    : 72   poison : 34   Mean   : 60.94   Mean   :3.69  
##  psychic: 53   fairy  : 29   3rd Qu.: 63.00   3rd Qu.:5.00  
##  fire   : 52   psychic: 29   Max.   :999.90   Max.   :7.00  
##  (Other):327   (Other):196                                  
##   is_legendary    
##  Min.   :0.00000  
##  1st Qu.:0.00000  
##  Median :0.00000  
##  Mean   :0.08739  
##  3rd Qu.:0.00000  
##  Max.   :1.00000  
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can learn a few things from this. For instance, if we look at the character type variables, “Levitate” is the most common ability (although note that this is only counting the cases where there is only a single ability – more on this later), “Dragon Pokémon” are the most common classfication [sic] (although the majority of the Pokémon are given a unique classification, which seems to go against the idea of classification), and “water” and “normal” type Pokémon are very common, and many Pokémon have flying as their secondary type. If we look at the numeric type variables, we can see that the &lt;code&gt;against_&lt;/code&gt; values appear to be numeric values between 0 and 4 in multiples of 0.25 (they represent the Pokémon’s strength against particular Pokémon types during battle), attack, defense and speed vary between 5 and 185, 230 and 180 respectively (so there is quite a big range depending on which Pokémon you have in a fight), there is at least one Pokémon that starts with a base happiness score of 0 (in fact there are 36, which is quite sad!), the percentage of males varies between 0 and 100 with 98 missing values (as expected). But overall there do not appear to be any strange outliers in these data. There are some pretty big Pokémon, but we will explore this in a little more detail later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;change-variable-class&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Change Variable Class&lt;/h2&gt;
&lt;p&gt;The m ajority of these data have been classified correctly by the &lt;code&gt;read.data()&lt;/code&gt; function as either numeric or characters (which are factorised automaticallyfor use in model and potting functions later). However, there are some changes that we may wish to make. Firstly, the &lt;code&gt;generation&lt;/code&gt; and &lt;code&gt;is_legendary&lt;/code&gt; variables should not be considered numeric, even though they are represented by numbers. For instance, a Pokémon could not come from Generation 2.345. So let’s make these changes from numeric to factorised character variables. Note that there is a natural order to the &lt;code&gt;generation&lt;/code&gt; values, so I will ensure that these are factorised as ordinal, just in case this comes into play later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[[&amp;quot;generation&amp;quot;]] &amp;lt;- factor(as.character(pokedat[[&amp;quot;generation&amp;quot;]]), order = TRUE)
pokedat[[&amp;quot;is_legendary&amp;quot;]] &amp;lt;- factor(as.character(pokedat[[&amp;quot;is_legendary&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These variabbles have been classified as factors even though they are represented by numeric values. The values representing the factor levels can actually be anything we choose, so it may be useful to change these to something more descriptive. For instance, it may be worth adding the word “Generation” to the &lt;code&gt;generation&lt;/code&gt; variable, whilst the &lt;code&gt;is_legendary&lt;/code&gt; variable may be more useful as a binary TRUE/FALSE value. It doesn’t matter what we put, since for later analyses dummy numeric variables will be used for calculations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;generation&amp;quot;]]) &amp;lt;- paste(&amp;quot;Generation&amp;quot;, levels(pokedat[[&amp;quot;generation&amp;quot;]]))
levels(pokedat[[&amp;quot;is_legendary&amp;quot;]]) &amp;lt;- c(FALSE, TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The defaults for the other factors seem to be suitable, so I will leave thse as they are.&lt;/p&gt;
&lt;p&gt;In comparison, the &lt;code&gt;capture_rate&lt;/code&gt; value is being classed as a factor, even though it should be a number. Why is this?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;capture_rate&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;100&amp;quot;                      &amp;quot;120&amp;quot;                     
##  [3] &amp;quot;125&amp;quot;                      &amp;quot;127&amp;quot;                     
##  [5] &amp;quot;130&amp;quot;                      &amp;quot;140&amp;quot;                     
##  [7] &amp;quot;145&amp;quot;                      &amp;quot;15&amp;quot;                      
##  [9] &amp;quot;150&amp;quot;                      &amp;quot;155&amp;quot;                     
## [11] &amp;quot;160&amp;quot;                      &amp;quot;170&amp;quot;                     
## [13] &amp;quot;180&amp;quot;                      &amp;quot;190&amp;quot;                     
## [15] &amp;quot;200&amp;quot;                      &amp;quot;205&amp;quot;                     
## [17] &amp;quot;220&amp;quot;                      &amp;quot;225&amp;quot;                     
## [19] &amp;quot;235&amp;quot;                      &amp;quot;25&amp;quot;                      
## [21] &amp;quot;255&amp;quot;                      &amp;quot;3&amp;quot;                       
## [23] &amp;quot;30&amp;quot;                       &amp;quot;30 (Meteorite)255 (Core)&amp;quot;
## [25] &amp;quot;35&amp;quot;                       &amp;quot;45&amp;quot;                      
## [27] &amp;quot;50&amp;quot;                       &amp;quot;55&amp;quot;                      
## [29] &amp;quot;60&amp;quot;                       &amp;quot;65&amp;quot;                      
## [31] &amp;quot;70&amp;quot;                       &amp;quot;75&amp;quot;                      
## [33] &amp;quot;80&amp;quot;                       &amp;quot;90&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aha. So there is a non-numeric value in there – &lt;code&gt;30 (Meteorite)255 (Core)&lt;/code&gt;. Let’s see which Pokémon this applies to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.character(subset(pokedat, capture_rate == &amp;quot;30 (Meteorite)255 (Core)&amp;quot;)[[&amp;quot;name&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Minior&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a “Meteor” type Pokémon that appears to have a different capture rate under different conditions. Baased on the online &lt;a href=&#34;https://pokemondb.net/pokedex&#34;&gt;Pokedex&lt;/a&gt;, the canonical value to use here is 30 for its Meteorite form, so let’s use this and convert it into a numeric value. One thing to bear in mind here is that factors can be a little funny. When I convert to a number, it will convert the level values to a number, not necessarily the values themselves. So in this case, it will not give me the values but instead will give me the order of the values. For instance. look at the output of the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- factor(c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;20&amp;quot;))
data.frame(factor = t, level = levels(t), number = as.numeric(t))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   factor level number
## 1      1     1      1
## 2      2     2      2
## 3      3    20      4
## 4     20     3      3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the levels are based on a character sort rather than a numeric sort, which puts “20” ahead of “4”. Then, to add to this further, the factor “20” is actually the 3&lt;sup&gt;rd&lt;/sup&gt; factor so gets the value 3, whilst the factor “3” is the 4&lt;sup&gt;th&lt;/sup&gt; level, and so gets a value 4. So the output is definitely not what we would expect when converting this to a number. To avoid this, we need to convert to a character value before we do anything else:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[[&amp;quot;capture_rate&amp;quot;]] &amp;lt;- as.character(pokedat[[&amp;quot;capture_rate&amp;quot;]])
pokedat[pokedat[[&amp;quot;name&amp;quot;]] == &amp;quot;Minior&amp;quot;, &amp;quot;capture_rate&amp;quot;] = &amp;quot;30&amp;quot;
pokedat[[&amp;quot;capture_rate&amp;quot;]] &amp;lt;- as.numeric(pokedat[[&amp;quot;capture_rate&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should now be a pretty clean data set ready for analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Exploratory Analyses&lt;/h1&gt;
&lt;p&gt;There are a lot of data here, and I could spend ages exploring every facet, but I just really wanrt to get a tester for these data here. First of all, let’s take a look at the distribution of some of the main statistics to see how they look across the dataset. For the majority of these plots, I will be using the &lt;code&gt;ggplot2&lt;/code&gt; library which offers a very nice way to produce publication-quality figures using a standardised lexicon, along with the &lt;code&gt;dplyr&lt;/code&gt; package which provides a simple way to rearrange data into suitable formats for producing plots. These are part of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;Tidyverse&lt;/a&gt; suite of packages from Hadley Wickham, and form a very useful suite of packages with a common grammar that can be used together to make Data Science more efficient and to produce beautiful plots easily. Handy &lt;em&gt;cheat sheets&lt;/em&gt; can be found for &lt;a href=&#34;https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;dplyr&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&#34;&gt;ggplot2&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
library(&amp;quot;ggrepel&amp;quot;)
library(&amp;quot;RColorBrewer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;attack&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Attack&lt;/h2&gt;
&lt;p&gt;First of all, let’s take a look at the attack strength of all Pokémon split by their main type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(x = attack, fill = MainType)) + 
  geom_density(alpha = 0.2) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/attack_dist-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general, there is a wide range of attack strengths and they are largely distributed with a roughly normal distribution around the mean attack strength of 77.86 that we saw earlier. However, we do see that there are some differences in the different Pokémon types available. However, this probably isn’t the easiest way to see differences between the groups. Instead, let’s use a boxplot. Here, we can see the overall distribution, with the 25&lt;sup&gt;th&lt;/sup&gt; percentile, the median (50&lt;sup&gt;th&lt;/sup&gt; percentile), and the 75&lt;sup&gt;th&lt;/sup&gt; percentile making up the range of the box, and outliers (I believe those with values in the bottom 2.5&lt;sup&gt;th&lt;/sup&gt; percentile or upper 97.5&lt;sup&gt;th&lt;/sup&gt; percentile) highlighted as points on the plot. I have added notches to the boxplots, which show the confidence interval represeneted by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[median \pm \frac{1.58*IQR}{\sqrt{n}}\]&lt;/span&gt;
Where IQR is the interquartile range. Essentially, if the notch values do not overlap between two boxes, it suggests that there may be a statistically significant difference between them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(y = attack, x = MainType, fill = MainType)) + 
  geom_boxplot(notch = TRUE) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text.x  = element_text(size = 18, angle = 90, hjust = 1),
    axis.text.y  = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/attack_box-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, we can see that ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ type Pokémon have higher attack strength, whilst ‘fairy’ and ‘psychic’ type seem to have slightly lower attack strength. This would make sense based on the names, and we can test this a little more formally by using an ANOVA (analysis of variance) analysis to look for significant effects of the Pokémon type on the attack strength. This can be done quite simply by using linear models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_vs_type1 &amp;lt;- lm(attack ~ type1, data = pokedat)
summary(attack_vs_type1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = attack ~ type1, data = pokedat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -70.16 -22.31  -3.50  19.26 114.88 
## 
## Coefficients:
##               Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)    70.1250     3.6263  19.338 &amp;lt; 0.0000000000000002 ***
## type1dark      17.6681     6.7675   2.611             0.009208 ** 
## type1dragon    36.2824     6.9439   5.225          0.000000223 ***
## type1electric   0.6955     6.1178   0.114             0.909515    
## type1fairy     -8.0139     8.1087  -0.988             0.323308    
## type1fighting  29.0536     6.8531   4.239          0.000025078 ***
## type1fire      11.3750     5.5998   2.031             0.042561 *  
## type1flying    -3.4583    18.1316  -0.191             0.848783    
## type1ghost      2.6157     6.9439   0.377             0.706501    
## type1grass      3.6442     5.0288   0.725             0.468871    
## type1ground    24.6875     6.5375   3.776             0.000171 ***
## type1ice        3.1793     7.3700   0.431             0.666301    
## type1normal     5.0369     4.7082   1.070             0.285036    
## type1poison     2.5313     6.5375   0.387             0.698719    
## type1psychic   -4.5590     5.5691  -0.819             0.413253    
## type1rock      20.5417     5.8473   3.513             0.000468 ***
## type1steel     22.9583     7.2527   3.166             0.001608 ** 
## type1water      3.1820     4.6320   0.687             0.492311    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 30.77 on 783 degrees of freedom
## Multiple R-squared:  0.1039, Adjusted R-squared:  0.08448 
## F-statistic: 5.343 on 17 and 783 DF,  p-value: 0.00000000002374&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is not a great model by any means, as the R-squared values suggest that Pokémon type alone explains only a small proportion of the variance in the data. From the summary of the model, we can see that the fit model has an intercept of 70.1 (which is close to the overall mean of 77.86), with an offset specific to each of the different types. As we saw from the boxplot, ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ all have significant increases in attack (as can be seen by the resulting p-value), but also ‘dark’ and ‘fire’ type which I missed from looking at the boxplot. We see also that ‘fairy’ and ‘psychic’ type Pokémon show a decrease in attack strength, however this difference is not significant. So if you want a strong Pokémon, then these types are your best shot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dark&lt;/li&gt;
&lt;li&gt;dragon&lt;/li&gt;
&lt;li&gt;fighting&lt;/li&gt;
&lt;li&gt;fire&lt;/li&gt;
&lt;li&gt;ground&lt;/li&gt;
&lt;li&gt;rock&lt;/li&gt;
&lt;li&gt;steel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is also worth mentioning that there are a few outliers here, which represent ridiculously powerful Pokémon. In general, these are those with a strength greater than 150, but there are also two fairy Pokémon with higher strength than is typically seen within the ‘fairy’ type Pokémon. Let’s use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cook%27s_distance&#34;&gt;Cook’s Distance&lt;/a&gt; to identify the outliers from the above model. The idea here is to test the influence of each Pokémon by comparing the least-squares regression with and without the sample included. A higher value indicates a possible outlier. So let’s have a look at the outliers, which we are going to class as those with a Cook’s Distance greater than 4 times the mean of the Cook’s Distance over the entire data set (note that this is a commonly used threshold, but is entirely arbitrary):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggrepel&amp;quot;)
library(&amp;quot;RColorBrewer&amp;quot;)
cookdist &amp;lt;- cooks.distance(attack_vs_type1)
cookdist_lim &amp;lt;- 4*mean(cookdist, na.rm=T)#0.012
data.frame(Pokenum   = pokedat[[&amp;quot;pokedex_number&amp;quot;]], 
           Pokename  = pokedat[[&amp;quot;name&amp;quot;]], 
           CooksDist = cookdist,
           Pokelab   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;name&amp;quot;]]), &amp;quot;&amp;quot;),
           Outlier   = ifelse(cookdist &amp;gt; cookdist_lim, TRUE, FALSE),
           PokeCol   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;type1&amp;quot;]]), &amp;quot;&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) +
  geom_point() + 
  scale_shape_manual(values = c(16, 8)) +
  scale_size_manual(values = c(2, 5)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, colorRampPalette(brewer.pal(9, &amp;quot;Set1&amp;quot;))(length(table(pokedat[[&amp;quot;type1&amp;quot;]]))))) +
  geom_text_repel(nudge_x = 0.2, size = 8) +
  geom_hline(yintercept = cookdist_lim, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  xlab(&amp;quot;Pokedex Number&amp;quot;) + ylab(&amp;quot;Cook&amp;#39;s Distance&amp;quot;) +
  theme_bw() +
  theme(#legend.position = &amp;quot;name&amp;quot;,
        axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/strength_cooks-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The flying Pokémon Tornadus and Noibat are the biggest outliers. However, interestingly whilst Tornadus has a particularly high attack strength of 100 for a flying type Pokémon, Noibat has a low attack strength of 30. So both are outliers compared to the flyiong type Pokémon as a whole, with a mean of 66.6666667 ± 35.1188458. It is worth mentioning that Tornadus is a Legendary Pokémon which probably explains why it has a higher attack strength.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/641-incarnate.png&#34; alt=&#34;Tornadus&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Tornadus&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Whilst these two do not show up on the barplots, it is likely that these are responsible for the strange distribution of the notched boxplot. I suspect this odd shape is a result of the calculation of the confidence interval for the notches exceeding the 25&lt;sup&gt;th&lt;/sup&gt; and 75&lt;sup&gt;th&lt;/sup&gt; percentiles due to a wide range of values, but I do enjoy the fact that it looks like it is itself flying!&lt;/p&gt;
&lt;p&gt;The two outlying fairy type Pokémon are Xerneas and Togepi, another favourite of my daughter’s:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/175.png&#34; alt=&#34;Togepi&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Togepi&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;defence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Defence&lt;/h2&gt;
&lt;p&gt;Let’s repeat this to look at the defence values across the different Pokémon types:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(x = defense, fill = MainType)) + 
  geom_density(alpha = 0.2) + 
  xlab(&amp;quot;Defence Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/defence_dist-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be a bit more of a distinction between the different types as compared to the attack distribution, so let’s also look at the boxplots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(y = defense, x = MainType, fill = MainType)) + 
  geom_boxplot(notch = TRUE) + 
  xlab(&amp;quot;Defence Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text.x  = element_text(size = 18, angle = 90, hjust = 1),
    axis.text.y  = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/defence_box-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see more outliers here as compared to the attack strength and a very clear increase in defence for steel and rock type Pokémon (unsurprisingly), as well as dragon type. There are no clear types with lower defence, similar again to what we saw in the attack strength distribution plots. Let’s once again look at this using linear models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;defence_vs_type1 &amp;lt;- lm(defense ~ type1, data = pokedat)
summary(defence_vs_type1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = defense ~ type1, data = pokedat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -70.208 -20.847  -3.031  16.518 159.153 
## 
## Coefficients:
##                Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)    70.84722    3.37247  21.008 &amp;lt; 0.0000000000000002 ***
## type1dark      -0.32998    6.29376  -0.052               0.9582    
## type1dragon    15.41204    6.45779   2.387               0.0172 *  
## type1electric  -9.02671    5.68954  -1.587               0.1130    
## type1fairy     -2.68056    7.54108  -0.355               0.7223    
## type1fighting  -4.45437    6.37337  -0.699               0.4848    
## type1fire      -3.05876    5.20784  -0.587               0.5571    
## type1flying    -5.84722   16.86236  -0.347               0.7289    
## type1ghost      8.67130    6.45779   1.343               0.1797    
## type1grass      0.02457    4.67678   0.005               0.9958    
## type1ground    13.05903    6.07981   2.148               0.0320 *  
## type1ice        1.06582    6.85403   0.156               0.8765    
## type1normal   -11.15198    4.37865  -2.547               0.0111 *  
## type1poison    -0.81597    6.07981  -0.134               0.8933    
## type1psychic   -1.58307    5.17923  -0.306               0.7599    
## type1rock      25.41944    5.43795   4.674    0.000003469793310 ***
## type1steel     49.36111    6.74494   7.318    0.000000000000623 ***
## type1water      2.63523    4.30777   0.612               0.5409    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 28.62 on 783 degrees of freedom
## Multiple R-squared:  0.1534, Adjusted R-squared:  0.135 
## F-statistic: 8.347 on 17 and 783 DF,  p-value: &amp;lt; 0.00000000000000022&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, we see very clear significant increase in defence strength for rock and steel type Pokémon, with a slight increase seen also for dragon and ground type Pokémon. There is also a slight reduction for normal type Pokémon. Flying Pokémon again show an odd distribution, so I expect again to see outliers for this class. Let’s look at the outliers now, again using the COok’s Distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cookdist &amp;lt;- cooks.distance(defence_vs_type1)
cookdist_lim &amp;lt;- 4*mean(cookdist, na.rm=T)#0.012
data.frame(Pokenum   = pokedat[[&amp;quot;pokedex_number&amp;quot;]], 
           Pokename  = pokedat[[&amp;quot;name&amp;quot;]], 
           CooksDist = cookdist,
           Pokelab   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;name&amp;quot;]]), &amp;quot;&amp;quot;),
           Outlier   = ifelse(cookdist &amp;gt; cookdist_lim, TRUE, FALSE),
           PokeCol   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;type1&amp;quot;]]), &amp;quot;&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) +
  geom_point() + 
  scale_shape_manual(values = c(16, 8)) +
  scale_size_manual(values = c(2, 5)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, colorRampPalette(brewer.pal(9, &amp;quot;Set1&amp;quot;))(length(table(pokedat[[&amp;quot;type1&amp;quot;]]))))) +
  geom_text_repel(nudge_x = 0.2, size = 8) +
  geom_hline(yintercept = cookdist_lim, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  xlab(&amp;quot;Pokedex Number&amp;quot;) + ylab(&amp;quot;Cook&amp;#39;s Distance&amp;quot;) +
  theme_bw() +
  theme(axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/defence_cooks-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, Noibat and Tornadus are outliers here, and are joined by Noivern (which is an evolution of Noibat). As it turns out, these are the only 3 Pokémon in the flying type, which explains why they show up as outliers and why the boxplot has such a strange distribution. Other outliers include ice-type Pokémon Avalugg (defence 184), steel-type Pokémon Steelix and Aggron (both with defence 230), and bug-type Pokémon Shuckle (defence 230) which doubles up as a rock type Pokémon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/213.png&#34; alt=&#34;Shuckle&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Shuckle&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As seen with Shuckle, we should bear in mind that we have only focussed on the primary Pokémon type here, and have not considered the &lt;code&gt;type2&lt;/code&gt; values. So it is possible that we are missing the full picture when looking only at the first class for some of these Pokémon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Other&lt;/h2&gt;
&lt;p&gt;We could do this for all of the different values, and there are many different ways that we may want to examine these data. We can look at eaxch variable in the data set and examine them for odd distributions, we can look for outliers (as we have done above), we can start to examine relationships between variables, and we can look for correlation between different values (which we will do further below). Indeed a considerable amount of time &lt;em&gt;should&lt;/em&gt; be spent exploring data in this way to ensure it is of good quality – the old saying “garbage in, garbage out” is very true. But to be honest there are more interesting things that I want to do with these data, so let’s crack on…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensuring-we-use-accurate-data-classes-throughout&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Ensuring we use accurate data classes throughout&lt;/h2&gt;
&lt;p&gt;It is important to ensure that we are using the data in the correct way for our analyses. First of all let’s take a look at the abilities that each Pokémon has. As it stands, the &lt;code&gt;abilities&lt;/code&gt; variable is not terribly useful, as it fails to link Pokémon who may share abilities but not exactly. We could spend some time decoding the abilities, and create a “dictionary” of different abilities for each Pokémon, but perhaps a better measure may be to simply look at the &lt;em&gt;number&lt;/em&gt; of abilities that each Pokémon has:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abilities &amp;lt;- strsplit(as.character(pokedat[[&amp;quot;abilities&amp;quot;]]), &amp;quot;, &amp;quot;)
abilities &amp;lt;- sapply(abilities, FUN = function(x) gsub(&amp;quot;\\[|\\&amp;#39;|\\]&amp;quot;, &amp;quot;&amp;quot;, x))
names(abilities) &amp;lt;- rownames(pokedat)
pokedat[[&amp;quot;number_abilities&amp;quot;]] &amp;lt;- sapply(abilities, FUN = length)
table(pokedat[[&amp;quot;number_abilities&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   6 
## 109 245 427   7  13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the majority of Pokémon have only 1, 2 or 3 abilities, with only a small number of Pokémon have more than 3 abilities, and around half having exactly 3.&lt;/p&gt;
&lt;p&gt;I want to now look only at the variables that might theoretically be descriptive of the Pokémon in some kind of model. So we can remove the individual abilties (although the &lt;em&gt;number&lt;/em&gt; of abiltities will likely be of interest), the names and Pokedex number, and the classification which seems to be fairly ambiguous. We also need to ensure that we use the dummy variables for the variables encoded as factors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Normalization&lt;/h1&gt;
&lt;p&gt;Prior to doing this, I want to look at a few different ways to normalize all of the variables to ensure that they all have values that are comparable. Some algorithms are more sensitive to normalization than others, and it becomes quite obvious why this may be something to consider here when you consider that whilst the stats value &lt;code&gt;attack&lt;/code&gt; lies between 5 and 185, the &lt;code&gt;experience_growth&lt;/code&gt; value ranges between 600,000 and 1,640,000. That’s a &amp;gt;10,000-fold increase. This variable will completely dominate the calculations for certain machine-learning algorithms like K-Nearest Neighbours (KNN) and Support Vector Machines (SVM) where the distance between data points is important. SO let’s look at how we might want to normalize these data prior to analysis.&lt;/p&gt;
&lt;div id=&#34;min-max-normalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Min-Max Normalization&lt;/h2&gt;
&lt;p&gt;There are a few different ways to do this. One way is to &lt;em&gt;standardize&lt;/em&gt; the data, so that we bring them all into a common scale of &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;, but we maintain the distribution for each specific variable. So if one variable has a very skewed distriburtion, this will be maintained. A simple way to do this is to use the &lt;em&gt;min-max scaling&lt;/em&gt; approach, where you subtract the lowest value (so that the minimum value is always zero) and then divide by the range of the data so that the data are spread in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x&amp;#39; = \frac{x - x_{min}}{x_{max} - x_{min}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s compare the standardized and non-standardized attack values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_std &amp;lt;- (pokedat[[&amp;quot;attack&amp;quot;]] - min(pokedat[[&amp;quot;attack&amp;quot;]]))/(max(pokedat[[&amp;quot;attack&amp;quot;]]) - min(pokedat[[&amp;quot;attack&amp;quot;]]))
data.frame(Raw          = pokedat[[&amp;quot;attack&amp;quot;]],
           Standardized = attack_std) %&amp;gt;%
  tidyr::gather(&amp;quot;class&amp;quot;, &amp;quot;attack&amp;quot;, Raw, Standardized) %&amp;gt;%
  mutate(class = factor(class, levels = c(&amp;quot;Raw&amp;quot;, &amp;quot;Standardized&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = attack, fill = class)) +
  geom_density(alpha = 0.2) +
  facet_wrap(. ~ class, scales = &amp;quot;free&amp;quot;) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(legend.position = &amp;quot;none&amp;quot;,
    axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    axis.text       = element_text(size = 18),
    legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    legend.text     = element_text(size = 20),
    strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/standardize_attack-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So as you can see here, the distribution of the attack scores remains the same, but the scales over which the distribution is spread are different.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z-score-normalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Z-Score Normalization&lt;/h2&gt;
&lt;p&gt;However, sometimes it is preferable to instead that every feature has a standard distribution that can be easily described to make them more comparable. In this case, you can &lt;em&gt;normalize&lt;/em&gt; the data so that the distribution of all features is shifted towards that of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34;&gt;normal (Gaussian) distribution&lt;/a&gt;. This is a typical “bell-shaped” curve, which can be exclusively described by the mean and the standard deviation. An example where I use this approach regularly is in gene-expression analysis, where we normalize the data such that the levels of expression of each gene is represented by a normal distribution, so that we can test for samples where the expression is significantly outside of the confines of this distribution. We can then look at all of the genes that seem to show significantly different expression than we would expect using hypothesis testing approaches, and look for common functions of these genes through the use of network analysis, gene ontology analysis and other downstream analyses.&lt;/p&gt;
&lt;p&gt;A simple normalization approach is the z-score normalization, which will transform the data so that the distribution has a mean of 0 and a standard deviation of 1. The transformation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x&amp;#39; = \frac{x - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we simply subtract the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (to center the data), and divide by the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Let’s again visualise this by comparing the raw and normalized data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_norm &amp;lt;- (pokedat[[&amp;quot;attack&amp;quot;]] - mean(pokedat[[&amp;quot;attack&amp;quot;]]))/sd(pokedat[[&amp;quot;attack&amp;quot;]])
data.frame(Raw          = pokedat[[&amp;quot;attack&amp;quot;]],
           Normalized   = attack_norm) %&amp;gt;%
  tidyr::gather(&amp;quot;class&amp;quot;, &amp;quot;attack&amp;quot;, Raw, Normalized) %&amp;gt;%
  mutate(class = factor(class, levels = c(&amp;quot;Raw&amp;quot;, &amp;quot;Normalized&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = attack, fill = class)) +
  geom_density(alpha = 0.2) +
  facet_wrap(. ~ class, scales = &amp;quot;free&amp;quot;) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(legend.position = &amp;quot;none&amp;quot;,
    axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    axis.text       = element_text(size = 18),
    legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    legend.text     = element_text(size = 20),
    strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/normalize_attack-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the normalized data now has a mean of 0, with the majority of values lying between -2 and 2. Note that since this is a fairly simple normalization technique, the distribution itself is not really changed, but there are other methods such as quantile normalization which can reshape the data to a Gaussian curve, or indeed any other dstribution as required.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-normalization-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Choosing a Normalization Method&lt;/h2&gt;
&lt;p&gt;The choice of which method to use will largely depend on what you are trying to do. The min-max scaling method is a common method used in machine learning, but does not handle outliers very well – a sample with an extreme value in the raw data will still have an extreme value in the scaled data, resulting in bunching up of the remaining data since it is all bounded within the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. The Z-score normalization is better at dealing with outliers as they will appear far away from the mean value of 0, but the range of data is no longer bounded meaning that different variables will now have different ranges. Other normalization processes may make your data more homogenous, but at the cost of potentially losing aspects of the data that may be useful.&lt;/p&gt;
&lt;p&gt;Many machine learning algorithms use some kind of &lt;em&gt;distance&lt;/em&gt; measure, in order to look for similarity between different items. This can be a simple Euclidean distance, which is a k-dimensional measure of “as the crow flies”. If the scales are very different between your variables, then this will cause a significant issue.&lt;/p&gt;
&lt;p&gt;For the following analyses, I will use Z-score normalization, since I know that there are a few extreme outliers in these data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-for-patterns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Looking for Patterns&lt;/h1&gt;
&lt;div id=&#34;correlation-of-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Correlation of Variables&lt;/h2&gt;
&lt;p&gt;One thing that it is worth doing before heading down a route of data modelling is to look at the correlation structure between the variables in the data set. If there are highly correlated variables (for instance one might imagine height and weight to be highly correlated), then both will offer the same information to the model – adding both will give no additional predictive power than adding only one. There is then a danger of over-fitting the data, which can happen if you create a model so complicated that, whilst it may fit the training data very well, it is so complex as to no longer be applicable to external data sets making it essentially useful. An extreme example is if we create a model where we have one variable for each sample, that is equal to 1 for that specific sample, and 0 for every other sample. This would fit the data perfectly, but would be of absolutely no use for any other data. In addition, we can risk biasing the data by including multiple variables that essentially encode the same information.&lt;/p&gt;
&lt;p&gt;So we can calculate a pairwise correlation matrix, which will give us a measure of similarity between each pair of variables by comparing the two vectors of values for the 801 Pokémon. There are multiple measures of correlation that can be used. The standard is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&#34;&gt;Pearson Correlation&lt;/a&gt;, which is a measure of the linearity of the relationship between two values. A value of 1 represents an entirely linear monotonic relationship (as one value increases, so does the other, but with every unit increase in one variable matching a unit increase in the other in a linear way), a value of 0 represents no linearity between the values (no clear relationship between the two), and a value of -1 represents an inverse monotonic linear relationship (anti-correlated).&lt;/p&gt;
&lt;p&gt;For this analysis, I will be using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&#34;&gt;Spearman Correlation&lt;/a&gt; coefficient. Instead of using the values themselves, this method first ranks the data, and then looks at the correlation. The idea here is that the unit difference between each successive value is kept constant, meaning that outliers do not have a big impact. This is therefore independent of the distribution of the data, and is therefore a non-parametric method. Since we identified some outliers with some of these variables, this method will remove the effect that these might have.&lt;/p&gt;
&lt;p&gt;We can represent these values by using a heatmap, which is a way of representing 3-dimensional data. The value of the correlation, rather than being represented on an axis as a value, will be represented by a colour. Values of the SPearman correlation closer to 1 will appear more red, whilst those closer to -1 will appear more blue. Those closer to 0 will appear white.&lt;/p&gt;
&lt;p&gt;In addition, I will apply a hierarchical clustering method to ensure that the variables most similar to one another are located close to one another on the figure. Pairwise distances are calculated by looking at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34;&gt;Euclidean distance&lt;/a&gt;, and similar variables are clustered together, allowing us to pick out by eye those most similar to one another.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, we first want to ensure that we are looking at numerical values, or at least numerical representations of factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat_vals &amp;lt;- pokedat[, !names(pokedat) %in% c(&amp;quot;abilities&amp;quot;, &amp;quot;classfication&amp;quot;, &amp;quot;japanese_name&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;pokedex_number&amp;quot;)]
pokedat_vals[[&amp;quot;type1&amp;quot;]]        &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;type1&amp;quot;]])
pokedat_vals[[&amp;quot;type2&amp;quot;]]        &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;type2&amp;quot;]])
pokedat_vals[[&amp;quot;generation&amp;quot;]]   &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;generation&amp;quot;]])
pokedat_vals[[&amp;quot;is_legendary&amp;quot;]] &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;is_legendary&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give us a purely numeric data set for use in numeric calculations. Finally we will normalize the data using Z-score normalization:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat_norm &amp;lt;- scale(pokedat_vals, center = TRUE, scale = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally let’s take a look at the correlation plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;pheatmap&amp;quot;)
pokecor_var &amp;lt;- cor(pokedat_norm, method = &amp;quot;spearman&amp;quot;)        
colors  &amp;lt;- colorRampPalette(c(&amp;#39;dark blue&amp;#39;,&amp;#39;white&amp;#39;,&amp;#39;dark red&amp;#39;))(255)#colorRampPalette( rev(brewer.pal(9, &amp;quot;Blues&amp;quot;)) )(255)
pheatmap(pokecor_var, 
         clustering_method = &amp;quot;complete&amp;quot;,
         show_colnames = FALSE,
         show_rownames = TRUE,
         col=colors, 
         fontsize_row = 24)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/correlation_var-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see a few clear cases of correlated variables here, especially between a few of the values giving attack strength against certain Pokémon types. For instance, attack against ghost- and dark-type Pokémon are very similar (which makes sense), as are attacks against electric- and rock-type Pokémon (makes less sense). The clearest aspect of the figure is a block of high positive correlations between a number of variables associated with the Pokémon’s vital statistics. So the most similar variables are those like speed, defence strength, attack strength, height, weight, health points, experience growth, etc. This makes a lot of sense, with bigger Pokémon having better attack and defence, more health points, etc. We also see that these variables are very highly anti-correlated with the capture-rate, which again makes sense – the better Pokémon are harder to catch. However, none of these correlations seem significantly high to require the remobval of any variables prior to analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;height-vs-weight&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; Height vs Weight&lt;/h2&gt;
&lt;p&gt;Let’s take a look at the height vs the weight. We can also include a few additional variables to see how the Pokémon strength and defence affect the relationship. I am going to scale both the x-and y-axes by using a &lt;span class=&#34;math inline&#34;&gt;\(log_{10}\)&lt;/span&gt; transformation to avoid much larger Pokémon drowing out the smaller ones:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = height_m, y = weight_kg, color = attack, size = defense), data = pokedat) +
  geom_point(alpha = 0.2) +
  scale_color_gradient2(name = &amp;quot;Attack&amp;quot;, midpoint = mean(pokedat[[&amp;quot;attack&amp;quot;]]), low = &amp;quot;blue&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;red&amp;quot;) +
  scale_size_continuous(name = &amp;quot;Defence&amp;quot;, range = c(1, 10)) +
  xlab(&amp;quot;Height (m)&amp;quot;) + ylab(&amp;quot;Weight (Kg)&amp;quot;) +
  scale_x_log10() +
  scale_y_log10() +
  #theme_bw() + 
  theme(axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/height_vs_weight-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there is a clear relationship, and in general taller Pokémon (such as Walor, the largest of the Pokémon) also weigh more as we might expect:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/321.png&#34; alt=&#34;Walord, the largest Pokemon&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Walord, the largest Pokemon&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We also see that in general the attack strength of the larger Pokémon is higher than the smaller ones, although there are definitely some outliers, such as Cosmeom, a legendary Pokémon only 10 cm tall that weighs nearly 1,000 kg!&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/cosmoem.png&#34; alt=&#34;Cosmoem, the smallest Pokemon&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Cosmoem, the smallest Pokemon&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-between-pokemon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Correlation between Pokémon&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokecor_sample &amp;lt;- cor(t(pokedat_norm), method = &amp;quot;spearman&amp;quot;)        
colors  &amp;lt;- colorRampPalette(c(&amp;#39;dark blue&amp;#39;,&amp;#39;white&amp;#39;,&amp;#39;dark red&amp;#39;))(255)
pheatmap(pokecor_sample, 
         clustering_method = &amp;quot;complete&amp;quot;,
         show_colnames = FALSE,
         show_rownames = FALSE,
         annotation = pokedat[, c(&amp;quot;generation&amp;quot;, &amp;quot;type1&amp;quot;, &amp;quot;type2&amp;quot;, &amp;quot;is_legendary&amp;quot;)],
         col=colors)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/correlation_pokemon-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So here we see the correlation structure between the 801 Pokémon, and it very clearly shows similarities between groups of Pokémon. The red boxes that we see down the diagonal represent highly similar groups, and at the top I have annotated the factor variables generation, type1, type2 and is_legendary. Almost all of the legendary Pokémon cluster together, suggesting that these are all similar to one another across these variables. Similarly there are a number of Pokémon types that clearly cluster together, such as rock type and dark type Pokémon. Interestingly the Generation number seems to be unrelated to Pokémon similarity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;principal-component-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.4&lt;/span&gt; Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;A good way to explore data such as these to look for underlying trends in the data is to use a dimensional-reduction algorithm to reduce these high-dimensional data down into asmaller number of easy to digest chunks. Principal component analysis (PCA) is one such approach, and can be used to look for the largest sources of variation within a high dimensional data set. For a data set with n variables, we can think of these data existing in an n-dimensional space. PCA is a mathematical trick that rotates these axes in n-dimensional space so that the x-axis of the rotation explains the largest possible amount of variation in the data, the y-axis then explains the next largest possible amount of variation, the z-axis the next largest amount, etc. In many cases, a very large amount of the total variation in the original n-dimensional data set can be captured by only a small number of so-called principal components. This can be used to look for underlying trends in the data. So let’s have a look at how this looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- prcomp(pokedat_vals, scale. = TRUE)
pc_plot &amp;lt;- as.data.frame(pc[[&amp;quot;x&amp;quot;]])
pc_plot &amp;lt;- cbind(pc_plot, pokedat[rownames(pc_plot), c(&amp;quot;generation&amp;quot;, &amp;quot;type1&amp;quot;, &amp;quot;type2&amp;quot;, &amp;quot;is_legendary&amp;quot;)])
explained_variance &amp;lt;- 100*((pc[[&amp;quot;sdev&amp;quot;]])^2 / sum(pc[[&amp;quot;sdev&amp;quot;]]^2))
screeplot(pc, type = &amp;quot;line&amp;quot;, main = &amp;quot;Principal Component Loadings&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/PCA-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we see that PC1 and PC2 explain a lot of variance compared to the other PCs, but the drop off is not complete at this point. In a lot of data sets, the first few PCs explain the vast majority of the variance, and so this plot drops off considerably to a flat line by PC3 or PC4. Let’s take a look at how discriminatory these PCs are to these data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = PC1, y = PC2, shape = is_legendary, color = type1), data = pc_plot) +
  geom_point(size = 5, alpha = 0.7) +
  xlab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 1, explained_variance[1])) +
  ylab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 2, explained_variance[2])) +
  theme(axis.title   = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 12),
        legend.title = element_text(size = 18, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/PCA_PC1_vs_PC2-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that this PCA approach clearly discriminates the legendary Pokémon (triangles) from the other Pokémon (circles) in PC1, which is the new axis that explains the most variance in the data (although note that this is still only 17.2%, so not a huge amount really). There is some separation seen between the different Pokémon groups, which seems to represent the second most significant source of variation, accounting for another 10.4%. Let’s look at PC3 as well to see if this is able to discriminate the samples further:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = PC2, y = PC3, shape = is_legendary, color = type1), data = pc_plot) +
  geom_point(size = 5, alpha = 0.7) +
  xlab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 2, explained_variance[2])) +
  ylab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 3, explained_variance[3])) +
  theme(axis.title   = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 12),
        legend.title = element_text(size = 18, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15-building-a-pokemon-recommendation-machine/index_files/figure-html/PCA_PC2_vs_PC3-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By looking at the 2&lt;sup&gt;nd&lt;/sup&gt; and 3&lt;sup&gt;rd&lt;/sup&gt; PCs, we do see a slight separation between the Pokémon types, but it is not strong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-based-recommendation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.5&lt;/span&gt; Correlation-Based Recommendation&lt;/h2&gt;
&lt;p&gt;As a little aside, I want to see whether it is possible to recommend similar Pokémon to any Pokémon that you might suggest. For instance, my daughter’s absolute favourite is Eevee (a cuddly fox-like Pokémon):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/133.png&#34; alt=&#34;Eeevee&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Eeevee&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So let’s see whether there are any other Pokémon that are similar to Eevee, based on a simple correlation match. To do this, I will calculate the Spearman correlation coefficient between Eevee and every other Pokémon, and see which the most similar are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eevee &amp;lt;- as.numeric(pokedat_vals[&amp;quot;Eevee&amp;quot;, ])
eevee_cor &amp;lt;- apply(pokedat_vals, MAR = 1, FUN = function (x) cor(x, eevee, method = &amp;quot;spearman&amp;quot;))
head(sort(eevee_cor, decreasing = TRUE), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Eevee     Aipom   Sentret Teddiursa    Patrat Zigzagoon   Rattata 
## 1.0000000 0.9781018 0.9728923 0.9695460 0.9693680 0.9690263 0.9689643 
##    Meowth  Raticate    Bidoof 
## 0.9687552 0.9676089 0.9669312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here are a few of the most similar Pokémon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/190.png&#34; alt=&#34;Aipom&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Aipom&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/161.png&#34; alt=&#34;Sentret&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sentret&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/216.png&#34; alt=&#34;Teddiursa&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Teddiursa&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/504.png&#34; alt=&#34;Patrat&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Patrat&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Well they all look kind of cuddly like Eevee, except for the really creepy evil beaver Patrat at the end!&lt;/p&gt;
&lt;p&gt;However, what would be even more efficient (maybe not in this case, but in the case of a much higher-dimensional data set like a gene-expression data set of 30,000 genes) is to use the reduced dataset after using PCA. The first 10 PCs explained around two thirds of the variance on the data, so by using these 10 values rather than the 37 values originally, we reduce the computations with a relatively small loss of data. Let’s see what the outputs are in this case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc_df_sub &amp;lt;- as.data.frame(pc[[&amp;quot;x&amp;quot;]])[, 1:10]
eevee &amp;lt;- as.numeric(pc_df_sub[&amp;quot;Eevee&amp;quot;, ])
eevee_cor_pca &amp;lt;- apply(pc_df_sub, MAR = 1, FUN = function (x) cor(x, eevee, method = &amp;quot;spearman&amp;quot;))
head(sort(eevee_cor_pca, decreasing = TRUE), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Eevee    Spinda  Delcatty   Watchog     Aipom    Furret  Smeargle 
## 1.0000000 0.9636364 0.9515152 0.9515152 0.9393939 0.9272727 0.9030303 
##   Herdier Vanillish    Meowth 
## 0.9030303 0.9030303 0.8909091&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/327.png&#34; alt=&#34;Spinda&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Spinda&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/301.png&#34; alt=&#34;Delcatty&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Delcatty&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/505.png&#34; alt=&#34;Watchog&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Watchog&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/162.png&#34; alt=&#34;Furret&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Furret&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Similarly cute, but with another creepy one! After checking with the experiment subject (my daughter), it seems that the best hits are definitely Watchog and Furret, so this seems to cope well at picking out similarly cuddly looking Pokémon.&lt;/p&gt;
&lt;p&gt;Obviously here we are simply looking for Pokémon with similar characteristics. This is not as in-depth as a collaboritive filtering method where we have some subjective ranking of items to help us to determine the &lt;em&gt;best&lt;/em&gt; Pokémon to match somebody’s needs. However, by using the reduced PCA data set we are able to find very close matches using only a reduced subset of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-legendary-pokemon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Predicting Legendary Pokémon&lt;/h1&gt;
&lt;p&gt;Based on our previous analyses, we see that there is a clear discrimination between normal and Legendary Pokémon. These are incredibly rare, and very powerful Pokémon in the game. So is it possible to identify a Legendary Pokémon based on the variables available from this database? And if so, which variables are the most important?&lt;/p&gt;
&lt;p&gt;To do predictive modelling, we need to first split our data up into a training data set to use to train the model, and then a validation data set to use to confirm the accuracy of the predictions that this model makes. There are more robust ways to do this, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;cross-validation&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrapping&lt;/a&gt;, which allow you to assess the accuracy of the model. Cross validation can be applied simply by using the &lt;code&gt;caret&lt;/code&gt; package in R.&lt;/p&gt;
&lt;p&gt;We will also split the data randomly into two data sets – one containing 80% of the Pokémon for training the model, and one containing 20% of the Pokémon for validation purposes. For the following model fitting approaches, we do not need to have the categorical data converted into numbers as we did for the correlation analysis, as the factors will be treated correctly automatically We do however still need to ignore the non-informative variables. In addition, I am going to remove the “against_” columns to avoid overfitting of the data. So let’s generate our two data sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;caret&amp;quot;)
set.seed(0)
split_index &amp;lt;- createDataPartition(y = pokedat[[&amp;quot;is_legendary&amp;quot;]], p = 0.8, list = FALSE)
rm_index       &amp;lt;- which(names(pokedat) %in% c(&amp;quot;abilities&amp;quot;, &amp;quot;classfication&amp;quot;, &amp;quot;japanese_name&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;pokedex_number&amp;quot;))
rm_index       &amp;lt;- c(grep(&amp;quot;against&amp;quot;, names(pokedat)), rm_index)
training_dat   &amp;lt;- pokedat[split_index,  -rm_index]
validation_dat &amp;lt;- pokedat[-split_index, -rm_index]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s try a few different common methods used for classification purposes.&lt;/p&gt;
&lt;div id=&#34;support-vector-machine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; Support-Vector Machine&lt;/h2&gt;
&lt;p&gt;We can then train our SVM model, incorporating a preprocessing step to center and scale the data, and performing 10-fold repeated cross validation which we repeat 3 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_svm &amp;lt;- train(is_legendary ~., 
                   data = training_dat, 
                   method = &amp;quot;svmLinear&amp;quot;,
                   trControl = trctrl,
                   preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                   tuneLength = 10)
model_svm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Support Vector Machines with Linear Kernel 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.9802138  0.8782632
## 
## Tuning parameter &amp;#39;C&amp;#39; was held constant at a value of 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this model is able to predict whether the Pokémon is Legendary based on these 18 predictor variables with 98.02% accuracy (correct predictions). The Kappa value is normalised to account for the fact that we could get pretty good accuracy if we just called everything not Legendary due to the imbalance in the classes.&lt;/p&gt;
&lt;p&gt;This accuracy is based on resampling of the training data. How does it cope with the validation data? Let’s take a look at the confusion matrix for the predicted outcomes compared to the true values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_svm, newdata = validation_dat)
svm_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
svm_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   144    1
##      TRUE      2   13
##                                           
##                Accuracy : 0.9812          
##                  95% CI : (0.9462, 0.9961)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.000314        
##                                           
##                   Kappa : 0.8863          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 1.000000        
##                                           
##             Sensitivity : 0.9863          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9931          
##          Neg Pred Value : 0.8667          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9000          
##    Detection Prevalence : 0.9062          
##       Balanced Accuracy : 0.9574          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SO according to these multiple statistics, of the 14 Legendary Pokémon in the validation data, we correctly identified 13 of them, but missed 1. 2 were identified incorrectly. Our ultimate accuracy is 98.12%, which seems to be pretty good. However, it is possible to further tune this model, by adjusting the tuning parameter &lt;code&gt;C&lt;/code&gt;, by tweaking the parameters included in the model, and moving from a linear SVM model. However, all told, this is a pretty good result.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbour&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; k-Nearest Neighbour&lt;/h2&gt;
&lt;p&gt;Another classification method is the k-Nearest Neighbour (kNN) algorithm. The idea here is that a record is kept of all of the data in the training data, and a new sample is compared to find the k samples “closest” to it. The classification is then calculated based on some average of the classifications of these nearest neighbours. The method of determining the “nearest” neighbour can be one of a number of different methods, including Euclidean distance as described earlier.&lt;/p&gt;
&lt;p&gt;Also, the value of k is very important. Using the &lt;code&gt;caret&lt;/code&gt; package in R, we are able to test using multiple different values of k to find the value that optimises the model accuracy. So let’s train our model, again performing 10-fold repeated cross validation which we repeat 3 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_knn &amp;lt;- train(is_legendary ~., 
                   data = training_dat, 
                   method = &amp;quot;knn&amp;quot;,
                   trControl = trctrl,
                   preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                   tuneLength = 10)
model_knn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## k-Nearest Neighbors 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    5  0.9470304  0.5515501
##    7  0.9444345  0.5158836
##    9  0.9428470  0.4854601
##   11  0.9376461  0.4246775
##   13  0.9350660  0.3903400
##   15  0.9355788  0.3898290
##   17  0.9376542  0.4161790
##   19  0.9376784  0.4063420
##   21  0.9355786  0.3765296
##   23  0.9381750  0.4070014
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 5.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the accuracy is optimised by using k = 5 nearest neighbours, which gives an accuracy of 94.7% – below that of the SVM accuracy. Now let’s test it on the validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_knn, newdata = validation_dat)
knn_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
knn_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   145    4
##      TRUE      1   10
##                                           
##                Accuracy : 0.9688          
##                  95% CI : (0.9286, 0.9898)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.004163        
##                                           
##                   Kappa : 0.7833          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.371093        
##                                           
##             Sensitivity : 0.9932          
##             Specificity : 0.7143          
##          Pos Pred Value : 0.9732          
##          Neg Pred Value : 0.9091          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9062          
##    Detection Prevalence : 0.9313          
##       Balanced Accuracy : 0.8537          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can already see that the results of this model are less positive than the SVM model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; Logistic Regression&lt;/h2&gt;
&lt;p&gt;For classification problems with only two groups, linear regression is often a good first option. This is a generalised linear model, where we require a transformation of the response variable to ensure that it fits a continuous scale. In this case, the response variable is the probability of being in the Legendary class, so we need to map this to the simple yes/no outcome of the input training data. The logit function is often used, which is a transformation between a probability &lt;code&gt;p&lt;/code&gt; and a real number:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(p) = log(\frac{p}{1-p})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So let’s fit a logistic regression model containing all of our data, again using 3 repeats of 10-fold cross validation, and see what we get back:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_logreg &amp;lt;- train(is_legendary ~., 
                      data = training_dat, 
                      method = &amp;quot;glm&amp;quot;,
                      family = &amp;quot;binomial&amp;quot;,
                      trControl = trctrl,
                      preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                      tuneLength = 10)
model_logreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized Linear Model 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results:
## 
##   Accuracy   Kappa   
##   0.9667197  0.802881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we actually get a better accuracy from the cross-validation than for SVM or kNN at 96.67% accuracy. So let’s test it on the validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_logreg, newdata = validation_dat)
logreg_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
logreg_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   141    1
##      TRUE      5   13
##                                           
##                Accuracy : 0.9625          
##                  95% CI : (0.9202, 0.9861)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.01131         
##                                           
##                   Kappa : 0.792           
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.22067         
##                                           
##             Sensitivity : 0.9658          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9930          
##          Neg Pred Value : 0.7222          
##              Prevalence : 0.9125          
##          Detection Rate : 0.8812          
##    Detection Prevalence : 0.8875          
##       Balanced Accuracy : 0.9472          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The accuracy of this model on the validation data set is 96.25%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; Random Forest&lt;/h2&gt;
&lt;p&gt;As a final model, we will look at using a random forest classifier. A random forest is essentially created by creating a number of decision trees and averaging over them all. I will use the same cross-validation scheme as previously, and will allow &lt;code&gt;caret&lt;/code&gt; to identify the optimum parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_rf &amp;lt;- train(is_legendary ~., 
                  data = training_dat, 
                  method = &amp;quot;rf&amp;quot;,
                  trControl = trctrl,
                  preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                  tuneLength = 10)
model_rf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Random Forest 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9911611  0.9396362
##    8    0.9984292  0.9905178
##   14    0.9984292  0.9905178
##   20    0.9984292  0.9905178
##   26    0.9984292  0.9905178
##   32    0.9984292  0.9905178
##   38    0.9984292  0.9905178
##   44    0.9984292  0.9905178
##   50    0.9984292  0.9905178
##   56    0.9984292  0.9905178
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 8.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach identified an accuracy of 99.84% (with a normalised kappa value of 99.05%) with an &lt;code&gt;mtry&lt;/code&gt; value of 8. This is incredibly good, and is the best outcome so far. However, it is worth noting that this approach is significantly slower than the others. Let’s check against our validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_rf, newdata = validation_dat)
confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   145    1
##      TRUE      1   13
##                                           
##                Accuracy : 0.9875          
##                  95% CI : (0.9556, 0.9985)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.00005782      
##                                           
##                   Kappa : 0.9217          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9932          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9932          
##          Neg Pred Value : 0.9286          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9062          
##    Detection Prevalence : 0.9125          
##       Balanced Accuracy : 0.9609          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So whilst the training accuracy is nearly 100%, here we see slightly lower accuracy on the test data set. It is still pretty good going, but could still be improved.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;We have managed to explore these data in a number of different ways and have identified some interesting themes and patterns. However, there is a lot of additional work that can be done. Correlation-based recommendation seems to work to identify similar Pokémon, at least in the small number of cases that I looked at. My wife showed me &lt;a href=&#34;https://www.amazon.co.uk/Pokemon-Trainer/dp/B07GYQC8C7&#34;&gt;this&lt;/a&gt; toy that can guess any Pokémon that you think of, which probably uses a similar approach. I’m in the wrong business…&lt;/p&gt;
&lt;p&gt;It is worth noting that here I have concentrated predominantly on the accuracy of the model, but in general this is not the only metric that we should be interested in. As mentioned previously, given how many more non-Legendary Pokémon there are, we would get a pretty good accuracy if we just assumed that none of the test samples were Legendary. Sensitivity and Specificity for instance are useful to look at, which define the proportion of samples called correctly as Legendary (True Positives) and the proportion of those called correctly as not Legendary (Truse Negatives). Often, if we think of the opposite of these, False Negatives are more of an issue than False Positives – e.g. for disease prediction we would probably prefer not to miss any potential cases.&lt;/p&gt;
&lt;p&gt;Also, whilst it was interesting to play with some of the most commonly used machine learning methods, I have spent no real time tuning the model parameters. The variables themselves used in the data set could be tweaked to identify those variables most associated with the response variable. Given that these are incredibly rare and powerful Pokémon, we will inevitably find that the fighting based variables like attack and defence are highly discriminative, but also those such as the capture rate and experience growth associated. In addition, we have looked only at additive models here and have not considered interaction terms. We can also spend time fine-tuning the parameters of the models themselves to improve the accuracy further.&lt;/p&gt;
&lt;p&gt;However, despite this, we got some very good results from the default model parameters for several commonly used methods. kNN, SVM, logistic regression and random forests gave very similar results, but with SVM and random forest giving the best predictive outcomes.&lt;/p&gt;
&lt;p&gt;Ultimately, the key to developing any machine learning algorithm is to trial multiple different approaches and tune the parameters for the specific data in question. Also, a machine learning algorithm is only as good as the data that it is trained on, so feeding in more good quality cleaned data will help the model to learn. Ultimately, machine learning is aimed at developing a model that is able to predict something from new data, so it needs to be as generalised as possible.&lt;/p&gt;
&lt;p&gt;So hopefully when I come across a new Pokémon not currently found in my Pokedex, I can easily check whether it is Legendary by asking it a few questions about its vital statistics. Very useful.&lt;/p&gt;
&lt;p&gt;Gotta catch ’em all!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] caret_6.0-84       lattice_0.20-38    pheatmap_1.0.12   
## [4] RColorBrewer_1.1-2 ggrepel_0.8.1      dplyr_0.8.3       
## [7] ggplot2_3.2.0     
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.2          lubridate_1.7.4     tidyr_1.0.0        
##  [4] class_7.3-15        assertthat_0.2.1    zeallot_0.1.0      
##  [7] digest_0.6.20       ipred_0.9-9         foreach_1.4.7      
## [10] R6_2.4.0            plyr_1.8.4          backports_1.1.4    
## [13] stats4_3.6.1        evaluate_0.14       e1071_1.7-2        
## [16] blogdown_0.16       pillar_1.4.2        rlang_0.4.0        
## [19] lazyeval_0.2.2      data.table_1.12.2   kernlab_0.9-27     
## [22] rpart_4.1-15        Matrix_1.2-17       rmarkdown_1.14     
## [25] labeling_0.3        splines_3.6.1       gower_0.2.1        
## [28] stringr_1.4.0       munsell_0.5.0       compiler_3.6.1     
## [31] xfun_0.8            pkgconfig_2.0.2     htmltools_0.3.6    
## [34] nnet_7.3-12         tidyselect_0.2.5    tibble_2.1.3       
## [37] prodlim_2018.04.18  bookdown_0.12       codetools_0.2-16   
## [40] randomForest_4.6-14 crayon_1.3.4        withr_2.1.2        
## [43] MASS_7.3-51.4       recipes_0.1.7       ModelMetrics_1.2.2 
## [46] grid_3.6.1          nlme_3.1-140        gtable_0.3.0       
## [49] lifecycle_0.1.0     magrittr_1.5        scales_1.0.0       
## [52] stringi_1.4.3       reshape2_1.4.3      timeDate_3043.102  
## [55] ellipsis_0.2.0.1    generics_0.0.2      vctrs_0.2.0        
## [58] lava_1.6.6          iterators_1.0.12    tools_3.6.1        
## [61] glue_1.3.1          purrr_0.3.3         survival_2.44-1.1  
## [64] yaml_2.2.0          colorspace_1.4-1    knitr_1.23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Center for Enzyme Innovation</title>
      <link>/project/2019-06-18-center-for-enzyme-innovation/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/project/2019-06-18-center-for-enzyme-innovation/</guid>
      <description>&lt;p&gt;The Centre for Enzyme Innovation is a new initiative set up at the University of Portsmouth, with a goal to find solutions to some of the most pressing global environmental problems. In particular, we aim to examine environmental samples to find how the natural world has solved the problem of plastic pollution, by identifying microorganisms who have evolved to be able to break down plastics in the environment. By identifying these microorgansisms, and further understanding the enzymatic methods that they use towards this goal, we aim to harness the amazing work that nature has done, and find ways to enhance this and scale it up on an industrial scale to allow their use in circular recycling of plastics.&lt;/p&gt;

&lt;p&gt;The Centre was &lt;a href=&#34;https://www.port.ac.uk/enquire/news/portsmouth-scientists-awarded-major-plastics-funding&#34; target=&#34;_blank&#34;&gt;recently awarded £5.8 million&lt;/a&gt; in funding from the Research England Expanding Excellence Fund. Together with significant investment from the University of Portsmouth, we aim to use this funding to speed up our progress towards finding a solution to one of the world’s greatest environmental challenges – plastic waste.&lt;/p&gt;

&lt;p&gt;We will be working in the Discovery phase of the pipeline, aiming to use next generation &lt;a href=&#34;https://nanoporetech.com&#34; target=&#34;_blank&#34;&gt;Nanopore&lt;/a&gt; sequencing to explore microorganisms present on samples taken from environmental sites of interest (e.g. plastic receycling centres) to try and profile what is present and how they have developed to survive on plastic substrates. We aim to use this bioprospecting approach to explore metagenomics and metatranscriptomics of samples to identify mechanisms of action for plastic degaradation, and in particular to compare with previously explored species to identify what makes these species unique. This information, and information on the make-up of enzymes involved in the processes, will feed through the pipeline to be further engineered to create enzymes that can be scaled up on an industrial scale. These data will be combined with data from public repositories to feed into novel machine learning algorithms to help to train algorithms for the identification of further potential candidates for feeding through the pipeline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Portsmouth Heritage Hub</title>
      <link>/project/2019-06-18-portsmouth-heritage-hub/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/project/2019-06-18-portsmouth-heritage-hub/</guid>
      <description>&lt;p&gt;The Portsmouth Heritage Hub was set up by Dr Sam Robson, Dr Robert Inkpen and Dr Garry Scarlett, and aims to provide a centralised hub for researchers within the University of Portsmouth currently working on projects involving preservation, conservation, interpretation and education about our local cultural heritage.&lt;/p&gt;

&lt;p&gt;The Solent, and the surrounding Wessex area has a rich array of history. Portsmouth’s Historic Dockyard, home to world famous ships such as the Mary Rose and HMS Victory, attracts thousands of visitors from around the world, and provides a window into the life of sailors throughout the ages and how this has shaped the City today. The Isle of Wight has proved to be one of the richest sources of dinosaur fossils in all of Europe,  and has earned it the title of the UK’s Dinosaur Capital. Fishbourne Roman. Palace in Chichester is the largest Roman residential building to have been discovered in Britain, and offers a unique glimpse into the life of  Britons following the Roman conquest. And of course, Portsmouth will today play host to Royalty and international leaders to celebrate its part in the D-Day landings in Normandy, one of the most important operations in World War II.&lt;/p&gt;

&lt;p&gt;The skills and expertise of researchers throughout the University are multifaceted and span a wide range of themes and fields. These include (but are not limited to) expertise with ancient DNA analysis to understand our past, identifying novel ways to preserve monuments and buildings, interpretation of ancient texts and oral histories, and using modern technology such as virtual reality to provide interactive ways to engage the public with their past.&lt;/p&gt;

&lt;p&gt;There are many more projects besides, and our aim is to provide a framework for these researchers to work together with local stakeholders in historical monuments, buildings and other sites to identify ways to improve the heritage of our local area, to interpret our history to better inform our understanding of our past, and to provide new and interesting ways to engage the local area with our history.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Portsmouth Heritage Hub Inaugural Workshop</title>
      <link>/post/2019-06-16-portsmouth-heritage-hub-inaugural-workshop/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-06-16-portsmouth-heritage-hub-inaugural-workshop/</guid>
      <description>&lt;p&gt;On Friday 7th June, the Mary Rose Museum in the Portsmouth Historic Dockyard hosted the inaugural meeting of the Portsmouth Heritage Hub (PHH). The workshop aimed to bring together around 30 researchers from the University of Portsmouth with interests and expertise in the heritage field, with members of 15 organisations associated with local historical artefacts, architecture and monuments in Portsmouth and the surrounding area. It was a fantastic opportunity to foster collaborative research networks between people with the skills and expertise to solve problems, with stakeholders in our history with problems to solve. The ultimate aim of the PHH is to enrich the community through preservation, conservation, interpretation and education about our rich and unique regional history &amp;ndash; from the dinosaurs of the Cretaceous Period, through the maritime history of our island port city, to our place in history as the launch point for much of the D-Day Landing fleet in 1944, recently celebrated by World Leaders on the 75th Anniversary of this pivotal turning point to World War II. In this way, we can help to protect our heritage, understand how it has shaped our life today, and how it might inform our future.&lt;/p&gt;

&lt;p&gt;The day began with an introduction from Prof. Leila Choukroune, Director of the University of Portsmouth Democratic Citizenship Theme, who kindly sponsored the event. She outlined the goals of the hub, and the need to develop close working relationships between the University and our surrounding community.&lt;/p&gt;

&lt;p&gt;A series of short talks followed, which highlighted the vast array of multidisciplinary work currently ongoing within the field. The talks were opened by Dr. Sam Robson, a researcher from the School of Pharmacy and Biomedical Science, who talked about how he and other researchers at the University have applied cutting edge genetic research to learn more about members of the Mary Rose crew, in a program of work recently highlighted in the Channel 4 documentary  &lt;a href=&#34;https://www.channel4.com/programmes/skeletons-of-the-mary-rose-the-new-evidence&#34; target=&#34;_blank&#34;&gt;Skeletons of the Mary Rose: The New Evidence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Next, Dr. Joy Watts from the School of Biological Sciences talked about work that she has been conducting with &lt;a href=&#34;https://sussexpast.co.uk/properties-to-discover/fishbourne-roman-palace&#34; target=&#34;_blank&#34;&gt;Fishbourne Roman Palace&lt;/a&gt;, looking at ways of understanding the effects of microorganisms growing on the ruins, and how these can be targeted to better preserve the architecture.&lt;/p&gt;

&lt;p&gt;Dr Richard Madgwick, an osteological researcher from Cardiff University who has previously worked on projects with the Mary Rose Museum, talked about some recent work looking at methods of tracking the origin of early settlers through analysis of livestock remains. His work showed that feasts and rituals in Late Neolithic sites such as Stonehenge may have attracted attendees from as far away as &lt;a href=&#34;https://www.bbc.co.uk/news/uk-england-wiltshire-47554926&#34; target=&#34;_blank&#34;&gt;Scotland, North East England and West Wales&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Martin Munt, Curator and General Manager of the &lt;a href=&#34;http://www.dinosaurisle.com/newhomepage.aspx&#34; target=&#34;_blank&#34;&gt;Dinosaur Isle Museum on the Isle of Wight&lt;/a&gt;, moved us away from the scientific to give us a fascinating overview of the Isle of Wight’s rich treasure trove of fossilized dinosaur remains. With over 30,000 specimens, the region is the richest dinosaur locality in Europe, and the museum works hard not only to engage in significant research projects with the University, but also to engage the public through guided tours along the shore line where many specimens were discovered.&lt;/p&gt;

&lt;p&gt;Dr Karl Bell, a researcher in the School of Social, Historical and literary Studies, next led a fascinating talk about his work on understanding the social and cultural context of Portsmouth and port towns across the globe, as unique sites of socio-cultural exchange. In particular, he gave an insight into his work on assembling and preserving local folklore and urban myth as a form of intangible heritage.&lt;/p&gt;

&lt;p&gt;Dr Tarek Teba from Portsmouth School of Architecture gave an introduction to some of the many modern methods that he is currently using to create digital reconstructions of ruins as a way to bring context to visitors that may otherwise be missing when visiting the sites. The ability to explore long-dead civilisations in “game-ified” recreations offers a tangible way to interact with our past history.&lt;/p&gt;

&lt;p&gt;Finally, Dr Alexzandra Hildred, Head of research and Curator of Ordnance and Human Remains at the Mary Rose Trust, gave an incredibly revealing talk about the wealth of research projects undertaken by the trust. From developing ways to preserve the timbers of the ship that have led to the incredible museum exhibit that we see today, through using cutting edge technology to glean understanding from the many artefacts discovered, to using genetics, osteology and morphological analysis to understand who the men of the Mary Rose really were.&lt;/p&gt;

&lt;p&gt;Following an incredibly incisive tour around the Mary Rose Museum, where the immense work conducted to preserve this 500 year old ship and its inhabitants can be seen, participants returned to take part in a series of workshops to help guide the direction of the PHH. The first workshop focussed on identifying key focus themes for members of the hub, to understand where the critical mass of our members might lie. The focus areas that were identified and discussed throughout the workshop were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Heritage buildings and structures&lt;/li&gt;
&lt;li&gt;New ways of interpreting the past&lt;/li&gt;
&lt;li&gt;Conserving and preserving&lt;/li&gt;
&lt;li&gt;A changing Portsmouth&lt;/li&gt;
&lt;li&gt;How we got here: How the past has influenced the present&lt;/li&gt;
&lt;li&gt;Recreating the past&lt;/li&gt;
&lt;li&gt;Heritage and the community&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Discussions were insightful and provide a huge amount of information to help us to develop the direction in which the hub should be headed. Much of the discussion is currently being digested to determine the next steps that we hope to take.&lt;/p&gt;

&lt;p&gt;The second workshop had a more focussed aim; to identify short term and long-term projects that might be easily developed into funding-generating projects able to capitalize on the networks that we had built up through the day. The session did not disappointment, and a number of incredibly exciting projects arose that instantly show the worth of collaborative events such as this.&lt;/p&gt;

&lt;p&gt;Looking forward to the future, we aim to maintain the momentum that the workshop has generated, and attract even more members to promote further cross-disciplinary working to help galvanise work within the heritage sphere. The first projects identified for short-term grant applications will be explored in more specific workshops, whilst in the meantime we will explore the incredibly exciting longer term goals that have come out from our work together. The outcome of the PHH working in partnership with local heritage stakeholders will be an improvement to maintenance of our local community history, helping to show the benefits and positive role that academic institutions can play in developing the local community. We look fo&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Development of Bioinformatics Tools</title>
      <link>/project/2017-08-17-development-of-bioinformatics-tools/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/project/2017-08-17-development-of-bioinformatics-tools/</guid>
      <description>&lt;p&gt;As well as collaborating with researchers from throughout the School of Pharmacy and Biomedical Sciences, we are also working to develop several tools for use alongside commonly used and freely available analysis tools. These tools will be made available via the University of Portsmouth &lt;a href=&#34;https://github.com/uopbioinformatics&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; account.&lt;/p&gt;

&lt;p&gt;This includes development of a next generation sequencing data analysis platform created by Dr. Sam Robson, which allows for processing, mapping and analysis of a number of data types, including data generated from ChIP-Seq, RNA-Seq, 16S.rDNA amplicon sequencing, whole genome sequencing and ancient DNA sequencing approaches.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interaction of Sox2 with RNA binding proteins in mouse embryonic stem cells</title>
      <link>/publication/2019_interaction_of_sox2_with_rna_binding_proteins_in_mouse_embryonic_stem_cells/</link>
      <pubDate>Thu, 09 May 2019 00:00:00 +0000</pubDate>
      <guid>/publication/2019_interaction_of_sox2_with_rna_binding_proteins_in_mouse_embryonic_stem_cells/</guid>
      <description></description>
    </item>
    
    <item>
      <title>METTL1 Promotes let-7 MicroRNA Processing via m7G Methylation</title>
      <link>/publication/2019_mettl1_promotes_let-7_microrna_processing_via_m7g_methylation/</link>
      <pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/2019_mettl1_promotes_let-7_microrna_processing_via_m7g_methylation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dystrophic mdx mouse myoblasts exhibit elevated ATP/UTP-evoked metabotropic purinergic responses and alterations in calcium signalling</title>
      <link>/publication/2019_dystrophic_mdx_mouse_myoblasts_exhibit_elevated_atp_utp-evoked_metabotropic_purinergic_responses_and_alterations_in_calcium_signalling/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/publication/2019_dystrophic_mdx_mouse_myoblasts_exhibit_elevated_atp_utp-evoked_metabotropic_purinergic_responses_and_alterations_in_calcium_signalling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SRPK1 maintains acute myeloid leukemia through effects on isoform usage of epigenetic regulators including BRD4</title>
      <link>/publication/2018_srpk1_maintains_acute_myeloid_leukemia_through_effects_on_isoform_usage_of_epigenetic_regulators_including_brd4/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/publication/2018_srpk1_maintains_acute_myeloid_leukemia_through_effects_on_isoform_usage_of_epigenetic_regulators_including_brd4/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phosphorylation of histone H4T80 triggers DNA damage checkpoint recovery</title>
      <link>/publication/2018_phosphorylation_of_histone_h4t80_triggers_dna_damage_checkpoint_recovery/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/publication/2018_phosphorylation_of_histone_h4t80_triggers_dna_damage_checkpoint_recovery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Strava Data Mining: Assessing Mimi Anderson&#39;s World Record Run Across the USA</title>
      <link>/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-are-key&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#get-to-the-point&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#full-disclosure&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#delayed-uploads&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-is-pausing-her-watch&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dodgy-fluctuations-in-cadence-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-running-in-the-185-195-steps-per-minute-range&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spoofed-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I ran most of this analysis one evening at the beginning of the week before the sad news that Mimi was going to give up her World Record attempt due to injury. Whether Mimi’s data is valid to claim the World Record is now moot, but the effect of the damage to her reputation is not. She is understandably devastated at the turn of events, and I can only hope to alleviate some of her grief with what I have written here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;On 7th September 2017, the &lt;a href=&#34;http://www.mimirunsusa.com&#34;&gt;Marvellous Mimi Anderson&lt;/a&gt; began a world record attempt to run across the United States of America. She is planning on running the 2,850 miles in 53 days to beat the current &lt;a href=&#34;http://www.guinnessworldrecords.com/world-records/fastest-crossing-of-america-(usa)-on-foot-(female)/&#34;&gt;Guinness World Record&lt;/a&gt; of 69 days 2 hours 40 minutes by Mavis Hutchinson from South Africa back in 1979.&lt;/p&gt;
&lt;p&gt;Mimi is somewhat of an institution in the UK, and can often be seen both running and crewing at many races in her trademark pink ensemble. I have run with her on many occasions, and have seen first hand her amazing ability at running stupid races, even going so far as to make them more stupid by running there and back again on races such as &lt;a href=&#34;http://www.badwater.com&#34;&gt;Badwater&lt;/a&gt;, the &lt;a href=&#34;http://www.gucr.co.uk&#34;&gt;Grand Union Canal Race&lt;/a&gt;, and &lt;a href=&#34;http://www.spartathlon.gr/en/&#34;&gt;Spartathlon&lt;/a&gt;. She holds records for running across Ireland and across the UK, so going for the USA record is a natural progression for somebody who loves hunting for ever bigger challenges.&lt;/p&gt;
&lt;p&gt;Unfortunately, Over the past year, we have seen some controversial stunt runs from the likes of &lt;a href=&#34;http://www.telegraph.co.uk/news/2016/05/13/chariots-of-fire-or-walter-mitty-doubts-raised-over-runners-incr/&#34;&gt;Mark Vaz&lt;/a&gt; (who drove 860 miles very slowly from Lands End to John O’ Groats to “smash” the previous running record), &lt;a href=&#34;http://www.derehamtimes.co.uk/news/how-controversy-hit-norfolk-man-s-world-record-run-attempt-1-5060552&#34;&gt;Dave Reading&lt;/a&gt; (who tried to do the same, but was “cyber-bullied” into giving up apparently), and Marathon Man UK himself &lt;a href=&#34;https://www.theguardian.com/sport/blog/2016/oct/02/robert-young-marathon-sponsor-stands-tall&#34;&gt;Robert Young&lt;/a&gt; (who sat in the back of an RV and was slowly driven across America until some geezers made him run a bit and he hurt himself).
I confess that when the Mark Vaz thing happened I could genuinely not believe that somebody would actually do that. I mean, what a waste of time - and for what?! I did not believe that somebody would go to that much effort for such a niche accolade, but that was exactly the point. To most people, running across the country is already pretty ridiculous, but they don’t have any baseline for what a good time would be. Is 7 days, 18 hours and 45 minutes good? Of course the running community knew that the time was bloody incredible for the top ultrarunners in the country, never mind an overweight window cleaner in a puffa jacket.&lt;/p&gt;
&lt;p&gt;When Robert Young attempted his transcontinental run, it was a different kettle of fish. Robert had developed somewhat of a following as an inspirational runner, having started running on a whim to complete 370 marathons in a year, beating Dean Karnazes “record” for running without sleeping, and winning the Race Across the USA. So he had a history of running long, but that didn’t stop questions from being asked. In particular, posters at the &lt;a href=&#34;http://www.letsrun.com&#34;&gt;LetsRun&lt;/a&gt; forum smelled a rat very quickly, after one of their own went out to run with him in the middle of the night only to find a slow moving RV with no runner anywhere to be seen. After a lot of vehement denial from the Robert Young fans, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=7355147&#34;&gt;sleuths over at LetsRun&lt;/a&gt; were able to provide enough evidence for Robert’s main sponsor, &lt;a href=&#34;https://www.skins.net&#34;&gt;Skins&lt;/a&gt;, to hire independent experts to prepare a report on whether or not any subterfuge had occurred. The results were not good for the Marathon Man UK brand (although to this day he denies any wrongdoings).&lt;/p&gt;
&lt;p&gt;The main take-home message from the Robert Young (RY) affair was that data transparency is key. RY refused to provide his Strava data for people to check, but to be fair he had a very good reason - he hadn’t got around to doctoring it yet. So anybody looking to take on this record would have to work very hard to ensure that their data was squeaky clean and stood up to scrutiny.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-are-key&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/h1&gt;
&lt;p&gt;In leading up to the challenge, Mimi gave several interviews where the RY affair and, in particular, the importance of data transparency were discussed. And it was pretty clear that this was most definitely clear to Mimi. She was aware of the LetsRun community, and the importance of making her attempt as free from controversy as possible. She had arranged for a &lt;a href=&#34;http://www.racedrone.net&#34;&gt;RaceDrone&lt;/a&gt; tracker to be used to follow her progress in real time, and would be using four different GPS watches (two at any one time for redundancy) uploading the data immediately following every run. In addition, as required by Guinness World Records, they would obtain witness testimonies along the way. Add in social media to provide a running commentary and it would seem to be foolproof.&lt;/p&gt;
&lt;p&gt;Unfortunately it did not work out that way. The LetsRun board was already lighting up with &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8347797&#34;&gt;posts&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8420326&#34;&gt;questioning&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8449925&#34;&gt;her&lt;/a&gt; attempt (EDIT while the posts were skeptical from the start, accusations of foul play did not begin until after the first few weeks with the tracker issues). In addition, a second runner - &lt;a href=&#34;http://www.sandyacrossamerica.com&#34;&gt;Sandra Villines&lt;/a&gt; (aka Sandra Vi) - was joining in the fun, and was planning to also attempt the record (albeit taking a different route) 4 days days later. Suddenly we had a race on.&lt;/p&gt;
&lt;p&gt;Unfortunately, within the first few weeks, questions surrounding Mimi’s attempts had surfaced, largely as a result of failures in the tracker. There are contradicting reports about what happened in this time, and I don’t pretend to understand all of them. Mimi’s crew claim that the issues were due to lack of coverage, whilst RaceDrone’s founder Richard Weremuik claims that the trackers were intentionally turned off. If Richard’s claims are correct, it raises a lot of serious concerns.&lt;/p&gt;
&lt;p&gt;In addition, there have been several other events that have received a lot of criticism, including against the reaction of Mimi’s fans to questions of her integrity (“trolls”, “haterz gonna hate”, etc.), and an incident whereby a stalker presumed (by Mimi’s crew) to be a poster on the LetsRun forum was arrested, despite no record of an arrest being found and no poster admitting to it (which they likely would). Since then, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&#34;&gt;whole run has been torn apart&lt;/a&gt;, and in particular she is now accused of faking the only source of data that is consistently available for review - &lt;a href=&#34;https://www.strava.com/athletes/13566252&#34;&gt;her Strava data&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-the-point&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/h1&gt;
&lt;p&gt;There are definitely things to do with this whole affair that I cannot comment on, such as the Race Drone incident and the arrest report. I also agree with many of the detractors that Sandy’s set up seems to be a far more open approach, and seems to be a good gold standard to use in the future. You will get no arguments from me that mistakes were made and perhaps things should have been done differently. But I genuinely believe that Mimi went out to the USA in the belief that what she had planned was foolproof and would cover all bases and supply the necessary evidence to convince anybody that might choose to look into it. Any mistakes were a result of the fact that Mimi has limited knowledge of technology - by her own omission she is a Luddite. Having said that, it is clear that the focus was on satisfying primarily the requirements of Guinness, which most runners in my experience consider to be very weak.&lt;/p&gt;
&lt;p&gt;However, what does concern me is the insinuation of fabricating her data, so I want to tackle some of these allegations to see if I can help defend her reputation. If I do find evidence of fabrication, so be it. But the idea that “data can be faked therefore we cannot believe any of it” is absurd. All data can be faked. Of course they can. But if we followed this impetus to discount all data out of hand, then scientific research would very quickly stall. What we have here is a peer review process - of course the burden of proof is on Mimi to provide evidence of her claims, but she has done that with daily Strava uploads (already a big improvement over Rob Young). If you subsequently suspect the data are forged, the onus is on you to show evidence of that.&lt;/p&gt;
&lt;p&gt;There are certainly some inconsistencies that need addressing and I am hoping that I can address some of these here. I don’t for one minute believe that I am covering all of the issues here. I am sure there are many more that will be pointed out to me (at over 200 pages, I really don’t feel like wading through the entire thread to pull everything out), but I figured I would make a start. This is all fairly rough and these are basic analyses conducted over a very short period of time, but I hope to look into things using some more statistical methods in a follow-up post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-disclosure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/h1&gt;
&lt;p&gt;In full disclosure, I consider Mimi to be a friend. I have run with her many times, including running over 50 miles together on a recce for the &lt;a href=&#34;http://www.vikingwayultra.com&#34;&gt;Viking Way&lt;/a&gt;, and have seen first hand that she is an incredibly accomplished runner who has achieved amazing things. I don’t expect this to mean anything, and completely understand that plenty of people came out saying similar things for Robert Young, I just wanted to lay my cards in the table. I am, however, typically objective when it comes to data, so I am trying to look at this without letting too many of my biases interfere. For the record, I think that her pedigree and the changes seen in her body over the last few weeks should themselves clearly distinguish her from Rob Young. She is clearly doing &lt;em&gt;something&lt;/em&gt; out there and not just riding in an RV.&lt;/p&gt;
&lt;p&gt;Having said that, I am not of the opinion that anybody that questions these runs (or indeed anything) are haterz and trolls. Extraordinary claims require extraordinary evidence, and if you are not prepared to provide that and have it interrogated then you shouldn’t be doing it. With fame comes a loss of anonymity. I believe that mistakes were made at the start of this run, and I believe that the impulse to fight back against criticism with a similar tact used by Mark Vaz, Robert Young and Dave Reading was the wrong choice. But then I am a naive fool who thinks that everybody is reasonable and open to logical discourse - I suspect I am about to be schooled on this thought.&lt;/p&gt;
&lt;p&gt;In all honesty I have stayed away from this whole debacle for exactly that reason. I do not like the “them vs us” mentality that seems to crop up in all of these discussions. Be that Brits vs Americans, URC vs LR, whatever. I think that the LetsRun community have done an amazing job at routing out cheats over the years, and without them many people would be reaping the benefits of false claims. I have no problem with witch hunts. Witch hunts are only a problem if you don’t live in a world where witches demonstrably exist. I don’t want to feed into that, I don’t want to make enemies here - I just want to help a friend by providing an alternative perspective.&lt;/p&gt;
&lt;p&gt;My aim here then is to look (hopefully) objectively at some of the claims against Mimi in her transcon attempt. I work in a data-heavy scientific discipline, and I believe that I can remain objective in this, although it should be fairly clear already that I know what I am hoping to see. But believe me when I say that if I find something that I do not like I will not hide it. I do not claim anything I say is any more valid than what anybody else says, and I do not want confirmation bias to creep in from Mimi’s fellow supporters. I just want all of the facts to be available to allow people to make an informed assessment. If anyone disagrees with any of my conclusions, or if you identify errors, by all means get in touch and I will try and follow up.&lt;/p&gt;
&lt;p&gt;I decided to write this up as a blog post as the message that I put together for the LetsRun forum got a bit ridiculous, and I thought that this way I could attach and annotate my figures, flesh out my thoughts, and importantly include my code for full transparency.&lt;/p&gt;
&lt;p&gt;A lot of the data that I am showing here is based on runs between 1st October and 7th October. I chose these as these overlap with the faked data generated by &lt;code&gt;Scam_Watcheroo&lt;/code&gt; &lt;del&gt;a user on LetsRun (who also runs the &lt;a href=&#34;https://www.marathoninvestigation.com&#34;&gt;Marathon Investigation&lt;/a&gt; website)&lt;/del&gt; (EDIT) has looked at a lot of these data and made many of the claims of faked data. In addition, he was able to &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8462935&#34;&gt;generate spoofed data files&lt;/a&gt; for a fake world record run to show how easy it is to do, which I will incorporate into these analyses.&lt;/p&gt;
&lt;p&gt;I have also included a short run that Mimi did before beginning her transcon run, which is the only other run available on Strava (presumably this was done to test the data upload). I figured that this could be useful as a baseline of her “normal” running, but of course you could always argue that this was also fabricated.&lt;/p&gt;
&lt;p&gt;I obtained the &lt;code&gt;.gpx&lt;/code&gt; files directly from Strava by using &lt;a href=&#34;https://mapstogpx.com/strava/&#34;&gt;a tool&lt;/a&gt; that is able to slurp the data directly from the Strava session webpage. I can obviously repeat any of this for other data sets, but I had to start somewhere and my free time to look at these things is limited.&lt;/p&gt;
&lt;p&gt;For each day in this period, I downloaded runs for Mimi, Sandra, and the spoofed files. I also downloaded a few of my own runs (much shorter) as a baseline as I am quite confident that these files are genuine. My thesis is that the spoofed data should be identifiable as such, while the other data sets should (hopefully) stand up to scrutiny. Note that Sandra’s data are single runs per day, whereas Mimi uploads two per day.&lt;/p&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.r-project.org&#34;&gt;R&lt;/a&gt;, which is a freely available and well maintained statistical programming language, for these analyses. I haven’t had time to do too much yet, so here I am concentrating on the cadence data as that seems to be the most contentious point at the moment. In a follow up post I will look at the whole run thus far and look at more factors such as the distance traveled and correlations with terrain (which I haven’t even touched so far).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/h1&gt;
&lt;p&gt;First of all let’s load in the packages I will be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(XML)
library(plyr)
library(geosphere)
library(pheatmap)
library(ggplot2)
library(benford.analysis)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I will load in the &lt;code&gt;.gpx&lt;/code&gt; files. They are basically &lt;code&gt;.xml&lt;/code&gt; (Extensible Markup Language) files, so a bit of parsing is required to get them into a usable &lt;code&gt;data.frame&lt;/code&gt; format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## I/O directory
root_dir &amp;lt;- &amp;quot;../../static/post/2017-10-18-assessing-Mimi-Andersons-World_Record-run_files/&amp;quot;

## Load in the gpx data for each run
all_gpx &amp;lt;- list()
for (n in c(&amp;quot;Mine&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;MimiAnderson&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  dirname &amp;lt;- paste0(root_dir, n)
  all_fnames &amp;lt;- list.files(dirname)
  for (fname in all_fnames) {
    
    ## Load .gpx and parse to data.frame
    gpx_raw     &amp;lt;- xmlTreeParse(paste0(dirname, &amp;quot;/&amp;quot;, fname), useInternalNodes = TRUE)
    rootNode    &amp;lt;- xmlRoot(gpx_raw)
    gpx_rawlist &amp;lt;- xmlToList(rootNode)[[&amp;quot;trk&amp;quot;]]
    gpx_list    &amp;lt;- unlist(gpx_rawlist[names(gpx_rawlist) == &amp;quot;trkseg&amp;quot;], recursive = FALSE)
    gpx         &amp;lt;- do.call(rbind.fill, 
                           lapply(gpx_list, function(x) as.data.frame(t(unlist(x)), stringsAsFactors=F)))
    
    ## Convert cadence and GPS coordinates to numeric
    for (i in c(&amp;quot;extensions.cadence&amp;quot;, &amp;quot;.attrs.lat&amp;quot;, &amp;quot;.attrs.lon&amp;quot;)) {
      gpx[[i]] &amp;lt;- as.numeric(gpx[[i]])
    }
    
    ## Convert cadence to steps per minute
    gpx[[&amp;quot;extensions.cadence&amp;quot;]] &amp;lt;- gpx[[&amp;quot;extensions.cadence&amp;quot;]] * 2
    
    ## Convert time to POSIXct date-time format
    gpx[[&amp;quot;time&amp;quot;]] &amp;lt;- as.POSIXct(gpx[[&amp;quot;time&amp;quot;]], format=&amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;)
    
    ## Calculate the time difference between data points
    gpx[[&amp;quot;time.diff&amp;quot;]] &amp;lt;- c(0, (gpx[-1, &amp;quot;time&amp;quot;] - gpx[-nrow(gpx), &amp;quot;time&amp;quot;]))
    
    ## Calculate the shortest distance between successive points (in miles)
    gpx[[&amp;quot;dist.travelled&amp;quot;]] &amp;lt;- c(0, 
                                 distHaversine(gpx[-nrow(gpx), c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               gpx[-1, c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               r = 3959)) # Radius of earth in miles
    
    ## Save to main list
    all_gpx[[n]][[gsub(&amp;quot;\\.gpx&amp;quot;, &amp;quot;&amp;quot;, fname)]] &amp;lt;- gpx
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s consider some of the specific criticisms being made against Mimi.&lt;/p&gt;
&lt;div id=&#34;delayed-uploads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/h2&gt;
&lt;p&gt;One of the criticisms that comes up regarding Mimi’s practice is that it sometimes takes a long time for the data to be uploaded to Strava. I believe that it is typically up within a couple of hours (EDIT - there were also times when the uploads were not made for several days which is obviously more than a delay in tranferring the data), but many people suggest that anything longer than 15 minutes is unacceptable as it provides time to doctor the data. I mean, I guess that this is true, but it is my understanding that, given Mimi’s lack of technological prowess, her crew is using the method that requires the least amount of knowledge; that being syncing to a phone via Bluetooth, which will then upload to Movescount when there is a wifi or mobile data signal. I do this sometimes with my own runs and it takes bloody ages to sync, and that doesn’t take into account the time to then takes to upload it from the phone to Movescount, dealing with blackspots, etc. So it does not surprise me in the least that she rarely uploads things within 15 minutes of stopping. This is not evidence based by any means, just an observation from my own experience (and something others have pointed out). Is this good practice for somebody out to claim a world record? Perhaps not. Is it evidence of subterfuge? I don’t know, but personally I doubt it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-is-pausing-her-watch&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/h2&gt;
&lt;p&gt;In page 201 of the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&amp;amp;page=200&#34;&gt;LetsRun thread&lt;/a&gt;, user &lt;code&gt;So Far Today&lt;/code&gt; says the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elapsed time is total time from start of the day until end of the day. Sandy’s Strava is based on total elapsed time. Mimi’s excludes lunch breaks, and it appears it it also excludes other mini-breaks. If you want to figure out actual running time for Sandy remove the lunch breaks from the total elapsed time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, these guys have been looking into this in a heck of a lot more detail than I have, so I apologise if I have got this wrong here or misunderstood. But the idea that Mimi’s data excludes mini-breaks disagrees with something that I noticed right at the start of this analysis. Let’s look at the time difference between successive data points for Mimi’s data using the &lt;code&gt;table()&lt;/code&gt; function which will simply count the number of occurrences of each time delay between data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;MimiAnderson&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_170828_preUSA&lt;/p&gt;
&lt;p&gt;0 1 2 5
1 4160 4 1&lt;/p&gt;
&lt;p&gt;$MimiAnderson_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 25883    11 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 26174    31 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 29046    44     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 10926    12 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 30896     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3     4 
1 24567    19     1     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28343     6 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 18492    42 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 32400    34     2 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 26461     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     7 
1 27747    42     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 21215    36     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 32114    27 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171012_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28411    17 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So every run has one &lt;code&gt;0&lt;/code&gt; (which is a result of the way that I have calculated the time difference, so the first value is always 0), a few sporadic &lt;code&gt;2&lt;/code&gt;-&lt;code&gt;7&lt;/code&gt; sec intervals (presumably due to brief signal drop out and the like), but the vast majority are &lt;code&gt;1&lt;/code&gt; sec. This is because Mimi has her watch set to 1 sec recording, and leaves it on for the duration of the run. From this I suggest that the assertion made that Mimi stops her watch for lunch breaks etc. is false.&lt;/p&gt;
&lt;p&gt;My watch is set to the same sampling rate, and I see exactly the same thing with my data (albeit with fewer errant digits):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Mine&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$mine_170514&lt;/p&gt;
&lt;p&gt;0 1 2
1 9790 1&lt;/p&gt;
&lt;p&gt;$mine_170521&lt;/p&gt;
&lt;p&gt;0 1 2
1 6860 1&lt;/p&gt;
&lt;p&gt;$mine_170525&lt;/p&gt;
&lt;p&gt;0 1
1 3056&lt;/p&gt;
&lt;p&gt;$mine_170603&lt;/p&gt;
&lt;p&gt;0 1
1 5531&lt;/p&gt;
&lt;p&gt;$mine_170618&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 11297 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, however, when we look at Sandra’s data we see a completely different result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;SandraVi&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$SandraVi_171002&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
1 82 79 77 61 57 48 51 65 126 332 348 335 160 51 14 4 1
18 21 22 29 30 32 35 38 39 43 47 48 51 54 55 61 63 66
1 1 2 1 1 1 1 1 3 1 1 1 2 1 1 1 1 1
86 89 133
1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171003&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 243 238 233 187 192 172 210 305 1459 1424 401 142 54 40
15 16 17 18 19 20 21 22 23 24 25 27 28 30 31
17 7 1 2 4 1 4 2 1 2 2 1 1 1 1
33 34 35 36 37 39 40 41 44 45 46 47 49 50 51
2 1 1 1 1 1 2 1 1 1 1 1 1 1 1
52 53 55 57 58 59 60 63 64 65 66 73 79 82 83
2 3 3 1 1 1 1 1 1 1 1 1 1 1 1
88 89 97 100 102 113 118 119 133 136 162 166 168 183 223
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
249 266
1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171004&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 299 425 356 333 267 311 311 697 1708 1022 327 157 76 33
15 16 17 18 19 20 21 22 23 24 26 27 28 29 32
26 15 6 6 1 1 1 3 1 3 1 1 3 1 1
33 35 36 37 39 41 43 44 49 50 51 52 55 56 57
2 2 1 1 4 1 1 1 1 1 2 1 1 2 1
58 59 61 62 64 67 70 74 75 84 88 94 98 130 149
1 1 2 1 1 2 1 1 1 1 1 3 1 1 1
169
1&lt;/p&gt;
&lt;p&gt;$SandraVi_171005&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 173 167 157 133 156 129 158 400 1891 1393 259 109 53 55
15 16 17 18 19 20 21 22 23 24 26 27 29 30 32
16 6 5 2 1 5 1 1 2 1 1 1 1 1 1
33 34 35 36 37 39 41 44 46 49 50 52 61 68 74
2 1 1 1 2 1 1 1 1 1 3 1 1 2 1
75 76 93 128 138 140 145 153 163 233
1 1 1 1 2 1 1 2 2 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171006&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 112 128 123 132 115 122 101 291 1658 1696 358 85 42 33
15 16 17 18 19 20 24 27 28 32 35 37 42 46 49
8 5 4 2 1 2 1 2 1 2 1 1 1 1 2
51 57 58 60 65 67 76 77 78 86 88 90 96 98 99
1 1 1 1 1 1 1 1 1 1 2 1 1 1 1
106 108 109 126 128 129 147 164 173 196 205
1 1 1 1 1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171007&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 57 85 83 73 81 74 64 197 2038 1707 275 95 64 39
15 16 17 18 22 23 27 28 31 32 38 39 40 42 45
17 11 2 1 2 1 1 5 2 1 1 1 3 3 1
46 49 53 54 55 60 65 67 77 84 109 115 126 128 149
2 1 1 2 1 1 1 1 1 1 1 1 1 1 1
151 154 157 181 182 189 225
1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;There is no one time difference that stands out as the most common. Instead, the time differences between her data points span a large range, with the majority being about &lt;code&gt;8&lt;/code&gt;-&lt;code&gt;11&lt;/code&gt; secs apart. I suspect that this means that Sandra’s watch is set to sample every 10 seconds or so. In addition, there are a lot more longer pauses seen, sometimes up to 4 minutes. Whether this is a result of random fluctuations due to the higher sampling rate, or is the result of pausing the watch at certain times, I do not know. I am most definitely not suggesting there is anything wrong with this, I just think that the better approach is to leave it running the whole time, and it is Mimi who is doing this and not Sandra.&lt;/p&gt;
&lt;p&gt;Note also that this means that Sandra’s data therefore has an order of magnitude fewer data points than Mimi’s does, since Mimi’s data is less smoothed out. This can be seen if we calculate the number of data points for each run and then show the distribution for Mimi and Sandra separately (note that each of Mimi’s runs is actually only half of her distance for the day):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;)) {
  plot_dat[[n]] &amp;lt;- data.frame(Name = n,
                              nPoints = sapply(all_gpx[[n]], FUN = nrow))
}
plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
ggplot(aes(x = nPoints, fill = Name), data = plot_dat) + 
  geom_density(alpha = 0.25)                           +
  ggtitle(&amp;quot;Number of Data Points Per Run&amp;quot;)             +
  xlab(&amp;quot;Number of Data Points&amp;quot;)                        + 
  ylab(&amp;quot;Density&amp;quot;)                                      +
  theme(axis.title = element_text(size = 10), 
        plot.title = element_text(size = 15, 
                                  face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_vs_sandra_data_points-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’m not sure whether the people on LetsRun have been working on the assumption that both data sets were using the same parameters, but it is pretty clear to me that the sampling rate at least is different between the two runners. There may also be differences in the accuracy - perhaps the crews could confirm one way or another. Whilst the overall approach taken by Sandra is clearly the better of the two, the 1 sec sampling rate used by Mimi is the better option for the Strava data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dodgy-fluctuations-in-cadence-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/h2&gt;
&lt;p&gt;One issue that has been raised is the fact that in most of Mimi’s runs, we occasionally see severe fluctuations in the cadence, which spikes up above 200 at times. This is absolutely true, which can be seen when we plot the cadence values over time. The following function will plot the raw cadence values against the cumulative time (in secs) from the start of each run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time &amp;lt;- function (n, smooth = FALSE) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (smooth) { ## Smooths the data if requested - see below
      cad &amp;lt;- runmed(all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]], 11)
    } else {
      cad &amp;lt;- all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]
    }
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Time    = cumsum(all_gpx[[n]][[r]][[&amp;quot;time.diff&amp;quot;]]),
                                Cadence = cad)
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  ggplot(aes(x = Time, y = Cadence, color = Run), data = plot_dat) + 
    geom_point()                                                   + 
    facet_grid(Run ~ .)                                            + 
    ggtitle(paste(n, &amp;quot;Cadence Over Time&amp;quot;))                         + 
    xlab(&amp;quot;Time (sec)&amp;quot;)                                             + 
    ylab(&amp;quot;Cadence (spm)&amp;quot;)                                          +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))                + 
    ylim(0,500) # Limits the plot to a maximum of 500 which will exclude a small number of outliers for Mimi
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can look at Mimi’s raw cadence over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 120 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there is no denying that these very high cadence values do exist in Mimi’s data. If, however, we look at Sandra’s data we do not see as many of these fluctuations. However, fluctuations are indeed still present, particularly for the shorter day on 2nd October, although they are nowhere near as high as those of Mimi (250 rather than 500):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/sandra_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that here we can make out the lunch breaks in Sandra’s data, whereas Mimi has her runs split into morning and afternoon.&lt;/p&gt;
&lt;p&gt;However, as discussed above, Mimi’s data is much deeper than Sandra’s. Sandra’s data has already undergone some smoothing, so it is likely that these blips are cancelled out by smoothing over a 10 second interval. Indeed, if we smooth Mimi’s data using a running median over an 11 sec window (which replaces the data points with a running average of the data point with the 5 data points either side) to approximate the 10 sec capture, we indeed see a much smoother distribution with these extreme values reduced to be more in keeping with what we see for Sandra.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;, smooth = TRUE)
## Warning: Removed 85 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It would appear that these very high fluctuations are a result of the increased sampling rate, although I do note that I do not see these sorts of fluctuations in my own 1 sec capture data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/my_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This may simply be due to the fact that I am not running through so many different areas, many of which may have different GPS signals that affect the capture. Another possibility is that the accuracy of our watches is set to different modes. Mine is set to “Best”, but I have no idea what Mimi’s is set to. One idea that I had was to ask her to set one of her watches to 10 sec capture and upload both in parallel at the end of one of her runs. However, unfortunately it seems that this is no more an option. I am going to hunt through to find some longer runs in my personal data to see if this crops up in any of my more remote jaunts, but for now I don’t have an answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-running-in-the-185-195-steps-per-minute-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/h2&gt;
&lt;p&gt;Continuing with the cadence data, another issue that has cropped up several times is the fact that Mimi regularly runs in the 185-195 spm. Indeed, if we look at the distribution of the cadence in a histogram rather than looking at it over time, this certainly seems to be the case.&lt;/p&gt;
&lt;p&gt;The following function will plot the above data as a series of overlaid density plots (note that I am smoothing the density estimates here slightly to make the overall distribution clearer and less spiky for the samples with fewer data points):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density &amp;lt;- function (n) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (r == &amp;quot;MimiAnderson_170828_preUSA&amp;quot;) next # Skip the pre-transcon run
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Cadence = all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]])
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  vwalk    &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 100 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 150]) ## Walking
  vrun     &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 150 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 200]) ## Running
  ggplot(aes(x = Cadence, color = Run), data = plot_dat) + 
    geom_density(alpha = 0.25, adjust = 3)               + 
    xlim(0,300)                                          + 
    ggtitle(paste(n, &amp;quot;Cadence Distribution&amp;quot;))            + 
    xlab(&amp;quot;Cadence (spm)&amp;quot;)                                + 
    ylab (&amp;quot;Density&amp;quot;)                                     +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))      + 
    geom_vline(xintercept = vwalk)                       + 
    geom_vline(xintercept = vrun)                        +
    annotate(&amp;quot;text&amp;quot;, x = vwalk, y = 0.05, 
             angle = 90, label = paste(vwalk, &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)                     +
    annotate(&amp;quot;text&amp;quot;, x = vrun,  y = 0.05, 
             angle = 90, label = paste(vrun,  &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am going annotate the peaks of these plots using a very basic method of taking the modal value (the one that occurs the most) over the entire data set for the walking and running distributions (very roughly defined, but as long as the modal value lies ion the range it should give the “correct” answer) . To do this, however, I need a function to calculate the mode since one does not exist in base R (for some odd reason):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mode &amp;lt;- function(v) {
   uniqv &amp;lt;- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s see the distribution for Mimi:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 244 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So Mimi runs with a cadence of around 134 spm for running and 182 spm for running. Now let’s look at Sandra’s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/sandra_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sandra has generally lower cadence of 120 spm for walking, and 170 spm for running. She also appears from this to walk a lot less than Mimi, who seems to do a fairly even split between running and walking in general. In addition, the variation of Mimi’s running cadence is much higher than Sandra’s, so it appears that Sandra tends to run at a relatively constant cadence with a small amount of walking, whereas Mimi’s is much more variable and seems to be split in a 50:50 run/walk. Together with the longer differences between successive time-points, this may indicate that Sandra’s watch is set to pause automatically below a certain speed.&lt;/p&gt;
&lt;p&gt;I also decided to look at my own data to see what that looked like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/my_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I also run with a fairly high cadence (just lower than Mimi’s but not dissimilar), and see more variation than Sandra. Now obviously I am not running across a continent in these runs - I am usually running with a belligerent dog who insists on stopping to sniff every bloody tree on the way. But it is not too dissimilar, and I see the distribution spreads out over 200 for some of the readings just like with Mimi. I’m an okay runner - probably not particularly good compared to many of the posters on LetsRun, but I do okay at shorter stuff and longer stuff. But it’s just a hobby for me, so I’m perfectly happy to self-associate as a hobby jogger. I don’t really know much about cadence, so I’m not sure if averaging 180+ is high or not? If nobody had suggested this was “garbage” and unbelievable, I would just assume that Mimi had a higher than normal cadence, similar to my own. I am a forefoot runner, and I think that Mimi is as well, and I believe that higher cadence tends to go hand in hand, but I am happy to bow to the experience of people more knowledgeable than myself in the matter.&lt;/p&gt;
&lt;p&gt;To get an idea of the level of variation in the data (specifically the running cadence), let’s look at some aspects of that main distribution (excluding the outliers):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;)) {
  cad &amp;lt;- unlist(lapply(all_gpx[[n]], FUN = function (x) x[[&amp;quot;extensions.cadence&amp;quot;]]))
  cad &amp;lt;- cad[cad &amp;gt; 150 &amp;amp; cad &amp;lt; 250]
  cat(sprintf(&amp;quot;%12s: mode   = %d\n&amp;quot;, n, Mode(cad)))
  cat(sprintf(&amp;quot;%12s: mean   = %.2f\n&amp;quot;, n, mean(cad)))
  cat(sprintf(&amp;quot;%12s: median = %d\n&amp;quot;, n, median(cad)))
  cat(sprintf(&amp;quot;%12s: SD     = %.2f\n&amp;quot;, n, sd(cad)))
  cat(sprintf(&amp;quot;%12s: SEM    = %.3f\n&amp;quot;, n, sd(cad)/sqrt(length(cad))))
  cat(&amp;quot;\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MimiAnderson: mode = 182
MimiAnderson: mean = 181.56
MimiAnderson: median = 182
MimiAnderson: SD = 11.07
MimiAnderson: SEM = 0.026&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SandraVi: mode   = 170
SandraVi: mean   = 171.13
SandraVi: median = 170
SandraVi: SD     = 4.54
SandraVi: SEM    = 0.029

    Mine: mode   = 178
    Mine: mean   = 177.96
    Mine: median = 178
    Mine: SD     = 7.53
    Mine: SEM    = 0.042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So clearly the standard deviation (SD) is much higher for Mimi’s data, but the standard error of the mean (SEM) is actually pretty comparable. SD and SEM, whilst both estimates of variability, tell you different things. The standard deviation is simply a measure of how different each individual data point is from the mean. It is descriptive of the data at hand. The SEM on the other hand is a measure of how far the mean of your sample is likely to be from the true population mean (under the assumption that each run is a random sampling of cadence values given Mimi’s true “normal” cadence). As your sample size increases, you more closely estimate the true mean of the population. This tells us that there is high variability in the sampling of cadence values for Mimi, but the precision is comparable with Sandra’s. This suggests nothing of whether the mean itself is actually believable of course, it is just worth noting the benefits of the increased sampling in these data.&lt;/p&gt;
&lt;p&gt;So my overall feeling is that, whilst high, this was just the natural running gait of Mimi. Given recent events, this entire post ended up being highly expedited so that &lt;em&gt;something&lt;/em&gt; was out there to provide a counter point to the accusations that have been made about data forgery and cheating, so in a rushed effort I looked around for some video of Mimi running to get an idea of her natural cadence. I found &lt;a href=&#34;https://www.youtube.com/watch?v=2eGb_goiPfc&#34;&gt;this video of her running at the end of her 7-day treadmill record&lt;/a&gt;. For the 18 seconds between 0:20 (when she begins to run properly) and 0:38 (when the camera pans away) I count 27-28 swings of her left hand/steps with her right foot, which would equate to a cadence of 180-187. Similarly, for the 16 seconds between 0:59 and 1:15, I count 24-25 swings/steps , which also equates to a cadence of 180-187. I’m not saying this is definitive proof, but this is at least evidence of her running with cadence similar to her average cadence across the USA, even at the end of 7 days on a treadmill. Adrenelin and a “sprint finish” mentality may play a role in achieving this as well of course. I would like to see more evidence of her running, and hopefully we will see some of that from the film crew that was with Mimi.&lt;/p&gt;
&lt;p&gt;So here I have shown that, yes Mimi runs with a higher cadence than Sandra, but there is evidence that this is simply her natural gait. As to the fact that she regularly runs in the 185-195 range; well yes she does, but so do I. And I am no elite, particularly for these particular runs which are fairly perambulatory if I am honest. I can assure you that I have not doctored these data to be this mediocre. You can look through and even work out the points where my dog stopped to piss up a tree if you like. It’s not proof, but it is evidence.&lt;/p&gt;
&lt;p&gt;I also wanted to look a bit into how the cadence actually corresponds to the speed at which the women are running. Below is a distribution plot showing the pace at each time point for each of the data sets considered here. Notice that I am excluding the data points where the runners are not moving to avoid divide by 0 errors in the pace calculation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_cor_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  cor_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    cor_dat[[r]]          &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;time.diff&amp;quot;, &amp;quot;dist.travelled&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
    cor_dat[[r]][[&amp;quot;Run&amp;quot;]]  &amp;lt;- r
    cor_dat[[r]][[&amp;quot;Pace&amp;quot;]] &amp;lt;- (cor_dat[[r]][[&amp;quot;time.diff&amp;quot;]]/60)/(cor_dat[[r]][[&amp;quot;dist.travelled&amp;quot;]])
  }
  cor_dat &amp;lt;- do.call(rbind.data.frame, cor_dat)
  cor_dat[[&amp;quot;Name&amp;quot;]] &amp;lt;- n
  cor_dat           &amp;lt;- subset(cor_dat, dist.travelled != 0)
  all_cor_dat[[n]]  &amp;lt;- cor_dat
}
all_cor_dat &amp;lt;- do.call(rbind.data.frame, all_cor_dat)
ggplot(aes(x = Pace, color = Name), data = all_cor_dat) +
  geom_density(alpha = 0.25) +
  xlim(0, 20) +
  xlab(&amp;quot;Pace (min/mile)&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;) +
  ggtitle(&amp;quot;Pace Comparison Between Data Sets&amp;quot;) +
  theme(axis.title = element_text(size = 10),
        plot.title = element_text(size = 15, face = &amp;quot;bold&amp;quot;))
## Warning: Removed 53290 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/cadence_vs_speed-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, we can see that there is a big difference in how the women are approaching the race. As noted before, Mimi runs in a fairly even 50:50 split of running and walking. This graph confirms that with a fairly even split between faster running of about 8.5 mins/mile and slower walking of about 11 mins/mile. Sandra on the other hand appears to be very steady in her approach, moving consistently at a 170 spm cadence run of about 11.5 mins/mile. This was earlier in the run and no doubt changed over time as Sandra began to close in on Mimi over the past week. My runs are predominantly spent jogging at around 8 mins/mile (with the occasional downhill thrown in for fun). The distribution for the fake data however does not follow the same type of distribution as the other runs (with a clearly delineated multimodal distribution for run/walk/sprint segments), and again stands out when compared with the ostensibly genuine data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spoofed-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/h2&gt;
&lt;p&gt;So now I am getting to the nitty gritty of this post. The main accusation that I am attempting to quash is that of doctoring of the data. I have no answers regarding other perceived issues with the run, but the doctoring accusation I believe is a step too far. I have never denied that it would be possible to spoof the data. Of course it would. They are raw text files containing numbers - nothing more impressive than that. I did however think that spoofing it through Movescount would be very difficult, but it seems that I was wrong about that. It’s not simple to do, but it is doable with a little bit of know-how.&lt;/p&gt;
&lt;p&gt;However, I do believe that it would be impossible to generate spoofed data that did not stand out as such when compared with genuine data. Faking data is notoriously hard. That’s not to say that people don’t do it all of the time, and sometimes it takes a while to pick up on. But I think that creating data out of thin air that also matched with what is going on with the tracker (I appreciate people have issues with the tracker, but I’m not getting into that), what is going on with reports from the crew, matched with environmental effects and the terrain that she was running over, what will ultimately come out from the film crew, and importantly what is self consistent, would be near impossible to manage. The cadence and times would have to make sense given the position, terrain and environmental effects into account. LetsRun user &lt;code&gt;Scam_Watcheroo&lt;/code&gt; developed a tool to spoof the data, but he had the benefit of being able to track the things that might give the data away in advance. Mimi would have had to develop her method (or more accurately get somebody else to develop the method) blind, with no idea what sort of things might show it up as being faked. Sounds incredibly risky to me. So in thus section, I wanted to look at a few things to see if the different data sets stand up to scrutiny.&lt;/p&gt;
&lt;p&gt;I am only touching the surface here, and I am looking into some more in depth methods to run statistical tests over the entire data set so far to check that the data are consistent and show the patterns one would expect. My hope in advance was that doing this would highlight the faked data as such. So to start with, I am not looking at consistency between the data sets, I am merely looking at the raw cadence data to look at a few potential things that might highlight anything that looked incongruous.&lt;/p&gt;
&lt;p&gt;First of all I went back to simply looking at the time differences between the data points. For my data and Mimi’s data, they are almost all 1s differences, but there is the occasional blip (presumably when it is not able to update with the satellite straight away) leading to a few counts of around 2-7 secs. The spoofed data has none of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Fake&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;25895 25894&lt;/p&gt;
&lt;p&gt;$Fake_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;26206 26205&lt;/p&gt;
&lt;p&gt;$Fake_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;29092 29091&lt;/p&gt;
&lt;p&gt;$Fake_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 19626 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 27499 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most recent ones (7th October) which were I think generated from scratch are ALL 1s differences, whilst the 2nd October ones (which were generated based on fiddling with Mimi’s uploads) were split half and half between 0s and 1s (a 0.5s sampling rate perhaps?). Either way, they do not have these little blips – the faked data appear to be too perfect. Being able to account for this and other such data imperfections heuristically (especially without knowing ahead of time that one would need to) would be bloody difficult and very very risky in my opinion.&lt;/p&gt;
&lt;p&gt;I am also looking at how well the spoofed data stand up to scrutiny using some other methods. One obvious test would be to see whether the cadence data obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Benford%27s_law&#34;&gt;Benford’s Law&lt;/a&gt;, which shows that the first digits (and indeed second digits, third digits, etc.) have a unique logarithmic distribution such that smaller numbers are more likely than bigger numbers. Notice here that I am looking at &lt;em&gt;half&lt;/em&gt; of the cadence, since the actual reported data in the &lt;code&gt;.gpx&lt;/code&gt; file was doubled to give the cadence in spm. However, in this case the first digits are somewhat constrained, since the cadence is typically in a very narrow range resulting in a huge proportion of 8s and 9s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/benford-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about if we look at the second digit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(as.numeric(substr(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, 2, 2)), number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/benford_2digit-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is now approaching a more standardised distribution. It does not appear to follow Benford’s Law, but instead these data appear to be somewhat uniformly distributed. One idea that I have looked at is whether the trailing (and thus least significant) digits of the cadence data follow any particular distribution. Perhaps one might imagine that they should follow &lt;em&gt;some&lt;/em&gt; distribution, such as the more uniform distribution seen above. Again remember here that I am plotting the raw cadence data, which is &lt;em&gt;half&lt;/em&gt; of the cadence values reported in the distribution plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Look at the final digit of the cadence data
all_digit_counts &amp;lt;- list()
all_digit_percent &amp;lt;- list()
for (n in names(all_gpx)) {
  all_digit &amp;lt;- matrix(0, ncol = 10, nrow = length(all_gpx[[n]]), dimnames = list(names(all_gpx[[n]]), as.character(0:9)))
  for (r in names(all_gpx[[n]])) {
      digit &amp;lt;- gsub(&amp;quot;^\\d*(\\d)$&amp;quot;, &amp;quot;\\1&amp;quot;, all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]/2)
      all_digit[r, ] &amp;lt;- table(digit)[as.character(0:9)]
  }
  all_digit_counts[[n]]  &amp;lt;- all_digit
  all_digit_percent[[n]] &amp;lt;- 100*all_digit/rowSums(all_digit)
}
  
## Plot heatmap
hm_dat &amp;lt;- do.call(rbind.data.frame, all_digit_percent)
rownames(hm_dat) &amp;lt;- gsub(&amp;quot;^.*\\.&amp;quot;, &amp;quot;&amp;quot;, rownames(hm_dat))
pheatmap(hm_dat, cluster_rows = FALSE, cluster_cols = FALSE, main = &amp;quot;Cadence - trailing digit percentage&amp;quot;)#, display.numbers = FALSE,  annotation_legend = TRUE, cluster_cols = TRUE, show_colnames = TRUE, show_rownames = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/final_digit_dist-1.png&#34; width=&#34;1800&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows the distribution of the percentage of trailing digits seen amongst the different data sets – Mimi’s, Sandra’s, the spoofed data, and some of my own runs. The colour of the heatmap indicates the percentage of times that digit is seen in the final position, with blue being less often and red being more often. In general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mine seem to be quite uniformly distributed (mainly blue)&lt;/li&gt;
&lt;li&gt;Mimi’s are pretty uniform (except for the pre-USA run that I put in as well) with the exception of a regular depletion of 2s and 5s&lt;/li&gt;
&lt;li&gt;Sandra’s seem to show a depletion of 1s and 9s, and enrichment of several digits in most runs (but nothing consistent)&lt;/li&gt;
&lt;li&gt;The fake data however seem very consistent in their prevalence of 6s and 7s (and to a lesser extent 4s and 8s).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is not really enough data here to identify a pattern, but from what is here the spoofed data stands out with a distribution that is different from that seen with my own data. Mimi’s is actually the most alike to data that I know to be real, although the depletion of certain digits is quite odd. But then the same is true for Sandra’s data as well to a greater degree (albeit different numbers). I plan to look into this in more detail using more data, including all of Sandra’s and Mimi’s runs, and a lot more genuine data taken from Strava as a base line to see if this distribution holds.&lt;/p&gt;
&lt;p&gt;Obviously none of this “proves” these data are not fabricated. That is impossible. I do however thus far see no sufficient evidence to suggest that these data are not real (for both runners, although Sandra was never on “trial” here). And really I have only scratched the surface on how to test the validity of these data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;I am very saddened about what has happened to Mimi on this journey. There are questions that I hope will be answered regarding certain aspects of the run, and I’m sure that these by themselves are enough to convince some people of wrong-doings. But I truly do not believe that Mimi has set out to flim flam, bamboozle or otherwise beffudle people into believing that she ran across America when she didn’t. I wrote this post before Mimi announced her intentions to stop, but the damage that she has done to herself must surely be evidence that she is doing it. And if you accept that the Strava data are genuine, there is no way to deny what she has done. Perhaps what I have introduced here will help a little to bring more people to that way of thinking, but others likely need more convincing. I will continue to try to provide reasoned explanations for some of the remaining inconsistencies where I can.&lt;/p&gt;
&lt;p&gt;So my feeling is that there is a zebra hunt going on here. What is more likely; that a 55 year old grandma is running across the country at world record pace, or that she has convinced several people to go on a month-long trip to fake a run, and in the process developed an incredibly sophisticated piece of software (which accounts for specific nuances) to spoof the data (even though she is clearly doing &lt;em&gt;something&lt;/em&gt; out there as she is losing weight and suffering exactly as one might expect for somebody running across America)? I’m going with the world record grandma. I do not think that Mimi is a witch.&lt;/p&gt;
&lt;p&gt;There are likely many questions outstanding which I have not addressed here. This is a fairly rudimentary piece of work compared to the amount of time and effort that others have put into looking into this. I am interested to look into &lt;code&gt;Scam_Watcheroo&#39;s&lt;/code&gt; blog post about this to see what other issues he addresses. I would also like to look at how Mimi’s performance changed over time, and in particular how it changed following the LetsRun forum taking off and her ultimate switch from Race Drone to the Garmin tracker. In addition, something that I have not considered is whether or not the data are modified in the move from MovesCount to Strava. Although the overal trends would not change drastically, the raw data themselves (and therefore the digit distributions) might. This is probably worth considering in due time.&lt;/p&gt;
&lt;p&gt;I genuinely hope that this is useful to some of you in addressing some of the concerns. Nothing I can do can change people’s opinions on what happened in the first 2 weeks, but I hope that I can at least start to alleviate fears that there is any duplicity in the Strava data. It is all academic now following Mimi’s recent announcement that she will be pulling out from the event, but hopefully this should also help to assure people of the validity of Sandra’s run as well (although her regular updating and constant tracking have allayed any such fears already). All I can do now is wish Sandra good luck in getting the record (she looks to be on excellent form), and wish Mimi a very speedy recovery. It is sad that it had to go like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There are aspects of the spoofed data that make it stand out when compared to Mimi’s and Sandra’s (and my own) data&lt;/li&gt;
&lt;li&gt;I just do not think that it would be possible to create a forged data set that stands up to intense scrutiny - this is fairly basic scrutiny and it stands out&lt;/li&gt;
&lt;li&gt;Mimi is using 1s capture mode on constant capture for her runs, with the very occasional 3 or 4 sec delay&lt;/li&gt;
&lt;li&gt;Sandra is using 10s capture and has longer pauses in data retrieval of several minutes at a time (auto-pause?)&lt;/li&gt;
&lt;li&gt;Mimi’s data sets are therefore an order of magnitude denser than Sandra’s&lt;/li&gt;
&lt;li&gt;Mimi’s cadence blips of 200+ spm are likely just random c*ck ups in data capture - they disappear if you smooth out to a 10s capture rate (I was trying to contact her crew to ask her to set a second watch to 10s capture for one of her runs to confirm this, but unfortunately it was too late)&lt;/li&gt;
&lt;li&gt;You probably don’t see them for Sandra because they get averaged out&lt;/li&gt;
&lt;li&gt;Mimi is running with a high cadence but the average seems to fit with previous evidence (albeit very limited and definitely open to scrutiny) of her running gait (evidence from the film crew videos in the future will also help if/when released)&lt;/li&gt;
&lt;li&gt;Mimi is often running in 185-195 range, but then so do I - granted I am not running across a continent on a day by day basis, but I am also nowhere near elite&lt;/li&gt;
&lt;li&gt;Mimi’s cadence is about 10 spm quicker than Sandra’s for both walking and running&lt;/li&gt;
&lt;li&gt;Mimi seems to have a fairly even split of walking and running, whilst Sandra seems to consistently run but at a slower overall pace&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inhibition of the acetyltransferase NAT10 normalizes progeric and aging cells by rebalancing the Transportin-1 nuclear import pathway</title>
      <link>/publication/2018_inhibition_of_the_acetyltransferase_nat10_normalizes_progeric_and_aging_cells_by_rebalancing_the_transportin-1_nuclear_import_pathway/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/publication/2018_inhibition_of_the_acetyltransferase_nat10_normalizes_progeric_and_aging_cells_by_rebalancing_the_transportin-1_nuclear_import_pathway/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How Predictable Are Ultra Runners?</title>
      <link>/post/2018-04-24-how-predictable-are-ultrarunners/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-24-how-predictable-are-ultrarunners/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#natural-language-processing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-improvements&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Model Improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;This post is a continuation from my &lt;a href=&#34;/post/2018-04-18-Suunto-Or-Garmin/index.html&#34;&gt;previous post&lt;/a&gt;, looking at various aspects of the posting habits of the Ultra Running Community (URC). This was originally intended to be just an additional section in that blog post, but it was getting a little unwieldy so I decided to split it off into its own post to stop it from getting too crazy. This post is probably a lot less interesting than the last post, as it is really looking at one central question; can I predict which group member is posting based on the content of the message? Spoiler alert, you can’t! The posters on this forum are apparently &lt;em&gt;all&lt;/em&gt; sarcastic SOBs, so it is difficult to pick them apart. But it was quite interesting trying.&lt;/p&gt;
&lt;p&gt;So as a bit of an experiment, I decided to play around with these data to see if the language that people use in their posts is specific enough to allow us to predict who has written something based on what the post says. This is a job for &lt;em&gt;machine-learning&lt;/em&gt;, which is really a lot less grandiose than it sounds. Essentially what we are doing here is using data to train a model of some description that can be applied to a new set of data to make predictions. In this case, we are looking to fit a model that is able to classify posts into one of a number of groups, where each group represents a single user. As an example of a classification problem, think of the spam filter in your email client. This is essentially a model that has been trained to look at the email message content and determine whether it is spam or not (e.g. if it is full of words like &lt;em&gt;viagra&lt;/em&gt;, &lt;em&gt;Nigerian Prince&lt;/em&gt;, &lt;em&gt;penis enlargement&lt;/em&gt;, &lt;em&gt;make money today&lt;/em&gt;, etc. then it is clearly all kosher). This would be a 2-class classification problem.&lt;/p&gt;
&lt;p&gt;For classification problems such as this, we require a training set on which to fit our model, and a validation set to determine the quality of the model. The validation set must be independent of the training set, as we want to test how the model will generalize to new data. The idea of &lt;em&gt;cross validation&lt;/em&gt; is essentially to split your training data into a training set and a validation set such that the validation is independent of the model fitting (to avoid the effects of over-fitting in the training set). There are various ways to split your data in this way. For now I will simply randomly select a subset for training and a smaller subset for validation (the &lt;em&gt;Holdout Method&lt;/em&gt;), but for true cross-validation this should then be repeated several times so that the average over several validation sets is used. For example, in &lt;em&gt;k-fold cross validation&lt;/em&gt; you would randomly distribute the data into &lt;code&gt;k&lt;/code&gt; equally sized subsets, and use exactly one of these as the validation set and &lt;code&gt;k-1&lt;/code&gt; as the training set. This is then repeated &lt;code&gt;k&lt;/code&gt; times, each time using a different subset as the validation set.&lt;/p&gt;
&lt;p&gt;It makes sense to restrict this analysis to the most active posters, and so I will limit the analysis to only users who have contributed 50 or more posts to the forum. This gives us 5,233 posts, from 48 users. I will randomly select 4,000 posts for the training set, and use the remainder for validation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts50 &amp;lt;- URC                 %&amp;gt;% 
           group_by(from_name) %&amp;gt;%    ## Group by poster
           filter(n() &amp;gt;= 50)   %&amp;gt;%    ## Select only posters with &amp;gt;50 posts
           select(from_name, message) ## Keep poster name and message content
set.seed(0) ## Set seed for random number generation for reproducibility
ids   &amp;lt;- sample(1:nrow(posts50), 4000) ## Randomly select 4000
train &amp;lt;- posts50[ids,]  ## Keep random ids as training set
test  &amp;lt;- posts50[-ids,] ## Use remaining ids as validation&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;natural-language-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Natural Language Processing&lt;/h1&gt;
&lt;p&gt;The model that we will be using is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bag-of-words_model&#34;&gt;Bag Of Words&lt;/a&gt; model, which is a natural language processing technique that aims to represent text based on the frequency of words within it. There are some things that we can do to reduce the vector space of available terms, such as removing capital letters and removing so called “stop words” (common words like “is”, “and”, “but”, “the”, etc.). We can also limit the analysis to only words that occur frequently in the text, although there is a possibility of missing specific terms used by only one or two individuals, say, that may help the predictiveness of the model.&lt;/p&gt;
&lt;p&gt;I will be using the &lt;a href=&#34;https://cran.r-project.org/web/packages/text2vec&#34;&gt;text2vec&lt;/a&gt; package in R which is efficient at generating the required document-term matrix (DTM) for fitting our model. In particular, it generates unique tokens for each term rather than using the terms themselves, which reduces computational overheads. An iterative function can then be applied to generate the DTM. So let’s generate such an iterator over the term tokens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(text2vec)
train_tokens &amp;lt;- train$message                      %&amp;gt;%
                iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;% # Convert to ASCII format
                tolower                            %&amp;gt;% # Make lower case
                word_tokenizer                         # Break terms into tokens
it_train &amp;lt;- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;itoken&amp;gt;
##   Inherits from: &amp;lt;iterator&amp;gt;
##   Public:
##     chunk_size: 400
##     clone: function (deep = FALSE) 
##     counter: 0
##     ids: Jean-François Tantin Gary Kiernan Richard Lendon Iain Ed ...
##     initialize: function (iterable, ids = NULL, n_chunks = 10, progress_ = interactive(), 
##     is_complete: active binding
##     iterable: list
##     length: active binding
##     nextElem: function () 
##     preprocessor: list
##     progress: FALSE
##     progressbar: NULL
##     tokenizer: list&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we use this iterator to create a vocabulary DTM for fitting the model. To start with, I will use all of the words, but later we could look at filtering out stop words and less frequent terms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vocab      &amp;lt;- create_vocabulary(it_train)
vectorizer &amp;lt;- vocab_vectorizer(vocab)
train_dtm  &amp;lt;- create_dtm(it_train, vectorizer)
dim(train_dtm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4000 13922&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a matrix with 4,000 rows (the number of messages in the training set) and 13,922 columns (the number of unique terms in the training set). So each message is now represented as a vector of counts for all possible terms in the search space. The hope now is that we will be able to fit a model that is able to discriminate different users based on their word usage. Unlikely, but hey let’s give it a shot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Random Forest&lt;/h1&gt;
&lt;p&gt;In this case, our dependent variable is the name of the user who posted the message which is a categorical variable. The independent variables are the counts for each of the 13,922 terms across the data set. I am going to start by using a random forest model, which is one of the more popular classification models available. A decision tree is a quite simple (although incredibly powerful) stepwise model that you can think of like a flow chart. The model fitting will create a series of nodes where your independent variables are used to discrimate between one choice and another, eventually leading to a certain prediction depending on the values of the variables in your model. A random forest essentially fits a whole load of these classification decision trees and outputs the &lt;em&gt;modal&lt;/em&gt; (most common) class across all of them.&lt;/p&gt;
&lt;p&gt;One benefit of using random forests over something like generalised linear models (see later) is that, since they rely on fairly independent tests at each stage in the tree, they are more robust to correlated variables in the model. With such a large set of term variables there is undoubtedly correlation between many of these terms, particularly as many of these variables are likely to be largely made of zeroes. Of course, this sparsity itself causes somewhat of a problem, and should be taken into account in the analysis. But for now I will ignore it and just hope that it isn’t a problem…&lt;/p&gt;
&lt;p&gt;To begin with,let’s fit a simple random forest model and see how it looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;randomForest&amp;quot;)
library(&amp;quot;caret&amp;quot;)
rf_model &amp;lt;- randomForest(x = as.matrix(train_dtm), y = as.factor(rownames(train_dtm)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I set the &lt;code&gt;y&lt;/code&gt; parameter to be a factor so that it is clear that this is a classification model. Now we can test our model by seeing how it performs at predicting the user for our test data set. First we generate a similar DTM for the test data set. Note that we use the same &lt;code&gt;vectorizer&lt;/code&gt; as we used for the training set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_tokens &amp;lt;- test$message                       %&amp;gt;%
               iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;% # Convert to ASCII format
               tolower                            %&amp;gt;% # Make lower case
               word_tokenizer                         # Break terms into tokens
it_test     &amp;lt;- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    &amp;lt;- create_dtm(it_test, vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we use our model to predict the user for each of the posts in our test data set. To do this we use the &lt;code&gt;predict()&lt;/code&gt; method for &lt;code&gt;randomForest&lt;/code&gt; objects, and output the response class with the majority vote amongst all of the decision trees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_predict &amp;lt;- predict.train(rf_model, test_dtm, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, how did we do? Let’s see how many of these were correctly predicted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test_predict == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   760   473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this model predicts the poster only 38.4 % of the time, which isn’t particularly good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-improvements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Model Improvements&lt;/h1&gt;
&lt;p&gt;So can we improve this? Yes, probably. The first thing that I can try is to be a little more clever in the way that I parameterise the data. So rather than simply counting words, I will instead use &lt;em&gt;n-grams&lt;/em&gt; – combinations of &lt;code&gt;n&lt;/code&gt; words that will be more sensitive to the types of phrases that different people typically use. Obviously increasing &lt;code&gt;n&lt;/code&gt; in this case will also increase the memory and run time considerably, so there are limits to what we can feasibly do. Also, it is probably worth noting that removal of stop words is less likely to be the best way to go about this, as this will affect the structure of the n-grams. So this time let us leave the stop words in, but parameterise with &lt;code&gt;3-grams&lt;/code&gt;. I will also limit the count to those n-grams used at least 10 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vocab &amp;lt;- create_vocabulary(it_train, ngram = c(1L, 3L)) ## use 1-, 2- and 3-grams
vocab &amp;lt;- vocab %&amp;gt;% 
         prune_vocabulary(term_count_min = 10) ## Only keep n-grams with count greater than 10
vectorizer &amp;lt;- vocab_vectorizer(vocab)
dtm_train  &amp;lt;- create_dtm(it_train, vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that we used the notation &lt;code&gt;1L, 3L&lt;/code&gt;, which tells &lt;code&gt;R&lt;/code&gt; to explicitly use integer values rather than numeric values. In many cases this has little to no effect, but in programming an integer variable will take up much less memory (4 bytes per element) than a double precision floating point number (8 bytes per element).&lt;/p&gt;
&lt;p&gt;Another thing that we can do to improve the model fit is that we can attempt to normalise our DTM to account for the fact that different Facebook messages may be longer or shorter than others. Typically the “documents” in this case (the messages) are very small so I imagine this will have only a minimal effect. Here I will use the &lt;em&gt;term frequency-inverse document frequency&lt;/em&gt; (TF-IDF) transformation. The idea here is to not only normalise the data, but also to scale the terms such that terms that are more common (i.e. those used regularly in all posts) are down-weighted, whilst those that are more specific to a small number of users (and will thus be more predictive) are up-weighted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tfidf           &amp;lt;- TfIdf$new()
train_dtm_tfidf &amp;lt;- fit_transform(train_dtm, tfidf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally there is some fine tunning that can be made to the model fitting procudure. Here we are dealing with a very sparse set of data, since most of the counts are zero in this matrix (not everybody uses every word or set of words). This can cause issues with the random forest model. In addition, there may be some imbalance in the classes (for instance as we saw above different individuals post more often than others).&lt;/p&gt;
&lt;p&gt;One particular aspect to explore is that different selections for the parameters can have big effects on the quality of the model. The two main parameters for a random forest are the number of trees (&lt;code&gt;ntree&lt;/code&gt;) and the number of features that are evaluated at each branch in the trees (&lt;code&gt;mtry&lt;/code&gt;). The higher the better for the number of trees, although run-time can be a hindrance on this. For the second parameter, I have seen it suggested that the square root of the number of features is a good place to start, and this is the default for classification anyway. So let’s try increasing the number of trees, and running this on the TF-IDF transformed 3-gram data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_model_tfidf &amp;lt;- randomForest(x = as.matrix(train_dtm_tfidf), 
                               y = as.factor(rownames(train_dtm_tfidf)),
                               ntree = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One note to make here is that this is &lt;em&gt;slllllllloooooooooowwwwwwwwww&lt;/em&gt;! This needed to be run overnight to finish. Using something like python is probably a better bet when running machine learning algorithms like this, and I will probably do another post later in the future to look at some alternative ways to do this.&lt;/p&gt;
&lt;p&gt;So let’s take a look at whether or not this model is more effective at predicting the user:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm_test       &amp;lt;- create_dtm(it_test, vectorizer)
test_dtm_tfidf &amp;lt;- fit_transform(test_dtm, tfidf)
test_predict &amp;lt;- predict(rf_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;quot;response&amp;quot;)
table(test_predict == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   751   482&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, so now we have improved the prediction to a whopping 39.1%. Hmm. An improvement of 0.7% was not &lt;em&gt;quite&lt;/em&gt; as much as I was hoping for.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Logistic Regression&lt;/h1&gt;
&lt;p&gt;Okay, so let’s try a different model to see if that has any effect. I am going to fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;logistic regression&lt;/a&gt;. Regression is simply an attempt to fit a linear approximation to a set of data that minimises the difference between the modeled value and the true value (the &lt;em&gt;residuals&lt;/em&gt;). I will do a more thorough post on statistical modelling in the future, but for now think of regression models as being an attempt to fit a line of best fit between some variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; that you suspect is dependent on some other variables &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, ..., x_n\)&lt;/span&gt;. The idea then is to use this model to predict &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on new measurements of &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, ..., x_n\)&lt;/span&gt;. So here we are trying to fit a model that will provide us with an estimate of the user based on the words used in the post.&lt;/p&gt;
&lt;p&gt;Here I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/index.html&#34;&gt;glmnet&lt;/a&gt; package to fit the logistic regression. Logistic regression is a subset of Generalised Linear Models (GLM), which are an extension of ordinary linear regression allowing for errors that are not normally distributed through the use of a link function. Since we have multiple possible classes in the dependent variable, this will be a multinomial logistic regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;glmnet&amp;quot;)
glm_model_tfidf &amp;lt;- cv.glmnet(x = train_dtm_tfidf, y = as.factor(train$from_name), 
                              family = &amp;#39;multinomial&amp;#39;, 
                              alpha = 1,
                              type.measure = &amp;quot;deviance&amp;quot;,
                              nfolds = 5,
                              thresh = 1e-3,
                              maxit = 1e3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an n-fold cross-validated GLM (hence &lt;code&gt;cv.glmnet&lt;/code&gt;), which is a method of validation for the model that splits the data into &lt;code&gt;n&lt;/code&gt; equally sized subsets, then uses &lt;code&gt;n-1&lt;/code&gt; subsets as training data and the remaining subset as the validation data to test the accuracy of the model. This is repeated &lt;code&gt;n&lt;/code&gt; times, and the average is used. This is actually a better method than I have used in these data (selecting a test data set and running the model on the remaining subset) as every sample is used in the validation, which avoids over-fitting.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;family&lt;/code&gt; parameter gives the model family that defines the error model, which in turn determines the link function to be used. In this case we are using multinomial logistic regression, so the predicted response is a vector of probabilities between 0 and 1 – one for each potential response – all adding to 1. The link function, which defines the relationship between the linear predictor and the mean of the distribution function, is the &lt;code&gt;logit&lt;/code&gt; function, which in the binary case gives the log odds of the prediction:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X\beta = ln \frac{\mu}{(1-\mu)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;alpha&lt;/code&gt; value will utilise an L1 regularisation of the data to account for the sparsity that we see in the data. The &lt;code&gt;type.measure&lt;/code&gt; value determines the measurement to use to determine the cross validation, in this case the misclassification error. &lt;code&gt;nfolds&lt;/code&gt; gives the value of &lt;code&gt;k&lt;/code&gt; for the k-fold cross validation, &lt;code&gt;thresh&lt;/code&gt; gives the threshold for the convergence of the coordinate descent loop, and &lt;code&gt;maxit&lt;/code&gt; gives the maximum number of iterations to perform.&lt;/p&gt;
&lt;p&gt;So let’s see if this is any better:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_predict &amp;lt;- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;quot;response&amp;quot;)
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   752   481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope. We still only see about 39% accurately assigned.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Discussion&lt;/h1&gt;
&lt;p&gt;Okay, so it is possible (highly probable?) that I have made some mistakes in this analysis, and that I could vastly improve the creation of the DTM, but I think it is more propbable that these posts are simply not distinct enough to determine individuals writing styles. I guess in a group with such a narrow focus, it is inevitable that people will be posting very similar content to one another. There is after all only so many ways to ask “Suunto or Garmin”.&lt;/p&gt;
&lt;p&gt;Let’s examine why we are struggling to distinguish these posts in a little more detail. Below is a heatmap showing the probability for each of the 48 potential posters, predicted for all 1,233 of the posts in the validation data set. A heatmap is a kind of 3-dimensional plot, where colour is used to represent the third dimension. So the 48 potential posters are shown on the x-axis, the 1,233 posts are shown on the y-axis, and the magnitude of the estimated probability for user &lt;code&gt;i&lt;/code&gt; based on post &lt;code&gt;j&lt;/code&gt; is represented by a colour from red (0% probability) to white (100% probability). Note that here I have scaled the data using a square root so that smaller probabilities (which we expect to see) are more visible. The rows and columns are arranged such that more similar values are closer together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;gplots&amp;quot;)
heatmap.2(sqrt(test_predict[,,1]), trace = &amp;quot;none&amp;quot;, margins = c(10,0), labRow = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;similar_posts.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the biggest problem here is that the vast majority of the posts are estimated as most likely coming from either Neil Bryant, Stef Schuermans or James Adams. And actually, the ones that it gets correct are almost all from one of these posters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pie(sort(table(rownames(test_dtm)[colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;similar_posts_pie.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I wonder whether these guys are skewing the model because of their, ahem, above average posting habits. But frankly at this stage I’m kind of bored, so I think that I will leave it there. Another time maybe. Ultimately I believe that these posts are simply too short and bereft of salient information to be useful for making predictions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Suunto Or Garmin? The Age Old Question.</title>
      <link>/post/2018-04-18-suunto-or-garmin/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-18-suunto-or-garmin/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rfacebook&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#temporary-token&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fboauth&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ultra-running-community&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likes-comments-and-shares&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-contributors&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-are-people-posting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-popular-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-often-do-people-actually-talk-about-ultras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suunto-or-garmin&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summing-up&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;In &lt;a href=&#34;/post/2018-04-15-ultrarunner-or-ultra-runner/&#34;&gt;my last post&lt;/a&gt;, I took a look at ways to pull data down from Twitter and analyse some specific trends. It was quite interesting for me to see how easy it is to access these data, and there is a huge amount to be gleened from these sorts of data. The idea of this post is to use a similar approach to pull data from the Ultra Running Community page on &lt;a href=&#34;https://www.facebook.com&#34;&gt;Facebook&lt;/a&gt;, and then to use these data to play around further with the &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; R packages. Just for funsies, I’m also going to have a bit of a crack at some machine learning concepts. In particular, a question that seems to comes up pretty regularly is whether the best GPS watch for running is from &lt;a href=&#34;http://www.suunto.com&#34;&gt;Suunto&lt;/a&gt; or &lt;a href=&#34;https://www.garmin.com&#34;&gt;Garmin&lt;/a&gt;. I figured I could save us all some time and answer the question once and for all…&lt;/p&gt;
&lt;p&gt;Just a little aside here; I think that some people missed the point last time. I honestly don’t care much about these questions, they are just a jumping off point for me to practice some of the data analysis techniques that come up in my work. The best way to get better at something is to practice, so these posts are just a way of combining something I love with a more practical purpose. The idea of this blog is for me to practice these things until they become second nature. Of course in the process, I may just find something interesting along the way.&lt;/p&gt;
&lt;p&gt;Probably not though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rfacebook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/h1&gt;
&lt;p&gt;Following on from my experiences playing around with the &lt;a href=&#34;https://twitter.com&#34;&gt;Twitter&lt;/a&gt; API, I decided to have a look to see if there were similar programmatic ways to access Facebook data. This can be accomplished using the &lt;a href=&#34;https://cran.r-project.org/web/packages/Rfacebook/&#34;&gt;Rfacebook&lt;/a&gt; package in R, which is very similar to the &lt;code&gt;TwitteR&lt;/code&gt; package that I used previously. This package accesses the Facebook &lt;a href=&#34;https://developers.facebook.com/docs/graph-api&#34;&gt;Graph API Explorer&lt;/a&gt;, allowing access to a huge amount of data from the Facebook social &lt;em&gt;graph&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So first of all, let’s install the &lt;code&gt;Rfacebook&lt;/code&gt; package. We can install the stable version from the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or install the more up-to-date but less stable developmental version from Github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;devtools&amp;quot;)
install_github(&amp;quot;pablobarbera/Rfacebook/Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am using the developmental version here. There are several additional packages that also need to be installed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;httr&amp;quot;, &amp;quot;rjson&amp;quot;, &amp;quot;httpuv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;code&gt;TwitteR&lt;/code&gt;, access to the API is controlled through the use of API tokens. There are two ways of doing this - either by registering as a developer and generating an app as I did with &lt;code&gt;TwitteR&lt;/code&gt;, or through the use of a temporary token which gives you access for a limited period of 2 hours. Let’s look at each of these in turn:&lt;/p&gt;
&lt;div id=&#34;temporary-token&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/h2&gt;
&lt;p&gt;To generate a temporary token, just go to the &lt;a href=&#34;https://developers.facebook.com/tools/explorer/&#34;&gt;Graph API Explorer&lt;/a&gt; page and generate a new token by clicking on &lt;code&gt;Get Token&lt;/code&gt; -&amp;gt; &lt;code&gt;Get Access Token&lt;/code&gt;. You need to select the permissions that you want to grant access for, which will depend on what you are looking do:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Graph-API-Explorer.png&#34; alt=&#34;Create Temporary Access Token&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Create Temporary Access Token&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I just granted permission to everything for this analysis. Once you have an access token, this can be used as the &lt;code&gt;token&lt;/code&gt; parameter when using functions such as &lt;code&gt;getUsers()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fboauth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/h2&gt;
&lt;p&gt;The above is the most simple method, but this access token will only last for 2 hours, at which point you will need to generate a new one. If you want a longer term solution, you can set up &lt;a href=&#34;https://en.wikipedia.org/wiki/OAuth&#34;&gt;Open Authorization&lt;/a&gt; access in a similar way to for the &lt;code&gt;TwitteR&lt;/code&gt; package. The downside is that you lose the ability to search friend networks unless your friends are also using the app that you generate - and I don’t want to inflict that on people just so that I can &lt;del&gt;steal their identity&lt;/del&gt; analyse their networks.&lt;/p&gt;
&lt;p&gt;This method is almost identical to the process used for generating the OAuth tokens in the &lt;code&gt;TwitteR&lt;/code&gt; app, and a good description of how to do it can be found in this &lt;a href=&#34;http://thinktostart.com/analyzing-facebook-with-r/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, I am feeling pretty lazy today and so I will just use the temporary method.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ultra-running-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/h1&gt;
&lt;p&gt;With nearly 18,000 members, the Ultra Running Community Facebook page is a very active community of ultrarunners from around the world. Runners are able to ask questions, share blogs, and generally speak with like-minded individuals about everything from gear selection to how best to prevent chaffing when running. It’s been going since June 2012, so there are a fair few posts available to look through.&lt;/p&gt;
&lt;p&gt;So let’s load in all of the posts from the URC page:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Rfacebook&amp;quot;)
token &amp;lt;- &amp;quot;XXXXXX&amp;quot;  ## Insert your temporary token from Graph API Explorer
URC &amp;lt;- getGroup(&amp;quot;259647654139161&amp;quot;, token, n=50000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will create an object of class &lt;code&gt;data.frame&lt;/code&gt; containing the most recent 50,000 posts available in the Facebook Group with ID &lt;code&gt;259647654139161&lt;/code&gt; (which is the internal ID for the Ultra Running Community page). The page was set up in June 2012 By Neil Bryant, and currently (as of 20th March 2017) contains a total of 24,836 posts. So this command will actually capture every single post.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;data.frame&lt;/code&gt; is somewhat of the workhorse of R, and looks to the user like a spreadsheet like you would expect to see in Excel. Behind the scene it is actually a list of lists, with each column representing a particular measurement or descriptive annotation of that particular datum. The ideal situation is to design your data frame such that every row is an individual measurement, and every column is some aspect relating to that measurement. This can sometimes go against the instinctual way that you might store data, but makes downstream analyses much simpler.&lt;/p&gt;
&lt;p&gt;As an example, suppose that you were measuring something (blood glucose levels, weight, lung capacity, VO2 max, etc.) at three times of the day for 2 individuals. Your natural inclination may be to design your table in this way:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement 1&lt;/th&gt;
&lt;th&gt;Measurement 2&lt;/th&gt;
&lt;th&gt;Measurement 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But actually the optimum way to represent this is to treat each measurement as a different row in your data table, and use a descriptive categorical variable to represent the repeated measurements:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement&lt;/th&gt;
&lt;th&gt;Replicate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can then add additional information relating to each individual measurement, which can be factored into your model down the line.&lt;/p&gt;
&lt;p&gt;In this case, we have a data frame where each row is a post on the URC feed, and each column gives you information on the post such as who wrote it, what the post says, when it was written, any links involved, and how many likes, comments and shares each post has. We can take a quick look at what the data.frame looks like by using the &lt;code&gt;str()&lt;/code&gt; function. This will tell us a little about each column, such as the data format (character, numeric, logical, factor, etc.) and the first few entries in each column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(URC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    24836 obs. of  11 variables:
##  $ from_id       : chr  &amp;quot;10203232759527000&amp;quot; &amp;quot;10154582131480554&amp;quot; &amp;quot;10206266800967107&amp;quot; &amp;quot;10153499987966664&amp;quot; ...
##  $ from_name     : chr  &amp;quot;Steph Wade&amp;quot; &amp;quot;Esther Bramley&amp;quot; &amp;quot;Tom Chapman&amp;quot; &amp;quot;Polat Dede&amp;quot; ...
##  $ message       : chr  &amp;quot;Anyone who runs in Speedcross 3s tried Sportiva? I&amp;#39;ve had several pairs of Speedcross  but thinking about tryin&amp;quot;| __truncated__ &amp;quot;Hi guys server pain in knee two weeks after 41miler. Ran 3 miles Tuesday no problem. Pain started at 5m and got&amp;quot;| __truncated__ &amp;quot;mega depressed; need advice. Running really well over xmas, since then, painful hip &amp;amp; groin, chasing itb, glute&amp;quot;| __truncated__ NA ...
##  $ created_time  : chr  &amp;quot;2017-03-19T08:38:15+0000&amp;quot; &amp;quot;2017-03-18T12:19:26+0000&amp;quot; &amp;quot;2017-03-18T16:30:54+0000&amp;quot; &amp;quot;2017-03-19T08:42:38+0000&amp;quot; ...
##  $ type          : chr  &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;photo&amp;quot; ...
##  $ link          : chr  NA NA NA &amp;quot;https://www.facebook.com/tahtaliruntosky/photos/a.659614340816057.1073741827.659609490816542/1145262965584523/?type=3&amp;quot; ...
##  $ id            : chr  &amp;quot;259647654139161_1064067560363829&amp;quot; &amp;quot;259647654139161_1063418100428775&amp;quot; &amp;quot;259647654139161_1063562803747638&amp;quot; &amp;quot;259647654139161_1064068937030358&amp;quot; ...
##  $ story         : logi  NA NA NA NA NA NA ...
##  $ likes_count   : num  0 0 0 0 0 2 2 58 7 1 ...
##  $ comments_count: num  5 23 9 0 25 9 4 64 77 12 ...
##  $ shares_count  : num  0 1 0 0 0 0 0 0 3 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likes-comments-and-shares&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/h1&gt;
&lt;p&gt;The first step in any data analysis is to check that the data make sense. You’ve probably heard the old adage “garbage in; garbage out”, so data cleaning is an essential first step to ensure that we are not basing our conclusions on erroneous information from the beginning. There are far too many posts here to look at them all by hand, but there are a few things we can certainly have a look at to check that the values make sense.&lt;/p&gt;
&lt;p&gt;For instance, let’s take a look at the number of likes, comments and shares. We would expect all of these values to be positive whole numbers, so this is something that is easy to check. To do this, I will be making use of the &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;ggplot2&lt;/a&gt; package, which allows for some incredibly powerful plotting in R. The idea is to define the plot in terms of &lt;em&gt;aesthetics&lt;/em&gt;, where different elements of the plot (x and y values, colour, size, shape, etc.) can be mapped to elements of your data.&lt;/p&gt;
&lt;p&gt;In this case I want to plot a distribution plot where the colour of the plot is mapped to whether we are looking at likes, comments or shares. To do this, I need to rearrange the data such that the &lt;code&gt;likes_count&lt;/code&gt;, &lt;code&gt;comments_count&lt;/code&gt; and &lt;code&gt;shares_count&lt;/code&gt; columns are in a single column &lt;code&gt;counts&lt;/code&gt;, with an additional column defining whether it is a like, a comment, or a share count (as described in the example above).&lt;/p&gt;
&lt;p&gt;I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/tidyr.pdf&#34;&gt;tidyr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html&#34;&gt;stringr&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html&#34;&gt;dplyr&lt;/a&gt; packages to rearrange the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyr&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
library(&amp;quot;stringr&amp;quot;)
like_comment_share &amp;lt;- URC                                                 %&amp;gt;% 
                      gather(count_type, count, likes_count:shares_count) %&amp;gt;%
                      select(count_type, count)
head(like_comment_share)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    count_type count
## 1 likes_count     0
## 2 likes_count     0
## 3 likes_count     0
## 4 likes_count     0
## 5 likes_count     0
## 6 likes_count     2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;stringr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; are incredibly powerful packages written by Hadley Wickham, which provide a simple to understand &lt;em&gt;grammar&lt;/em&gt; to apply to the filtering and tweaking of data frames in R. In particular, these can be used to convert the data into the &lt;em&gt;tidy&lt;/em&gt; format described above, allowing very simple and intuitive plotting with &lt;code&gt;ggplot2&lt;/code&gt;. One particularly useful feature is the ability to use the &lt;code&gt;%&amp;gt;%&lt;/code&gt; command to pipe the output to perform multiple data frame modifications.&lt;/p&gt;
&lt;p&gt;In the above code, we pipe the raw data &lt;code&gt;URC&lt;/code&gt; into the &lt;code&gt;gather()&lt;/code&gt; function, which will take the three columns from &lt;code&gt;likes_count&lt;/code&gt; through to &lt;code&gt;shares_count&lt;/code&gt; and split them into two new columns: &lt;code&gt;count_type&lt;/code&gt; which will be one of &lt;code&gt;shares_count&lt;/code&gt;, &lt;code&gt;likes_count&lt;/code&gt; and &lt;code&gt;comments_count&lt;/code&gt; , and &lt;code&gt;count&lt;/code&gt; which will take the value from the specified column. So essentially this produces a new data set with 3 times as many rows. This is then piped into &lt;code&gt;select()&lt;/code&gt; which will select the relevant columns.&lt;/p&gt;
&lt;p&gt;First let’s just check that they are are all positive integers as expected:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count == as.integer(like_comment_share$count))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count &amp;gt;= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;p&gt;Annoyingly there is no easy way to check that a vector of numbers is made up of integers, so this line will check that the numbers do not change after converting to integers. Similarly, we use &lt;code&gt;all()&lt;/code&gt; to check that all of the counts are greater than or equal to zero. They are as we would hope.&lt;/p&gt;
&lt;p&gt;Then we can use &lt;code&gt;ggplot2&lt;/code&gt; for the plotting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
ggplot(like_comment_share, aes(x = log10(count+1), col = count_type, fill = count_type)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Count (log10)&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-like_comment_share-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we use this output to plot a distribution plot showing the distribution of counts for the three different metrics – shares, comments and likes. We use the &lt;code&gt;count&lt;/code&gt; as the &lt;code&gt;x&lt;/code&gt; aesthetic, and &lt;code&gt;count_type&lt;/code&gt; as both the &lt;code&gt;col&lt;/code&gt; and &lt;code&gt;fill&lt;/code&gt; aesthetics to colour them. The main function &lt;code&gt;ggplot()&lt;/code&gt; will specify the aesthetics, and then we add additional elements to the plot by using the &lt;code&gt;+&lt;/code&gt; command. Here we add the &lt;code&gt;geom_density()&lt;/code&gt; element to plot the data in a density plot (the &lt;code&gt;alpha&lt;/code&gt; value will make the colours transparent for overplotting), the &lt;code&gt;labs()&lt;/code&gt; function will change the plot labels, and the &lt;code&gt;theme()&lt;/code&gt; function let’s you change aspects of the figure text, such as the size.&lt;/p&gt;
&lt;p&gt;Note that here I have plotted the &lt;span class=&#34;math inline&#34;&gt;\(log_{10}\)&lt;/span&gt; of the counts, which reduces the effects of outliers. Also note that I have added 1 to the counts. This is because &lt;span class=&#34;math inline&#34;&gt;\(log_{10}(0)\)&lt;/span&gt; is undefined. The idea here is that a count of 1 will get a value of 0, a count of 10 gets a value of 1, a count of 100 gets a value of 2, etc.&lt;/p&gt;
&lt;p&gt;So what does this tell us? Well not too much really. Not many people share posts from the page, but there aren’t too many that don’t get comments or likes. So it is a very active community. Posts tend to have more comments than likes, which makes sense because you can only like something once, but can comment as many times as you want. But the main thing that this shows is that these counts all seem to be in the right sort of expected range.&lt;/p&gt;
&lt;p&gt;Often exploratory plots like this can be useful to highlight problems in the raw data. One such example might be if a negative count existed in these data, which could happen due to input errors but quite clearly does not represent a valid count. As it happens, since these data are not manually curated, it is highly unlikely that such errors will be present, but you should never assume anything about any given data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-contributors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the all-time most common contributors to the page, again using the &lt;code&gt;ggplot2&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_contributors &amp;lt;- URC              %&amp;gt;% 
                    count(from_name) %&amp;gt;%
                    top_n(50, n)     %&amp;gt;%
                    arrange(desc(n))
ggplot(top_contributors, 
       aes(x = factor(from_name, levels = top_contributors$from_name), 
           y = n,
           fill = n)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_gradient(low=&amp;quot;blue&amp;quot;, high=&amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) +
  theme(axis.title  = element_text(size = 18),
        axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 14),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-contributors-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have first used &lt;code&gt;dplyr&lt;/code&gt; to count up the number of posts per user and select the top 50 contributors, then used &lt;code&gt;ggplot2&lt;/code&gt; to plot a barplot showing the number of posts per person. I have used the &lt;code&gt;scale_fill_gradient()&lt;/code&gt; element to colour the bars based on their height, such that those with the highest number of posts are coloured red, whilst those with the lowest are coloured blue.&lt;/p&gt;
&lt;p&gt;The top contributor to the page is Neil Bryant (757 posts), who is the founder member so this makes sense. James Adams is the second biggest contributor (489 posts), and he has less of an excuse really.&lt;/p&gt;
&lt;p&gt;Let’s take a look at James’ posting habits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;xts&amp;quot;)
jamesadams &amp;lt;- URC                                                        %&amp;gt;%
              filter(from_name == &amp;quot;James Adams&amp;quot;)                         %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
jamesadams_xts &amp;lt;- xts(jamesadams$n, order.by = jamesadams$created_time)
jamesadams_month &amp;lt;- apply.monthly(jamesadams_xts, FUN = sum)
plot(jamesadams_month, ylab = &amp;quot;Number of Posts&amp;quot;, main = &amp;quot;&amp;quot;, cex.lab = 1.7, cex.axis = 1.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-JamesAdams-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have used the package &lt;code&gt;xts&lt;/code&gt; to deal with the &lt;code&gt;POSIXct&lt;/code&gt; date format. In particular this will deal correctly with months with zero counts. James has been pretty active since summer 2013 (probably publicising &lt;a href=&#34;https://www.amazon.com/Running-Stuff-James-Adams/dp/1784622621&#34;&gt;his book&lt;/a&gt;), but his activity has been on the decline throughout 2016 – the price you pay when your family size doubles I guess.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-are-people-posting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/h1&gt;
&lt;p&gt;Next we can break the posts down by the day on which they are posted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;URC      &amp;lt;- URC %&amp;gt;%
            mutate(dow = factor(weekdays(as.POSIXct(created_time)), labels = c(&amp;quot;Monday&amp;quot;, &amp;quot;Tuesday&amp;quot;, &amp;quot;Wednesday&amp;quot;, &amp;quot;Thursday&amp;quot;, &amp;quot;Friday&amp;quot;, &amp;quot;Saturday&amp;quot;, &amp;quot;Sunday&amp;quot;)))
post_day &amp;lt;- URC %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-posting_day-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly to previously, here I have used &lt;code&gt;dplyr&lt;/code&gt; to reduce the full data down to a table of counts of posts per day of the week, then plotted them using &lt;code&gt;ggplot2&lt;/code&gt;. Surprisingly (to me anyway) there is no increase in activity during the weekend. I guess most of us are checking Facebook during working hours and busy running at the weekend…&lt;/p&gt;
&lt;p&gt;Wednesdays are interestingly bereft of posts though for some reason. Could this be people checking URC at work on Monday and Tuesday through boredom, only to find themselves told off and having to catch up on work by Wedesday? Then by the time the weekend rolls around we’re all back liking away with impunity ready to go through the whole process again the next week.&lt;/p&gt;
&lt;p&gt;Let’s look at the same plot for the 1,000 most popular posts (based on likes):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_day &amp;lt;- URC                      %&amp;gt;%
            top_n(1000, likes_count) %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-posting_day_most_likes-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So from this it is clear that if you want people to like your post, you should post on a Tuesday or a Thursday. Quite why people might be feeling so much more likely to click that all important like button on these dayas, I have no idea. But hey, stats don’t lie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-popular-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/h1&gt;
&lt;p&gt;So looking at the popular posts above got me thinking about how best to actually define a “popular” post. Is it a post with a lot of likes, or a post that everybody is commenting on? Let’s take a look at the top 5 posts based on each criteria:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(likes_count)) %&amp;gt;% top_n(5, likes_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                       message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 Thinking how far I’ve come and getting a bit emotional. .3 year ago i was a size 20/22 and couldnt run to end of the street. Yesterday i ran 30 mile as a training run and wasn’t even aching afterwards. Now nearly 45 and a size 10 and never felt better. I love my life!!!!
2 I saw this picture couple years ago and I found it very inspiring so I thought I’d share it. is a 12-year-old childhood cancer survivor who loves to run with his dad.
3 Not sure if swear words are accepted. 088208820882
4 “What you doing up here?” said the sheep.Pike last night, not a soul to be seen…
5 0880&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(comments_count)) %&amp;gt;% top_n(5, comments_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 For me claiming something you haven’t earned is not only immoral it’s fraudulent. And it’s a huge insult to all who’ve attempted the feat before you and legitimately fallen short. What do you guys think?
2 Anyone else do race to the stones and found it a rip off? I was not impressed with most things. were good such as medics and lots of water but who wants fudge bars and cadburies at aid stations. like a money making race to me, especially considering all the sponsorship they had. ’ve got lots of other rants about it but let’s hear anyone else’s thoughts first
3 New Game.am trying to convince some new ultra runners that you do not need to spend a load of money on kit to put one foot in front of the other a few more times. This is difficult given that half the posts in forums seems to be asking for recommendations or giving recommendations as to how one might waste money on kit.out of interest, what was the value of the kit you wore in your last ultra? Including nutrition. Obviously you will have to guess if you had them as a gift or can’t remember. Surely someone is going to have less than £100?
4 I had a small sabre rattling session last night with someone on this group. Nothing major by any stretch of the imagination - we just have opposing views on DNF. But it got me curios to what the opinions of others are on this subject. Is failing to finish something that you would risk your life to avoid? Is it something to fear? Is it something that will eventually happen to us all? Is it something that we can learn from? Etc,etc. Your thoughts please
5 i cannot wait to watch the EPSN footage - amazing stuff. It is a shame Robert has his doubters though.: was described as “trolling”, which was an over the top description (agree with the comments there)&lt;/p&gt;
&lt;p&gt;It seems to me that the posts with more likes tend to be posts with a much more positive message than those with most comments. The top liked posts are those from people who have overcome some form of adversity (such as the top liked post with 1,369 likes from Mandy Norris who had awesomely run 30 miles after losing half her body weight), whilst the top commented posts tend to be more controversial posts (such as the top commented post with 287 comments about Mark Vaz’s fraudulent JOGLE “World Record”).&lt;/p&gt;
&lt;p&gt;Let’s take a look at how closely these two poularity measures are correlated in the Ultra Running Community:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(URC, aes(x = comments_count, y = likes_count)) +
  geom_point(shape = 19, alpha = 0.2, size = 5) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  geom_point(data = URC %&amp;gt;% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = &amp;quot;red&amp;quot;, size = 5)) +
  geom_point(data = URC %&amp;gt;% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = &amp;quot;blue&amp;quot;, size = 5)) +
  labs(x = &amp;quot;Number of Comments&amp;quot;, y = &amp;quot;Number of Likes&amp;quot;) + 
  theme(axis.text   = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-likes_vs_comments-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are plotting a correlation scatterplot between the number of likes and the number of comments for each post. I have a set an alpha value of 0.2 for the scatterplot so that the individual points are made more see-through. That way, overplotting can be seen by darker coloured plots. I have also added in a line of best fit generated by fitting a linear model (&lt;code&gt;method= lm&lt;/code&gt;), together with an estimate of the standard error shown by the grey surrounding of the blue line (&lt;code&gt;se = TRUE&lt;/code&gt;). Finally I have highlighted the top 5 commented posts in blue, and the top 5 liked posts in red.&lt;/p&gt;
&lt;p&gt;It is pretty clear from this plot that there is virtually no correlation between the number of comments and the number of likes, particularly for those with more likes or comments. In general the posts with more likes do not have the most comments (and vice versa), suggesting that in general we like the nice posts, but comment on the ones that upset us. In fact, it looks as if Mandy’s post is the only exception, with both the highest number of likes but also a high number of comments (220).&lt;/p&gt;
&lt;p&gt;We can calculate the correlation between these measures, which is a measure of the linear relationship between two variables. A value of 1 indicates that they are entirely dependent on one another, whilst a value of 0 indicates that the two are entirely independent of one another. A value of -1 indicates an inverse depdnedancy, such that an increase in one variable is associated with a similar decrease in the other variable. Given the difference in the scales between likes and comments, I will use the &lt;em&gt;Spearman correlation&lt;/em&gt;, which looks at correlation between the ranks of the data and therefore ensures that each unit increment is 1 for both variables meaning that it is robust to outliers. The Spearman correlation between these two variables is &lt;code&gt;0.27&lt;/code&gt;, so there is virtually no correlation between likes and comments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-often-do-people-actually-talk-about-ultras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/h1&gt;
&lt;p&gt;It seems recently that there is more talk of charlatans and frauds than races, and a lot of people have commented on the fact that there seems to be less and less actual discussion of ultras recently. So let’s see if this is the case, by tracking how often the term &lt;em&gt;ultra&lt;/em&gt; is actually used over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ultraposts &amp;lt;- URC                                                        %&amp;gt;%
              filter(str_detect(message, &amp;quot;ultra&amp;quot;))                       %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
ultraposts_xts &amp;lt;- xts(ultraposts$n, order.by = ultraposts$created_time)
ultraposts_month &amp;lt;- apply.monthly(ultraposts_xts, FUN = sum)
plot(ultraposts_month, ylab = &amp;quot;Number of Ultra Posts&amp;quot;, main = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-ultra_usage-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Over the last year or so, the number of people in the group has risen dramatically, and yet it certainly seems that fewer people are actually discussing ultras these days. I guess read into that what you will – perhaps the feed is indeed dominated by Suunto vs Garmin questions?&lt;/p&gt;
&lt;p&gt;Hell, let’s find out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suunto-or-garmin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/h1&gt;
&lt;p&gt;So let’s take a look at the real question that everybody cares about – which is more popular, Suunto or Garmin. All my cards on the table; I have a Suunto Peak Ambit 3, but if it helps I had to Google that because I really don’t keep up on these things. I’m really not a gear not, and prefer to make do. The only reason that I got this is that my previous watch died a death, and I like to use a watch for navigation. I didn’t pay for it – at that price I couldn’t bring myself to fork out the money. But it was a gift, and I am very pleased with it. It has a great battery life, and is pretty simple when loading data to my computer. Despite being a stats guy, I don’t normally focus much on my own data, but actually it has been interesting to see how slow I have become recently due to an ongoing injury. Perhaps it will help me to push myself in training onece it is sorted.&lt;/p&gt;
&lt;p&gt;But as I understand it, the Garmin Fenix 3 does exactly the same stuff. Is one better than the other? I couldn’t possibly say. Many people have tried, but I suspect that it comes down to personal preference rather than there being some objective difference between the two.&lt;/p&gt;
&lt;p&gt;But just for the hell of it, let’s see how often people talk about the two. I will be simply using fuzzy matching to look for any use of the terms “suunto” or “garmin” in the post. Fuzzy matching is able to spot slight misspellings, such as “Sunto” or “Garmin”, and is carried out using the base &lt;code&gt;agrep()&lt;/code&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suunto &amp;lt;- agrep(&amp;quot;suunto&amp;quot;, URC$message, ignore.case = TRUE)
garmin &amp;lt;- agrep(&amp;quot;garmin&amp;quot;, URC$message, ignore.case = TRUE)
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c(&amp;quot;Suunto&amp;quot;, &amp;quot;Garmin&amp;quot;, &amp;quot;Both&amp;quot;), cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So of the 24,836 posts on the URC page, 237 (0.95 %) mention Suunto, whilst 552 (2.22 %) mention Garmin. Only 77 (0.3 %) mention both, which I assume are the posts specifically asking which of the two is best. Given the way some people moan about how often this question comes up, these numbers are actually surprisingly small I think. But based on this it seems that Garmin is more popular, although it would be interesting to look at the actual responses on those “VS” posts to see what the outcome actually was in each case.&lt;/p&gt;
&lt;p&gt;Having said that, there is nothing to suggest what these posts about Garmin’s are actually saying. They may be generally saying that they hate Garmins. So I am going to play around with a bit of sentiment analysis using the &lt;code&gt;qdap&lt;/code&gt; package. Essentially this is a machine learning algorithm that has been trained to identify the sentiment underlying a post, with positive values representing a generally positive sentiment (and vice versa). So let’s break down the Garmin and Suunto posts to see how they stack up:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;qdap&amp;quot;)

## Convert to ASCII and get rid of upper case
suunto_msg &amp;lt;- URC$message[suunto]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                         
garmin_msg &amp;lt;- URC$message[garmin]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                            

## Calculate the sentiment polarity
suunto_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, suunto_msg))
garmin_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, garmin_msg))

## Plot in a stacked barplot
sent_dat &amp;lt;- data.frame(Watch     = c(rep(&amp;quot;Suunto&amp;quot;, length(suunto)), 
                                     rep(&amp;quot;Garmin&amp;quot;, length(garmin))),
                       Sentiment = c(suunto_sentiment$all$polarity,
                                     garmin_sentiment$all$polarity))
ggplot(sent_dat, aes(x = Sentiment, col = Watch, fill = Watch)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Sentiment Polarity&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin_sentiment-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So a value of zero on this distribution plot suggests a neutral sentiment to the post, a positive number suggests a positive sentiment, and a negative number suggests a negative sentiment. While the majority of the posts seem to be fairly neutral in both cases, it certainly seems that the majority of the Garmin posts are positive whilst the Suunto posts are more neutral with many positive &lt;em&gt;and&lt;/em&gt; negative posts.&lt;/p&gt;
&lt;p&gt;We can actually put a number on this, for whether or not there is truly a statistically significant difference between the distribution of sentiment scores for the two watches. To do this, we will us a statistical test that checks how likely it is that we would see a difference of the magnitude seen here given that there is no difference between what people actually think of the watch. This is the so-called “null-hypothesis”, and essentially says that there is no difference, and any differences that we do see are purely random errors. We can test this hypothesis using one of a number of different tests, with the aim to see whether there is evidence that we can reject this null hypothesis, thus suggesting that there is indeed a true difference between the distributions. So we never really “prove” that there is a difference, but instead show that there is suitable evidence to disprove the null hypothesis.&lt;/p&gt;
&lt;p&gt;To do this some test statistic is calculated and is tested to see if it is significantly different than what you would expect to see by chance. Typically this is assessed using a “p-value”, which is one of the most misunderstood measurements in statistics. This value represents the probability that you would see a test statistic &lt;em&gt;at least as high&lt;/em&gt; as that measured purely by chance, even if both sets of data were drawn from the same distribution. So both the Garmin and Suunto scores are a tiny subset of all possible opinions of people in the world, the population distribution. Our two sample populations are either drawn from an overall population where there is no difference, or from two distinct populations for people who have a view one way or the other.&lt;/p&gt;
&lt;p&gt;It is pretty clear from the above figure that these values are not normally distributed (a so-called “bell-curve” distribution), so we cannot use a simple t-test which basically tests the difference in the means between two distributions (after taking the variance into account). Instead we would be better off using a non-parametric test which does not require the data to be parameterised into some fixed probability density function. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test&#34;&gt;Kolmogorov Smirnov test&lt;/a&gt; is one method that can be used, and works by looking at how the cummulative distribution functions of two distinct samples differ:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]],
## subset(sent_dat, : p-value will be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## D = 0.093653, p-value = 0.1091
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we run a two-sided test, which simply means that we have no &lt;em&gt;a priori&lt;/em&gt; reason to suspect one distribution to be greater than the other. We could do a one-sided test where the alternative hypothesis that we are testing is “A is greater than B”, rather than the two-sided test where we are testing the alternative hypothesis that “A is not equal to B”. &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the maximum distance between the empirical distribution functions, and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the p-value. Typically, a threshold used to reject the null hypothesis is for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be less than 0.05 (5 %), although it is fairly arbitrary. But in this case, we would conclude that there is not sufficient evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;As an alternative, we can instead use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann–Whitney_U_test&#34;&gt;Wilcoxon rank sum test&lt;/a&gt; (also called the Mann-Whitney &lt;em&gt;U&lt;/em&gt; test). The idea is to rank all of the data, sum up the ranks from one of the samples, and use this to calculate the test statistic &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; (which takes into account the sample sizes). So if the distributions are pretty similar, the sum of the ranks will be similar for sample 1 and sample 2. If there is a big difference, one sample will have more higher ranked values than the other, resulting in a higher value for &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. Let’s take a look at this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## W = 59157, p-value = 0.03066
## alternative hypothesis: true location shift is not equal to 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the test statistic here is actually &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, but in this case &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are equivalent. So this test would result in us rejecting the null hypothesis with the same threshold as above. So which one is correct? Well, this is a great example of why you should never trust statistics. Both of these are perfectly reasonable tests to perform in this case but give different results. Many people would simply choose the one that gives the lowest p-value, but this is pretty naughty and is often called “p-hacking”. At the end of the day, a p-value higher than 0.05 does not mean that there is &lt;em&gt;not&lt;/em&gt; a true difference between the distributions, just that the current data does not provide enough evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;So my conclusion from this is that I made the wrong decision, and will from now on look upon my useless Suunto watch with hatred and resentment. I can only hope that this post will save anyone else from making such an awful mistake.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/h1&gt;
&lt;p&gt;It has been quite &lt;em&gt;fun&lt;/em&gt; playing around with these data tonight, and I have had an opportunity to try out a few new techniques that I have wanted to play with for a while. As ever, there is loads more that can be gleaned from these data, but I should probably do something a little more productive right now. Like sleeping. I have actually done a while load more playing with machine learning algorithms of my own, but this post has already become a little too unruly so I will post this later as a separate post.&lt;/p&gt;
&lt;p&gt;But in summary, people on the Ultra Running Community page spend far too much time posting during working hours, seem to be talking less and less about ultra running, and definitely prefer Garmins to Suuntos. So this has all been completely worth it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] qdap_2.3.2             RColorBrewer_1.1-2     qdapTools_1.3.3       
##  [4] qdapRegex_0.7.2        qdapDictionaries_1.0.7 xts_0.11-2            
##  [7] zoo_1.8-6              ggplot2_3.2.0          stringr_1.4.0         
## [10] dplyr_0.8.3            tidyr_1.0.0            randomForest_4.6-14   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.2          lattice_0.20-38     xlsxjars_0.6.1     
##  [4] gtools_3.8.1        assertthat_0.2.1    zeallot_0.1.0      
##  [7] digest_0.6.20       slam_0.1-45         R6_2.4.0           
## [10] plyr_1.8.4          chron_2.3-54        backports_1.1.4    
## [13] evaluate_0.14       blogdown_0.16       pillar_1.4.2       
## [16] rlang_0.4.0         lazyeval_0.2.2      data.table_1.12.2  
## [19] gdata_2.18.0        rmarkdown_1.14      gender_0.5.2       
## [22] labeling_0.3        igraph_1.2.4.1      RCurl_1.95-4.12    
## [25] munsell_0.5.0       compiler_3.6.1      xfun_0.8           
## [28] pkgconfig_2.0.2     htmltools_0.3.6     reports_0.1.4      
## [31] tidyselect_0.2.5    tibble_2.1.3        gridExtra_2.3      
## [34] bookdown_0.12       codetools_0.2-16    XML_3.98-1.20      
## [37] crayon_1.3.4        withr_2.1.2         bitops_1.0-6       
## [40] openNLP_0.2-6       grid_3.6.1          gtable_0.3.0       
## [43] lifecycle_0.1.0     magrittr_1.5        scales_1.0.0       
## [46] xlsx_0.6.1          stringi_1.4.3       reshape2_1.4.3     
## [49] NLP_0.2-0           openNLPdata_1.5.3-4 xml2_1.2.0         
## [52] venneuler_1.1-0     ellipsis_0.2.0.1    vctrs_0.2.0        
## [55] wordcloud_2.6       tools_3.6.1         glue_1.3.1         
## [58] purrr_0.3.3         plotrix_3.7-6       parallel_3.6.1     
## [61] yaml_2.2.0          tm_0.7-6            colorspace_1.4-1   
## [64] rJava_0.9-11        knitr_1.23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultramarathon, Ultra Marathon or Ultra-Marathon?</title>
      <link>/post/2018-04-15-ultrarunner-or-ultra-runner/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-15-ultrarunner-or-ultra-runner/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#welcome&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Welcome!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-up-the-twitter-api&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Setting up the Twitter API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#twitter&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; TwitteR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ultramarathon-ultra-marathon-or-ultra-marathon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Ultramarathon, Ultra Marathon, or Ultra-Marathon?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ultrarunning-ultra-running-or-ultra-running&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Ultrarunning, Ultra Running or Ultra-Running?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#word-cloud&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Word Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-word&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Final Word&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;welcome&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Welcome!&lt;/h1&gt;
&lt;p&gt;Well, welcome to my new blog I guess. I have had a bit of a hiatus from writing recently but I am trying to get back into the habit over on my running-related blog &lt;a href=&#34;http://constantforwardmotion.blogspot.com&#34; class=&#34;uri&#34;&gt;http://constantforwardmotion.blogspot.com&lt;/a&gt;. Over there I will be mainly moaning about my injury woes (and maybe talking about some of my more ridiculous races if I ever get back to being able to run again), but this blog is a little bit different. In the past I have played around with various bits of data for some quite interesting (I think anyway) posts (e.g. &lt;a href=&#34;http://constantforwardmotion.blogspot.co.uk/2013/08/thames-path-100-2013-race-analysis.html&#34;&gt;this post looking at the 2013 Centurion Running Thames Path 100 mile race&lt;/a&gt;). I am a data analyst by trade, and I am about to start a whole new stage of my career, working as a Senior Post Doc at the &lt;a href=&#34;http://www.port.ac.uk&#34;&gt;University of Portsmouth&lt;/a&gt; where I will be building my own Bioinformatcs lab. Scary stuff.&lt;/p&gt;
&lt;p&gt;Anyway, I decided to set this blog up as a more technical place to play around with various data analysis techniques, new algorithms, new packages, etc. Since it is something that I am pretty passionate about, there is likely to be a bit of a running theme throughout, but really I will be looking at data from a whole load of different sources. I often play around with “fun” challenges like those set by &lt;a href=&#34;https://projecteuler.net&#34;&gt;Project Euler&lt;/a&gt; and &lt;a href=&#34;https://www.kaggle.com&#34;&gt;Kaggle&lt;/a&gt;, so I figured that it may be useful for me to put some of these out there in case my dumb mistakes when learning these things can help somebody else in the future. As the great philosopher Jake from &lt;a href=&#34;https://en.wikipedia.org/wiki/Adventure_Time&#34;&gt;Adventure Time&lt;/a&gt;, once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dude, sucking at something is the first step towards being sorta good at something&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, here is the first post in what I hope will become a regular source of potentially interesting data wrangling. I just wanted to do something simple to start with, and one thing that I have always wanted to play with is the Twitter API for accessing the myriad completely valid and interesting opinions of the millions of Twitter users out there… Hopefully I will keep away from the shadier parts of the interwebz, but in all seriousness there is a huge amount of useful stuff floating around out there.&lt;/p&gt;
&lt;p&gt;So quite why I picked this particular question as my first post I have no idea. With billions of opinions and social interactions available to me, I have chosen to answer the following rather inconsequential question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is it an “Ultramarathon”, “Ultra Marathon” or “Ultra-Marathon”?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My idea of a good time is running a really, really long way, usually for a whole day or sometimes even more. Any race over a marathon in distance is called an “ultramarathon” - i.e. it is beyond the marathon. Now, there is a huge amount of baggage attached to this, and some people seem to really get their panties in a bunch over the term. Does it “count” as an ultramarathon if I run back to the car after a marathon? Does it “count” as an ultramarathon if I walk most of it? Does it “count” as an ultramarathon if I run a marathon a day for a whole week? There’s a lot of questions about “counting”, but I’ve never been very good at counting personally (says the mathematician…). I actually really dislike the word myself as it smacks a little of elitism, and I prefer to just think of it as running. A 10K is a running race, a marathon is a running race, and a 100 miler is a running race. Let’s just leave it at that.&lt;/p&gt;
&lt;p&gt;Anyway, one thing that I have never seen definitively answered is what the correct nomenclature should actually be, and I find myself switching willy nilly between the three possible spellings as the winds change. I’ve probably used all three in this post. So I thought that I would let the people speak, and see what the general consensus is of the Twitterati. And let’s face it, no ultrarunner worth their salt would run without Tweeting about it. So let’s take a look at which term is used most often on Twitter and settle this mass debate that I am having with myself (chortle) once and for all.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-the-twitter-api&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Setting up the Twitter API&lt;/h1&gt;
&lt;p&gt;Twitter uses &lt;a href=&#34;https://apps.twitter.com&#34;&gt;OAuth&lt;/a&gt; as a way to control programmatic access to its information without requiring passwords. Essentially, Twitter grants you an access token which is used to grant you access to the client information via a web service without actually giving you direct access to the client machine. It’s pretty easy to set up. Once you have a Twitter account set up, go to &lt;a href=&#34;https://apps.twitter.com&#34; class=&#34;uri&#34;&gt;https://apps.twitter.com&lt;/a&gt; and click on the “Create New App” button. This will bring up the following page:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Twitter_create_app.png&#34; alt=&#34;Create a new app in Twitter&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Create a new app in Twitter&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I filled this in with details of this blog, and created a new app. Note that to avoid errors further down the line, you need to set the “Callback URL” field to &lt;code&gt;http://127.0.0.1:1410&lt;/code&gt; which will return to your localhost on port 1410 following authentication. Press “Create your Twitter application” to create your app. This will take you to a page with information about your new app, including a tab at the top of the page called “Keys and Access Tokens”. The “API Key” and “API Secret” can then be used to access the API.&lt;/p&gt;
&lt;p&gt;One important change to make to the basic settings is to click on the “Permissions” tab and make sure that your app is set up to have permissions to “Read, Write and Access direct messages”. Be sure to regenerate your access tokens after making any changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;twitter&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; TwitteR&lt;/h1&gt;
&lt;p&gt;Since R is my go-to analysis package, I will be using the &lt;code&gt;TwitteR&lt;/code&gt; package from Jeff Gentry to access the API. You can also access through scripting languages like perl and python, which I will likely explore in the future. You can install &lt;code&gt;TwitteR&lt;/code&gt; from the &lt;a href=&#34;https://cran.r-project.org&#34;&gt;Comprehensive R Archive Network&lt;/a&gt; by doing the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;twitteR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or alternatively you can install the less stable, but more up-to-date, development version from GitHub:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install_github(&amp;quot;geoffjentry/twitteR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This requires the &lt;code&gt;devtools&lt;/code&gt; package to be installed as well.&lt;/p&gt;
&lt;p&gt;We should now be set up, but actually I found that I also needed to install some additional packages so that OAuth credentials can be correctly captured in the browser-based authentication:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;httpuv&amp;quot;)
install.packages(&amp;quot;httr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to set up our authorisation (put your API key and secret in place of these placeholders):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;twitteR&amp;quot;)
setup_twitter_oauth(&amp;quot;API key&amp;quot;, &amp;quot;API secret&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open up a browser session where you can authenticate your API app. After this, close the browser and you are ready to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ultramarathon-ultra-marathon-or-ultra-marathon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Ultramarathon, Ultra Marathon, or Ultra-Marathon?&lt;/h1&gt;
&lt;p&gt;So now we are all set up and we can take a look at how to access the API. The workhorse of the &lt;code&gt;twitteR&lt;/code&gt; package is the &lt;code&gt;searchTwitter()&lt;/code&gt; function. This can search for something like a hashtag or key word, and can use basic boolean logic such as &lt;code&gt;AND&lt;/code&gt; (&lt;code&gt;+&lt;/code&gt;) and &lt;code&gt;OR&lt;/code&gt; (&lt;code&gt;-&lt;/code&gt;). The API actually only allows you to access information from a short time in the past, so we can only get Tweets from the last week or so. So let’s get the most recent Tweets relating to ultrarunning and count how many Tweets over the last few days have used the three different terms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;twitteR&amp;quot;)
numtweets      &amp;lt;- 500000
um_tweets_all  &amp;lt;- searchTwitter(&amp;quot;ultramarathon|ultra marathon&amp;quot;, n = numtweets)
um_tweets_trim &amp;lt;- strip_retweets(um_tweets_all, strip_manual=TRUE, strip_mt=TRUE)
um_tweets_text &amp;lt;- sapply(um_tweets_trim, function (x) x$getText())
um_tweets_text &amp;lt;- um_tweets_text[grep(&amp;quot;ultramarathon|ultra marathon|ultra-marathon&amp;quot;, um_tweets_text, ignore.case = TRUE)]
um_count &amp;lt;- NULL
for (t in c(&amp;quot;ultramarathon&amp;quot;, &amp;quot;ultra-marathon&amp;quot;, &amp;quot;ultra marathon&amp;quot;)) {
  um_count[[t]] &amp;lt;- length(grep(t, um_tweets_text, ignore.case = TRUE))
}
par(mar = c(0,0,0,0))
pie(um_count, col = c(&amp;quot;grey90&amp;quot;,&amp;quot;grey70&amp;quot;,&amp;quot;grey50&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;ultramarathon_tweet_pie.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s look at this code. After loading the &lt;code&gt;twitteR&lt;/code&gt; package and specifying the number of Tweets to load, we access the Twitter API using &lt;code&gt;searchTwitter&lt;/code&gt; and load in the 500,000 most recent Tweets with the terms “ultra”, “running”, or “ultrarunning” in them (there will be many fewer than this, but I want to capture every Tweet possible). This produces a list of 961 objects of class &lt;code&gt;status&lt;/code&gt;, which is a specially defined reference class as a container for Twitter statuses. Next we strip out all of the retweets to leave 585 Tweets, then pull out the text from the list objects (&lt;code&gt;sapply()&lt;/code&gt; applies the accessor function &lt;code&gt;getText()&lt;/code&gt; to all elements of the list). Since I am only interested in three specific terms, I use regular expressions to look only at Tweets containing one of these terms (giving 316 Tweets), and then count how many Tweets contain each of the three specific terms. Finally I generate a pie chart of the results (yes, yes, I know - I hate pie charts as well).&lt;/p&gt;
&lt;p&gt;A couple of things here. First of all, there are a lot of retweets. Of the 961 Tweets originally analysed, only 585 remain after removing the retweets. This means that 39.13% of these Tweets were retweets. Man, we ultrarunners aren’t very original are we? Unfortunately this therefore drastically reduces the number of Tweets that I am analysing. Secondly, because of the way the pattern matching is done we end up with a lot of Tweets with “ultra” or “running” in them that don’t match any of the three specific terms that I am looking at here. Also, this counting may be double counting some Tweets if both versions are used in a single Tweet. But I can’t be bothered taking such stupidity into account right now! ;)&lt;/p&gt;
&lt;p&gt;So with these caveats in place, it seems pretty clear that the correct term is most definitely “Ultra Marathon”. So there you go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ultrarunning-ultra-running-or-ultra-running&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Ultrarunning, Ultra Running or Ultra-Running?&lt;/h1&gt;
&lt;p&gt;Okay cool. So we know how to define the event. How about the act of running an ultra marathon? So let’s do the same again, this time looking at whether I should be saying “ultrarunning”, “ultra running”, or “ultra-running”. The code is practically identical, just using slightly different words in the regular expression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ur_tweets_all  &amp;lt;- searchTwitter(&amp;quot;ultrarunning|ultra running&amp;quot;,  n = numtweets)
ur_tweets_trim &amp;lt;- strip_retweets(ur_tweets_all, strip_manual=TRUE, strip_mt=TRUE)
ur_tweets_text &amp;lt;- sapply(ur_tweets_trim, function (x) x$getText())
ur_tweets_text &amp;lt;- ur_tweets_text[grep(&amp;quot;ultrarunning|ultra running|ultra-running&amp;quot;, ur_tweets_text, ignore.case = TRUE)]
ur_count &amp;lt;- NULL
for (t in c(&amp;quot;ultrarunning&amp;quot;, &amp;quot;ultra-running&amp;quot;, &amp;quot;ultra running&amp;quot;)) {
  ur_count[[t]] &amp;lt;- length(grep(t, ur_tweets_text, ignore.case = TRUE))
}
par(mar = c(0,0,0,0))
pie(ur_count, col = c(&amp;quot;grey90&amp;quot;,&amp;quot;grey70&amp;quot;,&amp;quot;grey50&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;ultrarunning_tweet_pie.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are a lot more Tweets relating to ultra “running” compared to ultra “marathon”, with 11,984 Tweets in the starting data set. However, again we lose a lot of Tweets through retweets leaving us with only 2,112 Tweets to play with. After trimming out Tweets that don’t follow the format that I am looking at here, we are left with only 179 – even less than in the last analysis.&lt;/p&gt;
&lt;p&gt;In this case, it is less clear cut, and whilst the single word term “ultrarunning” is used most often, the two word “ultra running” is not far behind. Damn, I wanted a clearly defined outcome, but I guess I will let you off whichever one you choose to use. But god help anybody who chooses to hyphenate either term…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-cloud&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Word Cloud&lt;/h1&gt;
&lt;p&gt;Since we have these Tweets available, let’s generate a word cloud to see what other terms are being talked about in relation to ultra marathons and ultrarunning. A word cloud takes some text and works out the most common words within it, then represents them in a cloud of words (funnily enough) with more common words being more prominent. Here we use the text mining package &lt;code&gt;tm&lt;/code&gt; for identifying and processing unique words from these Tweets, and the &lt;code&gt;wordcloud&lt;/code&gt; package for plotting them. The Tweets are loaded into a &lt;code&gt;Corpus&lt;/code&gt; object, and various mappings are performed to remove irrelevant text like punctuation, as well as commonly used words in English like &lt;em&gt;I&lt;/em&gt;, &lt;em&gt;We&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt;, &lt;em&gt;the&lt;/em&gt;, etc. Note that I have converted the encoding of all of these Tweets into UTF-8 encoding as I was having issues using the &lt;code&gt;tolower()&lt;/code&gt; function when some Tweets contained non-UTF-8 characters. I have coloured the plot using the &lt;code&gt;brewer.pal()&lt;/code&gt; function from the &lt;code&gt;RColorBrewer&lt;/code&gt; package, which in this case generates a palette of 9 equally spaced colours ranging from Red to blue (via white). The parameters here will plot a maximum of 1,000 words, and will only consider a word if it is present more than 5 times. By not using a random order, the most prominent words are plotted at the center of the cloud:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tm&amp;quot;)
library(&amp;quot;wordcloud&amp;quot;)
library(&amp;quot;SnowballC&amp;quot;)
all_tweets &amp;lt;- c(ur_tweets_text, um_tweets_text)
all_tweets &amp;lt;- iconv(all_tweets, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;)         ## Convert encodings
cloud_dat &amp;lt;- Corpus(VectorSource(all_tweets))                        ## Create Corpus
cloud_dat &amp;lt;- tm_map(cloud_dat, PlainTextDocument)                    ## Make plain text
cloud_dat &amp;lt;- tm_map(cloud_dat, content_transformer(tolower))         ## Convert to lower case
cloud_dat &amp;lt;- tm_map(cloud_dat, removePunctuation)                    ## Remove punctuation
cloud_dat &amp;lt;- tm_map(cloud_dat, removeWords, stopwords(&amp;quot;english&amp;quot;))    ## Remove common English words
par(mar = c(0,0,0,0))
wordcloud(cloud_dat, max.words = 1000, min.freq = 5, random.order = FALSE, colors = brewer.pal(9, &amp;quot;RdBu&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;ultramarathon_tweet_wordcloud.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Kind of what you would expect, with &lt;em&gt;ultra&lt;/em&gt;, &lt;em&gt;marathon&lt;/em&gt;, &lt;em&gt;running&lt;/em&gt;, &lt;em&gt;ultramarathon&lt;/em&gt;, and &lt;em&gt;ultrarunning&lt;/em&gt; being most prominent. &lt;em&gt;training&lt;/em&gt; is also pretty common, so it’s good to know that there is some of that going on between Tweets. I’m also pleased to see that &lt;em&gt;tom&lt;/em&gt; is quite enriched, which I can only assume to be Tom from Bognor’s take-over of the internet. It’s also nice to see people talking about &lt;em&gt;shorts&lt;/em&gt;, but come on people – it’s always shorts weather!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-word&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Final Word&lt;/h1&gt;
&lt;p&gt;This has been quite useful for me to get the hang of using the Twitter API, and I hope it has been a little interesting or useful for some of you. I am hoping that this will kick off regular use of this blog, and I will try and update it more regularly along with my less technical &lt;a href=&#34;http://constantforwardmotion.blogspot.com&#34;&gt;running blog&lt;/a&gt; as I start using it to play with new toys in my work. And hey, at least now you know that they are called &lt;strong&gt;Ultra Marathons&lt;/strong&gt; and that I love &lt;strong&gt;Ultrarunning&lt;/strong&gt;. So we’ve all learned something today. And knowing is half the battle.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to the University of Portsmouth Bioinformatics Group Website</title>
      <link>/post/2018-04-01-welcome-to-the-university-of-portsmouth-bioinformatics-group-website/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-01-welcome-to-the-university-of-portsmouth-bioinformatics-group-website/</guid>
      <description>&lt;p&gt;Welcome to the website of Dr Sam Robson, Lead Bioinformatician at the Centre for Enzyme Innovation at the University of Portsmouth. I set up this group having recently moved from the University of Cambridge, and hope to develop the group further over the coming years. I am going to aim to post regularly on this blog with posts relating to data analysis, experimental design, advances in sequencing technology, and other related matters that may be of interest to visitors.&lt;/p&gt;

&lt;p&gt;I have kicked things off with a &lt;a href=&#34;/resource/RTutorial/index.html&#34;&gt;tutorial&lt;/a&gt; for those of you that are interested in learning how to use the statistical programming language &lt;a href=&#34;https://cran.r-project.org&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt;. This language is incredibly useful for anybody looking to perform any kind of statistical analysis, and the &lt;a href=&#34;https://bioconductor.org&#34; target=&#34;_blank&#34;&gt;Bioconductor&lt;/a&gt; packages offer countless extensions to the base functionality to allow you to work with data of any type that you might care to mention. If you are interested in data analysis, I thoroughly recommend checking it out, and if you find it useful or if you splot any issues please comment.&lt;/p&gt;

&lt;p&gt;In addition, I have begun to build a list of commonly used &lt;a href=&#34;../../resource/bioinformaticstools/index.html&#34;&gt;bioinformatics tools&lt;/a&gt;, which may help other people to see which tools are currently available for a variety of different bioinformatics tasks. It is a work in progress, so please bear with me while I continue to populate it. I hope to keep this up to date as I discover new tools.&lt;/p&gt;

&lt;p&gt;In the short time that I have been at the University of Portsmouth, I have had similar conversations with a number of researchers and so will focus on discussing some of these commonly occurring questions in my first few posts (e.g. sample size for experimental design, number of reads required for a sequencing experiment, where to sequence your data, differences between sequecning technologies, etc.). Some of these may then develop further into additional resources.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Promoter-bound METTL3 maintains myeloid leukaemia by m6A-dependent translation control</title>
      <link>/publication/2017_promoter-bound_mettl3_maintains_myeloid_leukaemia_by_m6a-dependent_translation_control/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/publication/2017_promoter-bound_mettl3_maintains_myeloid_leukaemia_by_m6a-dependent_translation_control/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sequencing Facilities</title>
      <link>/resources/sequencingfacilities/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      <guid>/resources/sequencingfacilities/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#commercial-sequencing-facilities&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Commercial Sequencing Facilities&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#theragen-etex&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; &lt;span&gt;Theragen Etex&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#source-bioscience&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; &lt;span&gt;Source Bioscience&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lgc-sequencing-service&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3&lt;/span&gt; &lt;span&gt;LGC Sequencing Service&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mr.dna-lab&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4&lt;/span&gt; &lt;span&gt;Mr. DNA Lab&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gatc-biotech&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.5&lt;/span&gt; &lt;span&gt;GATC Biotech&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genome-scan&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.6&lt;/span&gt; &lt;span&gt;Genome Scan&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparison-table&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Comparison Table&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#platforms&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Platforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#technologies&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Technologies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#services&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Services&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;On this page you will find a list of commercial and academic sequencing facilities that you may wish to consider when looking for sequencing solutions for your project. In cases where the facility have been used by a member of the faculty here at the University of Portsmouth, you will find links to the researchers with experience. I will attempt to collate some feedback from these researchers on this page over time, but if you have any further questions, please contact the researchers indicated directly.&lt;/p&gt;
&lt;div id=&#34;commercial-sequencing-facilities&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Commercial Sequencing Facilities&lt;/h1&gt;
&lt;div id=&#34;theragen-etex&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; &lt;a href=&#34;http://www.theragenetex.com/bio/&#34;&gt;Theragen Etex&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A company in South Korea who offer a number of different sequencing services, including whole genome, exome, transcriptome, and epigenome sequencing technologies. They offer sequencing using Illumina HiSeq 4000/2500, Ion Torrent PGM and Ion Torrent Proton sequencers. Their services include whole genome sequencing, exome sequencing, RNA seq, epigenomics and &lt;strong&gt;de novo&lt;/strong&gt; assembly. They also offer library preparation and bioinformatics support if required. They offer a guarentee on sequence on coverage, although this results from pooling of samples across multiple lanes which may cause batch difficulties in downstream analyses. They are very quick to respond to emails and in our experience seem very keen to provide us with good quality data. Typically they seem to offer very competitive rates for sequencing needs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-pharmacy-and-biomedical-sciences/staff/prof-darek-gorecki.html&#34;&gt;Prof. Darek Gorecki&lt;/a&gt; - RNA seq&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;source-bioscience&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; &lt;a href=&#34;https://www.sourcebioscience.com/?gclid=CJKgxLDd99YCFe6w7QodMpsMeg&#34;&gt;Source Bioscience&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A company based in Nottingham who seem to offer a very comprehensive catalogue of sequencing services, including RNA seq, ChIP seq, whole genome sequencing, exome sequencing, targeted sequening and metagenomics. They offer DNA/RNA extraction, library preparation, and bioinformatics support if required. They offer Illumina MiSeq, Illumina HiSeq and Illumina NextSeq services. They are one of the leading providers of commercial sequencing in Europe. Data that we have recieved from them in the past has been of high quality, however we have had one case of lost data which is yet to be resolved in a satisfactory manner. Communication when requesting services and data generation are very fast, but communication when problems have arisen has been very poor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-pharmacy-and-biomedical-sciences/staff/prof-darek-gorecki.html&#34;&gt;Prof. Darek Gorecki&lt;/a&gt; - RNA seq&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/maria-salta.html&#34;&gt;Dr. Maria Salta&lt;/a&gt; - 16S rDNA Sequencing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/prof-matt-guille.html&#34;&gt;Prof. Matt Guille&lt;/a&gt; - RNA Seq&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lgc-sequencing-service&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; &lt;a href=&#34;https://www.lgcgroup.com/services/dna-sequencing/#.WeYcaUw7lBw&#34;&gt;LGC Sequencing Service&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;LGC (Laboratory of the Government Chemist) are a UK-based sequencing company with a long history in bioanalytics. They offer a range of services as well as next generation sequencing, including consultancy, product testing and forensic science. They are able to provide services including RNA seq, ChIP seq, metagenomics, microbial sequencing, and genotyping. They offer assistance with DNA/RNA extraction, library preparation, project design, and bioinformatics support.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/dr-joanne-preston.html&#34;&gt;Joanne Preston&lt;/a&gt; - 16S rDNA Sequencing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/dr-joy-watts.html&#34;&gt;Joy Watts&lt;/a&gt; - Transcriptome Assembly&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mr.dna-lab&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; &lt;a href=&#34;http://www.mrdnalab.com&#34;&gt;Mr. DNA Lab&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;They offer sequencing using Illumina MiSeq, Illumina HiSeq, Pacific Biosciences Sequel (for longer read lengths), Ion Torrent S5 XL and Ion Torrent PGM machines. They offer support for many sequencing services, including whole genome sequencing, exome sequencing, &lt;em&gt;de novo&lt;/em&gt; assembly, metagenomics, RNA seq and ChIP seq. They offer a guarentee on sequence on coverage, although this results from pooling of samples across multiple lanes which may cause batch difficulties in downstream analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gatc-biotech&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; &lt;a href=&#34;https://www.gatc-biotech.com/en/index.html&#34;&gt;GATC Biotech&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;GATC Biotech are a German-based commercial sequencing facility providing a comprehensive list of next generation sequencing services including whole genome sequencing, &lt;strong&gt;de novo&lt;/strong&gt; sequencing, targeted sequencing, amplicon sequencing, metagenomics, RNA seq, ChIP seq and BS seq. They have expertise in Illumina MiSeq, Illumina HiSeq and PacBio RS II. Their &lt;strong&gt;INVIEW&lt;/strong&gt; and &lt;strong&gt;NGSELECT&lt;/strong&gt; packages provide a complete service, from DNA/RNA extraction, through library preparation and sequencing, to bioinformatics analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;genome-scan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.6&lt;/span&gt; &lt;a href=&#34;https://www.genomescan.nl&#34;&gt;Genome Scan&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Through their ServiceXS department, Genome Scan provide a comprehensive series of research services, including whole genome sequencing, exome sequencing, RNA seq, small RNA seq and bisulfite seq. They offer sequencing using Illumina HiSeq 2500/4000, Illumina NextSeq 500 and Pacific Bioscience RS II (for longer read lengths). They provide a full sequencing service, including library proeration and downstream bioinformatics support.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-table&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Comparison Table&lt;/h1&gt;
&lt;p&gt;Below is a table that I have compiled based on the instruments available and the analyses that each facility is able to provide. Please note that I may have missed something - if I have made a mistake please let me know at &lt;a href=&#34;mailto:samuel.robson@port.ac.uk&#34; class=&#34;email&#34;&gt;samuel.robson@port.ac.uk&lt;/a&gt;. It is highly likely that any of these facilities would have the capacity to run whatever sequencing experiment you are hoping to conduct, but this table is based on the expertise highlighted on their websites:&lt;/p&gt;
&lt;div id=&#34;platforms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Platforms&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;em&gt;Facility&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Theragen&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;LGC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Mr DNA&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;GATC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Genome Scan&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;HiSeq&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;MiSeq&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;NextSeq&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;PacBio&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Ion Torrent&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;technologies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Technologies&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;em&gt;Facility&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Theragen&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;LGC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Mr DNA&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;GATC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Genome Scan&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;strong&gt;De Novo&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Transcriptomics&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Epigenetics&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;WGS&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;WES&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Metagenomics&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Epigenomics&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;F&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;services&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Services&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;em&gt;Facility&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Theragen&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;LGC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Mr DNA&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;GATC&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Genome Scan&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;DNA/RNA Extraction&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Library Prep&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Bioinformatics&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;td&gt;T&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Changes in microbial communities of Cliona celeta during observed changes in sponge health</title>
      <link>/project/2017-09-29-microbial-community-of-cliona-celeta-during-observed-changes-in-sponge-health/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-09-29-microbial-community-of-cliona-celeta-during-observed-changes-in-sponge-health/</guid>
      <description>&lt;p&gt;Cliona celeta is a yellow boring sponge found around the waters of the UK. The diversity of sponges in the Skomer Marine Nature Reserve has been surveyed since the 1990s. In 2013 divers observed apparent disease in C. celeta individuals across Skomer Island and the Marloes Peninsula. The previously healthy yellow sponge became colonised by fouling organisms followed by tissue decay and patches of black necrotic sponge. This issue appears to be seasonal, largely seen between the months of March and October, and is the first recorded incidence of sponge disease in temperate coastal waters. In addition, several sponges were seen to be fouled by algal and invertebrate species, without progression to the black state, which is indicative of reduced anti-fouling mechanisms in the sponge.&lt;/p&gt;

&lt;p&gt;Working together with &lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/dr-joanne-preston.html&#34; target=&#34;_blank&#34;&gt;Dr Joanne Preston&lt;/a&gt;, we are working to identify the change in microbial communities following the shift between healthy and fouled or necrotic tissue in sponges based on microbiological culturing and 16S rDNA sequencing of the sponge host and the microbial communities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Genotyping of Ancient DNA from Crew Members from the Mary Rose</title>
      <link>/project/2017-09-29-genotyping-of-crew-members-of-the-mary-rose/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-09-29-genotyping-of-crew-members-of-the-mary-rose/</guid>
      <description>&lt;p&gt;Built in Portsmouth in 1510, the Mary Rose was the flagship of the fleet of Henry VIII, and had a long and lustrious career until it capsized north of the Isle of Wight in the Battle of the Solent in 1545. Fewer than 35 of the roughly 400 crew members escaped. The wreck was discovered in 1971, and following years of excavation activities the hull was raised from the seabed in 1982 and transported to its home port of Portsmouth. Since then, years of archaeological work has been carried out on the hull itself and on the nearly 20,000 artefacts that have so far been discovered.&lt;/p&gt;

&lt;p&gt;Working together with &lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/dr-garry-scarlett.html&#34; target=&#34;_blank&#34;&gt;Dr Garry Scarlett&lt;/a&gt; at the University of Portsmouth, and the &lt;a href=&#34;http://www.maryrose.org&#34; target=&#34;_blank&#34;&gt;Mary Rose Museum&lt;/a&gt;, we will be looking to understand more about the members of the crew by working on genotyping of the ancient DNA from the skeletons of crew members raised from the wreckage. Using whole genome sequencing techniques, we hope to develop an understanding of phenotypic characteristics, disease traits and geographic information of crew members of one of the most famous ship wrecks in history.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bioinformatics Tools</title>
      <link>/resources/bioinformaticstools/</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/resources/bioinformaticstools/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sequence-alignment&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Sequence Alignment&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#clustalw&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.1&lt;/span&gt; &lt;span&gt;ClustalW&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seaview&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.2&lt;/span&gt; &lt;span&gt;SeaView&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pagan&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.3&lt;/span&gt; &lt;span&gt;PAGAN&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ncbi-blast&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.4&lt;/span&gt; &lt;span&gt;NCBI BLAST&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hmmer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.5&lt;/span&gt; &lt;span&gt;HMMer&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fasta&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.6&lt;/span&gt; &lt;span&gt;FASTA&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#muscle&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1.7&lt;/span&gt; &lt;span&gt;MUSCLE&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motif-and-domain-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Motif and Domain Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#meme-suite&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; &lt;span&gt;MEME Suite&lt;/span&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#meme&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.1&lt;/span&gt; MEME&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dreme&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.2&lt;/span&gt; DREME&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mast&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.3&lt;/span&gt; MAST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tomtom&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.4&lt;/span&gt; TOMTOM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gomo&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.5&lt;/span&gt; GOMO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#meme-chip&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1.6&lt;/span&gt; MEME-CHIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#homer-suite&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; &lt;span&gt;HOMER Suite&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#domain-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Domain Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interproscan&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; &lt;span&gt;InterProScan&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pfam&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; &lt;span&gt;Pfam&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smart&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; &lt;span&gt;SMART&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-repositories&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Data Repositories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gene-expression-omnibus-geo&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.5&lt;/span&gt; &lt;span&gt;Gene Expression Omnibus (GEO)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#arrayexpress&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.6&lt;/span&gt; &lt;span&gt;ArrayExpress&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequence-read-archive-sra&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.7&lt;/span&gt; &lt;span&gt;Sequence Read Archive (SRA)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genome-mapping&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Genome Mapping&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bwa&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; &lt;span&gt;BWA&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bowtie&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; &lt;span&gt;Bowtie&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maq&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; &lt;span&gt;MAQ&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tophat&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; &lt;span&gt;TopHat&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#star&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.5&lt;/span&gt; &lt;span&gt;STAR&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#novoalign&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.6&lt;/span&gt; &lt;span&gt;NovoAlign&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shrimp&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.7&lt;/span&gt; &lt;span&gt;SHRiMP&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eland&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.8&lt;/span&gt; ELAND&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#de-novo-assembly&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; De Novo Assembly&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#soap&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; &lt;span&gt;SOAP&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#abyss&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; &lt;span&gt;ABySS&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trinity&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.3&lt;/span&gt; &lt;span&gt;Trinity&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#read-quantification&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Read Quantification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#htseq&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; &lt;span&gt;HTseq&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kallisto&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; &lt;span&gt;Kallisto&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gene-ontology-go-and-network-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Gene Ontology (GO) and Network Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#david&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; &lt;span&gt;DAVID&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gsea&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; &lt;span&gt;GSEA&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#go&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; &lt;span&gt;GO&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#great&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; &lt;span&gt;GREAT&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#panther&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.5&lt;/span&gt; &lt;span&gt;PANTHER&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#blast2go&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.6&lt;/span&gt; &lt;span&gt;blast2GO&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quality-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Quality Control&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fastqc&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.1&lt;/span&gt; &lt;span&gt;FastQC&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiqc&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8.2&lt;/span&gt; &lt;span&gt;MultiQC&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#peak-calling&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Peak Calling&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#findpeaks&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.1&lt;/span&gt; &lt;span&gt;FindPeaks&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#macs&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.2&lt;/span&gt; &lt;span&gt;MACS&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hpeak&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.3&lt;/span&gt; &lt;span&gt;HPeak&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#peakseq&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.4&lt;/span&gt; &lt;span&gt;PeakSeq&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.5&lt;/span&gt; &lt;span&gt;QuEST&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#homer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.6&lt;/span&gt; &lt;span&gt;HOMER&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#peakranger&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9.7&lt;/span&gt; &lt;span&gt;PeakRanger&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequence-manipulation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Sequence Manipulation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bedtools&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.1&lt;/span&gt; &lt;span&gt;BEDTools&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#samtools&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.2&lt;/span&gt; &lt;span&gt;SAMtools&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adapter-trimming&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Adapter Trimming&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cutadapt&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.1&lt;/span&gt; &lt;span&gt;Cutadapt&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trimmomatic&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.2&lt;/span&gt; &lt;span&gt;Trimmomatic&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trim-galore&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.3&lt;/span&gt; &lt;span&gt;Trim Galore&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reaper&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.4&lt;/span&gt; &lt;span&gt;Reaper&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pathway-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Pathway Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ingenuity&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.1&lt;/span&gt; &lt;span&gt;Ingenuity&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kegg&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.2&lt;/span&gt; &lt;span&gt;KEGG&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cytoscape&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.3&lt;/span&gt; &lt;span&gt;Cytoscape&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#string&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12.4&lt;/span&gt; &lt;span&gt;STRING&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#genome-browsers&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13&lt;/span&gt; Genome Browsers&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#university-of-california-santa-cruz-genome-browser-ucsc&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13.1&lt;/span&gt; &lt;span&gt;University of California, Santa Cruz Genome Browser (UCSC)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensembl-genome-browser&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13.2&lt;/span&gt; &lt;span&gt;Ensembl Genome Browser&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#integrative-genomics-viewer-igv&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13.3&lt;/span&gt; &lt;span&gt;Integrative Genomics Viewer (IGV)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;sequence-alignment&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Sequence Alignment&lt;/h1&gt;
&lt;div id=&#34;clustalw&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; &lt;a href=&#34;http://www.ebi.ac.uk/Tools/msa/clustalw2/&#34;&gt;ClustalW&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Useful for aligning a small number of sequences, such as homologs from multiple species. Not so useful for multiple alignment of a large amount of sequences from ChIP seq for instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seaview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; &lt;a href=&#34;http://doua.prabi.fr/software/seaview&#34;&gt;SeaView&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Graphical visualisation of multiple sequence alignment. Can perform alignment of sequences, or can be used to visualise the results of alignments from other algorithms. Can be manually adjusted which can be useful when incorporating prior knowledge into the alignment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pagan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; &lt;a href=&#34;https://code.google.com/p/pagan-msa/wiki/PAGAN&#34;&gt;PAGAN&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A phylogeny-aware general purpose aligner which can be more useful for clustering when there are likely to be large gaps in some sequences compared to others (for instance if comparing gene sequences from different species with non-homologous exons).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ncbi-blast&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; &lt;a href=&#34;http://blast.st-va.ncbi.nlm.nih.gov/Blast.cgi&#34;&gt;NCBI BLAST&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Suite of tools for searching a protein or nucleotide database to find potential matches or homologs, or for aligning multiple sequences. This tool is particularly useful when you have a query sequence and want to find either known homologs amongst different species, or to find additional proteins with similar structural domains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hmmer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.5&lt;/span&gt; &lt;a href=&#34;http://hmmer.janelia.org&#34;&gt;HMMer&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Similar to BLAST, HMMer uses hidden Markov models to improve the accuracy and ability to detect remote homologs by using a stronger underlying mathematical model. This suite of tools can be used to search for homologs in protein and transcript databases in a similar way to BLAST.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fasta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.6&lt;/span&gt; &lt;a href=&#34;http://www.ebi.ac.uk/Tools/sss/fasta/&#34;&gt;FASTA&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;FASTA is the original sequence similarity search tool, and can be used to search for a query protein amongst known protein or ranscript databases such as UniProt. It uses a heuristic approach, which has been improved over the years, but BLAST is probably a better approach to use in general.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;muscle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.7&lt;/span&gt; &lt;a href=&#34;http://www.ebi.ac.uk/Tools/msa/muscle/&#34;&gt;MUSCLE&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MUSCLE is a multiple sequence comparison tool similar to ClustalW that may be slightly faster with more accurate results. It can be used to perform gapped alignment of a set of sequences in any supported format (e.g. fasta).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;motif-and-domain-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Motif and Domain Analysis&lt;/h1&gt;
&lt;div id=&#34;meme-suite&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; &lt;a href=&#34;http://meme.nbcr.net/meme/&#34;&gt;MEME Suite&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The MEME suite is a series of tools for motif analysis and consists of the following tools:&lt;/p&gt;
&lt;div id=&#34;meme&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.1&lt;/span&gt; MEME&lt;/h3&gt;
&lt;p&gt;De-Novo discovery of motifs from a series of related protein or transcript sequences. This method is more designed for comparison of sequences from, say, a handful of transcription factor sites and is not so well designed for large number of sequences (e.g. from ChIP seq experiments). This tool is better for the identification of wider motifs. For narrower motifs use DREME.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dreme&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.2&lt;/span&gt; DREME&lt;/h3&gt;
&lt;p&gt;Like MEME above, but better for the discovery of narrower motifs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mast&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.3&lt;/span&gt; MAST&lt;/h3&gt;
&lt;p&gt;MAST is a motif alignment and search tool for the identification of known (or identified) motifs amongst some database of sequences. This can be either a protein or transcript database, or for instance a known motif can be searched within a set of sequences such as from a ChIP seq experiment. A match score is calculated by comparing the position weight matrix of the motif with each sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tomtom&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.4&lt;/span&gt; TOMTOM&lt;/h3&gt;
&lt;p&gt;TOMTOM is a way to compare de novo motifs to a database of known motifs contained within the MEME database. A list of matching motifs is returned, so this can be used to see if for instance your motif matches the known binding motif of a transcription factor within the database. One of the databases that is queried is the JASPAR database (see resources) which contains a large well-maintained and curated set of known TF binding sites.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gomo&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.5&lt;/span&gt; GOMO&lt;/h3&gt;
&lt;p&gt;GOMO takes an input motif and scores all genes based on the binding affinity of the motif at their upstream promoter region. It then performs a gene ontology analysis to see if this motif is associated with the promoter regions of genes involved in any particular function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;meme-chip&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1.6&lt;/span&gt; MEME-CHIP&lt;/h3&gt;
&lt;p&gt;MEME-CHIP is a combined motif discovery tool for ChIP seq results. It relies on the fact that the mid-100bp region of the supplied sequences represent the likely binding location of the peak, performs motif discovery across a large number of sequences, and compares against known motif databases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;homer-suite&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; &lt;a href=&#34;http://homer.ucsd.edu/homer/&#34;&gt;HOMER Suite&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The HOMER (Hypergeometric Optimization of Motif EnRichment) suite of tools is ostensibly a package (like MEME) for &lt;em&gt;de novo&lt;/em&gt; identification of enriched motifs from genomic data, but has expanded considerably to provide resources for many aspects of NGS analysis. It has tools for analysis of ChIP-Seq, RNA-Seq, GRO-Seq, Hi-C and BS-Seq data. For instance, for ChIP-seq it has some very useful functions for annotation of peaks (annotatePeaks.pl), motif analysis on called peaks (findMotifsGenome.pl), and differential peak analysis (getDifferentialPeaksReplicates.pl).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;domain-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Domain Analysis&lt;/h1&gt;
&lt;div id=&#34;interproscan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; &lt;a href=&#34;http://www.ebi.ac.uk/interpro/&#34;&gt;InterProScan&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Interpro is a database of protein families and known structural domains from the EBI. InterProScan allows users to scan a query sequence for known structural domains or associated protein families, comparing against proteins in the UniProt Knowledge Database.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pfam&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; &lt;a href=&#34;http://pfam.xfam.org&#34;&gt;Pfam&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Pfam is a database from the EBI containing identified protein famillies based on multiple sequence alignments and HMM analysis of known sequences. The Pfam website can be used to view protein families, or view the protein domain structure of known proteins, or alternative can be used to search a query sequence for known domains.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; &lt;a href=&#34;http://smart.embl-heidelberg.de&#34;&gt;SMART&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SMART is a well-maintained modular database (based on SwissProt, TReMBL and the like) of annotated and functional protein domains. Domain structure of known proteins as well as identification of known domains within novel protein sequences can be achieved using this tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-repositories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Data Repositories&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;gene-expression-omnibus-geo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.5&lt;/span&gt; &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/geo/&#34;&gt;Gene Expression Omnibus (GEO)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;GEO is the NCBI repository of high throughput data, including sequencing and microarray data. All data maintained within this database is MIAME-compliant, and is synched with projects in ArrayExpress as well. Most experiments contain both raw data (raw chip data for microarrays, or raw read files for sequencing), as well as processed data files (mapped data, called peaks, normalised microarray results, etc).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arrayexpress&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.6&lt;/span&gt; &lt;a href=&#34;http://www.ebi.ac.uk/arrayexpress/&#34;&gt;ArrayExpress&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ArrayExpress is the EBI-maintained repository for high-throughput data. As with GEO, it contains data for microarray experiments and sequencing experiments of all kinds, all of which are MIAME-compliant. Most experiments contain both raw data (raw chip data for microarrays, or raw read files for sequencing), as well as processed data files (mapped data, called peaks, normalised microarray results, etc).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-read-archive-sra&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.7&lt;/span&gt; &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/sra&#34;&gt;Sequence Read Archive (SRA)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The SRA is probably the single most useful source for the storage of sequence level data in the world. GEO and ArrayEpxress are linked to the SRA, and this is where the raw and mapped data for most sequencing experiments are stored. These data are stored in a proprietary XML-style format and the SRA Toolkit applications can be used to dump the files into one of a number of useful formats (e.g. fastq-dump and sam-dump).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;genome-mapping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Genome Mapping&lt;/h1&gt;
&lt;div id=&#34;bwa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; &lt;a href=&#34;http://bio-bwa.sourceforge.net&#34;&gt;BWA&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Burrows-Wheeler Aligner is one of the most widely used mapping algorithms available, and is particularly useful for mapping low-divergent sequences against a large reference genome. Each sequence is mapped with a mapping quality score, which takes into account gapped alignments and mismatches, and this can be used to define accurate mapping. The output SAM files also contain the location of mismatches in CIGAR format. The benefits of using this algorithm are its speed and accuracy. One negative is that if a sequence maps to multiple locations, only one mapped loci is returned (obviously with a low mapping score). If you want to see all mapped locations (e.g. if you are interested in regions of low complexity) then Bowtie may be better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bowtie&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; &lt;a href=&#34;http://bowtie-bio.sourceforge.net/index.shtml&#34;&gt;Bowtie&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Bowtie is one of the most commonly used mapping algorithms, and uses indexing using Burrows-Wheeler to produce fast and memory efficient alignments. Bowtie can be used to provide multiple mapping coordinates for reads from regions of low complexity and repetitive sequences in the genome. Bowtie is well maintained (with version 2 currently in circulation).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maq&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; &lt;a href=&#34;http://maq.sourceforge.net&#34;&gt;MAQ&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MAQ is an older aligner that does not seem to be maintained any longer. It has been superseded by BWA and Bowtie and should probably not be used any longer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tophat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; &lt;a href=&#34;http://ccb.jhu.edu/software/tophat/index.shtml&#34;&gt;TopHat&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;TopHat is a splice-junction aware mapper for the alignment of RNA Seq data to the genome. It is based on the Bowtie aligner above, but analyses mapped data to identify splice junctions between exons. Alternatively a list of known exon splice junctions can be provided which can be used to guide the alignment across splice junctions. This is one of the most commonly used tools for the mapping of RNA seq data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;star&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.5&lt;/span&gt; &lt;a href=&#34;https://github.com/alexdobin/STAR&#34;&gt;STAR&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Like TopHat, STAR is a splice aware aligner that is used to map RNA Seq reads to the genome. However, STAR uses a much faster algorithm than TopHat and tends to produce slightly better results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;novoalign&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.6&lt;/span&gt; &lt;a href=&#34;http://www.novocraft.com/main/page.php?s=novoalign&#34;&gt;NovoAlign&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Novoalign is a licensed mapping software tool (although a free version is available). For multi-mapping reads, you can choose to return all, none, or just one randomly selected mapping in your output.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shrimp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.7&lt;/span&gt; &lt;a href=&#34;http://compbio.cs.toronto.edu/shrimp/&#34;&gt;SHRiMP&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SHRiMP is another older free mapping algorithm which no longer has active support from the authors. It is based on seeding together with the Smith-Waterman algorithm, but is surprisingly fast.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;eland&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.8&lt;/span&gt; ELAND&lt;/h2&gt;
&lt;p&gt;ELAND is the Illumina pipeline software that comes attached to each of the Illumina sequencing machines. It is their proprietary software and is not easy to get hold of, although as indirect customers we could probably obtain a copy if necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;de-novo-assembly&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; De Novo Assembly&lt;/h1&gt;
&lt;div id=&#34;soap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; &lt;a href=&#34;http://soap.genomics.org.cn&#34;&gt;SOAP&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SOAP is a suite of tools for analysis of short oligonucleotide sequences, including for de novo assembly, alignment against a target genome, splice mapping for RNA seq data, and SNP and indel calling for resequencing experiments. The website has not been updated since 2010, so this may not be maintained any more and may be an older tool that is not of any use any more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abyss&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; &lt;a href=&#34;http://www.bcgsc.ca/platform/bioinfo/software/abyss&#34;&gt;ABySS&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ABySS is a de novo assembler that is paired end aware and well suited for short reads. It is well maintained, with the latest version (1.5.2) out in July.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trinity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; &lt;a href=&#34;https://github.com/trinityrnaseq/trinityrnaseq/wiki&#34;&gt;Trinity&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Trinity is a complete package for assembly of transcripts from Illumina RNA Seq data, developed at the Broad Institute ant the Hebrew University of Jerusalem. There are three main stages to the assembly: 1) The &lt;em&gt;Inchworm&lt;/em&gt; program performs the read assembly, generating either full length transcripts, or unique portions for alternatively spliced transcripts; 2) The &lt;em&gt;Chrysalis&lt;/em&gt; program then clusters these unique contigs into “gene” clusters and constructs de Bruijn graphs for each cluster based on member assignment; 3) The &lt;em&gt;Butterfly&lt;/em&gt; program then deconvolutes these graphs and generates the full length transcripts for the alternatively spliced transcripts and identiying paralogous genes. It can be used in conjunction with the &lt;a href=&#34;https://trinotate.github.io&#34;&gt;Trinotate&lt;/a&gt; package, which uses a number of standard programs for annoattion of the resulting transcripts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;read-quantification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Read Quantification&lt;/h1&gt;
&lt;div id=&#34;htseq&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; &lt;a href=&#34;https://htseq.readthedocs.io/en/release_0.9.1/&#34;&gt;HTseq&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;HTseq is a python based tool for assessing the quantification of reads amongst specific genomic elements (typically used to count reads that map to individual genes in RNA Seq data analysis). It relies on mapped data using one of the (likely splice-aware) mapping elgorithms mentioned above. In particular, it allows you to specify how best to deal with reads that overlap multiple exons (e.g. due to multiple splicing isoforms, or due to reads mapping to genes that overlap on opposite strands).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kallisto&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; &lt;a href=&#34;https://pachterlab.github.io/kallisto/about&#34;&gt;Kallisto&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Kallisto is an alignment-free read quantification algorithm, used to quantify the abundance of transcripts from (say) RNA-Seq data. It uses an index of the target transcriptome and uses a pseudoalignment method which can quantify 30 million human reads in less than 3 minutes. This has been shown to be comparable to other assembly-based quantification methods (e.g. Tophat2, STAR), and actually is robust to read-level errors and so often out-performs these, whilst also being orders of magnitude faster.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gene-ontology-go-and-network-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Gene Ontology (GO) and Network Analysis&lt;/h1&gt;
&lt;div id=&#34;david&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; &lt;a href=&#34;http://david.abcc.ncifcrf.gov&#34;&gt;DAVID&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;DAVID is an online tool for performing gene ontology analysis of a set of target genes. As well as comparing gene lists against gene ontology classes, it is also able to compare against many other annotated gene lists, including pathways from KEGG and Panther, disease genes, literature searches, etc. It is a fantastic resource for looking for enriched functions within a given gene list.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gsea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; &lt;a href=&#34;http://www.broadinstitute.org/gsea/index.jsp&#34;&gt;GSEA&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;GSEA is the Broad Institute’s visualisation tool for looking for enrichment of functional annotations in a given gene set. You provide the tool with a molecular profile data set (e.g. from RNA seq or microarray) which allows you to rank all genes in the genome based on some measure of expression (or enrichment above a control). This list is compared against a database of gene sets (including curated sets such as gene ontology and disease data sets, as well as any sets that may be of interest specifically for your current experiment , such as results from a different experiment), the genes are ranked based on the profile data, and are scored based on whether or not they are present in the database. The idea is that if there is an enrichment of genes of a certain function in your data set, then you will see this as an enrichment in the graph of the score towards the end of the genes (it will peak at the start, and then tail off for instance). If however there is no enrichment (the genes in the current database are spread across the entire continuum of expression scores) then no such peak will be seen.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;go&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; &lt;a href=&#34;http://geneontology.org&#34;&gt;GO&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Gene Ontology consortium maintains a well maintained and fully curated database of gene ontologies for the vast majority of identified genes amongst a whole host of organisms. Gene ontology classes are subdivided into 3 groups: biological process, molecular function, and cellular process. The gene ontology classes also follow a hierarchical structure, with increased fidelity as you look at more fine-grained annotations. The GO website allows you to search for enrichment of certain GO terms within genes in a dataset that you input, such as from a microarray experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;great&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; &lt;a href=&#34;http://bejerano.stanford.edu/great/public/html/&#34;&gt;GREAT&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;GREAT is a software tool for predicting cis-regulatory regions for a given set of genomic regions. Essentially it takes a set of genomic loci and analyses the annotation of any nearby genes to see if the specified regions are close by to genes of a specific function. For instance, you might have identified a set of ncRNAs that are specifically involved in cis-regualtory activity of target genes of a specific class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;panther&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.5&lt;/span&gt; &lt;a href=&#34;http://www.pantherdb.org&#34;&gt;PANTHER&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The PANTHER (Protein ANalysis THrough Evolutionary Relationships) database is another database of gene/protein family and subfamily functional classes, that actually uses the GO terms from the gene ontology database. It uses evolutionarily conserved elements of novel genes/proteins to identify functions based on similar families present in the database. It is ultimately used in a similar way to the GO database.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;blast2go&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.6&lt;/span&gt; &lt;a href=&#34;https://www.blast2go.com&#34;&gt;blast2GO&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;blast2GO is a bioinformatics package for the identification of GO classes and other functional information from novel nucleotide level data (e.g. from &lt;em&gt;de novo&lt;/em&gt; sequencing). It utilises the BLAST algorithm to identify genes with similar sequence structure and known functional annotation that can be inferred across to the novel sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;quality-control&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Quality Control&lt;/h1&gt;
&lt;div id=&#34;fastqc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.1&lt;/span&gt; &lt;a href=&#34;http://www.bioinformatics.babraham.ac.uk/projects/fastqc/&#34;&gt;FastQC&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;FastQC from the Babraham Institute is a very easy to use tool for quality control of fastq files, such as those used in sequencing. This can be a fantastic first pass analysis for sequencing data to check that no obvious problems exist with the sequencing data prior to mapping. A variety of figures are generated, including the base-level base calling scores across each read which can show if there are any systematic regions that require trimming (e.g. the 3’ end of the reads where quality can fade), the per-base sequence composition which can help to identify the presence of contaminants (such as adapter sequences), and the levels of sequence duplication which might indicate significant PCR bias. It is usually a very good idea to run FastQC on the sequencing files as a matter of course prior to mapping to see what specific issues you should expect to encounter in the processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiqc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;8.2&lt;/span&gt; &lt;a href=&#34;http://multiqc.info&#34;&gt;MultiQC&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MultiQC is a tool that I have recently started to use which is able to create interactive reports aggregating the results from various QC programs into one simple to navigate web page. This provides a single report for each sample that can easily be perused to identify issues with the data. It is set up to deal with output from many common analysis tools, including cutadapt, fastQC, Trimmomatic, BWA, Tophat, Bowtie, STAR, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;peak-calling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Peak Calling&lt;/h1&gt;
&lt;div id=&#34;findpeaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.1&lt;/span&gt; &lt;a href=&#34;http://sourceforge.net/projects/vancouvershortr/&#34;&gt;FindPeaks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;FindPeaks is part of the Vancouver short read analysis package, and is a peak calling algorithm designed to identify regions of enrichment above control and background levels, in a set of sequencing data (e.g. ChIP seq). It works well at identifying tighter peaks but is not well designed for the identification of extended enriched regions (e.g. H3K36me3).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;macs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.2&lt;/span&gt; &lt;a href=&#34;http://liulab.dfci.harvard.edu/MACS/index.html&#34;&gt;MACS&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;MACS is a model-based peak-calling algorithm that uses the difference in the ChIP fragments from the positive and negative strands to pinpoint the peak center. A poisson model is used to model the background, and enriched regions above the background or above a control sample are estimated. MACS is useful for tight and medium width peaks, but is not so good at identifying wider peaks such as for gene-wide histone marks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hpeak&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.3&lt;/span&gt; &lt;a href=&#34;http://www.sph.umich.edu/csg/qin/HPeak/&#34;&gt;HPeak&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;HPeak uses hidden Markov models to identify regions of the genome with significantly more mapped reads than expected by chance. It was released in 2010 and version 2 is now out, although I do not think that it is maintained and updated any more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peakseq&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.4&lt;/span&gt; &lt;a href=&#34;http://info.gersteinlab.org/PeakSeq&#34;&gt;PeakSeq&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;PeakSeq is another recent peak calling algorithm that identifies local thresholds for peak calling based on a Poisson approximation. The ChIPseq data are first normalised to a control sample, and then enriched regions are identified based on this threshold.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.5&lt;/span&gt; &lt;a href=&#34;http://mendel.stanford.edu/sidowlab/downloads/quest/&#34;&gt;QuEST&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Quantitative enrichment of sequence tags (QuEST) is an older peak calling algorithm that works in a similar way to MACS. It is most suited for identifying clear tight peaks in ChIP seq data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;homer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.6&lt;/span&gt; &lt;a href=&#34;http://homer.salk.edu/homer/chipseq/peaks.html&#34;&gt;HOMER&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The HOMER suite of tools is very useful for performing various aspects of ChIPseq analyses, such as annotation of called peaks and motif finding. Their peak calling tool is designed specifically for ChIP style sequencing data, and relies on defining a constant peak length for all peaks. This is designed to keep things sinple for the motif analysis tool, but means that this algorithm is not well designed for identifying longer&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;peakranger&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;9.7&lt;/span&gt; &lt;a href=&#34;http://ranger.sourceforge.net&#34;&gt;PeakRanger&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;PeakRanger is a versatile peak calling algorithm that is useful for both narrow and broader peaks in ChIP seq data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-manipulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Sequence Manipulation&lt;/h1&gt;
&lt;div id=&#34;bedtools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.1&lt;/span&gt; &lt;a href=&#34;http://bedtools.readthedocs.org/en/latest/#&#34;&gt;BEDTools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Bedtools is a suite of Unix tools for dealing with files of bed, bedgraph, wig, etc. They can be used to do everything from intersecting, merging, counting, complementing and shuffling genomic interval files, as well as converting between different file formats such as BAM, BED, GFF/GTF and VCF.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;samtools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.2&lt;/span&gt; &lt;a href=&#34;http://www.htslib.org&#34;&gt;SAMtools&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SamtTools is a suite of Unix tools for dealing with SAM and BAM formatted files following mapping of genome wide sequencing data. This includes reading, writing, editing, indexing and viewing of these files, as well as filtering based on different fields in the file (mapping quality etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adapter-trimming&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Adapter Trimming&lt;/h1&gt;
&lt;div id=&#34;cutadapt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.1&lt;/span&gt; &lt;a href=&#34;https://code.google.com/p/cutadapt/&#34;&gt;Cutadapt&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Cutadapt is a tool allowing the trimming of adapter sequences from the 3’ and 5’ of called reads from sequencing studies. This can be used to trim off known sequences as well as trimming reads to a specified length. This method is not useful for paired end reads as it does not maintain synchronisation between the mate pair files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trimmomatic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.2&lt;/span&gt; &lt;a href=&#34;http://www.usadellab.org/cms/?page=trimmomatic&#34;&gt;Trimmomatic&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Trimmomatic is another tool for trimming sequences such as adapter sequences from called reads, that also allows you to crop reads to remove poor quality bases and maintains synchronisation between mate pair files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trim-galore&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.3&lt;/span&gt; &lt;a href=&#34;http://www.bioinformatics.babraham.ac.uk/projects/trim_galore/&#34;&gt;Trim Galore&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Trim galore is a wrapper script for cutadapt that allows users to maintain synchronisation between mate pair files.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reaper&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.4&lt;/span&gt; &lt;a href=&#34;https://www.ebi.ac.uk/research/enright/software/kraken&#34;&gt;Reaper&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Reaper is part of the Kraken package from EMBL/EBI, and is a fast and memory efficient method for demultiplexing, trimming and filtering short read sequencing data. It can be used for a variety of purposes, including barcode filtering, quality checking, and the llike.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pathway-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Pathway Analysis&lt;/h1&gt;
&lt;div id=&#34;ingenuity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.1&lt;/span&gt; &lt;a href=&#34;http://www.ingenuity.com&#34;&gt;Ingenuity&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Ingenuity pathway analysis is a platform for the analysis of pathways within sequencing or microarray results based on comparisons with the Ingenuity knowledge base. In particular, the pathway analysis tool provides an interactive approach to the analysis of complex ’omics data, by using known interactions between genes contained within the knowledgebase to generate connections between genes from your analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kegg&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.2&lt;/span&gt; &lt;a href=&#34;http://www.genome.jp/kegg/&#34;&gt;KEGG&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Like the gene ontology database, the Kyoto encylopedia of genes and genomes is a database resource of various aspects of biological systems such as pathways, functional hierarchies, protein modules, reactome associations, etc. KEGG pathway maps provide interaction information between genes in the given pathway, and expression data can be overlaid to understand the interaction effects in your data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cytoscape&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.3&lt;/span&gt; &lt;a href=&#34;http://www.cytoscape.org&#34;&gt;Cytoscape&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Cytoscape is an open source software platform for the visualisation of complex networks, such as those resulting from interaction datasets, gene ontology classes, etc. It can be used to look for potential interaction networks by combining data from multiple genome-wide experiments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;string&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;12.4&lt;/span&gt; &lt;a href=&#34;https://string-db.org&#34;&gt;STRING&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The STRING database provides protein-protein interaction networks identified using a variety of different methods (computer predictions, text mining from published reports and other databases, high throughput experiments, conserved co-expression, etc). This includes both direct interactions and functional interactions. This database can be very useful to understand protein interaction networks and to identify hub genes in gene expression results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;genome-browsers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;13&lt;/span&gt; Genome Browsers&lt;/h1&gt;
&lt;div id=&#34;university-of-california-santa-cruz-genome-browser-ucsc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;13.1&lt;/span&gt; &lt;a href=&#34;https://htseq.readthedocs.io/en/release_0.9.1/&#34;&gt;University of California, Santa Cruz Genome Browser (UCSC)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the most well-known genome browsers available. Describing everything that this browser can do is beyond the scope of this brief outline, but the UCSC browser allows you to represent genomic data from various sources in parallel tracks on the internet browser allowing you to identify correlations between different factors throughout the genome. You can upload your own data in a variety of formats (e.g. mapped RNA seq data in wiggle format, raw read data in BAM format, called peaks from ChIP Seq data in BED format, etc.), and can also access a huge amount of data available from UCSC’s own servers (e.g. instances of the vast ENCODE data). This is a great way of visualising and exploring your data and should be explored in more detail to see what you can do.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensembl-genome-browser&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;13.2&lt;/span&gt; &lt;a href=&#34;http://www.ensembl.org/index.html&#34;&gt;Ensembl Genome Browser&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The Ensembl Genome Browser is an alternative web based genome browser to the UCSC browser offered by Ensembl. It is important to be aware that, whislt both UCSC and Ensembl use the same assemblies, there are many key differences that make them fairly incompatible. One important difference is that chromosomes are labelled differently (e.g. chr1, chr2, chr3 for UCSC but 1, 2, 3, for Ensembl). Similarly the genome builds are named differently. It generally works out simpler to pick one of these specifications and stick to it. Additional data (your own files or puclished data such as that from ENCODE) can be added using the track hubs feature, which allows you to specify where to find these data and how to represent them in your browser session.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integrative-genomics-viewer-igv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;13.3&lt;/span&gt; &lt;a href=&#34;http://software.broadinstitute.org/software/igv/&#34;&gt;Integrative Genomics Viewer (IGV)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The IGV browser from the Broad Institute is an alternative to the UCSC browser. Unlike the UCSC browser, which accesses a centrally located database through a web broswer, the IGV runs locally and so does not suffer from the lag and slow-down often seen with the UCSC browser (particularly during high traffic times of the day). The IGV browser has some very nice personalisation and formatting options not present in UCSC (e.g. being able to easily resize all of your tracks). On first looks, it is a little more bare bones than UCSC due to the lack of data avaialble at your fingertips (as with UCSC), but these annotated published data sets can be added using well annotated track hubs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How To Use R</title>
      <link>/resources/rtutorial/</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/resources/rtutorial/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Installing R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basics-of-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Basics of R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-classes&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Data Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lists&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.5&lt;/span&gt; Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.6&lt;/span&gt; Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#printing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-packages&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Installing Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Data Frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-and-writing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Reading and Writing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-sequences&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Control Sequences&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#if-else&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; IF ELSE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; FOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#while&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; WHILE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loop-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; Loop Control&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#writing-functions-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Writing Functions in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-simple-statistics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Some Simple Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plotting-with-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Plotting With R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.1&lt;/span&gt; Scatterplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.2&lt;/span&gt; Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.3&lt;/span&gt; Quantile-Quantile Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#line-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.4&lt;/span&gt; Line Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.5&lt;/span&gt; Density Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.6&lt;/span&gt; Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bar-plots-and-pie-charts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.7&lt;/span&gt; Bar Plots and Pie Charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graphical-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.8&lt;/span&gt; Graphical Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.9&lt;/span&gt; Subplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-figures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.10&lt;/span&gt; Saving Figures&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Example Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.2&lt;/span&gt; Load Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculate-fold-change&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.3&lt;/span&gt; Calculate Fold Change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.4&lt;/span&gt; Compare Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;This tutorial is a basic introduction to R that was originally written for biologists to get a basic understanding of how R functions. R is a software package that is a free to use open-source version of the S programming language. It is designed mainly for running statistical analyses and is very powerful in this regard. Follow through the tutorial and run the example commands by typing them into the command line as you go to see what happens. Don’t be afraid to play around with things as you go – it’s the best way to find out what certain functions do.&lt;/p&gt;
&lt;p&gt;You will notice that I have added comments to some of the code using the &lt;code&gt;#&lt;/code&gt; comment character. Everything to the right of this character is ignored by R. This can be used to add comments to your code, for instance to explain what a particular code chunk does. You can NEVER have too many comments!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Installing R&lt;/h1&gt;
&lt;p&gt;First of all, you will need to download and install R. The R website can be found at &lt;a href=&#34;http://www.r-project.org&#34;&gt;r-project.org&lt;/a&gt;. R is updated quite regularly – there is an updated release roughly every 6 months, with various developmental versions released between the official versions. The functions in R are actively maintained to ensure that they run as they should, and new functionality is added all of the time.&lt;/p&gt;
&lt;p&gt;The current version is 3.3.2. To download it, go to the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org&#34;&gt;cran.r-project.org&lt;/a&gt;). There are ready-made binaries available for MAC, windows, and most Linux distributions, so follow the links and download as instructed. You can also download the source code in a tarball, and can compile and install it using &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is also worth taking a look at the Integrated Development Environment &lt;a href=&#34;https://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;, which is a great open-source interface for R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics-of-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basics of R&lt;/h1&gt;
&lt;div id=&#34;introduction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Open the R environment. This is a command line version allowing you to see the results of the commands that you enter as you run them.&lt;/p&gt;
&lt;p&gt;The command line is shown by the &lt;code&gt;&amp;gt;&lt;/code&gt; character. Simply type your command here and press return to see the results. If your command is not complete, then the command line character will change to a &lt;code&gt;+&lt;/code&gt; to indicate that more input is required, for instance a missing parenthesis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print (&amp;quot;Hello World!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error: &amp;lt;text&amp;gt;:2:0: unexpected end of input
## 1: print (&amp;quot;Hello World!&amp;quot;
##    ^&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R stores “variables” using names made up of characters and numbers. A variable, as the name suggests, is a data “object” that can take any value that you want, and can be changed.&lt;/p&gt;
&lt;p&gt;The variable name can be anything that you like, although it must begin with a character. Whilst it is perfectly acceptable to use simple variable names such as &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;i&lt;/code&gt;, I recommend using a more descriptive name (e.g. &lt;code&gt;patient_height&lt;/code&gt; instead of &lt;code&gt;x&lt;/code&gt;). There are lots of different variable naming conventions to choose from (e.g. see &lt;a href=&#34;https://en.wikipedia.org/wiki/Naming_convention_(programming)&#34;&gt;here&lt;/a&gt;), but once you have chosen one try and stick to it.&lt;/p&gt;
&lt;p&gt;To assign a value to the variable, use the &lt;code&gt;&amp;lt;-&lt;/code&gt; command (less-than symbol followed by minus symbol). You can also use the &lt;code&gt;=&lt;/code&gt; symbol, but this has other uses (for instance using &lt;code&gt;==&lt;/code&gt; to test for equality) so I prefer to use the &lt;code&gt;&amp;lt;-&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
x # Returns the value stored in &amp;#39;x&amp;#39; - currently 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 5
x # Returns the value stored in &amp;#39;x&amp;#39; - now 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simple arithmetic can be performed using the standard arithmetic operators (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;), as well as the exponent operator (&lt;code&gt;^&lt;/code&gt;). There is a level of precedence to these functions – the exponent will be calculated first, followed by multiplication and division, followed by plus and minus. For this reason, you must be careful that your arithmetic is doing what you expect it to do. You can get around this by encapsulating subsets of the sum in parentheses, which will be calculated from the inside out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1+2*3 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 + 2) * 3 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + (2 * 3) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Personally I think that you can NEVER have too many parentheses – they ensure that your equations are doing what they should, and they can help improve the readability of things making it easier to see what a calculation is trying to achieve.&lt;/p&gt;
&lt;p&gt;Another operator that you may not have seen before is the “modulo” operator (&lt;code&gt;%%&lt;/code&gt;), which gives you the remainder left after dividing by the number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6%%2 # 6 is divisible by 2 exactly three times&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6%%4 # 6 is divisible by 4 one time with a remainder of 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use other variables in these assignments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1
y &amp;lt;- x
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- x + y 
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-classes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Data Classes&lt;/h2&gt;
&lt;p&gt;Variables can take many forms, or “classes”. The most common are “numeric” (which you can do numerical calculations on), character (can contain letters, numbers, symbols etc., but cannot run numerical calculations), and logical (TRUE or FALSE). The speech marks character &lt;code&gt;&amp;quot;&lt;/code&gt; is used to show that the class of y is “character”. You can also use the apostrophe &lt;code&gt;&#39;&lt;/code&gt;. There &lt;em&gt;is&lt;/em&gt; a difference between these, but for now this is not important. You can check the class of a variable by using the &lt;code&gt;class()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 12345
class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- &amp;quot;12345&amp;quot;
class(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Addition is a well-defined operation on numerical objects, but is not defined on character class objects. Attempting to use a function which has not been defined for the object in question will throw an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x + 1 # x is numeric, so addition is well defined&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12346&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y + 1 # y is a character, so addition is not defined - produces an error&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in y + 1: non-numeric argument to binary operator&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see which objects are currently present in the R environment, use the &lt;code&gt;ls()&lt;/code&gt; command. To remove a particular object, use the &lt;code&gt;rm()&lt;/code&gt; command. &lt;em&gt;BE CAREFUL&lt;/em&gt; – once you have removed an object, it is gone forever!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 5
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(x)
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) # Removes all objects in the current R session
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also change the class of a variable by assigning to the &lt;code&gt;class()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- &amp;quot;12345&amp;quot;
x+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in x + 1: non-numeric argument to binary operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x) &amp;lt;- &amp;quot;numeric&amp;quot; 
class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other important data class is “logical”, which is simply a binary TRUE or FALSE value. There are certain operators that are used to compare two variables. The obvious ones are “is less than” (&lt;code&gt;&amp;lt;&lt;/code&gt;), “is greater than” (&lt;code&gt;&amp;gt;&lt;/code&gt;), “is equal to”&amp;quot; (&lt;code&gt;==&lt;/code&gt;). You can also combine these to see “is less than or equal to” (&lt;code&gt;&amp;lt;=&lt;/code&gt;) or “is greater than or equal to” (&lt;code&gt;&amp;gt;=&lt;/code&gt;). If the statement is true, then it will return the output “TRUE”. Otherwise it will return “FALSE”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 2
y &amp;lt;- 3
x &amp;lt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;gt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also combine these logical tests to ask complex questions by using the “AND” (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;) or the “OR” (&lt;code&gt;||&lt;/code&gt;) operators. You can also negate the output of a logical test by using the “NOT” (&lt;code&gt;!&lt;/code&gt;) operator. This lets you test for very specific events in your data. Again, I recommend using parentheses to break up your tests to ensure that the tests occur in the order which you expect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
y &amp;lt;- 7
z &amp;lt;- 6
(x &amp;lt;= 3 &amp;amp;&amp;amp; y &amp;gt;= 8) &amp;amp;&amp;amp; z == 6  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(x &amp;lt;= 3 &amp;amp;&amp;amp; y &amp;gt;= 8) || z == 6 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important set of functions are the log and exponential functions. The exponential function is the function &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt;, such that &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt; is its own derivative (&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{dx} e^x = e^x\)&lt;/span&gt;). The value e is the constant 2.718281828…, which is the limit &lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} (1+\frac{1}{n})^n\)&lt;/span&gt;. It is a very important value in mathematics (hence why it has its own constant). Logarithms are the inverse of exponents, with natural log being log base &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. Here are some examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log (8)     ## Natural logarithm - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.079442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log2 (8)    ## Log base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp (1)     ## e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.718282&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp (5)     ## e^5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 148.4132&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(exp(8)) ## log and exponential cancel out - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(log(8)) ## log and exponential cancel out - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2^(log2(8)) ## log and exponential cancel out - base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log2 (2^8)  ## log and exponential cancel out - base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Vectors&lt;/h2&gt;
&lt;p&gt;Single values are all well and good, but R has a number of ways to store multiple values in a single data structure. The simplest one of these is as a “vector” – simply a list of values of the same class. You create a vector by using the &lt;code&gt;c()&lt;/code&gt; (concatenate) function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(1,2,3,4,5) 
my_vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a very useful way of storing linked data together. You can access the individual elements of the vector by using square brackets (&lt;code&gt;[&lt;/code&gt;) to take a subset of the data. The elements in the vector are numbered from 1 upwards, so to take the first and last values we do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50)
my_vector[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, a value &lt;code&gt;NA&lt;/code&gt; (Not Applicable) is returned if you try to take an element that does not exist. The subset can be as long as you like, as long as it’s not longer than the full set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50)
my_vector[1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 20 30 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the &lt;code&gt;:&lt;/code&gt; in the brackets simply means to take all of the numbers from 1 through to 4, so this returns the first 4 elements of the vector. For instance, this is a simple way to take the numbers from 1 to 20:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:20&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To drop elements from an array, you use the minus symbol:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50) 
my_vector[-1] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20 30 40 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[-length(my_vector)] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 20 30 40&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[-c(1,3,5)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to generate a regular sequence in R is to use the &lt;code&gt;seq()&lt;/code&gt; command. You supply the start number and the end number, and then either supply the parameter &lt;code&gt;by&lt;/code&gt; to define the regular interval between values, or the parameter &lt;code&gt;length&lt;/code&gt; to specify the total number of values to return between the start and end value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, by = 1) # Returns the same as 1:20&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, by = 2) # Just the even numbers between 1 and 20 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  3  5  7  9 11 13 15 17 19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, length = 10) # Slightly different to above&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.000000  3.111111  5.222222  7.333333  9.444444 11.555556 13.666667
##  [8] 15.777778 17.888889 20.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the &lt;code&gt;rep()&lt;/code&gt; function to give a vector of the specified length containing repeated values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(10, times = 5)      # Returns vector containing five copies of the number 10 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 10 10 10 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(1,2,3), each = 5) # Returns five 1s, then five 2s, then five 3s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the most powerful features of R is the fact that arithmetic can be conducted on entire vectors rather than having to loop through all values in the vector. Vectorisation of calculations in this way can give huge improvements in performance. For instance, if you sum two vectors (of equal size), the result will be a vector where the i&lt;sup&gt;th&lt;/sup&gt; entry is the sum of the i&lt;sup&gt;th&lt;/sup&gt; entries from the input vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(2,3,2,4,5) 
y &amp;lt;- c(4,1,1,2,3) 
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6 4 3 6 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x*y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  8  3  2  8 15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Lists&lt;/h2&gt;
&lt;p&gt;Another data structure that is very useful is the “list”. A list contains a number of things in a similar way to the vector, but the things that it contains can all be completely different classes. They can even be vectors and other lists (a list of lists):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 12345
## 
## [[2]]
## [1] &amp;quot;12345&amp;quot;
## 
## [[3]]
## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To subset a list, the syntax is slightly different and you use double square brackets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your list contains lists or vectors, you can subset these as well by using multiple sets of square brackets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[3]][5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can you see the difference between subsetting using &lt;code&gt;[[&lt;/code&gt; and using &lt;code&gt;[&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[3]]  ## Returns a vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[3]    ## Returns a list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[3][5] ## Not defined!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can give names to the values in a vector or in a list by using the &lt;code&gt;names()&lt;/code&gt; function to make it easier to follow what the values are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector        &amp;lt;- c(1:5)
names(my_vector) &amp;lt;- c(&amp;quot;length&amp;quot;, &amp;quot;width&amp;quot;, &amp;quot;height&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;age&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use these names instead of the reference number to subset lists and vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector        &amp;lt;- c(1:5)
names(my_vector) &amp;lt;- c(&amp;quot;length&amp;quot;, &amp;quot;width&amp;quot;, &amp;quot;height&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;age&amp;quot;) 
my_vector[&amp;quot;age&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## age 
##   5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of values in a vector or list can be found by using the &lt;code&gt;length()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- 1:5 
length(my_vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort the data simply using the &lt;code&gt;sort()&lt;/code&gt; function. If we want to get the indeces of the sorted vector (for instance to order a second vector based on the values in the first), we can use the &lt;code&gt;order()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Some values and their corresponding names
my_vals  &amp;lt;- c( 0.2, 1.7, 0.5, 3.4, 2.7 ) 
my_names &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;, &amp;quot;val5&amp;quot;)

## Sort the data
my_sorted &amp;lt;- sort(my_vals)  ## Returns the values in sorted order 
my_order  &amp;lt;- order(my_vals) ## Returns the indeces of the sorted values

## What is the difference between the two?
my_sorted &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2 0.5 1.7 2.7 3.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_order&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 2 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get the sorted value names
sort(my_names)     ## This won&amp;#39;t work as this will order names alphabetically &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;val1&amp;quot; &amp;quot;val2&amp;quot; &amp;quot;val3&amp;quot; &amp;quot;val4&amp;quot; &amp;quot;val5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_names[my_order] ## This gives us the order based on the values themselves&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;val1&amp;quot; &amp;quot;val3&amp;quot; &amp;quot;val2&amp;quot; &amp;quot;val5&amp;quot; &amp;quot;val4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default the sort functions sort from lowest to highest. You can sort in decreasing by order by using the decreasing parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(my_vals , decreasing = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.4 2.7 1.7 0.5 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.5&lt;/span&gt; Matrices&lt;/h2&gt;
&lt;p&gt;Another data format is a “matrix”&amp;quot; (also known as an “array” in R). This is simply a table of values, and can be thought of as a multidimensional vector. To access specific values in the matrix, you again use the square bracket accessor function, but this time must specify both the row (first value) and column (second value):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    6   11   16
## [2,]    2    7   12   17
## [3,]    3    8   13   18
## [4,]    4    9   14   19
## [5,]    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix[3,4] &amp;lt;- 99999
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]  [,4]
## [1,]    1    6   11    16
## [2,]    2    7   12    17
## [3,]    3    8   13 99999
## [4,]    4    9   14    19
## [5,]    5   10   15    20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the values are added to the matrix in a column-wise fashion (from top to bottom for column 1, then the same for column 2, etc.). To fill the matrix in a row-wise fashion, use the byrow parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the square bracket accessor function to extract subsets of the matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    6   11   16
## [2,]    2    7   12   17
## [3,]    3    8   13   18
## [4,]    4    9   14   19
## [5,]    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sub_matrix &amp;lt;- my_matrix[1:2, 3:4]
sub_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   11   16
## [2,]   12   17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;cbind()&lt;/code&gt; (column bind) and &lt;code&gt;rbind()&lt;/code&gt; (row bind) functions can also be used to concatenate vectors together by row or by column to give a matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cbind(c(1,2,3), c(4,5,6), c(7,8,9)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(c(1,2,3), c(4,5,6), c(7,8,9)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the names of both the rows and the columns by using the &lt;code&gt;rownames()&lt;/code&gt; and &lt;code&gt;colnames()&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix           &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
rownames(my_matrix) &amp;lt;- c(&amp;quot;row1&amp;quot;, &amp;quot;row2&amp;quot;, &amp;quot;row3&amp;quot;, &amp;quot;row4&amp;quot;, &amp;quot;row5&amp;quot;) 
colnames(my_matrix) &amp;lt;- c(&amp;quot;col1&amp;quot;, &amp;quot;col2&amp;quot;, &amp;quot;col3&amp;quot;, &amp;quot;col4&amp;quot;) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      col1 col2 col3 col4
## row1    1    6   11   16
## row2    2    7   12   17
## row3    3    8   13   18
## row4    4    9   14   19
## row5    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dimensions of the matrix can be found by using the &lt;code&gt;dim()&lt;/code&gt; function, which gives the number of rows (first value) and the number of columns (second value) of the matrix. You can access the number of rows or columns directly by using the &lt;code&gt;nrows()&lt;/code&gt; or &lt;code&gt;ncols()&lt;/code&gt; functions respectively:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
dim(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.6&lt;/span&gt; Functions&lt;/h2&gt;
&lt;p&gt;R also uses functions (also known as methods, subroutines, and procedures) which simply take in one or more values, do something to them, and return a result. A simple example is the &lt;code&gt;sum()&lt;/code&gt; function, which takes in two or more values in the form of a vector, and returns the sum of all of the values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- 1:5
sum(my_vector) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the &lt;code&gt;sum()&lt;/code&gt; function takes only one variable (in this case a numeric vector). Sometimes functions take more than one variable (also known as “arguments”). These are named values that must be specified for the function to run. For example, the &lt;code&gt;cor()&lt;/code&gt; function returns the correlation between two vectors. This requires several variables to be supplied – two vectors, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, of equal length – and you can also supply a number of additional arguments to control how the function works, including the &lt;code&gt;method&lt;/code&gt; argument, which lets you specify which method to use to calculate the correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample1 &amp;lt;- c(0.9, 1.2, 8.9, -0.3, 6.4)
sample2 &amp;lt;- c(0.6, 1.3, 9.0, -0.5, 6.2)
cor(sample1, sample2 , method = &amp;quot;pearson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9991263&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(sample1, sample2 , method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we gave a name to the third argument (“method”), but not the first two. If you do not name arguments, they will be taken and assigned to the arguments in the order in which they are input. The first two arguments required by the function are &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; – the two vectors to compare. So there is no problem with not naming these (although you could, if you wanted to, say &lt;code&gt;x=sample1, y=sample2&lt;/code&gt;). Any arguments not submitted will use their default value. For instance, the Pearson correlation is the default for &lt;code&gt;method&lt;/code&gt;, so you could get this by simply typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pearson_cor &amp;lt;- cor(sample1 , sample2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there is another argument for &lt;code&gt;cor()&lt;/code&gt;, &lt;code&gt;use&lt;/code&gt;, for which we are happy to use the default value before we get to &lt;code&gt;method&lt;/code&gt;. We therefore need to name &lt;code&gt;method&lt;/code&gt; to make sure that “pearson”&amp;quot; is not assigned to the &lt;code&gt;use&lt;/code&gt; argument in the function. It is always safer to name the arguments if you are unsure of the order. You can check the arguments using the &lt;code&gt;args()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(cor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, y = NULL, use = &amp;quot;everything&amp;quot;, method = c(&amp;quot;pearson&amp;quot;, 
##     &amp;quot;kendall&amp;quot;, &amp;quot;spearman&amp;quot;)) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to find out what a function does, there is a lot of very helpful documentation available in R. To see the documentation for a specific function, use the &lt;code&gt;help()&lt;/code&gt; function. If you want to try and find a function, you can search using a keyword by using the &lt;code&gt;help.search()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(cor)
?cor # Alternative for help() 
help.search(&amp;quot;correlation&amp;quot;)
??correlation # Alternative for help.search()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;printing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;print()&lt;/code&gt; can be used to print whatever is stored in the variable the function is called on. Print is what is known as an “overloaded”&amp;quot; function, which means that there are many functions named &lt;code&gt;print()&lt;/code&gt;, each written to deal with a variable of a different class. The correct one is used based on the variable that you supply. So calling &lt;code&gt;print()&lt;/code&gt; on a numeric variable will print the value stored in the variable. Calling it on a vector prints all of the values stored in the vector. Calling it on a list will print the contents of the list split into an easily identifiable way. There are also many more classes in R for which print is defined, but there are too many to describe here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1
print (x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- 1:5
print (y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- list(val1 = 1:5, val2 = 6:10)
print (z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you notice, using &lt;code&gt;print()&lt;/code&gt; is the default when you just call the variable itself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- list(val1 = 1:5, val2 = 6:10) 
print (z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cat()&lt;/code&gt; is similar to print in that the results of calling it are that text is printed to the console. The main use for &lt;code&gt;cat()&lt;/code&gt; is to con&lt;strong&gt;CAT&lt;/strong&gt;enate two or more variables together and instantly print them to the console. The additional argument &lt;code&gt;sep&lt;/code&gt; specifies the character to use to separate the different variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;hello&amp;quot;, &amp;quot;world&amp;quot;, sep = &amp;quot; &amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hello world&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:5
y &amp;lt;- &amp;quot;bottles of beer&amp;quot;
cat(x, y, sep = &amp;quot;\t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1    2   3   4   5   bottles of beer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;\t&lt;/code&gt; is a special printing character that you can use with the &lt;code&gt;cat()&lt;/code&gt; function that prints a tab character. Another similar special character that you may need to use is &lt;code&gt;\n&lt;/code&gt; which prints a new line.&lt;/p&gt;
&lt;p&gt;Another similar function is the &lt;code&gt;paste()&lt;/code&gt; function, which also concatenates multiple values together. The differences between this and &lt;code&gt;cat()&lt;/code&gt; are that the results of &lt;code&gt;paste()&lt;/code&gt; can be saved to a different variable which requires a call to &lt;code&gt;print()&lt;/code&gt; to see the results, and &lt;code&gt;paste()&lt;/code&gt; can be used to concatenate individual elements of a vector by using the additional &lt;code&gt;collapse&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;hello&amp;quot;, &amp;quot;world&amp;quot;, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;hello\tworld&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;sample&amp;quot;, 1:5, sep=&amp;quot;_&amp;quot;)) # Returns a vector of values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample_1&amp;quot; &amp;quot;sample_2&amp;quot; &amp;quot;sample_3&amp;quot; &amp;quot;sample_4&amp;quot; &amp;quot;sample_5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;sample&amp;quot;, 1:5, sep=&amp;quot;_&amp;quot;, collapse=&amp;quot;\n&amp;quot;)) # Prints values separated by new lines&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample_1\nsample_2\nsample_3\nsample_4\nsample_5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do you notice the difference between &lt;code&gt;print()&lt;/code&gt; and &lt;code&gt;cat()&lt;/code&gt;? While &lt;code&gt;print()&lt;/code&gt; prints the &lt;code&gt;\t&lt;/code&gt; character as is, &lt;code&gt;cat()&lt;/code&gt; prints the actual tab space. This is a process known as “interpolation”. In many programming languages, using double quotes in strings results in special characters being interpolated, whilst single quotes will print as is. However, in R the two can be used relatively interchangeably.&lt;/p&gt;
&lt;p&gt;There are also other characters, such as &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&amp;quot;&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;\&lt;/code&gt;, which may require “escaping” with a backslash to avoid R interpreting the character in a different context. For instance, if you have a string containing an apostrophe within a string defined using apostrophes, the string will be interpreted as terminating earlier, and the code will not do what you expect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;It&amp;#39;s very annoying when this happens...&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error: &amp;lt;text&amp;gt;:1:9: unexpected symbol
## 1: cat(&amp;#39;It&amp;#39;s
##             ^&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the string submitted to &lt;code&gt;cat()&lt;/code&gt; is actual “It” rather than the intended “It’s very annoying when this happens…”. The function will not know what to do about the remainder of the string, so an error will occur. However, by escaping the apostrophe, the string will be interpreted correctly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;It\&amp;#39;s easily fixed though!&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another alternative is to use double apostrophes as the delimiter, which will avoid the single apostrophe being misinterpreted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;It&amp;#39;s easily fixed though!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One function that gives you slightly more control over the formatting of your data is the &lt;code&gt;sprintf()&lt;/code&gt; function. This function allows you to specify things like the width in which to print each variable, which is useful for arranging output in a table format (note that you need to use &lt;code&gt;cat()&lt;/code&gt; to actual print to the screen):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10s\t%5s\n&amp;quot;, &amp;quot;Hello&amp;quot;, &amp;quot;World&amp;quot;), 
    sprintf(&amp;quot;%10s\t%5s\n&amp;quot;, &amp;quot;Helloooooo&amp;quot;, &amp;quot;World&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Hello   World
##  Helloooooo  World&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sprintf()&lt;/code&gt; function takes as input a string telling R how you want your inputs to be formatted, followed by a list of the inputs. Within the formatting string, placeholders of the form &lt;code&gt;%10s&lt;/code&gt; are replaced by the given inputs, with the first being replaced by the first argument in the list, and so on (so the number of additional arguments to &lt;code&gt;sprintf&lt;/code&gt; must match the number of placeholders). The number in the placeholder defines the width to allocate for printing that argument (positive is right aligned, negative is left aligned), decimal numbers in the placeholder define precision of floating point numbers, and the letter defines the type of argument to print (e.g. &lt;code&gt;s&lt;/code&gt; for string, &lt;code&gt;i&lt;/code&gt; for integer, &lt;code&gt;f&lt;/code&gt; for fixed point decimal, &lt;code&gt;e&lt;/code&gt; for exponential decimal). Note that special characters are interpolated by &lt;code&gt;cat()&lt;/code&gt; as before. Here are some examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%20s\n&amp;quot;, &amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%-20s\n&amp;quot;, &amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10i\n&amp;quot;, 12345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      12345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10f\n&amp;quot;, 12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  12.345000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10e\n&amp;quot;, 12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1.234500e+01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Installing Packages&lt;/h1&gt;
&lt;p&gt;The main R package contains a large number of commonly used functions. There are also additional functions available in other “packages” that you can get hold of from the Comprehensive R Archive Network, or &lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;. To load in a package, first download and install the package from CRAN using the &lt;code&gt;install.packages()&lt;/code&gt; function (if it is not already downloaded), and then use the “library” command to make the libraries available to your current R session:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?xtable &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No documentation for &amp;#39;xtable&amp;#39; in specified packages and libraries:
## you could try &amp;#39;??xtable&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;xtable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in contrib.url(repos, &amp;quot;source&amp;quot;): trying to use CRAN without setting a mirror&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;xtable&amp;quot;) 
?xtable&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;xtable()&lt;/code&gt; is not available in the R environment until you have loaded the package. Only the most commonly used functions are made available in the R environment by default (for example the package “stats” is loaded by default, which contains all commonly used statistical fuctions). There are also a number of commonly used packages that are part of the R installation, but which are not automatically loaded when you start a new R session. There are also thousands of additional packages available, some written by users, which can perform most of the things that you would ever want to do. Chances are, if you want to do something it’s already available from somewhere. Don’t re-invent the wheel if you can help it.&lt;/p&gt;
&lt;p&gt;Since R is so useful for analysing biological data, the &lt;code&gt;bioconductor&lt;/code&gt; project was set up to bring together packages used for the analysis of high-throughput data (it started with microarrays, but now there are packages available for analysis of sequencing data). Bioconductor packages can be downloaded from &lt;a href=&#34;http://www.bioconductor.org&#34;&gt;bioconductor.org&lt;/a&gt;. However, there is also a simple way to install bioconductor packages directly from within R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;) # Load the biocLite() script 
biocLite() # Installs the basic packages required to use bioconductor 
biocLite(&amp;quot;DESeq&amp;quot;) # Installs a specific bioconductor package&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Data Frames&lt;/h1&gt;
&lt;p&gt;Data frames are the most powerful data types in R. They look similar to matrices, but the data structure is actually more similar to a list of vectors (all of the same length). The simplest way to think of them is as being similar to spreadsheets in Excel.&lt;/p&gt;
&lt;p&gt;You can create data frames either in a similar way to how you create a list, or also by converting a matrix object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(matrix(1:12, nrow = 3, ncol = 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3 V4
## 1  1  4  7 10
## 2  2  5  8 11
## 3  3  6  9 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how, in the second data frame, no column names are specified so R sets the defaults as &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt;, &lt;code&gt;V3&lt;/code&gt;, etc. Whilst data frames do have row names, it is the column names that are the most important. As with lists, these can be changed by using the &lt;code&gt;names()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df        &amp;lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) 
names(my_df) &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;)
my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You access the elements of a data frame either using single square bracket notation in the same way as for a matrix, or you can access the individual columns using double square bracket notation in the same way as for lists. You can also access the individual columns by using the special &lt;code&gt;$&lt;/code&gt; operator which is specifically used for data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df &amp;lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) 
names(my_df) &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;)
my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sub_df &amp;lt;- my_df[1:2, 3:4]
sub_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val3 val4
## 1    7   10
## 2    8   11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val1 &amp;lt;- my_df[[1]]
val1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val2 &amp;lt;- my_df[[&amp;quot;val2&amp;quot;]] 
val2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4 5 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val3 &amp;lt;- my_df$val3
val3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7 8 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The beauty of data frames is that the data frame columns can be dealt with as if they were individual variables. For this reason, the column names must be suitable variable names (i.e. alphanumeric and not starting with a number) and must be unique. If you attach a data frame, you can access the columns as if they were variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df &amp;lt;- data.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12))
attach(my_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked _by_ .GlobalEnv:
## 
##     val1, val2, val3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val1 + 1000 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1001 1002 1003&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(my_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a slight aside, I dislike using the attach/detach approach to accessing elements of a data frame, as it can make it difficult when reading through your code to tell which variable is being accessed. For instance, if you have a variable named &lt;code&gt;myname&lt;/code&gt;, and a data frame with a column &lt;code&gt;myname&lt;/code&gt;, then using &lt;code&gt;df$myname&lt;/code&gt; in your code makes it much clearer where you are accessing your data from than simply using &lt;code&gt;myname&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that to make changes to the data frame itself, we need to use the &lt;code&gt;$&lt;/code&gt; accessor function (or double square brackets), otherwise a new variable &lt;code&gt;val1&lt;/code&gt; will be created. Data frames should be set up in such a way that every row represents an independent observation, and the columns represent the independent variables that you may be interested in. For instance, if you have taken a measurement of say the weight of each sample in triplicate, you would not represent the data like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;SampleName&lt;/th&gt;
&lt;th&gt;Weight1&lt;/th&gt;
&lt;th&gt;Weight2&lt;/th&gt;
&lt;th&gt;Weight3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;66.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;80.3&lt;/td&gt;
&lt;td&gt;79.8&lt;/td&gt;
&lt;td&gt;79.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But instead you would ensure that the two independent variables (weight and replicate number) were in their own columns:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;SampleName&lt;/th&gt;
&lt;th&gt;Replicate&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;66.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;80.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;79.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;79.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now all of the weights are in a single column that can be analysed.&lt;/p&gt;
&lt;p&gt;Subsetting a data frame is also very powerful. The subset command allows you to look for the rows of a data frame that fit certain criteria. For instance, to pull out the genes that show more than 2-fold expression and a p-value less than 0.05, you would do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp &amp;lt;- data.frame(geneName   = paste(&amp;quot;gene&amp;quot;, 1:10, sep = &amp;quot;&amp;quot;), 
                       foldChange = rnorm(10, mean = 2, sd = 1),
                       pVal       = rnorm(10, mean = 0.05, sd = 0.05)) 
signif_genes &amp;lt;- subset(gene_exp, foldChange &amp;gt; 2 &amp;amp; pVal &amp;lt;= 0.05)
signif_genes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   geneName foldChange        pVal
## 1    gene1   2.234659 0.038236225
## 3    gene3   2.713056 0.047058009
## 5    gene5   2.982536 0.003146303&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice here that we use a single &lt;code&gt;&amp;amp;&lt;/code&gt; rather than the double &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; that we used earlier. This is because we are doing a vector-based logical test (that is performing the test on each element of the vector to get a vector of logical values at the end). It is very easy to forget this and accidentally use the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;, which will not give you what you want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fake_signif_genes &amp;lt;- subset(gene_exp, foldChange &amp;gt; 2 &amp;amp;&amp;amp; pVal &amp;lt;= 0.05) 
fake_signif_genes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    geneName foldChange        pVal
## 1     gene1  2.2346589 0.038236225
## 2     gene2  1.1171569 0.010953464
## 3     gene3  2.7130557 0.047058009
## 4     gene4  1.9352067 0.046603706
## 5     gene5  2.9825355 0.003146303
## 6     gene6  2.2315724 0.138186853
## 7     gene7  1.6141956 0.093650234
## 8     gene8  1.8556861 0.032846692
## 9     gene9  0.7700426 0.063940521
## 10   gene10  1.3124471 0.081082591&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another form of data that comes in very handy, particularly with data frames, is the “factor”. Factors are a way of dealing with categorical data, and simply encode the possible levels with numberic dummy values 0, 1, 2, etc. (which are used in modelling procedures such as ANOVA):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(&amp;quot;apples&amp;quot;, &amp;quot;pears&amp;quot;, &amp;quot;apples&amp;quot;, &amp;quot;oranges&amp;quot;, &amp;quot;pears&amp;quot;) 
my_vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;apples&amp;quot;  &amp;quot;pears&amp;quot;   &amp;quot;apples&amp;quot;  &amp;quot;oranges&amp;quot; &amp;quot;pears&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_factor &amp;lt;- as.factor(my_vector)
my_factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] apples  pears   apples  oranges pears  
## Levels: apples oranges pears&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(my_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;apples&amp;quot;  &amp;quot;oranges&amp;quot; &amp;quot;pears&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since data frames can hold data of different classes within its columns (a data frame is essentially a glorified list), it is very important to ensure that each column is assigned the correct class so that R functions that you use later do the correct thing with the data. For instance, R will automatically convert character entries to factors with all possible values as the factor levels. You can quickly see the class of all of your columns by using the &lt;code&gt;str()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(gene_exp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    10 obs. of  3 variables:
##  $ geneName  : Factor w/ 10 levels &amp;quot;gene1&amp;quot;,&amp;quot;gene10&amp;quot;,..: 1 3 4 5 6 7 8 9 10 2
##  $ foldChange: num  2.23 1.12 2.71 1.94 2.98 ...
##  $ pVal      : num  0.03824 0.01095 0.04706 0.0466 0.00315 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whilst factors are incredibly useful in modelling and plotting, they may not necessarily be exactly what you want. For instance, in this case the column &lt;code&gt;geneName&lt;/code&gt; has been converted into a factor, with levels &lt;code&gt;gene1&lt;/code&gt;, …, &lt;code&gt;gene10&lt;/code&gt;. If we try and add in a new gene, &lt;code&gt;gene11&lt;/code&gt;, this will not work as all entries of a factor must be one of the specified levels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp_plus &amp;lt;- rbind(gene_exp, c(&amp;quot;gene11&amp;quot;, 1.789, 0.0034))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in `[&amp;lt;-.factor`(`*tmp*`, ri, value = &amp;quot;gene11&amp;quot;): invalid factor
## level, NA generated&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp_plus&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    geneName        foldChange                pVal
## 1     gene1  2.23465888817809  0.0382362251121874
## 2     gene2  1.11715691372872  0.0109534636149735
## 3     gene3  2.71305571811009  0.0470580093671668
## 4     gene4  1.93520669112966  0.0466037061884548
## 5     gene5  2.98253553079485 0.00314630349135952
## 6     gene6  2.23157241340974   0.138186853004448
## 7     gene7  1.61419556341739   0.093650234491183
## 8     gene8  1.85568611921072  0.0328466921261732
## 9     gene9 0.770042566261392  0.0639405210499306
## 10   gene10  1.31244706165258  0.0810825912438105
## 11     &amp;lt;NA&amp;gt;             1.789              0.0034&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead we would be better off treating &lt;code&gt;geneName&lt;/code&gt; as a character vector, since we are unlikely to treat it as a categorical variable in later model fitting analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-and-writing-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Reading and Writing Data&lt;/h1&gt;
&lt;p&gt;Reading and writing data in R is quite simple, and is most easily done by using pure text files. Functions exist for reading other formats as well (e.g. Excel tables), but for now we will concentrate on raw text. There are some very basic example files available from &lt;a href=&#34;/files/RTutorial/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unless you give the complete path for a file, R will look in it’s current working directory for any files that you want to load in. By default, R will use your system’s home directory, but you can set this by using the setwd() function. You can check that the correct working directory is set by using the getwd() function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;/path/to/mydir/&amp;quot;) 
getwd ()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have a list of data in a file (e.g. a list of gene names separated by new lines), then the simplest method to use is &lt;code&gt;scan()&lt;/code&gt;. You must tell &lt;code&gt;scan()&lt;/code&gt; where to find the data file (either the full path, or a relative path from the current working directory), as well as the format that the data should be read in as (generally either “character” or “numeric”):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_file &amp;lt;- &amp;quot;gene_list.txt&amp;quot;
gene_list &amp;lt;- scan(my_file, what = &amp;quot;character&amp;quot;, sep = &amp;quot;\n&amp;quot;) 
gene_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;gene1&amp;quot;  &amp;quot;gene2&amp;quot;  &amp;quot;gene3&amp;quot;  &amp;quot;gene4&amp;quot;  &amp;quot;gene5&amp;quot;  &amp;quot;gene6&amp;quot;  &amp;quot;gene7&amp;quot; 
##  [8] &amp;quot;gene8&amp;quot;  &amp;quot;gene9&amp;quot;  &amp;quot;gene10&amp;quot; &amp;quot;gene11&amp;quot; &amp;quot;gene12&amp;quot; &amp;quot;gene13&amp;quot; &amp;quot;gene14&amp;quot;
## [15] &amp;quot;gene15&amp;quot; &amp;quot;gene16&amp;quot; &amp;quot;gene17&amp;quot; &amp;quot;gene18&amp;quot; &amp;quot;gene19&amp;quot; &amp;quot;gene20&amp;quot; &amp;quot;gene21&amp;quot;
## [22] &amp;quot;gene22&amp;quot; &amp;quot;gene23&amp;quot; &amp;quot;gene24&amp;quot; &amp;quot;gene25&amp;quot; &amp;quot;gene26&amp;quot; &amp;quot;gene27&amp;quot; &amp;quot;gene28&amp;quot;
## [29] &amp;quot;gene29&amp;quot; &amp;quot;gene30&amp;quot; &amp;quot;gene31&amp;quot; &amp;quot;gene32&amp;quot; &amp;quot;gene33&amp;quot; &amp;quot;gene34&amp;quot; &amp;quot;gene35&amp;quot;
## [36] &amp;quot;gene36&amp;quot; &amp;quot;gene37&amp;quot; &amp;quot;gene38&amp;quot; &amp;quot;gene39&amp;quot; &amp;quot;gene40&amp;quot; &amp;quot;gene41&amp;quot; &amp;quot;gene42&amp;quot;
## [43] &amp;quot;gene43&amp;quot; &amp;quot;gene44&amp;quot; &amp;quot;gene45&amp;quot; &amp;quot;gene46&amp;quot; &amp;quot;gene47&amp;quot; &amp;quot;gene48&amp;quot; &amp;quot;gene49&amp;quot;
## [50] &amp;quot;gene50&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For tables (for instance tab-delimited files saved from Excel), the easiest way is to use the &lt;code&gt;read.table()&lt;/code&gt; function. This works by using &lt;code&gt;scan()&lt;/code&gt; to read in each line from the table, then splitting the line by the specified delimiter. It is easier (or at least you are less prone to mistakes) to read such files when there are no empty cells, so try to fill empty data with a missing data character, such as &lt;code&gt;NA&lt;/code&gt; (the default):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  4 variables:
##  $ SampleName: Factor w/ 6 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;,..: 1 2 3 4 5 6
##  $ Treatment : Factor w/ 2 levels &amp;quot;Control&amp;quot;,&amp;quot;Drug&amp;quot;: 1 1 1 2 2 2
##  $ Replicate : int  1 2 3 1 2 3
##  $ CellType  : Factor w/ 1 level &amp;quot;HeLa&amp;quot;: 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are lots of additional arguments to the &lt;code&gt;read.table()&lt;/code&gt; function; &lt;code&gt;header&lt;/code&gt; is a boolean value that says whether or not the first row should be used to name the columns of the data frame, &lt;code&gt;sep&lt;/code&gt; gives the delimiter between column entries (e.g. &lt;code&gt;\t&lt;/code&gt; for tab-delimited files, or &lt;code&gt;,&lt;/code&gt; for comma-separated files), &lt;code&gt;skip&lt;/code&gt; tells R to skip the first &lt;code&gt;n&lt;/code&gt; rows of the input, and &lt;code&gt;nrow&lt;/code&gt; tells R to only load the first &lt;code&gt;n&lt;/code&gt; rows that it sees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;, nrow = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2 obs. of  4 variables:
##  $ SampleName: Factor w/ 2 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;: 1 2
##  $ Treatment : Factor w/ 1 level &amp;quot;Control&amp;quot;: 1 1
##  $ Replicate : int  1 2
##  $ CellType  : Factor w/ 1 level &amp;quot;HeLa&amp;quot;: 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = FALSE, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    7 obs. of  4 variables:
##  $ V1: Factor w/ 7 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;,..: 7 1 2 3 4 5 6
##  $ V2: Factor w/ 3 levels &amp;quot;Control&amp;quot;,&amp;quot;Drug&amp;quot;,..: 3 1 1 1 2 2 2
##  $ V3: Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;Replicate&amp;quot;: 4 1 2 3 1 2 3
##  $ V4: Factor w/ 2 levels &amp;quot;CellType&amp;quot;,&amp;quot;HeLa&amp;quot;: 1 2 2 2 2 2 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that when the header is not used, the numeric column &lt;code&gt;Replicate&lt;/code&gt; is now interpreted in the same way as the character columns, because now the first entry is non-numeric. By default, &lt;code&gt;read.table()&lt;/code&gt; converts character columns into factors, which can be avoided by setting the &lt;code&gt;stringsAsFactors&lt;/code&gt; argument to &lt;code&gt;FALSE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;, stringsAsFactors = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  4 variables:
##  $ SampleName: chr  &amp;quot;sample1&amp;quot; &amp;quot;sample2&amp;quot; &amp;quot;sample3&amp;quot; &amp;quot;sample4&amp;quot; ...
##  $ Treatment : chr  &amp;quot;Control&amp;quot; &amp;quot;Control&amp;quot; &amp;quot;Control&amp;quot; &amp;quot;Drug&amp;quot; ...
##  $ Replicate : int  1 2 3 1 2 3
##  $ CellType  : chr  &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;control-sequences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Control Sequences&lt;/h1&gt;
&lt;p&gt;One of the most useful things to be able to do with computers is to repeat the same command multiple times without having to do it by hand each time. For this, control sequences can be used to give you close control over the progress of your program.&lt;/p&gt;
&lt;div id=&#34;if-else&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; IF ELSE&lt;/h2&gt;
&lt;p&gt;The first control sequence to look at is the “if else” command, which acts as a switch to run one of a selection of possible commands given a switch that you specify. For instance, you may want to do something different depending on whether a value is odd or even:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_val &amp;lt;- 3
if (my_val%%2 == 0) { # If it is even (exactly divisible by 2)
  cat (&amp;quot;Value is even\n&amp;quot;)
} else {              # Otherwise it must be odd
  cat (&amp;quot;Value is odd\n&amp;quot;) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value is odd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the expression in the parentheses following “if” is evaluated, and if it evaluates to TRUE then the block of code contained within the following curly braces is evaluated. Otherwise, the block of code following the “else” statement is evaluated. You can add additional tests by using the “else if” statement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_val &amp;lt;- 27
if (my_val%%2 == 0) {
  cat(&amp;quot;Value is divisible by 2\n&amp;quot;)
} else if (my_val%%3 == 0) { 
  cat(&amp;quot;Value is divisible by 3\n&amp;quot;)
} else if (my_val%%4 == 0) {
  ...
} else if (my_val%%n == 0) {
  cat(&amp;quot;Value is divisible by n\n&amp;quot;)
} else {
  cat(&amp;quot;Value is not divisible by 1:n\n&amp;quot;) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each switch is followed by a block of code surrounded by curly braces, and the conditional statements are evaluated until one evaluates to TRUE, at which point R avaluates this code bloack then exits. If none of them evaluate to TRUE, then the default code block following “else” is evaluated instead. If no “else” block is present, then the default is to just do nothing. These blocks can be as complicated as you like, and you can have “if else” statements within the blocks to create a hierarchical structure. Note that this ifelse block will ony return the smallest factor of &lt;code&gt;myval&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; FOR&lt;/h2&gt;
&lt;p&gt;Another control structure is the “for loop”, which will conduct the code in the block multiple times for a variety of values that you specify at the start. For instance, here is a simple countdown script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 10:1) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
  if (i == 1) {
    cat(&amp;quot;Blastoff!\n&amp;quot;) 
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the index &lt;code&gt;i&lt;/code&gt; is taken from the set of numbers (10, 9, …, 1), starting at the first value 10, and each time prints out the number followed by a newline. Then an &lt;code&gt;if&lt;/code&gt; statement checks to see if we have reached the final number, at which point it is time for blast off! At this point, it returns to the start of the block, updates the number to the second value 9, and repeats. It does this until there are no more values to use.&lt;/p&gt;
&lt;p&gt;As a small aside, this is slightly inefficient. Evaluation of the &lt;code&gt;if&lt;/code&gt; statement is conducted every single time the loop is traversed (10 times in this example). It will only ever be true at the end of the loop, so we could always take this out of the loop and evaluate the final printout after the loop is finished and save ourselves 10 calculations. Whilst the difference here is negligible, thinking of things like this may save you time in the future:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 10:1) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;)
} 
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;while&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; WHILE&lt;/h2&gt;
&lt;p&gt;The final main control structure is the “while loop”. This is similar to the “for loop”, and will continue to evaluate the code chunk as long as the specified expression evaluates to TRUE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i &amp;lt;- 10
while (i &amp;gt; 0) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
  i &amp;lt;- i - 1
} 
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This does exactly the same as the “for loop” above. In general, either can be used for a given purpose, but there are times when one would be more “elegant” than the other. For instance, here the for loop is better as you do not need to manually subtract 1 from the index each time.&lt;/p&gt;
&lt;p&gt;However, if you did not know how many iterations were required before finding what you are looking for (for instance searching through a number of files), a “while loop” may be more suitable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HOWEVER&lt;/strong&gt;: Be aware that it is possible to get caught up in an “infinite loop”. This happens if the conditional statement never evaluates to FALSE. If this happens, press either ESCAPE or press the &lt;code&gt;CONROL&lt;/code&gt; key and the letter &lt;code&gt;c&lt;/code&gt; (&lt;code&gt;CTRL+c&lt;/code&gt;) to quit out of the current function (&lt;code&gt;CMD+c&lt;/code&gt;) for Mac). For instance, if we forget to decrement the index, &lt;code&gt;i&lt;/code&gt; will always be 10 and will therefore never be less than 0. This loop will therefore run forever:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i &amp;lt;- 10
while (i &amp;gt; 0) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loop-control&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; Loop Control&lt;/h2&gt;
&lt;p&gt;You can leave control loops early by using flow control constructs. &lt;code&gt;next&lt;/code&gt; skips out of the current loop and moves onto the next in the sequence:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:10) { 
  if (i == 5) {
    next 
  }
  cat (i, &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Finished loop\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4
## 6
## 7
## 8
## 9
## 10
## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;break&lt;/code&gt; will leave the loop entirely, and will return to the function after the last curly brace in the code chunk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:10) { 
  if (i == 5) {
    break 
  }
  cat (i, &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Finished loop\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4
## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-functions-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Writing Functions in R&lt;/h1&gt;
&lt;p&gt;There are many functions available in R, and chances are if you want to do something somebody has already written the function to do it. It is best to not re-invent the wheel if possible (or at least it is more efficient – sometimes it is good to reinvent the wheel to understand how it works), but very often you will want to create your own functions to save replicating code.&lt;/p&gt;
&lt;p&gt;A function takes in one or more variables, does something with them, and returns something (e.g. a value or a plot). For instance, calculating the mean of a number of values is simply a case of adding them together and dividing by the number of values. Let’s write a function to do this and check that it matches the &lt;code&gt;mean()&lt;/code&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_mean &amp;lt;- function (x) { # Here, x is a numeric vector 
  nvals &amp;lt;- length(x)
  valsum &amp;lt;- sum(x)
  return (valsum/nvals)
}
my_vals &amp;lt;- c(3,5,6,3,4,3,4,7) 
my_mean(my_vals) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, as with the loops earlier, the function is contained within a block of curly braces. A numeric vector is given to the function, the mean is calculated, and this value is returned to the user using the &lt;code&gt;return()&lt;/code&gt; function. This value can be captured into a variable of your choosing in the same way as with any function.&lt;/p&gt;
&lt;p&gt;You can also add further arguments to the function call. If you want an argument to have a default value, you can specify this in the function declaration. This is the value that will be used if no argument value is specified. Any arguments that do not have a default value must be specified when calling the function, or an error will be thrown:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- function(x, arg) { 
  return(paste(x, arg, sep = &amp;quot; &amp;quot;))
}
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in paste(x, arg, sep = &amp;quot; &amp;quot;): argument &amp;quot;arg&amp;quot; is missing, with no default&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try and add a default value for &lt;code&gt;arg&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- function(x, arg = &amp;quot;World!&amp;quot;) { 
  return(paste(x, arg, sep = &amp;quot; &amp;quot;))
}
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hello World!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a good point to mention an idea known as “scope”. After running the following function, have a look at the value &lt;code&gt;valsum&lt;/code&gt; calculated within the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_mean &amp;lt;- function (x) { # Here, x is a numeric vector 
  nvals &amp;lt;- length(x)
  valsum &amp;lt;- sum(x)
  return (valsum/nvals)
}
my_vals &amp;lt;- c(3,5,6,3,4,3,4,7)
my_mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(valsum) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in print(valsum): object &amp;#39;valsum&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what went wrong? The error message says that R cannot find the object &lt;code&gt;valsum&lt;/code&gt;. So where is it? The “scope” of an object is the environment where it can be found. Up until now, we have been using what are known as “global variables”. That is we have created all of our objects within the “global environment”, which is the top level where R searches for objects. These objects are available at all times.&lt;/p&gt;
&lt;p&gt;However, when we call a function, a new environment, or “scope”, is created, and all variables created within the function become “local variables” that can only be accessed from within the function itself. As soon as we leave the function, these local variables are deleted. If you think about it, this makes sense – otherwise, every time we called a function, memory would fill up with a whole load of temporary objects. Also, how many functions do you think create an object called &lt;code&gt;x&lt;/code&gt;? Pretty much all of them (it’s generally the name of the first argument, as in my example). If we created an object &lt;code&gt;x&lt;/code&gt;, then ran a couple of functions, and then went to use &lt;code&gt;x&lt;/code&gt; again, chances are it would no longer be what we thought it was.&lt;/p&gt;
&lt;p&gt;So, the function itself is completely self-contained. A copy of the input variable is stored in a new local variable called &lt;code&gt;x&lt;/code&gt;, something is done to this object (possibly creating additional objects along the way), something is returned, and then all of these objects in the scope of the function are removed, and we move back into the global environment.&lt;/p&gt;
&lt;p&gt;Functions are incredibly useful when we want to repeat the same set of actions on multiple sets of data. The “apply”&amp;quot; set of functions are very useful for running a single function multiple times on input data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;apply()&lt;/code&gt; works on a matrix or data frame, and applies the function named by the argument &lt;code&gt;FUN&lt;/code&gt; across either the rows or the columns of the table, as specified with the &lt;code&gt;MAR&lt;/code&gt; (margin) argument (&lt;code&gt;MAR=1&lt;/code&gt; for rows, &lt;code&gt;MAR=2&lt;/code&gt; for columns). For instance, suppose that you had a matrix of expression values from a microarray, where each row was a different gene, and each column is the signal from a different probe on the array. We may want to calculate the mean value across these probes for each gene:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probe_file  &amp;lt;- &amp;quot;probe_values.txt&amp;quot;
probe_dat   &amp;lt;- read.table(probe_file, header = TRUE, sep = &amp;quot;\t&amp;quot;) 
probe_means &amp;lt;- apply(probe_dat[, -1], MAR = 1, FUN = mean) 
probe_means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 21.2 29.8 85.8 63.6 25.6 44.8 70.8 88.4 47.6 35.8 90.2 57.8 24.2 23.2
## [15] 25.0 57.6 83.0 62.8 33.8 28.4 13.2 58.4 24.6 28.2 47.2  6.4 94.6 14.2
## [29] 39.6 53.4 80.2 47.8  9.8 58.8 59.8  0.4 63.8 33.0 22.4 53.8 37.8 68.8
## [43] 99.6 97.6  5.0 59.8 95.4 -0.2  1.4 52.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, apply can be used to apply a function to all values by using &lt;code&gt;MAR=c(1,2)&lt;/code&gt; to run across rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probe_file    &amp;lt;- &amp;quot;probe_values.txt&amp;quot;
probe_dat     &amp;lt;- read.table(probe_file, header = TRUE, sep = &amp;quot;\t&amp;quot;) 
probe_dat_log &amp;lt;- apply(probe_dat[, -1], MAR = c(1,2), FUN = exp) 
probe_dat_log&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Probe1       Probe2       Probe3       Probe4       Probe5
##  [1,] 4.424134e+05 7.896296e+13 2.648912e+10 2.980958e+03 3.931334e+12
##  [2,] 3.931334e+12 1.907347e+21 8.886111e+06 5.987414e+04 1.285160e+19
##  [3,] 2.235247e+37 5.052394e+31 1.811239e+41 1.220403e+39 8.223013e+36
##  [4,] 3.404276e+29 4.607187e+28 5.685720e+24 1.373383e+32 1.041376e+23
##  [5,] 3.584913e+09 1.068647e+13 1.484132e+02 7.896296e+13 8.659340e+16
##  [6,] 3.493427e+19 2.581313e+20 1.142007e+26 3.185593e+16 5.834617e+14
##  [7,] 2.758513e+33 1.545539e+25 6.837671e+30 1.373383e+32 1.373383e+32
##  [8,] 2.451246e+40 1.373383e+32 4.375039e+48 3.025077e+36 2.038281e+34
##  [9,] 3.185593e+16 1.907347e+21 1.041376e+23 1.545539e+25 2.353853e+17
## [10,] 2.146436e+14 6.565997e+07 1.285160e+19 2.581313e+20 1.171914e+16
## [11,] 2.235247e+37 6.076030e+37 5.399228e+44 6.837671e+30 1.467662e+45
## [12,] 1.142007e+26 8.438357e+26 4.201210e+25 7.016736e+20 1.142007e+26
## [13,] 1.784823e+08 1.784823e+08 4.311232e+15 5.987414e+04 4.311232e+15
## [14,] 9.744803e+09 1.627548e+05 9.744803e+09 2.353853e+17 6.565997e+07
## [15,] 1.957296e+11 7.200490e+10 4.851652e+08 2.146436e+14 1.318816e+09
## [16,] 2.091659e+24 3.493427e+19 1.858672e+31 1.014800e+33 8.659340e+16
## [17,] 2.038281e+34 2.451246e+40 4.923458e+41 5.540622e+34 1.252363e+29
## [18,] 9.253782e+29 3.831008e+22 4.093997e+35 1.545539e+25 1.041376e+23
## [19,] 4.311232e+15 1.907347e+21 1.957296e+11 2.146436e+14 7.200490e+10
## [20,] 1.446257e+12 1.318816e+09 5.834617e+14 5.320482e+11 7.896296e+13
## [21,] 8.103084e+03 2.648912e+10 3.584913e+09 5.459815e+01 1.096633e+03
## [22,] 6.235149e+27 5.685720e+24 7.694785e+23 2.515439e+30 9.496119e+19
## [23,] 3.584913e+09 6.398435e+17 2.415495e+07 1.318816e+09 3.584913e+09
## [24,] 3.584913e+09 5.987414e+04 4.311232e+15 8.659340e+16 2.146436e+14
## [25,] 1.409349e+22 8.659340e+16 1.285160e+19 4.201210e+25 4.727839e+18
## [26,] 1.096633e+03 7.389056e+00 1.957296e+11 1.096633e+03 4.539993e-05
## [27,] 1.811239e+41 1.112864e+36 4.093997e+35 7.307060e+43 4.375039e+48
## [28,] 1.627548e+05 3.931334e+12 1.353353e-01 1.627548e+05 4.851652e+08
## [29,] 1.739275e+18 1.285160e+19 1.907347e+21 8.659340e+16 2.648912e+10
## [30,] 7.016736e+20 4.201210e+25 2.091659e+24 1.739275e+18 8.438357e+26
## [31,] 4.093997e+35 1.858672e+31 6.663176e+40 4.093997e+35 6.837671e+30
## [32,] 1.409349e+22 2.830753e+23 3.831008e+22 8.659340e+16 4.727839e+18
## [33,] 2.415495e+07 1.484132e+02 2.648912e+10 1.000000e+00 2.008554e+01
## [34,] 6.837671e+30 1.409349e+22 1.041376e+23 1.694889e+28 2.830753e+23
## [35,] 1.252363e+29 1.041376e+23 6.837671e+30 3.104298e+26 2.581313e+20
## [36,] 5.459815e+01 3.059023e-07 4.424134e+05 8.315287e-07 1.202604e+06
## [37,] 9.253782e+29 1.142007e+26 4.093997e+35 1.041376e+23 7.694785e+23
## [38,] 1.068647e+13 3.185593e+16 5.320482e+11 1.586013e+15 1.586013e+15
## [39,] 2.415495e+07 9.744803e+09 1.586013e+15 3.931334e+12 2.980958e+03
## [40,] 7.694785e+23 6.398435e+17 5.685720e+24 1.041376e+23 2.293783e+27
## [41,] 5.834617e+14 1.068647e+13 4.727839e+18 1.446257e+12 2.830753e+23
## [42,] 9.253782e+29 1.858672e+31 5.540622e+34 7.694785e+23 3.404276e+29
## [43,] 1.338335e+42 6.493134e+50 1.084464e+46 1.220403e+39 1.651636e+38
## [44,] 1.811239e+41 4.093997e+35 3.989520e+45 1.986265e+44 1.467662e+45
## [45,] 5.459815e+01 5.459815e+01 6.565997e+07 6.144212e-06 5.987414e+04
## [46,] 4.607187e+28 4.201210e+25 7.694785e+23 3.831008e+22 1.252363e+29
## [47,] 4.489613e+38 2.178204e+47 2.758513e+33 3.989520e+45 1.338335e+42
## [48,] 7.389056e+00 3.678794e-01 1.831564e-02 1.125352e-07 6.565997e+07
## [49,] 5.459815e+01 1.234098e-04 6.565997e+07 9.118820e-04 2.718282e+00
## [50,] 9.496119e+19 7.694785e+23 1.142007e+26 6.398435e+17 4.201210e+25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same results can be generated by using a &lt;code&gt;for&lt;/code&gt; loop to loop over all entries, but this is much slower.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;lapply()&lt;/code&gt; (“list apply”) is similar but runs over a list of values, and returns the output as a list of values. In this example, the mean is calculated for a number of vectors, but these vectors can be different sizes (unlike for a matrix or data frame):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
lapply(my_list, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 3.142857
## 
## $val2
## [1] 1.5
## 
## $val3
## [1] 6.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, since the output is a list, the output could also be a list of vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
lapply(my_list, FUN = sort)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 2 2 3 3 4 4 4
## 
## $val2
## [1] 1 2
## 
## $val3
## [1]  2  5  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sapply()&lt;/code&gt; (“simple apply”) is similar to &lt;code&gt;lapply()&lt;/code&gt;, but returns the results as a vector rather than a list. This is a better method to use when returning a single value for each list entry:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
sapply(my_list, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     val1     val2     val3 
## 3.142857 1.500000 6.500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mapply()&lt;/code&gt; (“multivariate apply”) is very useful for vectorization, and works by applying the function &lt;code&gt;FUN&lt;/code&gt; to the first elements of each object, then to the second element, and so on. The following example will replicate the number &lt;code&gt;n&lt;/code&gt; n-times for numbers 1 to 5. This could also be done using loops, but loops do not scale as well as vectorised functions such as this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapply(rep, 1:5, 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2 2
## 
## [[3]]
## [1] 3 3 3
## 
## [[4]]
## [1] 4 4 4 4
## 
## [[5]]
## [1] 5 5 5 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tapply()&lt;/code&gt; is a little more complicated, but essentially applies a function after breaking data up based on some index variable. It is useful for calculating summary statistics of different groups of data, and uses a factor parameter &lt;code&gt;INDEX&lt;/code&gt; to define the groups over which to apply the function &lt;code&gt;FUN&lt;/code&gt;. So in the following code, &lt;code&gt;tapply&lt;/code&gt; will apply the function &lt;code&gt;mean()&lt;/code&gt; on the values of &lt;code&gt;Expression&lt;/code&gt; for the two different treatment classes defined in the &lt;code&gt;INDEX&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_dat &amp;lt;- data.frame(Treatment = c(&amp;quot;Control&amp;quot;, &amp;quot;Control&amp;quot;, &amp;quot;Control&amp;quot;, 
                                   &amp;quot;Treated&amp;quot;, &amp;quot;Treated&amp;quot;, &amp;quot;Treated&amp;quot;),
                     Expression = c(13, 17, 9,
                                    28, 37, 34))
tapply(my_dat$Expression, INDEX = my_dat$Treatment, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Control Treated 
##      13      33&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;some-simple-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Some Simple Statistics&lt;/h1&gt;
&lt;p&gt;R is mainly designed for easy computation of statistics and there are many in-built functions and additional libraries that allow you to carry out most tasks. Most simple statistics can be easily calculated using in-built functions. The following example creates two vectors of 100 random values sampled from a normal distribution with mean 0 and standard deviation 1, then calculates various basic summary statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sort(rnorm(100, mean = 0, sd = 1))
min(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.336472&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.939707&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07717416&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02808852&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum and maximum values are the smallest and largest values respectively. The mean is what most people would think of when you asked for the average, and is calculated by summing the values and dividing by the total number of values. The median is another way of looking at the average, and is essentially the middle value (&lt;code&gt;50^th^&lt;/code&gt; percentile). Other percentiles can be calculated, which can give you an idea of where the majority of your data lie:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = 0.25) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        25% 
## -0.6394031&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = 0.75) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       75% 
## 0.6994683&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = seq(0, 1, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          0%         10%         20%         30%         40%         50% 
## -2.33647209 -1.31947437 -0.87549325 -0.55566797 -0.15779236  0.02808852 
##         60%         70%         80%         90%        100% 
##  0.27175879  0.56818018  1.05753511  1.62287076  2.93970745&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summary()&lt;/code&gt; function will calculate many of these basic statistics for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -2.33647 -0.63940  0.02809  0.07717  0.69947  2.93971&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance is the average of the squared distances from the mean, and is a measure of how spread out the data are from the average. The standard deviation is simply the square root of this value &lt;span class=&#34;math inline&#34;&gt;\(var(x) = sd(x)^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.07332&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.152016&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(sd(x)^2, var(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum((x-mean(x))^2)/(length(x)-1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.152016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The covariance is a measure of how much two sets of data vary together:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- sort(rnorm(100, mean = 0, sd = 1))
var(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9048439&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.006295&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The covariance is related to the correlation between two data sets, which is a number between -1 and 1 indicating the level of dependance between the two variables. A value of 1 indicates perfect correlation, so that as one value increases so does the other. A value of -1 indicates perfect anti-correlation, so that as one value increases the other decreases. A value of 0 indicates that the two values change independently of one another:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9856189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(x, y)/(sd(x) * sd(y)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9856189&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This value is known as the Pearson correlation. An alternative method for calculating the correlation between two sets of values is to use the Spearman correlation, which is essentially the same as the Pearson correlation but is calculated on the ranks of the data rather than the values themselves. In this way, each value increases by only one unit at a time, meaning that the correlation score is more robust to the presence of outliers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y, method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So these values are pretty highly dependent on one another – not surprising considering that they are both drawn randomly from the same distribution. We can calculate the line of best fit between the two vectors by using linear regression, which searches for the best straight line model &lt;span class=&#34;math inline&#34;&gt;\(y = a + bx\)&lt;/span&gt; that minimises the squared distances between the line (estimated values) and the observed data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_lin_mod &amp;lt;- lm(y ~ x)
summary(my_lin_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.75519 -0.06913  0.03729  0.10091  0.29603 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.26517    0.01620  -16.37   &amp;lt;2e-16 ***
## x            0.87351    0.01513   57.74   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1616 on 98 degrees of freedom
## Multiple R-squared:  0.9714, Adjusted R-squared:  0.9712 
## F-statistic:  3334 on 1 and 98 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explaining this output is beyond the scope of this short tutorial, but the coefficient estimates give us the values for &lt;code&gt;a&lt;/code&gt; (&lt;code&gt;-0.2651651&lt;/code&gt;) and &lt;code&gt;b&lt;/code&gt; (&lt;code&gt;0.8735073&lt;/code&gt;) in the linear model. The p-value tells us how significant these estimates are. In statistical terms, we are testing the null hypothesis that the coefficient is actually equal to zero (i.e. there is not an association between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;). The p-value gives the probability of detecting a coefficient at least as large as the one that we calculated in our model given that the null hypothesis is actually true. If this probability is low enough, we can safely reject the null hypothesis and say that this variable is statistically significant. Often a value of 0.05 (5%) is used as the cutoff for rejection of the null hypothesis.&lt;/p&gt;
&lt;p&gt;Hypothesis testing is a large part of statistics. The t-test is a commonly used test for comparing the means of two sets of data. In simple terms we are looking to see if they are significantly different (e.g. does the expression of a particular gene change significantly following treatment with a drug). In statistical terms, we are testing to see if the change that we see in the means is greater than we would expect by chance alone.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, y) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and y
## t = 1.917, df = 195.18, p-value = 0.0567
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.007920033  0.557774135
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 -0.19775289&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are drawn from the same distribution, the test shows there is no evidence that there is a difference between the mean. Let’s try again with a different data set, drawn from a different distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- rnorm(100, mean = 10, sd = 1) 
t.test(x, z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.239515  -9.660461
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the p-value is much less than 0.05, so we can make the claim that the mean of &lt;code&gt;z&lt;/code&gt; is significantly different from that of &lt;code&gt;x&lt;/code&gt;. By default, the &lt;code&gt;t.test()&lt;/code&gt; function is 2-sided, meaning that it does not distinguish between whether or not the difference in the means is an increase or a decrease in &lt;code&gt;z&lt;/code&gt;. We can specify the &lt;code&gt;alternative&lt;/code&gt; parameter to define the alternative hypothesis that we want to test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, z, alternative = &amp;quot;less&amp;quot;)    ## Tests if mean(x) &amp;lt; mean(z) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##       -Inf -9.707361
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, z, alternative = &amp;quot;greater&amp;quot;) ## Tests if mean(x) &amp;gt; mean(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value = 1
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -10.19262       Inf
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us that the difference in the means between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; is less than 0, or that the mean of &lt;code&gt;z&lt;/code&gt; is greater than that of &lt;code&gt;x&lt;/code&gt; (as we expect).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Plotting With R&lt;/h1&gt;
&lt;p&gt;One of the most useful functions of R is the ability to plot publication-quality figures simply and easily. The vast number of tools available to users for plotting figures is beyond the scope of this tutorial, but I will mention a few of the most commonly used plotting functions to allow you to have a quick look at your data. These functions are all part of the &lt;code&gt;base&lt;/code&gt; plotting package, but I also recommend looking into the &lt;a href=&#34;http://ggplot2.org&#34;&gt;ggplot2()&lt;/a&gt; package for an incredibly intuative appraoch to plotting data.&lt;/p&gt;
&lt;div id=&#34;scatterplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.1&lt;/span&gt; Scatterplots&lt;/h2&gt;
&lt;p&gt;Scatterplots are probably the simplest plot that we can look at. Here we take two sets of values and plot one against the other to see how they correlate. This means that the two data sets are paired, such that the first element of each data set represents one event, the second represents another, and so on. For instance, for every student in a class, we may have scores from tests taken at the start and at the end of the year, and we want to compare them against one another to see how they compare. Here is how to plot a simple scatterplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, 
     pch = 19,                  ## Plot each point as a filled circle
     col = &amp;quot;red&amp;quot;,               ## Colour each point red
     xlab = &amp;quot;This is x&amp;quot;,        ## Add a label to the x-axis
     ylab = &amp;quot;This is y&amp;quot;,        ## Add a label to the y-axis
     main = &amp;quot;This is y vs. x&amp;quot;,  ## Add a main title to the plot
     cex.main = 1.4,            ## Change the size of the title
     cex.lab  = 1.2,            ## Change the size of the axis labels
     cex.axis = 1.1             ## Change the size of the axis values
     )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are lots of additional plotting arguments that can be set in the &lt;code&gt;plot()&lt;/code&gt; command. These are just a few. These arguments will typically work for any plotting function that you may want to use.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plot()&lt;/code&gt; is the standard plotting function, and works differently depending on the type of data on which it is called. Most of the following plots use this function in some way, even though it may not be obvious.&lt;/p&gt;
&lt;p&gt;Here we have coloured all of our points a single colour by using the &lt;code&gt;col = &amp;quot;red&amp;quot;&lt;/code&gt; argument. However, we can assign colours to each point separately by supplying a vector of colours that is the same length as &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. This means that we can set colours based on the data themselves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_cols &amp;lt;- rep(&amp;quot;black&amp;quot;, length(x)) 
my_cols[x &amp;gt; 1 &amp;amp; y &amp;gt;  1] &amp;lt;- &amp;quot;red&amp;quot;
my_cols[x &amp;gt; 1 &amp;amp; y &amp;lt; -1] &amp;lt;- &amp;quot;green&amp;quot; 
my_cols[x &amp;lt; 0 &amp;amp; y &amp;gt;  0] &amp;lt;- &amp;quot;blue&amp;quot;
plot(x, y, col = my_cols, pch = 19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since this plot is useful for observing the level of correlation between two data sets, it may be useful to add a couple of lines in to the plot to help us determine if there is a trend indicating that x is well correlated with y. First of all we will add lines in through the origin, and then we will add in a dotted line along the x = y line (since, if the two datasets were exactly correlated, the points would lie on this line). To do this, we use the &lt;code&gt;abline()&lt;/code&gt; function. This plots a straight line in one of three ways. We can either specify a horizontal line by specifying the &lt;code&gt;h&lt;/code&gt; argument, or we can specify a vertical line by using the &lt;code&gt;v&lt;/code&gt; argument, or we can specify a straight line in the format &lt;span class=&#34;math inline&#34;&gt;\(y = a + bx\)&lt;/span&gt; (where &lt;code&gt;a&lt;/code&gt; is the intercept term and &lt;code&gt;b&lt;/code&gt; is the gradient term):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, ylim = c(-3,3), xlim = c(-3,3))
abline(h = 0)
abline(v = 0)
abline(a = 0, b = 1, lty = 2) ## lty gives the line type - in this case dotted&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;abline()&lt;/code&gt; does not create a new plot, but instead adds to the plot that we already have. This is because it does not call the &lt;code&gt;plot.new()&lt;/code&gt; function, which would otherwise create a new plotting region.&lt;/p&gt;
&lt;p&gt;We may be particularly interested in how the line of best fit looks as compared to the &lt;span class=&#34;math inline&#34;&gt;\(x = y\)&lt;/span&gt; line, as this will show us if there is a general trend in the data or not. To do this we can use a linear model to predict &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; from the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, ylim = c(-3,3), xlim = c(-3,3))
my_lin_model &amp;lt;- lm(y ~ x) 
abline(my_lin_model, lty = 2, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want to explicitly pull out &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, use the &lt;code&gt;coef()&lt;/code&gt; function to get the coefficients:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(my_lin_model)[1] ## Get the intercept from the coefficients of the model &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##  -0.2651651&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(my_lin_model)[2] ## Get the gradient from the coefficients of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         x 
## 0.8735073&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.2&lt;/span&gt; Histograms&lt;/h2&gt;
&lt;p&gt;Now let’s look at the distribution of the data. A histogram is useful for this. Here we count up the number of values that fall into discrete bins. The size of the bins (or the number of bins) can be specified by using the &lt;code&gt;breaks&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- rnorm (1000)
par(mfrow=c(1,2))
hist(x) ## Shows a nice bell shape curve about mean 0 
hist(x, breaks = 200) ## More fine-grained&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.3&lt;/span&gt; Quantile-Quantile Plots&lt;/h2&gt;
&lt;p&gt;Quantile-quantile plots are a particular type of scatterplot that are used to see if two data sets are drawn from the same distribution. To do this, it plots the quantiles of each data set against each other. That is it plots the 0&lt;sup&gt;th&lt;/sup&gt; percentile of data set A (the minimum value) against the 0th percentile of data set B, the 50&lt;sup&gt;th&lt;/sup&gt; percentiles (the medians) against each other, the 100&lt;sup&gt;th&lt;/sup&gt; percentiles (the maximum values) against each other, etc. Simply, it sorts both data sets, and makes them both the same length by estimating any missing values, then plots a scatterplot of the sorted data. If the two data sets are drawn from the same distribution, this plot should follow the &lt;span class=&#34;math inline&#34;&gt;\(x = y\)&lt;/span&gt; identity line at all but the most extreme point.&lt;/p&gt;
&lt;p&gt;Here is a QQ plot for two data sets drawn from the same normal distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- rnorm(100, mean = 0, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 0, sd = 1) 
qqplot(x1, x2)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/QQplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is a QQ plot for two data sets drawn from different normal distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- rnorm(100, mean = 0, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 1, sd = 3) 
qqplot(x1, x2)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/QQplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.4&lt;/span&gt; Line Plots&lt;/h2&gt;
&lt;p&gt;Scatterplots are useful for generating correlation plots for pairs of data. Another form of data is a set of values along a continuum, for instance we may have the read count along the length of the genome. For this, a scatterplot may not be the most sensible way of viewing these data. Instead, a line plot may be a more fitting way of viewing the data. To do this we simply specify the &lt;code&gt;type&lt;/code&gt; argument to be &lt;code&gt;line&lt;/code&gt; (or simply &lt;code&gt;l&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;One thing to be careful of with data such as this is that you must make sure that the data are ordered from left to right (or right to left) on the x axis so that connecting the points makes sense on the continuum. For instance, the following plot is not terribly useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = c(2,4,5,3,1,7,9,8,6,10)
y = c(4,2,5,4,10,6,6,5,6,9)
plot(x = x, y = y, type = &amp;#39;l&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot1-1.png&#34; width=&#34;672&#34; /&gt;
But if we order the data from left to right then it will be a lot more useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = x[order(x)], y = y[order(x)], type = &amp;#39;l&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot2-1.png&#34; width=&#34;672&#34; /&gt;
You can also plot both points and lines by setting the &lt;code&gt;type&lt;/code&gt; argument to &lt;code&gt;both&lt;/code&gt; (or &lt;code&gt;b&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = x[order(x)], y = y[order(x)], type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.5&lt;/span&gt; Density Plots&lt;/h2&gt;
&lt;p&gt;We can use a line plot like this to plot the density of the data, which gives us a similar plot to the histogram. The benefit of this type of plot over a histogram is that you can overlay the distribution of multiple data sets. The &lt;code&gt;density()&lt;/code&gt; function is a kernal density estimator function that basically calculates the density of the data within each bin such that the total area under the resulting curve is 1. This makes these plots useful for comparing data sets of different sizes as they are essentially normalised. We can add a legend to this plot to make it clear which line represents which sample. Again, this does not call &lt;code&gt;plot.new()&lt;/code&gt; so will appear on top of the current plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Create 2 random normal distributions about 5 and 10 respectively
x1 &amp;lt;- rnorm(100, mean = 5, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 10, sd = 1)

## Calculate the density of each
x1dens &amp;lt;- density(x1) 
x2dens &amp;lt;- density(x2)

## Set up a plotting region explicitly
plot.new()
plot.window(xlim = c(0,15), 
            ylim = c(0,0.5))
range&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (..., na.rm = FALSE)  .Primitive(&amp;quot;range&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title(xlab = &amp;quot;Value&amp;quot;, ylab = &amp;quot;Density&amp;quot;, main = &amp;quot;Density Plot&amp;quot;) 
axis(1)
axis(2)

## Add the data (notice that these do not call plot.new() so will add onto the current figure
lines(x1dens , col = &amp;quot;red&amp;quot;) 
lines(x2dens , col = &amp;quot;blue&amp;quot;)

## Add a legend
legend(&amp;quot;topleft&amp;quot;, legend = c(&amp;quot;Mean = 5&amp;quot;, &amp;quot;Mean = 10&amp;quot;), col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/density1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.6&lt;/span&gt; Boxplots&lt;/h2&gt;
&lt;p&gt;Another way to compare the distribution of two (or more) data sets is by using a boxplot. A boxplot shows the overal distribution by plotting a box bounded by the first and third quartiles, with the median highlighted. This shows where the majority of the data lie. Additional values are plotted as whiskers coming out from the main box:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(x1, x2, names = c(&amp;quot;Mean = 5&amp;quot;, &amp;quot;Mean = 10&amp;quot;), ylab = &amp;quot;Value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/boxplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;boxplot()&lt;/code&gt; can also take the data in the form of a data frame, which is useful for instance if you want to compare the distribution of expression values over all genes for a number of different samples. This will automatically label the boxes with the column names from the data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_data &amp;lt;- data.frame(Sample1 = rnorm(100), 
                      Sample2 = rnorm(100), 
                      Sample3 = rnorm(100), 
                      Sample4 = rnorm(100), 
                      Sample5 = rnorm(100))
boxplot(my_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/boxplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-plots-and-pie-charts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.7&lt;/span&gt; Bar Plots and Pie Charts&lt;/h2&gt;
&lt;p&gt;Now let’s say that we have a data set that shows the number of called peaks from a ChIPseq data set that fall into distinct genomic features (exons, introns, promoters and intergenic regions). One way to look at how the peaks fall would be to look at a pie graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_peak_nums &amp;lt;- c(&amp;quot;exon&amp;quot;       = 1400, 
                  &amp;quot;intron&amp;quot;     = 900, 
                  &amp;quot;promoter&amp;quot;   = 200, 
                  &amp;quot;intergenic&amp;quot; = 150) 
pie(my_peak_nums)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/pie1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the majority of the peaks fall into exons. However, pie graphs are typically discouraged by statisticians, because your eyes can often misjudge estimates of the area taken up by each feature. A better way of looking at data such as this would be in the form of a barplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(my_peak_nums, 
        ylab = &amp;quot;Number of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s suppose that we had data showing the number of peaks in different genomic features for multiple samples. We could plot multiple pie charts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_peak_nums &amp;lt;- data.frame(GeneFeature = c(&amp;quot;exon&amp;quot;, &amp;quot;intron&amp;quot;, &amp;quot;promoter&amp;quot;, &amp;quot;intergenic&amp;quot;),
                           Sample1 = c( 1400, 900, 200, 150 ),
                           Sample2 = c( 2400, 1000, 230,250 ),
                           Sample3 = c( 40,30, 5,7 )
                           )
par(mfrow = c(1,3))
pie(my_peak_nums[[2]], main = &amp;quot;Sample1&amp;quot;, labels = my_peak_nums[[1]])
pie(my_peak_nums[[3]], main = &amp;quot;Sample2&amp;quot;, labels = my_peak_nums[[1]])
pie(my_peak_nums[[4]], main = &amp;quot;Sample3&amp;quot;, labels = my_peak_nums[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/pie2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1,1)) ## Reset the plotting region&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However comparing across multiple pie charts is very difficult. Instead, a single barplot will work better. Note that here the number of peaks is different for each sample, so it makes more sense to convert the data into a format whereby the bar height represents the percentage of peaks within a particular feature:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Convert to percentages so that the samples are comparable
my_peak_percent &amp;lt;- my_peak_nums[, 2:4] 
for (i in 1:3) {
  my_peak_percent[[i]] &amp;lt;- 100*my_peak_percent[[i]]/sum(my_peak_percent[[i]]) 
}

## Convert to a matrix to satisfy requirements for barplot()
my_peak_percent &amp;lt;- as.matrix(my_peak_percent)

## Plot the bar plot
barplot(my_peak_percent ,
        ylab = &amp;quot;Percentage of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;, 
        legend.text = my_peak_nums[[&amp;quot;GeneFeature&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot2-1.png&#34; width=&#34;672&#34; /&gt;
Notice that the default way that &lt;code&gt;barplot()&lt;/code&gt; works is to plot the bars in a single stack for each sample. This is fine for comparing the exons, but trying to compare the other classes is much harder. A better way to plot these data would be to plot the bars side by side for each sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(my_peak_percent ,
        ylab = &amp;quot;Percentage of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;, 
        legend.text = my_peak_nums[[&amp;quot;GeneFeature&amp;quot;]], 
        beside = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graphical-control&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.8&lt;/span&gt; Graphical Control&lt;/h2&gt;
&lt;p&gt;That covers the majority of the basic plotting functions that you may want to use. You can change the standard plotting arguments by using the par() command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(5,10,0,3))  ## Sets the figure margins (in &amp;#39;number of lines&amp;#39;) - b,l,t,r
par(las = 1)            ## Changes axis labels to always be horizontal
par(tcl = -0.2)         ## Change the size of the axis ticks
plot(x = rnorm(100), y = rnorm(100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.9&lt;/span&gt; Subplots&lt;/h2&gt;
&lt;p&gt;By default, the graphics device will plot a single figure only. There are several ways to create subfigures within this region. The first is to set the &lt;code&gt;mfrow&lt;/code&gt; argument in &lt;code&gt;par()&lt;/code&gt;. This will split the graphics region into equally sized subplots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(3, 2)) ## Creates a figure region with 3 rows and 2 columns 
for (i in 1:6) {
  plot(x = rnorm(100), y = rnorm(100)) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/subplot1-1.png&#34; width=&#34;672&#34; /&gt;
However, if you want more control over your plotting, you can use the &lt;code&gt;layout()&lt;/code&gt; function which allows you to specify the size and layout of the subplots. This function takes a matrix specifying where in the grid of subplots each plot should be drawn to. So the first call to &lt;code&gt;plot()&lt;/code&gt; will put its figure in the grid regions labelled &lt;code&gt;1&lt;/code&gt;, the scond call will put its figure anywhere that there is a &lt;code&gt;2&lt;/code&gt;, etc. Anywhere that you do not want a figure should have a &lt;code&gt;0&lt;/code&gt;. The heights and widths arguments allow you to specify the size of each grid region. You can check what the resulting figure layout will look like by using &lt;code&gt;layout.show(n)&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is the number of subplots in your figure. With a bit of work, you can get some very good layouts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_layout &amp;lt;- matrix(c(1,1,1,1,2,2,3,4,2,2,3,4,0,0,3,4,0,0,5,5), nrow = 5, ncol = 4, byrow = TRUE)
layout(my_layout, widths = c(10,10,2,2), heights = c(1,5,5,5,2)) 
my_layout&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    1    1    1
## [2,]    2    2    3    4
## [3,]    2    2    3    4
## [4,]    0    0    3    4
## [5,]    0    0    5    5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout.show(5) ## Can you see how this matrix leads to this layout? &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/subplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-figures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.10&lt;/span&gt; Saving Figures&lt;/h2&gt;
&lt;p&gt;By default, figures are generated in a seperate window from R. However, you can save the figure to an external file by using one of the functions &lt;code&gt;png()&lt;/code&gt;, &lt;code&gt;pdf()&lt;/code&gt;, &lt;code&gt;jpeg()&lt;/code&gt;, etc. These functions open a new “device”, which R can use to plot to. After the figure has been plotted, the device must be turned off again using the &lt;code&gt;dev.off()&lt;/code&gt; function. There are many arguments that can be used for these functions. In general, these define the dimensions and resolution of the resulting figure. It can be difficult to get these right, so play around to see how they affect things:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;png(&amp;quot;figures/test_figure.png&amp;quot;, height = 10, width = 10, unit = &amp;quot;in&amp;quot;, res = 300)
plot(1:10, 1:10, type = &amp;quot;l&amp;quot;, main = &amp;quot;My Test Figure&amp;quot;, xlab = &amp;quot;x axis&amp;quot;, ylab = &amp;quot;y axis&amp;quot;) 
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Example Analysis&lt;/h1&gt;
&lt;div id=&#34;introduction-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;This is just a simple example analysis to give you an idea of the sort of things that we can do with R. Suppose that we have two experiments, each looking at the effects on gene expression of using a particular drug (“Drug A” and “Drug B”). For each experiment we have two samples; one showing the gene expression when treated with the drug, and the other showing the gene expression when treated with some control agent. Obviously in a real experiment, we would have many replicates, but here we have &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;. We want to do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For each drug, we want to get the fold change for each gene&lt;/li&gt;
&lt;li&gt;For each drug, we want to identify the genes that are significantly changed when using the drug&lt;/li&gt;
&lt;li&gt;We want to compare the results for Drug A with those from Drug B to find genes that are affected similarly by both drugs&lt;/li&gt;
&lt;li&gt;We want to plot the correlation between the fold change values for the two drugs to see how similar they are&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, we will need four files. These files are in a tab-delimited text format. They are tables of values where each row is separated by a new line, and each column is separated by a tab character (&lt;code&gt;\t&lt;/code&gt;). These files can be created by and read into Excel for ease of use. To avoid errors when reading in files from text, it is good practice to ensure that there are no missing cells in your data. Instead try to get into the habit of using some “missing”&amp;quot; character (e.g. &lt;code&gt;NA&lt;/code&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;File Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_control.txt&#34;&gt;experiment1_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for ctrl in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_drug.txt&#34;&gt;experiment1_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_control.txt&#34;&gt;experiment2_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for control in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_drug.txt&#34;&gt;experiment2_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.2&lt;/span&gt; Load Data&lt;/h2&gt;
&lt;p&gt;First let’s load in the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expt1_ctrl &amp;lt;- read.table(&amp;quot;experiment1_control.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt1_drug &amp;lt;- read.table(&amp;quot;experiment1_drug.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt2_ctrl &amp;lt;- read.table(&amp;quot;experiment2_control.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt2_drug &amp;lt;- read.table(&amp;quot;experiment2_drug.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;head()&lt;/code&gt; to look at the data. Each of these files contains two columns; the gene name and some value that represents the expression level for that gene (assume that these values have been calculated after pre-processing, normalisation, etc.).&lt;/p&gt;
&lt;p&gt;In all of these cases, the list of gene names is identical, and in the same order which means that we could compare row 1 from the control-treated file with row 2 from the drug-treated file to get all of the comparisons. However, in a real data set you will not know for sure that the gene names match so I recommend merging the files together into a single data frame to ensure that all analyses are conducted on a gene by gene basis on the correct values.&lt;/p&gt;
&lt;p&gt;We therefore create a single data frame for both experiments using the merge() command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expt1 &amp;lt;- merge(expt1_ctrl, expt1_drug, 
               by = &amp;quot;GeneName&amp;quot;) ## The &amp;#39;by&amp;#39; variable tells merge which column to merge
names(expt1)[2] &amp;lt;- &amp;quot;Control&amp;quot; 
names(expt1)[3] &amp;lt;- &amp;quot;Drug&amp;quot; 
expt2 &amp;lt;- merge(expt2_ctrl, expt2_drug, 
               by = &amp;quot;GeneName&amp;quot;) 
names(expt2)[2] &amp;lt;- &amp;quot;Control&amp;quot;
names(expt2)[3] &amp;lt;- &amp;quot;Drug&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-fold-change&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.3&lt;/span&gt; Calculate Fold Change&lt;/h2&gt;
&lt;p&gt;Now we calculate the fold change for each gene by dividing the drug-treated expression by the control expression. To avoid divide by zero errors, we can set a minimum expression value. This will also ensure that we are only looking at expression changes between significant expression values. Since we want to do the same thing to both the experiment 1 and the experiment 2 data sets, it makes sense to write a single function to use for both:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_fold_change &amp;lt;- function (x, min_expression = 10) {
  ctrl_val &amp;lt;- as.numeric(x[&amp;quot;Control&amp;quot;]) 
  drug_val &amp;lt;- as.numeric(x[&amp;quot;Drug&amp;quot;])
  ctrl_val &amp;lt;- ifelse(ctrl_val &amp;lt;= min_expression, min_expression, ctrl_val)
  drug_val &amp;lt;- ifelse(drug_val &amp;lt;= min_expression, min_expression, drug_val)
  return(drug_val/ctrl_val) 
}
expt1[[&amp;quot;FoldChange&amp;quot;]] &amp;lt;- apply(expt1, MAR = 1, FUN = get_fold_change) 
expt2[[&amp;quot;FoldChange&amp;quot;]] &amp;lt;- apply(expt2, MAR = 1, FUN = get_fold_change)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.4&lt;/span&gt; Compare Data&lt;/h2&gt;
&lt;p&gt;Now let’s find the genes that are upregulated and downregulated in each experiment. Due to the lack of replicates, we do not have any estimate for the variance of these genes, so we will have to make do with using a threshold on the fold change:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fold_change_threshold &amp;lt;- 1.5
expt1_up   &amp;lt;- subset(expt1, FoldChange &amp;gt;= fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt1_down &amp;lt;- subset(expt1, FoldChange &amp;lt;= 1/fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt2_up   &amp;lt;- subset(expt2, FoldChange &amp;gt;= fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt2_down &amp;lt;- subset(expt2, FoldChange &amp;lt;= 1/fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
cat(&amp;quot;Upregulated in Experiment 1:&amp;quot;,   paste(expt1_up,   collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1:
## gene12
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 1:&amp;quot;, paste(expt1_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1:
## gene32
## gene46&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Upregulated in Experiment 2:&amp;quot;,   paste(expt2_up,   collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 2:
## gene18
## gene50
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 2:&amp;quot;, paste(expt2_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 2:
## gene22
## gene43&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we now have the genes that change when each of the drugs is used. But now we want to compare the two drugs together. First, let’s see if there are any genes similarly affected by both drugs. We can do this using the &lt;code&gt;intersect()&lt;/code&gt; function which gives the intersect of two lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_up   &amp;lt;- intersect(expt1_up, expt2_up) 
common_down &amp;lt;- intersect(expt1_down, expt2_down)
cat(&amp;quot;Upregulated in Experiment 1 and Experiment 2:&amp;quot;, paste(common_up, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1 and Experiment 2:
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 1 and Experiment 2:&amp;quot;, paste(common_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1 and Experiment 2:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see that only one gene is similarly affected by both drugs (“gene8”). Now let’s plot a figure to see how the fold change differs between the two drugs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fold_change_data &amp;lt;- merge(expt1[, c(&amp;quot;GeneName&amp;quot;, &amp;quot;FoldChange&amp;quot;)], 
                          expt2[, c(&amp;quot;GeneName&amp;quot;, &amp;quot;FoldChange&amp;quot;)], 
                          by = &amp;quot;GeneName&amp;quot;)
names(fold_change_data)[2] &amp;lt;- &amp;quot;Experiment1&amp;quot;
names(fold_change_data)[3] &amp;lt;- &amp;quot;Experiment2&amp;quot;
plot(x = log2(fold_change_data[[&amp;quot;Experiment1&amp;quot;]]), 
     y = log2(fold_change_data[[&amp;quot;Experiment2&amp;quot;]]), 
     pch = 19,
     xlab = &amp;quot;log2(Experiment1 Fold Change)&amp;quot;,
     ylab = &amp;quot;log2(Experiment2 Fold Change)&amp;quot;,
     main = &amp;quot;Experiment1 Fold Change vs Experiment2 Fold Change&amp;quot;, 
     cex.lab = 1.3,
     cex.axis = 1.2,
     cex.main = 1.4,
     xlim = c(-2,2),
     ylim = c(-2,2)
     )
abline(h = 0) 
abline(v = 0)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/foldchange-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the effect on the gene expression is actually quite different for the two drugs. We can also see this by looking at the correlation between the two experiments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x = fold_change_data[[&amp;quot;Experiment1&amp;quot;]], 
    y = fold_change_data[[&amp;quot;Experiment2&amp;quot;]], 
    method = &amp;quot;pearson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08381614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x = fold_change_data[[&amp;quot;Experiment1&amp;quot;]], 
    y = fold_change_data[[&amp;quot;Experiment2&amp;quot;]], 
    method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.02618115&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A lncRNA fine tunes the dynamics of a cell state transition involving Lin28, let-7 and de novo DNA methylation</title>
      <link>/publication/2017_a_lncrna_fine_tunes_the_dynamics_of_a_cell_state_transition_involving_lin28_let-7_and_de_novo_dna_methylation/</link>
      <pubDate>Fri, 18 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/publication/2017_a_lncrna_fine_tunes_the_dynamics_of_a_cell_state_transition_involving_lin28_let-7_and_de_novo_dna_methylation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of Anti-Fouling Coatings on Marine Biofilms</title>
      <link>/project/2017-08-17-anti-fouling-marine-biofilms/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-08-17-anti-fouling-marine-biofilms/</guid>
      <description>&lt;p&gt;Marine biofilms, or &amp;ldquo;biofouling&amp;rdquo;, are communities of micro-organisms bound together on a surface by an extracellular matrix composed of extracellular polymeric substances (EPS). Biofilms consist predominantly of phytoplankton (particularly diatoms which are photosynthesising algae) and bacteria, which bind to surfaces in the marine environment to form complex 3 dimensional surface structures.&lt;/p&gt;

&lt;p&gt;The formation of these biofilms can affect all types of surfaces in the marine environment, and substrates include such man-made structures as the hulls of ships, nets used by fishing trawlers, pipelines used to transport oil and gas, and boat propellors. Biofilm formation on ships can have a costly impact due to increased drag - for example the increased cost of fuel has been estimated at around $56 million per year for DDG-51 naval ships.&lt;/p&gt;

&lt;p&gt;For this reason, testing novel antifouling substrates that are environmentally benign is of great importance to many marine-based industries. Working together with &lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/maria-salta.html&#34; target=&#34;_blank&#34;&gt;Dr. Maria Salta&lt;/a&gt; we have been analysing the changes in bacterial communities on different man-made surfaces using next-generation sequencing. This project combines microbiological analysis with both 16S rDNA amplicon sequencing (for populationa analysis) and whole transcriptome sequencing using RNA seq (for gene expression analysis).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Effects of Radiation Exposure in the Environment</title>
      <link>/project/2017-08-17-effect-of-radiation-exposure-on-fish/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-08-17-effect-of-radiation-exposure-on-fish/</guid>
      <description>&lt;p&gt;Exposure to radiation can have deleterious effects on gene expression regulation, particularly due to increased DNA damage and deregulation of DNA damage checkpoint pathways. Working together with Dr. Adelaide Lerebours, we are performing differential expression analyses to understand the effects of radiation on 3-spined stickleback under laboratory conditions, and in particular attempting to characterise the differences between normal and abnormal offspring of exposed parents to understand how mutations are passed down through the lineage. In addition, we are looking at the effects of radiation exposure in the field, by looking at the effects of radiation exposure on lakes around the Chernobyl nuclear power station which suffered a catastrophic meltdown in 1986. The effects of this are still being felt today, and this project will allow us to understand the negative impact that this disaster has had on aquatic life in the area.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gene Expression Profiling of Duchenne Muscular Dystrophy</title>
      <link>/project/2017-08-17-expression-profiling-of-dmd/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-08-17-expression-profiling-of-dmd/</guid>
      <description>&lt;p&gt;Duchenne muscular dystrophy (DMD) is an X-linked recessive form of muscular dystrophy typically presenting in boys at around the age of four. DMD is caused by mutations in the DMD gene, which is the largest gene in the genome with 79 exons encoding a range of dystrophin proteins. These proteins are expressed in skeletal and cardiac muscles and also in brain and other non-muscle tissues, where they have specific roles. Their absence due to DMD mutations causes specific abnormalities such as progressive muscle weakness and wasting and cognitive deficits. Muscle weakness begins in the upper legs and pelvis but quickly spreads until boys are unable to walk and eventually cannot breathe unaided.  As yet, no cure exists for DMD, despite it being the most common form of muscular dystrophy.&lt;/p&gt;

&lt;p&gt;Together with &lt;a href=&#34;http://www.port.ac.uk/school-of-pharmacy-and-biomedical-sciences/staff/prof-darek-gorecki.html&#34; target=&#34;_blank&#34;&gt;Professor Darek Gorecki&lt;/a&gt;, we are using RNA seq analysis to identify the changes in expression associated with a point mutation in the Dmd gene in a dystrophic mouse model. By identifying the pathways most impacted by the dystrophic phenotype, we hope to identify novel areas for future therapeutic intervention.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Xenopus Development Project</title>
      <link>/project/2017-08-17-role-of-variant-histones-on-xenopus-development/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/project/2017-08-17-role-of-variant-histones-on-xenopus-development/</guid>
      <description>&lt;p&gt;Together with &lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/prof-matt-guille.html&#34; target=&#34;_blank&#34;&gt;Professor Matt Guille&lt;/a&gt; and &lt;a href=&#34;http://www.port.ac.uk/school-of-biological-sciences/staff/dr-fiona-myers.html&#34; target=&#34;_blank&#34;&gt;Dr. Fiona Myers&lt;/a&gt;, we are examining development in the model species &lt;em&gt;Xenopus laevis&lt;/em&gt;. We are using a wide range of techniques, including ChIP-seq to study epigenetic changes and to observe transcription factor binding throughout development, RNA-seq to study changes in gene expression, structural analysis of DNA tertiary structures, and Nanopore sequencing using the &lt;a href=&#34;https://nanoporetech.com&#34; target=&#34;_blank&#34;&gt;Oxford Nanopore MinION&lt;/a&gt; system to look at whole genome changes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RNA binding by the histone methyltransferases Set1 and Set2</title>
      <link>/publication/2017_rna-binding-by-the-histone-methyltransferases-set1-and-set2/</link>
      <pubDate>Tue, 25 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/publication/2017_rna-binding-by-the-histone-methyltransferases-set1-and-set2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Chemical Probe for the ATAD2 Bromodomain</title>
      <link>/publication/2016_a-chemical-probe-for-the-atad2-bromodomain/</link>
      <pubDate>Mon, 12 Sep 2016 00:00:00 +0000</pubDate>
      <guid>/publication/2016_a-chemical-probe-for-the-atad2-bromodomain/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Discovery of I-BRD9, a selective cell active chemical probe for bromodomain containing protein 9 inhibition</title>
      <link>/publication/2016_discovery_of_i-brd9_a_selective_cell_active_chemical_probe_for_bromodomain_containing_protein_9_inhibition/</link>
      <pubDate>Thu, 25 Feb 2016 00:00:00 +0000</pubDate>
      <guid>/publication/2016_discovery_of_i-brd9_a_selective_cell_active_chemical_probe_for_bromodomain_containing_protein_9_inhibition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generation of a selective small molecule inhibitor of the CBP/p300 bromodomain for leukemia therapy</title>
      <link>/publication/2015_generation_of_a_selective_small_molecule_inhibitor_of_the_cbp_p300_bromodomain_for_leukemia_therapy/</link>
      <pubDate>Tue, 01 Dec 2015 00:00:00 +0000</pubDate>
      <guid>/publication/2015_generation_of_a_selective_small_molecule_inhibitor_of_the_cbp_p300_bromodomain_for_leukemia_therapy/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The breast cancer oncogene EMSY represses transcription of antimetastatic microRNA miR-31</title>
      <link>/publication/2014_the_breast_cancer_oncogene_emsy_represses_transcription_of_antimetastatic_microrna_mir-31/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      <guid>/publication/2014_the_breast_cancer_oncogene_emsy_represses_transcription_of_antimetastatic_microrna_mir-31/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recurrent mutations, including NPM1c, activate a BRD4-dependent core transcriptional program in acute myeloid leukemia</title>
      <link>/publication/2014_recurrent_mutations_including_nmp1c_activates_a_brd4_dependent_core_transcriptional_program_in_acute_myeloid_leukemia/</link>
      <pubDate>Sat, 01 Feb 2014 00:00:00 +0000</pubDate>
      <guid>/publication/2014_recurrent_mutations_including_nmp1c_activates_a_brd4_dependent_core_transcriptional_program_in_acute_myeloid_leukemia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Glutamine methylation in histone H2A is an RNA-polymerase-I-dedicated modification</title>
      <link>/publication/2014_glutamine_methylation_in_histone_h2a_is_an_rna-polymerase-i-dedicated_modification/</link>
      <pubDate>Thu, 23 Jan 2014 00:00:00 +0000</pubDate>
      <guid>/publication/2014_glutamine_methylation_in_histone_h2a_is_an_rna-polymerase-i-dedicated_modification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BET protein inhibition shows efficacy against JAK2V617F-driven neoplasms</title>
      <link>/publication/2014_bet_protein_inhibition_shows_efficacy_against_jak2v617f-driven_neoplasms/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>/publication/2014_bet_protein_inhibition_shows_efficacy_against_jak2v617f-driven_neoplasms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The non-coding snRNA 7SK controls transcriptional termination, poising, and bidirectionality in embryonic stem cells</title>
      <link>/publication/2013_the_non-coding_snrna_7sk_controls_transcriptional_termination_poising_and_bidirectionality_in_embryonic_stem_cells/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      <guid>/publication/2013_the_non-coding_snrna_7sk_controls_transcriptional_termination_poising_and_bidirectionality_in_embryonic_stem_cells/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human RNA methyltransferase BCDIN3D regulates microRNA processing</title>
      <link>/publication/2012_human_rna_methyltransferase_bcdin3d_regulates_microrna_processing/</link>
      <pubDate>Fri, 12 Oct 2012 00:00:00 +0000</pubDate>
      <guid>/publication/2012_human_rna_methyltransferase_bcdin3d_regulates_microrna_processing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Three distinct patterns of histone H3Y41 phosphorylation mark active genes</title>
      <link>/publication/2012_three_distinct_patterns_of_histone_h3y41ph_phosphorylation_mark_active_genes/</link>
      <pubDate>Thu, 27 Sep 2012 00:00:00 +0000</pubDate>
      <guid>/publication/2012_three_distinct_patterns_of_histone_h3y41ph_phosphorylation_mark_active_genes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deciphering c-MYC-regulated genes in two distinct tissues</title>
      <link>/publication/2011_deciphering_c-myc-regulated_genes_in_two_distinct_tissues/</link>
      <pubDate>Thu, 01 Dec 2011 00:00:00 +0000</pubDate>
      <guid>/publication/2011_deciphering_c-myc-regulated_genes_in_two_distinct_tissues/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inhibition of BET recruitment to chromatin as an effective treatment for MLL-fusion leukaemia</title>
      <link>/publication/2011_inhibition_of_bet_recruitment_to_chromatin_as_an_effective_treatment_for_mll-fusion_leaukemia/</link>
      <pubDate>Sun, 02 Oct 2011 00:00:00 +0000</pubDate>
      <guid>/publication/2011_inhibition_of_bet_recruitment_to_chromatin_as_an_effective_treatment_for_mll-fusion_leaukemia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nucleosome-interacting proteins regulated by DNA and histone methylation</title>
      <link>/publication/2010_nucleosome-interacting_proteins_regulated_by_dna_and_histone_methylation/</link>
      <pubDate>Fri, 29 Oct 2010 00:00:00 +0000</pubDate>
      <guid>/publication/2010_nucleosome-interacting_proteins_regulated_by_dna_and_histone_methylation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Genome-wide association study of CNVs in 16,000 cases of eight common diseases and 3,000 shared controls</title>
      <link>/publication/2010_genome-wide_association_study_of_cnvs_in_16000_cases_of_eight_common_diseases_and_3000_shared_controls/</link>
      <pubDate>Thu, 01 Apr 2010 00:00:00 +0000</pubDate>
      <guid>/publication/2010_genome-wide_association_study_of_cnvs_in_16000_cases_of_eight_common_diseases_and_3000_shared_controls/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Origins and functional impact of copy number variation in the human genome</title>
      <link>/publication/2010_origins_and_functional_impact_of_copy_number_variation_in_the_human_genome/</link>
      <pubDate>Thu, 01 Apr 2010 00:00:00 +0000</pubDate>
      <guid>/publication/2010_origins_and_functional_impact_of_copy_number_variation_in_the_human_genome/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
