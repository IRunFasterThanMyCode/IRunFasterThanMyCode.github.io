<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MNIST | IRunFasterThanMyCode</title>
    <link>/tags/mnist/</link>
      <atom:link href="/tags/mnist/index.xml" rel="self" type="application/rss+xml" />
    <description>MNIST</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017 Sam Robson</copyright><lastBuildDate>Sat, 05 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/irunfasterthanmycode.jpg</url>
      <title>MNIST</title>
      <link>/tags/mnist/</link>
    </image>
    
    <item>
      <title>Deep Learning using TensorFlow Through the Keras API in RStudio</title>
      <link>/post/2019-10-05_deep_learning_with_keras_in_rstudio/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-05_deep_learning_with_keras_in_rstudio/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensorflow&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-database&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-transformation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequential-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropout-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-initial-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compile-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-the-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Machine learning and artificial intelligence is a hot topic in the tech world, but the expression “machine learning” can describe anything from fitting a straight line through some data, to a machine able to think, learn and react to the world in highly sophisticated ways (e.g. self-driving cars if you want to be positive about AI, or SkyNet from Terminator if you want to be a naysayer). Whilst common machine learning techniques like support vector machines and k-Nearest Neighbour algorithms can be used to solve a huge number of problems, deep learning algorithms like neural networks are required to create these highly sophisticated models.&lt;/p&gt;
&lt;p&gt;In this blog, I will explore the use of some commonly used tools for generating neural networks within the R programming language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/h1&gt;
&lt;p&gt;TensorFlow is one of the most powerful tools for deep learning, and in particular is widely used for training neural networks to classify various aspects of images. It is a freely distributed open-source library in python (but mainly written in C++) originally created by Google, but has become the cornerstone of many deep learning models currently out there&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Tensor&lt;/em&gt; is a multi-dimensional array, and the TensorFlow libraries represent a highly efficient pipeline for the myriad linear algebra calculations required to generate new tensors through the layers of the network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;keras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://keras.io&#34;&gt;Keras API&lt;/a&gt; is a high-level user-friendly neural network API (application programming interface) designed for accessing deep neural networks. One of the benefits is that it is able to run on GPUs as well as CPUs, which have been shown to work better for training neural networks since they are able more efficient at running the huge number of simple calculations required for model training (for example convolutions of image data).&lt;/p&gt;
&lt;p&gt;Keras can be used an interface to TensorFlow for training deep multi-level networks for use in deep learning applications. Both are developed in python, but here I am going to use the RStudio interface to run a few simple deep learning models to trial the process ahead of a more in-depth application. R and python are somewhat at war in the data science community, with (in my opinion) R being better for more general data analysis and visualisation (for instance, whilst the python &lt;code&gt;seaborn&lt;/code&gt; package produces beautiful images, the &lt;code&gt;ggplot2&lt;/code&gt; package is far more elaborate). However, with the Keras and TensorFlow packages (and the generally higher memory impact of using R), python is typically far more suited for deep learning applications.&lt;/p&gt;
&lt;p&gt;However, the ability to access the Keras API through RStudio, and the amazing power of using RStudio to develop workflows, will make this a perfect “one stop shop” for data science needs. Much of this work is developed from the &lt;a href=&#34;https://tensorflow.rstudio.com&#34;&gt;RStudio Keras and TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the &lt;code&gt;reticulate&lt;/code&gt; package to pipe python commands through R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;reticulate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then install and load the &lt;code&gt;keras&lt;/code&gt; package. When we load it using the &lt;code&gt;install_keras()&lt;/code&gt; function, we can define different backend engines and choose to use GPUs rather than CPUs, but for this example I will simply use the default TensorFlow backend on my laptop CPU:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/keras&amp;quot;)
library(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mnist-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/h1&gt;
&lt;p&gt;So let’s have a little play by looking at a standard machine learning approach, looking at the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST&lt;/a&gt; dataset. This is the Modified National Institute of Standards and Technology database, and contains a large amount of images of handwritten digits that is used to train models for handwriting recognition. Ultimately, the same models can be used for a huge number of classification tasks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mnist_dat &amp;lt;- dataset_mnist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset contains a training set of 60,000 images, and a test set of 10,000 images. Each image is pre-normalised such that each digit is a grayscale image that fits into a 28x28 pixel bounding box.Each image is also supplied with a label that tells us what the digit should really be. This dataset is commonly used as a kind of benchmark for new models, with people vying to build the model with the lowest error rate possible:&lt;/p&gt;
&lt;p&gt;So let’s define our data sets. We will require two main data sets; a &lt;strong&gt;training&lt;/strong&gt; set where we show the model the images and tell it what it should recognise, and a &lt;strong&gt;test&lt;/strong&gt; dataset where we can predict the result and check against the ground level label. For each data set, we will create a dataset &lt;code&gt;x&lt;/code&gt; containing all of the images, and a dataset &lt;code&gt;y&lt;/code&gt; containing the labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;x&amp;quot;]]
training_dat_y_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;y&amp;quot;]]
test_dat_x_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;x&amp;quot;]]
test_dat_y_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;y&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the images is essentially a 2D array, with 28 rows and 28 columns, with each cell representing the greyscale value of the pixel. So the &lt;code&gt;_dat_x&lt;/code&gt; data sets are 3D arrays. So accessing specific elements from these arrays in R is similar to accessing rows and columns using the &lt;code&gt;[x,y]&lt;/code&gt; style axis, but we need to specify a third element &lt;code&gt;z&lt;/code&gt; for the specific array that we want to access – &lt;code&gt;[z,x,y]&lt;/code&gt;. So lets take a look at an exampl of the input data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in 1:18) { 
  plot_dat &amp;lt;- training_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = training_dat_y_raw[i],
        cex  = 4, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_plot_entries-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/h1&gt;
&lt;p&gt;We can easily reduce this 3D data by essentially taking each 28x28 matrix and collapsing the 784 values down into a 1D vector. Then we can make one big 2D matrix containing all of the data. Ordinarily, this could be done by reassigning the dimensions of the array, but by using the &lt;code&gt;array_reshape()&lt;/code&gt; function, the data is adjusted to meet the requirements for Keras:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- array_reshape(training_dat_x_raw, c(nrow(training_dat_x_raw), 784))
test_dat_x     &amp;lt;- array_reshape(test_dat_x_raw,     c(nrow(test_dat_x_raw), 784))
dim(training_dat_x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000   784&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The values in these arrays are greyscale values, representing 256 integer values between 0 (black) and 255 (white). It will be useful for downstream analyses to rescale these values to real values in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- training_dat_x/255
test_dat_x     &amp;lt;- test_dat_x/255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The R-specific way to deal with categorical data would be to encode the values in the &lt;code&gt;y&lt;/code&gt; datasets to a factor with 10 levels (“0”, “1”, “2”, etc). However, Keras requires the data to be in a slightly different format, so we use the &lt;code&gt;to_categorical()&lt;/code&gt; function instead. This will encode the value in a new matrix with 10 columns and &lt;code&gt;n&lt;/code&gt; rows, such that every row contains exactly one &lt;code&gt;1&lt;/code&gt; (representing the label) and nine &lt;code&gt;0s&lt;/code&gt;. This is known as an identity matrix. Keras uses a lot of linear algebra, and this encoding makes these calculations much simpler:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_y &amp;lt;- to_categorical(training_dat_y_raw, 10)
test_dat_y     &amp;lt;- to_categorical(test_dat_y_raw, 10)
dim(training_dat_y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000    10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequential-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/h1&gt;
&lt;p&gt;A standard deep learning neural network model can be thought of as a number of sequential layers, with each layer representing a different abstraction of the data. For instance, consider a model looking at facial recognition from image data. The first layer might represent edges of different aspects of the image. The next layer might be designed to pick out nose shape. The next might pick out hair. The next might determine the orientation of the face. etc. Then by adding more and more layers, we can develop models able to classify samples based on a wide range of different features.&lt;/p&gt;
&lt;p&gt;Of course, there is a danger in statistics of &lt;em&gt;over-fitting&lt;/em&gt; data, which is when we create a model so specific for the training data that it becomes practically worthless. By definition, adding more variables into a model will always improve the fit, but at the cost of its applicability to other data sets. In models such as linear models, we look for parsimony – a model should be as complicated as it needs to be &lt;em&gt;and no more complicated&lt;/em&gt;. The old phrase is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you hear hooves, think horse not zebra&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, deep learning sequential models such as these are robust to these problems, since model training can back-propagate, allowing us to incorporate far more levels than would be possible with other machine learning techniques.&lt;/p&gt;
&lt;p&gt;The general steps involved in using Keras for deep learning are to first build your model, compile it to configure the parameters that will be used to develop the “best” model, train it using your training data, then test it on an additional data set to see how it copes.&lt;/p&gt;
&lt;p&gt;So let’s build a simple sequential neural network model object using &lt;code&gt;keras_model_sequential()&lt;/code&gt;, then add a series of additional layers that we hope will accurately identify our different categories. Adding sequential layers uses similar syntax to the tidyverse libraries such as &lt;code&gt;dplyr&lt;/code&gt;, by using the pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;%
  layer_dense(units = 28, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dense-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/h1&gt;
&lt;p&gt;The first layer is a densely connected neural network layer, which takes a set of nD input tensors (in this case 1D input tensors), and generate a weights matrix by breaking the tensor into subsets and using this to learn the weights by doing some linear algebra (vector and matrix multiplication). The output from the &lt;code&gt;dense&lt;/code&gt; layer is then generated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[output = activation(dot(input, kernel) + bias)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the weights kernel is generated and multiplied (dot product) with the input. If requested, a bias is also calculated and added to account for any systematic bias identified in the data. An &lt;code&gt;activation&lt;/code&gt; function is then used to generate the final tensor to go on to the following layer (in this case we have specified this is a separate layer).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activation-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/h1&gt;
&lt;p&gt;An activation function can often be necessary to ensure the back-propogation and gradient descent algorithms work. By default, no activation is used. However, this is a linear identity function, which is very limited. A common activation function is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectified Linear Unit&lt;/a&gt; (&lt;code&gt;ReLU&lt;/code&gt;), which is linear for positive values, but zero for negative values. This is usually a good starting point as it is very simple and fast. Another option is the &lt;code&gt;[softmax](https://en.wikipedia.org/wiki/Softmax_function)&lt;/code&gt; function, which transforms each input logit (the pre-activated values) by taking the exponential and normalizing by the sum of exponentials over all inputs so that the sum is exactly 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma(y_i) = \frac{e^{y_i}}{\sum^{K}_{j=1}e^{y_j}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is commonly used for multinomial logistic regression, where a different softmax function is applied for each class with a different probability incorporated, since it is able to transform input numbers into probabilities. The use of exponentials ensures that there are no negative values, no matter how negative the input logit. So the &lt;code&gt;softmax&lt;/code&gt; function outputs a probability distribution for potential outcomes in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dropout-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/h1&gt;
&lt;p&gt;Finally, we specify a dropout layer, which essentially drops a proportion of the nodes in the neural network to prevent over-fitting. In this case we have connections in the network between all of the tensor subsets generated. However, many of them are more useful in the model than others, so here we deactivate the 40% least useful nodes. Of course, this will reduce the training performance, but will prevent the issue of over-fitting making the model more generalised and applicable to other data sets. Model fitting is all about tweaking parameters and layers to get the most effective model, and this is one way in which we can improve the effectiveness of the model at predicting unseen data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;define-initial-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/h1&gt;
&lt;p&gt;So let’s generate a more thorough model of 4 dense layers, gradually filtering down to a final output of 10 probabilities using the &lt;code&gt;softmax&lt;/code&gt; activation function – the probabilities for the 10 digits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model &amp;lt;- keras_model_sequential()
MNIST_model %&amp;gt;%
  layer_dense(units = 256, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4) %&amp;gt;%
  layer_dense(units = 128) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units = 56) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.2) %&amp;gt;%
  layer_dense(units = 10) %&amp;gt;%
  layer_activation(activation = &amp;quot;softmax&amp;quot;)
summary(MNIST_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential_1&amp;quot;
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 256)                   200960      
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 256)                   0           
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 256)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 128)                   32896       
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 128)                   0           
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 128)                   0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 56)                    7224        
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 56)                    0           
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 56)                    0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 10)                    570         
## ___________________________________________________________________________
## activation_4 (Activation)        (None, 10)                    0           
## ===========================================================================
## Total params: 241,650
## Trainable params: 241,650
## Non-trainable params: 0
## ___________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see the change in shape of the tensors throughout the model, and the number of trainable parameters at each layer level. These are fully connected layers, so every neuron (values in the tensors) is connected to every other neuron. So the number of parameters (or connections) is given by multiplying the number of values in the input layer by the number in the previous layer plus one. So in total we have nearly a quarter of a million parameters to estimate here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compile-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/h1&gt;
&lt;p&gt;So next we can compile this model to tell it which methods we want to use to estimate these parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model %&amp;gt;% 
  compile(
    loss = &amp;quot;categorical_crossentropy&amp;quot;,
    optimizer = optimizer_rmsprop(),
    metrics = c(&amp;quot;accuracy&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss function here is the method that will be used to optimise the parameters by comparing the predicted value with that of the actual value. Categorical crossentropy is commonly used in classification models when the output is a probability. It increases logarithmically as the predicted value diverges from the true value.&lt;/p&gt;
&lt;p&gt;The optimizer is used to ensure that the algorithm converges in training. We are trying to minimise the loss function, so &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt; can be used to optimse by iteratively recalculating the weights and bias until the minima is found. There is a danger of getting stuck at a local minima value, so sometimes it may be necessary to tune the parameters to avoid this. In this case, we are using RMSProp optimizer, which is similar to Gradient Descent but attempts to avoid this by adding oscillations to the descent.&lt;/p&gt;
&lt;p&gt;Finally, we can specify which metrics we wish to output inorder to evaluate the model during training. Here we look at the accuracy to determine how often our model gives the correct prediction in the trained data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/h1&gt;
&lt;p&gt;So now let’s train our model with our training data to estimate the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_output &amp;lt;- MNIST_model %&amp;gt;% 
  fit(
    training_dat_x, training_dat_y,
    batch_size = 128,
    epochs = 30,
    verbose = 1,
    validation_split = 0.2
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are going to run the model using our reformated training data above. The &lt;code&gt;epoch&lt;/code&gt; argument determines the number of iterations used to optimize the model parameters. In each epoch, we will use &lt;code&gt;batch_size&lt;/code&gt; samples per epoch for the gradient update.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;validation_split&lt;/code&gt; argument is used for running cross-validation in order to evaluate the quality of the model. A portion of the training data is kept aside, and is used to validate the current model parameters and calculate the accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/h1&gt;
&lt;p&gt;Let’s take a look at how the accuracy and the loss (caluclated as categorical cross-entropy) change as the model training progresses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(training_output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_training_plot-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the loss is high and the accuracy low at the start of the training, but they quickly improve within the first 10 epochs. After this, they begin to plateau, resulting in a loss of 0.06 and accuracy of 98.43%.&lt;/p&gt;
&lt;p&gt;This is pretty good, so let’s see how it works with the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_output &amp;lt;- MNIST_model %&amp;gt;% evaluate(test_dat_x, test_dat_y)
test_output&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $loss
## [1] 0.1039611
## 
## $acc
## [1] 0.9786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So 97.86% of the 10,000 test cases were predicted accurately. So this means that 214 were wrong. Let’s take a look at some of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted &amp;lt;- MNIST_model %&amp;gt;% predict_classes(test_dat_x)
which_wrong &amp;lt;- which(predicted != test_dat_y_raw)
par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in which_wrong[1:18]) {
  plot_dat &amp;lt;- test_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = paste(&amp;quot;Predict =&amp;quot;, predicted[i], &amp;quot;\nReal =&amp;quot;, test_dat_y_raw[i]),
        cex  = 2, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_wrong-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we can see why the algorithm struggled with some of these, such as predicting 6s as 0s, and numbers that are slanted or squished. However, this obviously shows a lack of generalisation in the model, which is not brilliant for dealing with hand written numbers.&lt;/p&gt;
&lt;p&gt;Obviously this is a fairly basic example of a neural network model, and the sorts of models being used in technology like self-driving cars contain far more layers than this. Model tuning is essential to compare and contrast models to identify the optimum model.&lt;/p&gt;
&lt;!-- # Model Comparison --&gt;
&lt;!-- Models can be compared in various different ways, but one example is to use the TensorBoard, a visualisation tool from TensorFlow that shows dynamic graphs of the Keras training and test metrics. To compare multiple models, we can record the data, and then visualise it on TensorBoard.  --&gt;
&lt;!-- So let&#39;s try to compare a 1-layer model with a 4-layer model. We use the ```callback_tensorboard()``` function to save the data to add to TensorBoard.  --&gt;
&lt;!-- First let&#39;s run a 1-layer model: --&gt;
&lt;!-- ``` {r model_comparison_1layer} --&gt;
&lt;!-- model1 &lt;- keras_model_sequential() --&gt;
&lt;!-- model1 %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model1&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So we can see that this is much worse than our first model, with only an 82.5% accuracy. Let&#39;s now try a 10-layer model: --&gt;
&lt;!-- ``` {r model_comparison_10layer} --&gt;
&lt;!-- model10 &lt;- keras_model_sequential() --&gt;
&lt;!-- model10 %&gt;% --&gt;
&lt;!--   layer_dense(units = 500, input_shape = c(784)) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 450) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 400) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 350) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 300) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 250) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 200) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 150) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 100) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.1) %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model10&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So this model is better than the 1-layer model, but I have essentially added layers with fairly random parameters. The result is a model that is more complicated, but less accurate than the initial model that we generated. But we can compare these directly using Tensorboard: --&gt;
&lt;!-- ``` {r tensorboard} --&gt;
&lt;!-- tensorboard(c(&#34;model1&#34;, &#34;model10&#34;)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This allows us to explore differences betwen multiple models, and can be used to interactively identify the optimal model for our needs. --&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/h1&gt;
&lt;p&gt;According to Wikipedia, one of the best results for the MNIST database used a hierarchical system of convolutional neural networks and managed to get an error rate of 0.23%. Here I have an error rate of 2.14%, so I clearly have a way to go! Often in classification algorithms, using standard machine learning algorithms will get you pretty far with pretty good error rates. However, to tune the models further to get error rates down to these sorts of levels, more complex models are required. Neural networks can be used to push the error rates down further. Getting the right answer 96% of the time is pretty good, but if you’re relying on that classification to tell whether there is a pedestrian stood in front of a self-driving car, it is incredibly important to ensure that this error rate is as close to 0 as possible.&lt;/p&gt;
&lt;p&gt;However, this has been a very useful attempt at incorparting the powerful interface of Keras and the workflow of TensorFlow in R. Being able to incorporate powerful deep learning networks in R is incredobly useful, and will allow for incorporation with pre-existing pipelines already developed for bioinformatics analyses utilising the powerful pacakges available from &lt;a href=&#34;https://bioconductor.org&#34;&gt;Bioconductor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deep learning algorithms currently have a huge number of applications, from self-driving cars to facial recognition, and are being incorporated into technology in many industries. Development of deep learning algorithms and Big Data processing approaches will provide significant technological advancements. I am currently working on some potentially interesting applications, and hope to further my expertise in this area by working more with the R Keras API interface.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] keras_2.2.5.0   reticulate_1.13
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.5  xfun_0.8          remotes_2.1.0    
##  [4] reshape2_1.4.3    purrr_0.3.3       lattice_0.20-38  
##  [7] colorspace_1.4-1  generics_0.0.2    testthat_2.1.1   
## [10] htmltools_0.3.6   usethis_1.5.1     yaml_2.2.0       
## [13] base64enc_0.1-3   rlang_0.4.0       pkgbuild_1.0.3   
## [16] pillar_1.4.2      glue_1.3.1        withr_2.1.2      
## [19] sessioninfo_1.1.1 plyr_1.8.4        tensorflow_2.0.0 
## [22] stringr_1.4.0     munsell_0.5.0     blogdown_0.16    
## [25] gtable_0.3.0      devtools_2.1.0    codetools_0.2-16 
## [28] memoise_1.1.0     evaluate_0.14     labeling_0.3     
## [31] knitr_1.23        callr_3.3.0       ps_1.3.0         
## [34] tfruns_1.4        curl_3.3          Rcpp_1.0.2       
## [37] backports_1.1.4   scales_1.0.0      desc_1.2.0       
## [40] pkgload_1.0.2     jsonlite_1.6      fs_1.3.1         
## [43] ggplot2_3.2.0     digest_0.6.20     stringi_1.4.3    
## [46] dplyr_0.8.3       bookdown_0.12     processx_3.4.1   
## [49] grid_3.6.1        rprojroot_1.3-2   cli_1.1.0        
## [52] tools_3.6.1       magrittr_1.5      tibble_2.1.3     
## [55] lazyeval_0.2.2    pkgconfig_2.0.2   crayon_1.3.4     
## [58] whisker_0.4       zeallot_0.1.0     Matrix_1.2-17    
## [61] prettyunits_1.0.2 assertthat_0.2.1  rmarkdown_1.14   
## [64] R6_2.4.0          compiler_3.6.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
