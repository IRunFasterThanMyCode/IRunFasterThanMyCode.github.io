<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | IRunFasterThanMyCode</title>
    <link>/tags/r/</link>
      <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017 Sam Robson</copyright><lastBuildDate>Sat, 05 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/irunfasterthanmycode.jpg</url>
      <title>R</title>
      <link>/tags/r/</link>
    </image>
    
    <item>
      <title>Deep Learning using TensorFlow Through the Keras API in RStudio</title>
      <link>/post/2019-10-05_deep_learning_with_keras_in_rstudio/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-10-05_deep_learning_with_keras_in_rstudio/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tensorflow&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#keras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mnist-database&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-transformation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequential-models&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dense-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#activation-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dropout-layer&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#define-initial-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compile-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-the-model&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;Machine learning and artificial intelligence is a hot topic in the tech world, but the expression “machine learning” can describe anything from fitting a straight line through some data, to a machine able to think, learn and react to the world in highly sophisticated ways (e.g. self-driving cars if you want to be positive about AI, or SkyNet from Terminator if you want to be a naysayer). Whilst common machine learning techniques like support vector machines and k-Nearest Neighbour algorithms can be used to solve a huge number of problems, deep learning algorithms like neural networks are required to create these highly sophisticated models.&lt;/p&gt;
&lt;p&gt;In this blog, I will explore the use of some commonly used tools for generating neural networks within the R programming language.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tensorflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; TensorFlow&lt;/h1&gt;
&lt;p&gt;TensorFlow is one of the most powerful tools for deep learning, and in particular is widely used for training neural networks to classify various aspects of images. It is a freely distributed open-source library in python (but mainly written in C++) originally created by Google, but has become the cornerstone of many deep learning models currently out there&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Tensor&lt;/em&gt; is a multi-dimensional array, and the TensorFlow libraries represent a highly efficient pipeline for the myriad linear algebra calculations required to generate new tensors through the layers of the network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;keras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Keras&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://keras.io&#34;&gt;Keras API&lt;/a&gt; is a high-level user-friendly neural network API (application programming interface) designed for accessing deep neural networks. One of the benefits is that it is able to run on GPUs as well as CPUs, which have been shown to work better for training neural networks since they are able more efficient at running the huge number of simple calculations required for model training (for example convolutions of image data).&lt;/p&gt;
&lt;p&gt;Keras can be used an interface to TensorFlow for training deep multi-level networks for use in deep learning applications. Both are developed in python, but here I am going to use the RStudio interface to run a few simple deep learning models to trial the process ahead of a more in-depth application. R and python are somewhat at war in the data science community, with (in my opinion) R being better for more general data analysis and visualisation (for instance, whilst the python &lt;code&gt;seaborn&lt;/code&gt; package produces beautiful images, the &lt;code&gt;ggplot2&lt;/code&gt; package is far more elaborate). However, with the Keras and TensorFlow packages (and the generally higher memory impact of using R), python is typically far more suited for deep learning applications.&lt;/p&gt;
&lt;p&gt;However, the ability to access the Keras API through RStudio, and the amazing power of using RStudio to develop workflows, will make this a perfect “one stop shop” for data science needs. Much of this work is developed from the &lt;a href=&#34;https://tensorflow.rstudio.com&#34;&gt;RStudio Keras and TensorFlow tutorials&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We first load the &lt;code&gt;reticulate&lt;/code&gt; package to pipe python commands through R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;reticulate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then install and load the &lt;code&gt;keras&lt;/code&gt; package. When we load it using the &lt;code&gt;install_keras()&lt;/code&gt; function, we can define different backend engines and choose to use GPUs rather than CPUs, but for this example I will simply use the default TensorFlow backend on my laptop CPU:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;rstudio/keras&amp;quot;)
library(&amp;quot;keras&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mnist-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; MNIST Database&lt;/h1&gt;
&lt;p&gt;So let’s have a little play by looking at a standard machine learning approach, looking at the &lt;a href=&#34;https://en.wikipedia.org/wiki/MNIST_database&#34;&gt;MNIST&lt;/a&gt; dataset. This is the Modified National Institute of Standards and Technology database, and contains a large amount of images of handwritten digits that is used to train models for handwriting recognition. Ultimately, the same models can be used for a huge number of classification tasks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mnist_dat &amp;lt;- dataset_mnist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset contains a training set of 60,000 images, and a test set of 10,000 images. Each image is pre-normalised such that each digit is a grayscale image that fits into a 28x28 pixel bounding box.Each image is also supplied with a label that tells us what the digit should really be. This dataset is commonly used as a kind of benchmark for new models, with people vying to build the model with the lowest error rate possible:&lt;/p&gt;
&lt;p&gt;So let’s define our data sets. We will require two main data sets; a &lt;strong&gt;training&lt;/strong&gt; set where we show the model the images and tell it what it should recognise, and a &lt;strong&gt;test&lt;/strong&gt; dataset where we can predict the result and check against the ground level label. For each data set, we will create a dataset &lt;code&gt;x&lt;/code&gt; containing all of the images, and a dataset &lt;code&gt;y&lt;/code&gt; containing the labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;x&amp;quot;]]
training_dat_y_raw &amp;lt;- mnist_dat[[&amp;quot;train&amp;quot;]][[&amp;quot;y&amp;quot;]]
test_dat_x_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;x&amp;quot;]]
test_dat_y_raw     &amp;lt;- mnist_dat[[&amp;quot;test&amp;quot;]][[&amp;quot;y&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the images is essentially a 2D array, with 28 rows and 28 columns, with each cell representing the greyscale value of the pixel. So the &lt;code&gt;_dat_x&lt;/code&gt; data sets are 3D arrays. So accessing specific elements from these arrays in R is similar to accessing rows and columns using the &lt;code&gt;[x,y]&lt;/code&gt; style axis, but we need to specify a third element &lt;code&gt;z&lt;/code&gt; for the specific array that we want to access – &lt;code&gt;[z,x,y]&lt;/code&gt;. So lets take a look at an exampl of the input data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in 1:18) { 
  plot_dat &amp;lt;- training_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = training_dat_y_raw[i],
        cex  = 4, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_plot_entries-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Data Transformation&lt;/h1&gt;
&lt;p&gt;We can easily reduce this 3D data by essentially taking each 28x28 matrix and collapsing the 784 values down into a 1D vector. Then we can make one big 2D matrix containing all of the data. Ordinarily, this could be done by reassigning the dimensions of the array, but by using the &lt;code&gt;array_reshape()&lt;/code&gt; function, the data is adjusted to meet the requirements for Keras:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- array_reshape(training_dat_x_raw, c(nrow(training_dat_x_raw), 784))
test_dat_x     &amp;lt;- array_reshape(test_dat_x_raw,     c(nrow(test_dat_x_raw), 784))
dim(training_dat_x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000   784&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The values in these arrays are greyscale values, representing 256 integer values between 0 (black) and 255 (white). It will be useful for downstream analyses to rescale these values to real values in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_x &amp;lt;- training_dat_x/255
test_dat_x     &amp;lt;- test_dat_x/255&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The R-specific way to deal with categorical data would be to encode the values in the &lt;code&gt;y&lt;/code&gt; datasets to a factor with 10 levels (“0”, “1”, “2”, etc). However, Keras requires the data to be in a slightly different format, so we use the &lt;code&gt;to_categorical()&lt;/code&gt; function instead. This will encode the value in a new matrix with 10 columns and &lt;code&gt;n&lt;/code&gt; rows, such that every row contains exactly one &lt;code&gt;1&lt;/code&gt; (representing the label) and nine &lt;code&gt;0s&lt;/code&gt;. This is known as an identity matrix. Keras uses a lot of linear algebra, and this encoding makes these calculations much simpler:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_dat_y &amp;lt;- to_categorical(training_dat_y_raw, 10)
test_dat_y     &amp;lt;- to_categorical(test_dat_y_raw, 10)
dim(training_dat_y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 60000    10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequential-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Sequential Models&lt;/h1&gt;
&lt;p&gt;A standard deep learning neural network model can be thought of as a number of sequential layers, with each layer representing a different abstraction of the data. For instance, consider a model looking at facial recognition from image data. The first layer might represent edges of different aspects of the image. The next layer might be designed to pick out nose shape. The next might pick out hair. The next might determine the orientation of the face. etc. Then by adding more and more layers, we can develop models able to classify samples based on a wide range of different features.&lt;/p&gt;
&lt;p&gt;Of course, there is a danger in statistics of &lt;em&gt;over-fitting&lt;/em&gt; data, which is when we create a model so specific for the training data that it becomes practically worthless. By definition, adding more variables into a model will always improve the fit, but at the cost of its applicability to other data sets. In models such as linear models, we look for parsimony – a model should be as complicated as it needs to be &lt;em&gt;and no more complicated&lt;/em&gt;. The old phrase is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you hear hooves, think horse not zebra&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, deep learning sequential models such as these are robust to these problems, since model training can back-propagate, allowing us to incorporate far more levels than would be possible with other machine learning techniques.&lt;/p&gt;
&lt;p&gt;The general steps involved in using Keras for deep learning are to first build your model, compile it to configure the parameters that will be used to develop the “best” model, train it using your training data, then test it on an additional data set to see how it copes.&lt;/p&gt;
&lt;p&gt;So let’s build a simple sequential neural network model object using &lt;code&gt;keras_model_sequential()&lt;/code&gt;, then add a series of additional layers that we hope will accurately identify our different categories. Adding sequential layers uses similar syntax to the tidyverse libraries such as &lt;code&gt;dplyr&lt;/code&gt;, by using the pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- keras_model_sequential()
model %&amp;gt;%
  layer_dense(units = 28, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dense-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Dense Layer&lt;/h1&gt;
&lt;p&gt;The first layer is a densely connected neural network layer, which takes a set of nD input tensors (in this case 1D input tensors), and generate a weights matrix by breaking the tensor into subsets and using this to learn the weights by doing some linear algebra (vector and matrix multiplication). The output from the &lt;code&gt;dense&lt;/code&gt; layer is then generated as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[output = activation(dot(input, kernel) + bias)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the weights kernel is generated and multiplied (dot product) with the input. If requested, a bias is also calculated and added to account for any systematic bias identified in the data. An &lt;code&gt;activation&lt;/code&gt; function is then used to generate the final tensor to go on to the following layer (in this case we have specified this is a separate layer).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;activation-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Activation Layer&lt;/h1&gt;
&lt;p&gt;An activation function can often be necessary to ensure the back-propogation and gradient descent algorithms work. By default, no activation is used. However, this is a linear identity function, which is very limited. A common activation function is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectified Linear Unit&lt;/a&gt; (&lt;code&gt;ReLU&lt;/code&gt;), which is linear for positive values, but zero for negative values. This is usually a good starting point as it is very simple and fast. Another option is the &lt;code&gt;[softmax](https://en.wikipedia.org/wiki/Softmax_function)&lt;/code&gt; function, which transforms each input logit (the pre-activated values) by taking the exponential and normalizing by the sum of exponentials over all inputs so that the sum is exactly 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma(y_i) = \frac{e^{y_i}}{\sum^{K}_{j=1}e^{y_j}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is commonly used for multinomial logistic regression, where a different softmax function is applied for each class with a different probability incorporated, since it is able to transform input numbers into probabilities. The use of exponentials ensures that there are no negative values, no matter how negative the input logit. So the &lt;code&gt;softmax&lt;/code&gt; function outputs a probability distribution for potential outcomes in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dropout-layer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Dropout Layer&lt;/h1&gt;
&lt;p&gt;Finally, we specify a dropout layer, which essentially drops a proportion of the nodes in the neural network to prevent over-fitting. In this case we have connections in the network between all of the tensor subsets generated. However, many of them are more useful in the model than others, so here we deactivate the 40% least useful nodes. Of course, this will reduce the training performance, but will prevent the issue of over-fitting making the model more generalised and applicable to other data sets. Model fitting is all about tweaking parameters and layers to get the most effective model, and this is one way in which we can improve the effectiveness of the model at predicting unseen data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;define-initial-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Define Initial Model&lt;/h1&gt;
&lt;p&gt;So let’s generate a more thorough model of 4 dense layers, gradually filtering down to a final output of 10 probabilities using the &lt;code&gt;softmax&lt;/code&gt; activation function – the probabilities for the 10 digits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model &amp;lt;- keras_model_sequential()
MNIST_model %&amp;gt;%
  layer_dense(units = 256, input_shape = c(784)) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.4) %&amp;gt;%
  layer_dense(units = 128) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.3) %&amp;gt;%
  layer_dense(units = 56) %&amp;gt;%
  layer_activation(activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  layer_dropout(rate = 0.2) %&amp;gt;%
  layer_dense(units = 10) %&amp;gt;%
  layer_activation(activation = &amp;quot;softmax&amp;quot;)
summary(MNIST_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential_1&amp;quot;
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 256)                   200960      
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 256)                   0           
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 256)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 128)                   32896       
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 128)                   0           
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 128)                   0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 56)                    7224        
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 56)                    0           
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 56)                    0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 10)                    570         
## ___________________________________________________________________________
## activation_4 (Activation)        (None, 10)                    0           
## ===========================================================================
## Total params: 241,650
## Trainable params: 241,650
## Non-trainable params: 0
## ___________________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can see the change in shape of the tensors throughout the model, and the number of trainable parameters at each layer level. These are fully connected layers, so every neuron (values in the tensors) is connected to every other neuron. So the number of parameters (or connections) is given by multiplying the number of values in the input layer by the number in the previous layer plus one. So in total we have nearly a quarter of a million parameters to estimate here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compile-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Compile Model&lt;/h1&gt;
&lt;p&gt;So next we can compile this model to tell it which methods we want to use to estimate these parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MNIST_model %&amp;gt;% 
  compile(
    loss = &amp;quot;categorical_crossentropy&amp;quot;,
    optimizer = optimizer_rmsprop(),
    metrics = c(&amp;quot;accuracy&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The loss function here is the method that will be used to optimise the parameters by comparing the predicted value with that of the actual value. Categorical crossentropy is commonly used in classification models when the output is a probability. It increases logarithmically as the predicted value diverges from the true value.&lt;/p&gt;
&lt;p&gt;The optimizer is used to ensure that the algorithm converges in training. We are trying to minimise the loss function, so &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;Gradient Descent&lt;/a&gt; can be used to optimse by iteratively recalculating the weights and bias until the minima is found. There is a danger of getting stuck at a local minima value, so sometimes it may be necessary to tune the parameters to avoid this. In this case, we are using RMSProp optimizer, which is similar to Gradient Descent but attempts to avoid this by adding oscillations to the descent.&lt;/p&gt;
&lt;p&gt;Finally, we can specify which metrics we wish to output inorder to evaluate the model during training. Here we look at the accuracy to determine how often our model gives the correct prediction in the trained data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Training the Model&lt;/h1&gt;
&lt;p&gt;So now let’s train our model with our training data to estimate the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_output &amp;lt;- MNIST_model %&amp;gt;% 
  fit(
    training_dat_x, training_dat_y,
    batch_size = 128,
    epochs = 30,
    verbose = 1,
    validation_split = 0.2
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are going to run the model using our reformated training data above. The &lt;code&gt;epoch&lt;/code&gt; argument determines the number of iterations used to optimize the model parameters. In each epoch, we will use &lt;code&gt;batch_size&lt;/code&gt; samples per epoch for the gradient update.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;validation_split&lt;/code&gt; argument is used for running cross-validation in order to evaluate the quality of the model. A portion of the training data is kept aside, and is used to validate the current model parameters and calculate the accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;13&lt;/span&gt; Results&lt;/h1&gt;
&lt;p&gt;Let’s take a look at how the accuracy and the loss (caluclated as categorical cross-entropy) change as the model training progresses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(training_output)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_training_plot-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the loss is high and the accuracy low at the start of the training, but they quickly improve within the first 10 epochs. After this, they begin to plateau, resulting in a loss of 0.06 and accuracy of 98.43%.&lt;/p&gt;
&lt;p&gt;This is pretty good, so let’s see how it works with the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_output &amp;lt;- MNIST_model %&amp;gt;% evaluate(test_dat_x, test_dat_y)
test_output&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $loss
## [1] 0.1039611
## 
## $acc
## [1] 0.9786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So 97.86% of the 10,000 test cases were predicted accurately. So this means that 214 were wrong. Let’s take a look at some of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted &amp;lt;- MNIST_model %&amp;gt;% predict_classes(test_dat_x)
which_wrong &amp;lt;- which(predicted != test_dat_y_raw)
par(mfcol = c(3,6))
par(mar = c(0, 0, 3, 0), xaxs = &amp;#39;i&amp;#39;, yaxs = &amp;#39;i&amp;#39;)
for (i in which_wrong[1:18]) {
  plot_dat &amp;lt;- test_dat_x_raw[i, , ]
  plot_dat &amp;lt;- t(apply(plot_dat, MAR = 2, rev)) 
  image(1:28, 1:28, plot_dat, 
        col  = gray((0:255)/255), 
        xaxt =&amp;#39;n&amp;#39;, 
        main = paste(&amp;quot;Predict =&amp;quot;, predicted[i], &amp;quot;\nReal =&amp;quot;, test_dat_y_raw[i]),
        cex  = 2, axes = FALSE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-10-05_Deep_Learning_with_Keras_in_RStudio/index_files/figure-html/MNIST_wrong-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we can see why the algorithm struggled with some of these, such as predicting 6s as 0s, and numbers that are slanted or squished. However, this obviously shows a lack of generalisation in the model, which is not brilliant for dealing with hand written numbers.&lt;/p&gt;
&lt;p&gt;Obviously this is a fairly basic example of a neural network model, and the sorts of models being used in technology like self-driving cars contain far more layers than this. Model tuning is essential to compare and contrast models to identify the optimum model.&lt;/p&gt;
&lt;!-- # Model Comparison --&gt;
&lt;!-- Models can be compared in various different ways, but one example is to use the TensorBoard, a visualisation tool from TensorFlow that shows dynamic graphs of the Keras training and test metrics. To compare multiple models, we can record the data, and then visualise it on TensorBoard.  --&gt;
&lt;!-- So let&#39;s try to compare a 1-layer model with a 4-layer model. We use the ```callback_tensorboard()``` function to save the data to add to TensorBoard.  --&gt;
&lt;!-- First let&#39;s run a 1-layer model: --&gt;
&lt;!-- ``` {r model_comparison_1layer} --&gt;
&lt;!-- model1 &lt;- keras_model_sequential() --&gt;
&lt;!-- model1 %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model1&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So we can see that this is much worse than our first model, with only an 82.5% accuracy. Let&#39;s now try a 10-layer model: --&gt;
&lt;!-- ``` {r model_comparison_10layer} --&gt;
&lt;!-- model10 &lt;- keras_model_sequential() --&gt;
&lt;!-- model10 %&gt;% --&gt;
&lt;!--   layer_dense(units = 500, input_shape = c(784)) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 450) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.5) %&gt;% --&gt;
&lt;!--   layer_dense(units = 400) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 350) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.4) %&gt;% --&gt;
&lt;!--   layer_dense(units = 300) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 250) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.3) %&gt;% --&gt;
&lt;!--   layer_dense(units = 200) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 150) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.2) %&gt;% --&gt;
&lt;!--   layer_dense(units = 100) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;relu&#34;) %&gt;% --&gt;
&lt;!--   layer_dropout(rate = 0.1) %&gt;% --&gt;
&lt;!--   layer_dense(units = 10) %&gt;% --&gt;
&lt;!--   layer_activation(activation = &#34;softmax&#34;) %&gt;% --&gt;
&lt;!--   compile( --&gt;
&lt;!--     loss = &#34;categorical_crossentropy&#34;, --&gt;
&lt;!--     optimizer = optimizer_rmsprop(), --&gt;
&lt;!--     metrics = c(&#34;accuracy&#34;) --&gt;
&lt;!--   ) %&gt;% --&gt;
&lt;!--   fit( --&gt;
&lt;!--     training_dat_x, training_dat_y, --&gt;
&lt;!--     batch_size = 128, --&gt;
&lt;!--     epochs = 30, --&gt;
&lt;!--     verbose = 1, --&gt;
&lt;!--     validation_split = 0.2, --&gt;
&lt;!--     callbacks = callback_tensorboard(&#34;model10&#34;) --&gt;
&lt;!--   ) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- So this model is better than the 1-layer model, but I have essentially added layers with fairly random parameters. The result is a model that is more complicated, but less accurate than the initial model that we generated. But we can compare these directly using Tensorboard: --&gt;
&lt;!-- ``` {r tensorboard} --&gt;
&lt;!-- tensorboard(c(&#34;model1&#34;, &#34;model10&#34;)) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- This allows us to explore differences betwen multiple models, and can be used to interactively identify the optimal model for our needs. --&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;14&lt;/span&gt; Conclusions&lt;/h1&gt;
&lt;p&gt;According to Wikipedia, one of the best results for the MNIST database used a hierarchical system of convolutional neural networks and managed to get an error rate of 0.23%. Here I have an error rate of 2.14%, so I clearly have a way to go! Often in classification algorithms, using standard machine learning algorithms will get you pretty far with pretty good error rates. However, to tune the models further to get error rates down to these sorts of levels, more complex models are required. Neural networks can be used to push the error rates down further. Getting the right answer 96% of the time is pretty good, but if you’re relying on that classification to tell whether there is a pedestrian stood in front of a self-driving car, it is incredibly important to ensure that this error rate is as close to 0 as possible.&lt;/p&gt;
&lt;p&gt;However, this has been a very useful attempt at incorparting the powerful interface of Keras and the workflow of TensorFlow in R. Being able to incorporate powerful deep learning networks in R is incredobly useful, and will allow for incorporation with pre-existing pipelines already developed for bioinformatics analyses utilising the powerful pacakges available from &lt;a href=&#34;https://bioconductor.org&#34;&gt;Bioconductor&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deep learning algorithms currently have a huge number of applications, from self-driving cars to facial recognition, and are being incorporated into technology in many industries. Development of deep learning algorithms and Big Data processing approaches will provide significant technological advancements. I am currently working on some potentially interesting applications, and hope to further my expertise in this area by working more with the R Keras API interface.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;15&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] keras_2.2.5.0   reticulate_1.13
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.5  xfun_0.8          remotes_2.1.0    
##  [4] reshape2_1.4.3    purrr_0.3.3       lattice_0.20-38  
##  [7] colorspace_1.4-1  generics_0.0.2    testthat_2.1.1   
## [10] htmltools_0.3.6   usethis_1.5.1     yaml_2.2.0       
## [13] base64enc_0.1-3   rlang_0.4.0       pkgbuild_1.0.3   
## [16] pillar_1.4.2      glue_1.3.1        withr_2.1.2      
## [19] sessioninfo_1.1.1 plyr_1.8.4        tensorflow_2.0.0 
## [22] stringr_1.4.0     munsell_0.5.0     blogdown_0.16    
## [25] gtable_0.3.0      devtools_2.1.0    codetools_0.2-16 
## [28] memoise_1.1.0     evaluate_0.14     labeling_0.3     
## [31] knitr_1.23        callr_3.3.0       ps_1.3.0         
## [34] tfruns_1.4        curl_3.3          Rcpp_1.0.2       
## [37] backports_1.1.4   scales_1.0.0      desc_1.2.0       
## [40] pkgload_1.0.2     jsonlite_1.6      fs_1.3.1         
## [43] ggplot2_3.2.0     digest_0.6.20     stringi_1.4.3    
## [46] dplyr_0.8.3       bookdown_0.12     processx_3.4.1   
## [49] grid_3.6.1        rprojroot_1.3-2   cli_1.1.0        
## [52] tools_3.6.1       magrittr_1.5      tibble_2.1.3     
## [55] lazyeval_0.2.2    pkgconfig_2.0.2   crayon_1.3.4     
## [58] whisker_0.4       zeallot_0.1.0     Matrix_1.2-17    
## [61] prettyunits_1.0.2 assertthat_0.2.1  rmarkdown_1.14   
## [64] R6_2.4.0          compiler_3.6.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Pokémon Recomendation Machine</title>
      <link>/post/2019-07-15_gotta_catch_em_all_pokemon/</link>
      <pubDate>Mon, 15 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/2019-07-15_gotta_catch_em_all_pokemon/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#download-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Download data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data Cleaning&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Missing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numeric-range&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Numeric Range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#change-variable-class&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Change Variable Class&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploratory-analyses&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Exploratory Analyses&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#attack&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Attack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defence&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Defence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Other&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ensuring-we-use-accurate-data-classes-throughout&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.4&lt;/span&gt; Ensuring we use accurate data classes throughout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Normalization&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#min-max-normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.1&lt;/span&gt; Min-Max Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#z-score-normalization&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.2&lt;/span&gt; Z-Score Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-normalization-method&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5.3&lt;/span&gt; Choosing a Normalization Method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#looking-for-patterns&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Looking for Patterns&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-of-variables&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Correlation of Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#height-vs-weight&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; Height vs Weight&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-between-pokemon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Correlation between Pokémon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.4&lt;/span&gt; Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-based-recommendation&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.5&lt;/span&gt; Correlation-Based Recommendation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-legendary-pokemon&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Predicting Legendary Pokémon&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; Support-Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbour&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; k-Nearest Neighbour&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;When I saw the &lt;a href=&#34;https://www.kaggle.com/rounakbanik/pokemon&#34;&gt;Complete Pokémon Dataset on Kaggle&lt;/a&gt;, I just had to download it and have a look! When I was younger, I was a big fan of Pokémon and used to play it regularly and watch the TV show (to this day I can recite much of the original &lt;a href=&#34;https://www.youtube.com/watch?v=xMk8wuw7nek&#34;&gt;Pokémon Rap&lt;/a&gt;). More recently, my daughter has become a fan, and watches the show incessently (although it beats watching Peppa Pig…). So I am going to have a look over these data and see what they can show us about these pocket monsters.&lt;/p&gt;
&lt;p&gt;This is a fairly comprehensive analysis of the data, and will include introductions to a number of different data science techniques. I may further develop these into posts of their own in the future, so will only skim over most of them here. I hope that this post shows a fairly complete example of the types of analyses that it is possible to do with data such as these for prediction and recommendation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;download-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Download data&lt;/h1&gt;
&lt;p&gt;The data about each of the Pokémon can be downloaded directly from Kaggle &lt;a href=&#34;https://www.kaggle.com/rounakbanik/pokemon/download&#34;&gt;here&lt;/a&gt;, and I have also downloaded some images for each of the Pokémon (at least for Generations 1 to 6) from Kaggle &lt;a href=&#34;https://www.kaggle.com/kvpratama/pokemon-images-dataset/download&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main data are in the form of a compressed comma separated values (CSV) file. After unzipping the file, we are left with a plain text file where every row is a separate entry, and the various columns of the data set are separated by commas. So let’s load the data in using the &lt;code&gt;read.csv&lt;/code&gt; function, which will generate a &lt;code&gt;data.frame&lt;/code&gt; object, and take a look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat &amp;lt;- read.csv(&amp;quot;pokemon.csv&amp;quot;)
rownames(pokedat) &amp;lt;- pokedat[[&amp;quot;name&amp;quot;]]
dim(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 801  41&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have information for 41 variables for 801 unique Pokémon. Each Pokémon has a unique number assigned to it in the so called “Pokédex”, which is common between this data set and the list of Pokémon images. This makes it easy to link the two. Back in my day, there were only 151 Pokémon to keep track of, but many years later they have added more and more with each “Generation”. Pokémon Generation 8 is the most recent and has only recently been released, so we can see which generations are present in this data set by using the &lt;code&gt;table&lt;/code&gt; function, which will show us the number of entries with each value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(pokedat[[&amp;quot;generation&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   5   6   7 
## 151 100 135 107 156  72  80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see the 151 Generation 1 Pokémon, but also additional Pokémon from up to Generation 7. We can get a fairly broad overview of the data set by using the &lt;code&gt;str&lt;/code&gt; function to gain an overview of what is contained within each of the columns of this &lt;code&gt;data.frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    801 obs. of  41 variables:
##  $ abilities        : Factor w/ 482 levels &amp;quot;[&amp;#39;Adaptability&amp;#39;, &amp;#39;Download&amp;#39;, &amp;#39;Analytic&amp;#39;]&amp;quot;,..: 244 244 244 22 22 22 453 453 453 348 ...
##  $ against_bug      : num  1 1 1 0.5 0.5 0.25 1 1 1 1 ...
##  $ against_dark     : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_dragon   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_electric : num  0.5 0.5 0.5 1 1 2 2 2 2 1 ...
##  $ against_fairy    : num  0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 ...
##  $ against_fight    : num  0.5 0.5 0.5 1 1 0.5 1 1 1 0.5 ...
##  $ against_fire     : num  2 2 2 0.5 0.5 0.5 0.5 0.5 0.5 2 ...
##  $ against_flying   : num  2 2 2 1 1 1 1 1 1 2 ...
##  $ against_ghost    : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_grass    : num  0.25 0.25 0.25 0.5 0.5 0.25 2 2 2 0.5 ...
##  $ against_ground   : num  1 1 1 2 2 0 1 1 1 0.5 ...
##  $ against_ice      : num  2 2 2 0.5 0.5 1 0.5 0.5 0.5 1 ...
##  $ against_normal   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_poison   : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ against_psychic  : num  2 2 2 1 1 1 1 1 1 1 ...
##  $ against_rock     : num  1 1 1 2 2 4 1 1 1 2 ...
##  $ against_steel    : num  1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 1 ...
##  $ against_water    : num  0.5 0.5 0.5 2 2 2 0.5 0.5 0.5 1 ...
##  $ attack           : int  49 62 100 52 64 104 48 63 103 30 ...
##  $ base_egg_steps   : int  5120 5120 5120 5120 5120 5120 5120 5120 5120 3840 ...
##  $ base_happiness   : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ base_total       : int  318 405 625 309 405 634 314 405 630 195 ...
##  $ capture_rate     : Factor w/ 34 levels &amp;quot;100&amp;quot;,&amp;quot;120&amp;quot;,&amp;quot;125&amp;quot;,..: 26 26 26 26 26 26 26 26 26 21 ...
##  $ classfication    : Factor w/ 588 levels &amp;quot;Abundance Pokémon&amp;quot;,..: 449 449 449 299 187 187 531 546 457 585 ...
##  $ defense          : int  49 63 123 43 58 78 65 80 120 35 ...
##  $ experience_growth: int  1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1000000 ...
##  $ height_m         : num  0.7 1 2 0.6 1.1 1.7 0.5 1 1.6 0.3 ...
##  $ hp               : int  45 60 80 39 58 78 44 59 79 45 ...
##  $ japanese_name    : Factor w/ 801 levels &amp;quot;Abagouraアバゴーラ&amp;quot;,..: 200 201 199 288 417 416 794 334 336 80 ...
##  $ name             : Factor w/ 801 levels &amp;quot;Abomasnow&amp;quot;,&amp;quot;Abra&amp;quot;,..: 73 321 745 95 96 93 656 764 56 88 ...
##  $ percentage_male  : num  88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 50 ...
##  $ pokedex_number   : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ sp_attack        : int  65 80 122 60 80 159 50 65 135 20 ...
##  $ sp_defense       : int  65 80 120 50 65 115 64 80 115 20 ...
##  $ speed            : int  45 60 80 65 80 100 43 58 78 45 ...
##  $ type1            : Factor w/ 18 levels &amp;quot;bug&amp;quot;,&amp;quot;dark&amp;quot;,&amp;quot;dragon&amp;quot;,..: 10 10 10 7 7 7 18 18 18 1 ...
##  $ type2            : Factor w/ 19 levels &amp;quot;&amp;quot;,&amp;quot;bug&amp;quot;,&amp;quot;dark&amp;quot;,..: 15 15 15 1 1 9 1 1 1 1 ...
##  $ weight_kg        : num  6.9 13 100 8.5 19 90.5 9 22.5 85.5 2.9 ...
##  $ generation       : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ is_legendary     : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we gave a lot of different information contained in this data set, with the vast majority being represented by numerical values. As well as the name by which we likely know them, we have the original Japanese name, as well as their fighting statistics such as &lt;code&gt;hp&lt;/code&gt; (Health Points), &lt;code&gt;speed&lt;/code&gt;, &lt;code&gt;attack&lt;/code&gt; and &lt;code&gt;defense&lt;/code&gt;. We also get their specific abilities, which are given in a listed format within square brackets (e.g. three abilities – &lt;code&gt;[&#39;Adaptability&#39;, &#39;Download&#39;, &#39;Analytic&#39;]&lt;/code&gt; – for &lt;code&gt;Abomasnow&lt;/code&gt;). There are also various other things that we will explore now in the following sections. So let’s explore these data to see how they look, and to check for any inconsistencies that need to be corrected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Cleaning&lt;/h1&gt;
&lt;p&gt;The first step in any data analysis is to check the consistency of the data to ensure that there are no missing values (and if there are, decide the best thing to do with them), to make sure that the data are consistent and fit within expected bounds, and to generally make sure that these data make sense. These are data that somebody else has generated, so it is best not to assume that they are perfect. If there are any issues, this will propogate to our downstream analyses.&lt;/p&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Missing Data&lt;/h2&gt;
&lt;p&gt;First of all, let’s take a look to see which of the entries for each of the columns is missing (i.e. is coded as &lt;code&gt;NA&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;has_na &amp;lt;- apply(pokedat, MAR = 2, FUN = function(x) which(is.na(x))) 
has_na[sapply(has_na, length) &amp;gt; 0]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $height_m
##   Rattata  Raticate    Raichu Sandshrew Sandslash    Vulpix Ninetales 
##        19        20        26        27        28        37        38 
##   Diglett   Dugtrio    Meowth   Persian   Geodude  Graveler     Golem 
##        50        51        52        53        74        75        76 
##    Grimer       Muk Exeggutor   Marowak     Hoopa  Lycanroc 
##        88        89       103       105       720       745 
## 
## $percentage_male
##  Magnemite   Magneton    Voltorb  Electrode     Staryu    Starmie 
##         81         82        100        101        120        121 
##      Ditto    Porygon   Articuno     Zapdos    Moltres     Mewtwo 
##        132        137        144        145        146        150 
##        Mew      Unown   Porygon2     Raikou      Entei    Suicune 
##        151        201        233        243        244        245 
##      Lugia      Ho-Oh     Celebi   Shedinja   Lunatone    Solrock 
##        249        250        251        292        337        338 
##     Baltoy    Claydol     Beldum     Metang  Metagross   Regirock 
##        343        344        374        375        376        377 
##     Regice  Registeel     Kyogre    Groudon   Rayquaza    Jirachi 
##        378        379        382        383        384        385 
##     Deoxys    Bronzor   Bronzong  Magnezone  Porygon-Z      Rotom 
##        386        436        437        462        474        479 
##       Uxie    Mesprit      Azelf     Dialga     Palkia  Regigigas 
##        480        481        482        483        484        486 
##   Giratina     Phione    Manaphy    Darkrai    Shaymin     Arceus 
##        487        489        490        491        492        493 
##    Victini      Klink      Klang  Klinklang  Cryogonal     Golett 
##        494        599        600        601        615        622 
##     Golurk   Cobalion  Terrakion   Virizion   Reshiram     Zekrom 
##        623        638        639        640        643        644 
##     Kyurem     Keldeo   Meloetta   Genesect    Carbink    Xerneas 
##        646        647        648        649        703        716 
##    Yveltal    Zygarde    Diancie      Hoopa  Volcanion Type: Null 
##        717        718        719        720        721        772 
##   Silvally     Minior   Dhelmise  Tapu Koko  Tapu Lele  Tapu Bulu 
##        773        774        781        785        786        787 
##  Tapu Fini     Cosmog    Cosmoem   Solgaleo     Lunala   Nihilego 
##        788        789        790        791        792        793 
##   Buzzwole  Pheromosa  Xurkitree Celesteela    Kartana   Guzzlord 
##        794        795        796        797        798        799 
##   Necrozma   Magearna 
##        800        801 
## 
## $weight_kg
##   Rattata  Raticate    Raichu Sandshrew Sandslash    Vulpix Ninetales 
##        19        20        26        27        28        37        38 
##   Diglett   Dugtrio    Meowth   Persian   Geodude  Graveler     Golem 
##        50        51        52        53        74        75        76 
##    Grimer       Muk Exeggutor   Marowak     Hoopa  Lycanroc 
##        88        89       103       105       720       745&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, this seems to be a fairly complete data set, with only three of the variables showing any NA data. We can see that there are 20 Pokémon with no height nor weight data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(pokedat, is.na(height_m) | is.na(weight_kg))[, c(&amp;quot;name&amp;quot;, &amp;quot;height_m&amp;quot;, &amp;quot;weight_kg&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                name height_m weight_kg
## Rattata     Rattata       NA        NA
## Raticate   Raticate       NA        NA
## Raichu       Raichu       NA        NA
## Sandshrew Sandshrew       NA        NA
## Sandslash Sandslash       NA        NA
## Vulpix       Vulpix       NA        NA
## Ninetales Ninetales       NA        NA
## Diglett     Diglett       NA        NA
## Dugtrio     Dugtrio       NA        NA
## Meowth       Meowth       NA        NA
## Persian     Persian       NA        NA
## Geodude     Geodude       NA        NA
## Graveler   Graveler       NA        NA
## Golem         Golem       NA        NA
## Grimer       Grimer       NA        NA
## Muk             Muk       NA        NA
## Exeggutor Exeggutor       NA        NA
## Marowak     Marowak       NA        NA
## Hoopa         Hoopa       NA        NA
## Lycanroc   Lycanroc       NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many of these are Pokémon that I know from Generation 1, and in fact seem to be sets of evolutions. For instance, Rattata evolves into Raticate:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/19.png&#34; alt=&#34;Rattata&#34; /&gt;
&lt;img src=&#34;pokemon/20.png&#34; alt=&#34;Raticate&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sandshrew evolves into Sandslash:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/27.png&#34; alt=&#34;Sandshrew&#34; /&gt;
&lt;img src=&#34;pokemon/28.png&#34; alt=&#34;Sandslash&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And Vulpix evolves into Ninetales:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pokemon/37.png&#34; alt=&#34;Vulpix&#34; /&gt;
&lt;img src=&#34;pokemon/38.png&#34; alt=&#34;Ninetales&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are also a couple of other none-Generation 1 Pokémon, including Lycanroc which is one of my daughter’s favourited from Pokémon Sun and Moon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/lycanroc.png&#34; alt=&#34;Lycanroc&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Lycanroc&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;However, there is no obvious reason why values are missing. There are methods that can be used to account for missing data. One possible approach is to &lt;em&gt;impute&lt;/em&gt; the data – that is, we use the rest of the data to give us a rough idea of what we should see for these missing values. An example of this is to simply use the mean of the non-missing values for the missing variable. However, in this case, we can actually find these missing values by visiting an online &lt;a href=&#34;https://pokemondb.net/pokedex&#34;&gt;Pokedex&lt;/a&gt;, so let’s correct these, ensuring that we match the units for weight (kg) and height (m):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;missing_height &amp;lt;- list(Rattata   = c(height_m = 0.3, weight_kg =   3.5), 
                       Raticate  = c(height_m = 0.7, weight_kg =  18.5),
                       Raichu    = c(height_m = 0.8, weight_kg =  30.0),
                       Sandshrew = c(height_m = 0.6, weight_kg =  12.0),
                       Sandslash = c(height_m = 1.0, weight_kg =  29.5),
                       Vulpix    = c(height_m = 0.6, weight_kg =   9.9),
                       Ninetales = c(height_m = 1.1, weight_kg =  19.9),
                       Diglett   = c(height_m = 0.2, weight_kg =   0.8),
                       Dugtrio   = c(height_m = 0.7, weight_kg =  33.3),
                       Meowth    = c(height_m = 0.4, weight_kg =   4.2),
                       Persian   = c(height_m = 1.0, weight_kg =  32.0),
                       Geodude   = c(height_m = 0.4, weight_kg =  20.0),
                       Graveler  = c(height_m = 0.3, weight_kg = 105.0),
                       Golem     = c(height_m = 1.4, weight_kg = 300.0),
                       Grimer    = c(height_m = 0.9, weight_kg =  30.0),
                       Muk       = c(height_m = 1.2, weight_kg =  30.0),
                       Exeggutor = c(height_m = 2.0, weight_kg = 120.0),
                       Marowak   = c(height_m = 1.0, weight_kg =  45.0),
                       Hoopa     = c(height_m = 0.5, weight_kg =   9.0),
                       Lycanroc  = c(height_m = 0.8, weight_kg =  25.0))
missing_height &amp;lt;- t(rbind.data.frame(missing_height))
pokedat[match(rownames(missing_height), pokedat[[&amp;quot;name&amp;quot;]]), c(&amp;quot;height_m&amp;quot;, &amp;quot;weight_kg&amp;quot;)] &amp;lt;- missing_height&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also 98 Pokémon with a missing &lt;code&gt;percentage_male&lt;/code&gt; value. This value gives the proportion of the Pokémon out in the world that you might come across in the game that are male as a percentage. These seem to be spread throughout the entire list of Pokémon across all Generations, with no clear reason as to why they have missing values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(subset(pokedat, is.na(percentage_male)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                            abilities against_bug
## Magnemite      [&amp;#39;Magnet Pull&amp;#39;, &amp;#39;Sturdy&amp;#39;, &amp;#39;Analytic&amp;#39;]         0.5
## Magneton       [&amp;#39;Magnet Pull&amp;#39;, &amp;#39;Sturdy&amp;#39;, &amp;#39;Analytic&amp;#39;]         0.5
## Voltorb        [&amp;#39;Soundproof&amp;#39;, &amp;#39;Static&amp;#39;, &amp;#39;Aftermath&amp;#39;]         1.0
## Electrode      [&amp;#39;Soundproof&amp;#39;, &amp;#39;Static&amp;#39;, &amp;#39;Aftermath&amp;#39;]         1.0
## Staryu    [&amp;#39;Illuminate&amp;#39;, &amp;#39;Natural Cure&amp;#39;, &amp;#39;Analytic&amp;#39;]         1.0
## Starmie   [&amp;#39;Illuminate&amp;#39;, &amp;#39;Natural Cure&amp;#39;, &amp;#39;Analytic&amp;#39;]         2.0
##           against_dark against_dragon against_electric against_fairy
## Magnemite            1            0.5              0.5           0.5
## Magneton             1            0.5              0.5           0.5
## Voltorb              1            1.0              0.5           1.0
## Electrode            1            1.0              0.5           1.0
## Staryu               1            1.0              2.0           1.0
## Starmie              2            1.0              2.0           1.0
##           against_fight against_fire against_flying against_ghost
## Magnemite           2.0          2.0           0.25             1
## Magneton            2.0          2.0           0.25             1
## Voltorb             1.0          1.0           0.50             1
## Electrode           1.0          1.0           0.50             1
## Staryu              1.0          0.5           1.00             1
## Starmie             0.5          0.5           1.00             2
##           against_grass against_ground against_ice against_normal
## Magnemite           0.5              4         0.5            0.5
## Magneton            0.5              4         0.5            0.5
## Voltorb             1.0              2         1.0            1.0
## Electrode           1.0              2         1.0            1.0
## Staryu              2.0              1         0.5            1.0
## Starmie             2.0              1         0.5            1.0
##           against_poison against_psychic against_rock against_steel
## Magnemite              0             0.5          0.5          0.25
## Magneton               0             0.5          0.5          0.25
## Voltorb                1             1.0          1.0          0.50
## Electrode              1             1.0          1.0          0.50
## Staryu                 1             1.0          1.0          0.50
## Starmie                1             0.5          1.0          0.50
##           against_water attack base_egg_steps base_happiness base_total
## Magnemite           1.0     35           5120             70        325
## Magneton            1.0     60           5120             70        465
## Voltorb             1.0     30           5120             70        330
## Electrode           1.0     50           5120             70        490
## Staryu              0.5     45           5120             70        340
## Starmie             0.5     75           5120             70        520
##           capture_rate      classfication defense experience_growth
## Magnemite          190     Magnet Pokémon      70           1000000
## Magneton            60     Magnet Pokémon      95           1000000
## Voltorb            190       Ball Pokémon      50           1000000
## Electrode           60       Ball Pokémon      70           1000000
## Staryu             225  Starshape Pokémon      55           1250000
## Starmie             60 Mysterious Pokémon      85           1250000
##           height_m hp        japanese_name      name percentage_male
## Magnemite      0.3 25           Coilコイル Magnemite              NA
## Magneton       1.0 50   Rarecoilレアコイル  Magneton              NA
## Voltorb        0.5 40 Biriridamaビリリダマ   Voltorb              NA
## Electrode      1.2 60   Marumineマルマイン Electrode              NA
## Staryu         0.8 30  Hitodemanヒトデマン    Staryu              NA
## Starmie        1.1 60    Starmieスターミー   Starmie              NA
##           pokedex_number sp_attack sp_defense speed    type1   type2
## Magnemite             81        95         55    45 electric   steel
## Magneton              82       120         70    70 electric   steel
## Voltorb              100        55         55   100 electric        
## Electrode            101        80         80   150 electric        
## Staryu               120        70         55    85    water        
## Starmie              121       100         85   115    water psychic
##           weight_kg generation is_legendary
## Magnemite       6.0          1            0
## Magneton       60.0          1            0
## Voltorb        10.4          1            0
## Electrode      66.6          1            0
## Staryu         34.5          1            0
## Starmie        80.0          1            0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, by looking at a few of these in the Pokedex, it would appear that these are generally genderless Pokémon, which would explain the missing values. A sensible value to use in these cases would therefore be 0.5, representing an equal spit of male and female:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[is.na(pokedat[[&amp;quot;percentage_male&amp;quot;]]), &amp;quot;percentage_male&amp;quot;] &amp;lt;- 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also worth noting that there are also missing values for the type2 Pokémon. These were not picked up as NA values, because they are encoded as a blank entry &amp;quot;“. We can convert this to a more descriptive factor such as”none&amp;quot;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;type2&amp;quot;]])[levels(pokedat[[&amp;quot;type2&amp;quot;]]) == &amp;quot;none&amp;quot;] &amp;lt;- &amp;quot;none&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;numeric-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Numeric Range&lt;/h2&gt;
&lt;p&gt;The vast majority of these variables are numeric in nature, so it is worth checking the range of these values to ensure that they are within typical ranges that we might expect. For instance, we would not expect negative values, zero values, or values greater than some sensible limit for things like height, weight, etc. So let’s take an overall look at these data ranges by using the &lt;code&gt;summary()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pokedat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                      abilities    against_bug    
##  [&amp;#39;Levitate&amp;#39;]                             : 29   Min.   :0.2500  
##  [&amp;#39;Beast Boost&amp;#39;]                          :  7   1st Qu.:0.5000  
##  [&amp;#39;Shed Skin&amp;#39;]                            :  5   Median :1.0000  
##  [&amp;#39;Clear Body&amp;#39;, &amp;#39;Light Metal&amp;#39;]            :  4   Mean   :0.9963  
##  [&amp;#39;Justified&amp;#39;]                            :  4   3rd Qu.:1.0000  
##  [&amp;#39;Keen Eye&amp;#39;, &amp;#39;Tangled Feet&amp;#39;, &amp;#39;Big Pecks&amp;#39;]:  4   Max.   :4.0000  
##  (Other)                                  :748                   
##   against_dark   against_dragon   against_electric against_fairy  
##  Min.   :0.250   Min.   :0.0000   Min.   :0.000    Min.   :0.250  
##  1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.500    1st Qu.:1.000  
##  Median :1.000   Median :1.0000   Median :1.000    Median :1.000  
##  Mean   :1.057   Mean   :0.9688   Mean   :1.074    Mean   :1.069  
##  3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.000    3rd Qu.:1.000  
##  Max.   :4.000   Max.   :2.0000   Max.   :4.000    Max.   :4.000  
##                                                                   
##  against_fight    against_fire   against_flying  against_ghost  
##  Min.   :0.000   Min.   :0.250   Min.   :0.250   Min.   :0.000  
##  1st Qu.:0.500   1st Qu.:0.500   1st Qu.:1.000   1st Qu.:1.000  
##  Median :1.000   Median :1.000   Median :1.000   Median :1.000  
##  Mean   :1.066   Mean   :1.135   Mean   :1.193   Mean   :0.985  
##  3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:1.000   3rd Qu.:1.000  
##  Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :4.000  
##                                                                 
##  against_grass   against_ground   against_ice    against_normal 
##  Min.   :0.250   Min.   :0.000   Min.   :0.250   Min.   :0.000  
##  1st Qu.:0.500   1st Qu.:1.000   1st Qu.:0.500   1st Qu.:1.000  
##  Median :1.000   Median :1.000   Median :1.000   Median :1.000  
##  Mean   :1.034   Mean   :1.098   Mean   :1.208   Mean   :0.887  
##  3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:1.000  
##  Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :1.000  
##                                                                 
##  against_poison   against_psychic  against_rock  against_steel   
##  Min.   :0.0000   Min.   :0.000   Min.   :0.25   Min.   :0.2500  
##  1st Qu.:0.5000   1st Qu.:1.000   1st Qu.:1.00   1st Qu.:0.5000  
##  Median :1.0000   Median :1.000   Median :1.00   Median :1.0000  
##  Mean   :0.9753   Mean   :1.005   Mean   :1.25   Mean   :0.9835  
##  3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:2.00   3rd Qu.:1.0000  
##  Max.   :4.0000   Max.   :4.000   Max.   :4.00   Max.   :4.0000  
##                                                                  
##  against_water       attack       base_egg_steps  base_happiness  
##  Min.   :0.250   Min.   :  5.00   Min.   : 1280   Min.   :  0.00  
##  1st Qu.:0.500   1st Qu.: 55.00   1st Qu.: 5120   1st Qu.: 70.00  
##  Median :1.000   Median : 75.00   Median : 5120   Median : 70.00  
##  Mean   :1.058   Mean   : 77.86   Mean   : 7191   Mean   : 65.36  
##  3rd Qu.:1.000   3rd Qu.:100.00   3rd Qu.: 6400   3rd Qu.: 70.00  
##  Max.   :4.000   Max.   :185.00   Max.   :30720   Max.   :140.00  
##                                                                   
##    base_total     capture_rate          classfication    defense      
##  Min.   :180.0   45     :250   Dragon Pokémon  :  8   Min.   :  5.00  
##  1st Qu.:320.0   190    : 75   Mouse Pokémon   :  6   1st Qu.: 50.00  
##  Median :435.0   255    : 69   Mushroom Pokémon:  6   Median : 70.00  
##  Mean   :428.4   75     : 61   Balloon Pokémon :  5   Mean   : 73.01  
##  3rd Qu.:505.0   3      : 58   Fairy Pokémon   :  5   3rd Qu.: 90.00  
##  Max.   :780.0   120    : 55   Flame Pokémon   :  5   Max.   :230.00  
##                  (Other):233   (Other)         :766                   
##  experience_growth    height_m            hp        
##  Min.   : 600000   Min.   : 0.100   Min.   :  1.00  
##  1st Qu.:1000000   1st Qu.: 0.600   1st Qu.: 50.00  
##  Median :1000000   Median : 1.000   Median : 65.00  
##  Mean   :1054996   Mean   : 1.155   Mean   : 68.96  
##  3rd Qu.:1059860   3rd Qu.: 1.500   3rd Qu.: 80.00  
##  Max.   :1640000   Max.   :14.500   Max.   :255.00  
##                                                     
##              japanese_name         name     percentage_male 
##  Abagouraアバゴーラ :  1   Abomasnow :  1   Min.   :  0.00  
##  Absolアブソル      :  1   Abra      :  1   1st Qu.: 50.00  
##  Abulyアブリー      :  1   Absol     :  1   Median : 50.00  
##  Aburibbonアブリボン:  1   Accelgor  :  1   Mean   : 48.47  
##  Achamoアチャモ     :  1   Aegislash :  1   3rd Qu.: 50.00  
##  Agehuntアゲハント  :  1   Aerodactyl:  1   Max.   :100.00  
##  (Other)            :795   (Other)   :795                   
##  pokedex_number   sp_attack        sp_defense         speed       
##  Min.   :  1    Min.   : 10.00   Min.   : 20.00   Min.   :  5.00  
##  1st Qu.:201    1st Qu.: 45.00   1st Qu.: 50.00   1st Qu.: 45.00  
##  Median :401    Median : 65.00   Median : 66.00   Median : 65.00  
##  Mean   :401    Mean   : 71.31   Mean   : 70.91   Mean   : 66.33  
##  3rd Qu.:601    3rd Qu.: 91.00   3rd Qu.: 90.00   3rd Qu.: 85.00  
##  Max.   :801    Max.   :194.00   Max.   :230.00   Max.   :180.00  
##                                                                   
##      type1         type2       weight_kg        generation  
##  water  :114          :384   Min.   :  0.10   Min.   :1.00  
##  normal :105   flying : 95   1st Qu.:  9.00   1st Qu.:2.00  
##  grass  : 78   ground : 34   Median : 27.30   Median :4.00  
##  bug    : 72   poison : 34   Mean   : 60.94   Mean   :3.69  
##  psychic: 53   fairy  : 29   3rd Qu.: 63.00   3rd Qu.:5.00  
##  fire   : 52   psychic: 29   Max.   :999.90   Max.   :7.00  
##  (Other):327   (Other):196                                  
##   is_legendary    
##  Min.   :0.00000  
##  1st Qu.:0.00000  
##  Median :0.00000  
##  Mean   :0.08739  
##  3rd Qu.:0.00000  
##  Max.   :1.00000  
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can learn a few things from this. For instance, if we look at the character type variables, “Levitate” is the most common ability (although note that this is only counting the cases where there is only a single ability – more on this later), “Dragon Pokémon” are the most common classfication [sic] (although the majority of the Pokémon are given a unique classification, which seems to go against the idea of classification), and “water” and “normal” type Pokémon are very common, and many Pokémon have flying as their secondary type. If we look at the numeric type variables, we can see that the &lt;code&gt;against_&lt;/code&gt; values appear to be numeric values between 0 and 4 in multiples of 0.25 (they represent the Pokémon’s strength against particular Pokémon types during battle), attack, defense and speed vary between 5 and 185, 230 and 180 respectively (so there is quite a big range depending on which Pokémon you have in a fight), there is at least one Pokémon that starts with a base happiness score of 0 (in fact there are 36, which is quite sad!), the percentage of males varies between 0 and 100 with 98 missing values (as expected). But overall there do not appear to be any strange outliers in these data. There are some pretty big Pokémon, but we will explore this in a little more detail later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;change-variable-class&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Change Variable Class&lt;/h2&gt;
&lt;p&gt;The m ajority of these data have been classified correctly by the &lt;code&gt;read.data()&lt;/code&gt; function as either numeric or characters (which are factorised automaticallyfor use in model and potting functions later). However, there are some changes that we may wish to make. Firstly, the &lt;code&gt;generation&lt;/code&gt; and &lt;code&gt;is_legendary&lt;/code&gt; variables should not be considered numeric, even though they are represented by numbers. For instance, a Pokémon could not come from Generation 2.345. So let’s make these changes from numeric to factorised character variables. Note that there is a natural order to the &lt;code&gt;generation&lt;/code&gt; values, so I will ensure that these are factorised as ordinal, just in case this comes into play later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[[&amp;quot;generation&amp;quot;]] &amp;lt;- factor(as.character(pokedat[[&amp;quot;generation&amp;quot;]]), order = TRUE)
pokedat[[&amp;quot;is_legendary&amp;quot;]] &amp;lt;- factor(as.character(pokedat[[&amp;quot;is_legendary&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These variabbles have been classified as factors even though they are represented by numeric values. The values representing the factor levels can actually be anything we choose, so it may be useful to change these to something more descriptive. For instance, it may be worth adding the word “Generation” to the &lt;code&gt;generation&lt;/code&gt; variable, whilst the &lt;code&gt;is_legendary&lt;/code&gt; variable may be more useful as a binary TRUE/FALSE value. It doesn’t matter what we put, since for later analyses dummy numeric variables will be used for calculations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;generation&amp;quot;]]) &amp;lt;- paste(&amp;quot;Generation&amp;quot;, levels(pokedat[[&amp;quot;generation&amp;quot;]]))
levels(pokedat[[&amp;quot;is_legendary&amp;quot;]]) &amp;lt;- c(FALSE, TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The defaults for the other factors seem to be suitable, so I will leave thse as they are.&lt;/p&gt;
&lt;p&gt;In comparison, the &lt;code&gt;capture_rate&lt;/code&gt; value is being classed as a factor, even though it should be a number. Why is this?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(pokedat[[&amp;quot;capture_rate&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;100&amp;quot;                      &amp;quot;120&amp;quot;                     
##  [3] &amp;quot;125&amp;quot;                      &amp;quot;127&amp;quot;                     
##  [5] &amp;quot;130&amp;quot;                      &amp;quot;140&amp;quot;                     
##  [7] &amp;quot;145&amp;quot;                      &amp;quot;15&amp;quot;                      
##  [9] &amp;quot;150&amp;quot;                      &amp;quot;155&amp;quot;                     
## [11] &amp;quot;160&amp;quot;                      &amp;quot;170&amp;quot;                     
## [13] &amp;quot;180&amp;quot;                      &amp;quot;190&amp;quot;                     
## [15] &amp;quot;200&amp;quot;                      &amp;quot;205&amp;quot;                     
## [17] &amp;quot;220&amp;quot;                      &amp;quot;225&amp;quot;                     
## [19] &amp;quot;235&amp;quot;                      &amp;quot;25&amp;quot;                      
## [21] &amp;quot;255&amp;quot;                      &amp;quot;3&amp;quot;                       
## [23] &amp;quot;30&amp;quot;                       &amp;quot;30 (Meteorite)255 (Core)&amp;quot;
## [25] &amp;quot;35&amp;quot;                       &amp;quot;45&amp;quot;                      
## [27] &amp;quot;50&amp;quot;                       &amp;quot;55&amp;quot;                      
## [29] &amp;quot;60&amp;quot;                       &amp;quot;65&amp;quot;                      
## [31] &amp;quot;70&amp;quot;                       &amp;quot;75&amp;quot;                      
## [33] &amp;quot;80&amp;quot;                       &amp;quot;90&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aha. So there is a non-numeric value in there – &lt;code&gt;30 (Meteorite)255 (Core)&lt;/code&gt;. Let’s see which Pokémon this applies to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.character(subset(pokedat, capture_rate == &amp;quot;30 (Meteorite)255 (Core)&amp;quot;)[[&amp;quot;name&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Minior&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a “Meteor” type Pokémon that appears to have a different capture rate under different conditions. Baased on the online &lt;a href=&#34;https://pokemondb.net/pokedex&#34;&gt;Pokedex&lt;/a&gt;, the canonical value to use here is 30 for its Meteorite form, so let’s use this and convert it into a numeric value. One thing to bear in mind here is that factors can be a little funny. When I convert to a number, it will convert the level values to a number, not necessarily the values themselves. So in this case, it will not give me the values but instead will give me the order of the values. For instance. look at the output of the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- factor(c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;20&amp;quot;))
data.frame(factor = t, level = levels(t), number = as.numeric(t))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   factor level number
## 1      1     1      1
## 2      2     2      2
## 3      3    20      4
## 4     20     3      3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the levels are based on a character sort rather than a numeric sort, which puts “20” ahead of “4”. Then, to add to this further, the factor “20” is actually the 3&lt;sup&gt;rd&lt;/sup&gt; factor so gets the value 3, whilst the factor “3” is the 4&lt;sup&gt;th&lt;/sup&gt; level, and so gets a value 4. So the output is definitely not what we would expect when converting this to a number. To avoid this, we need to convert to a character value before we do anything else:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat[[&amp;quot;capture_rate&amp;quot;]] &amp;lt;- as.character(pokedat[[&amp;quot;capture_rate&amp;quot;]])
pokedat[pokedat[[&amp;quot;name&amp;quot;]] == &amp;quot;Minior&amp;quot;, &amp;quot;capture_rate&amp;quot;] = &amp;quot;30&amp;quot;
pokedat[[&amp;quot;capture_rate&amp;quot;]] &amp;lt;- as.numeric(pokedat[[&amp;quot;capture_rate&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should now be a pretty clean data set ready for analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Exploratory Analyses&lt;/h1&gt;
&lt;p&gt;There are a lot of data here, and I could spend ages exploring every facet, but I just really wanrt to get a tester for these data here. First of all, let’s take a look at the distribution of some of the main statistics to see how they look across the dataset. For the majority of these plots, I will be using the &lt;code&gt;ggplot2&lt;/code&gt; library which offers a very nice way to produce publication-quality figures using a standardised lexicon, along with the &lt;code&gt;dplyr&lt;/code&gt; package which provides a simple way to rearrange data into suitable formats for producing plots. These are part of the &lt;a href=&#34;https://www.tidyverse.org&#34;&gt;Tidyverse&lt;/a&gt; suite of packages from Hadley Wickham, and form a very useful suite of packages with a common grammar that can be used together to make Data Science more efficient and to produce beautiful plots easily. Handy &lt;em&gt;cheat sheets&lt;/em&gt; can be found for &lt;a href=&#34;https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf&#34;&gt;dplyr&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&#34;&gt;ggplot2&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
library(&amp;quot;ggrepel&amp;quot;)
library(&amp;quot;RColorBrewer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;attack&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Attack&lt;/h2&gt;
&lt;p&gt;First of all, let’s take a look at the attack strength of all Pokémon split by their main type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(x = attack, fill = MainType)) + 
  geom_density(alpha = 0.2) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/attack_dist-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general, there is a wide range of attack strengths and they are largely distributed with a roughly normal distribution around the mean attack strength of 77.86 that we saw earlier. However, we do see that there are some differences in the different Pokémon types available. However, this probably isn’t the easiest way to see differences between the groups. Instead, let’s use a boxplot. Here, we can see the overall distribution, with the 25&lt;sup&gt;th&lt;/sup&gt; percentile, the median (50&lt;sup&gt;th&lt;/sup&gt; percentile), and the 75&lt;sup&gt;th&lt;/sup&gt; percentile making up the range of the box, and outliers (I believe those with values in the bottom 2.5&lt;sup&gt;th&lt;/sup&gt; percentile or upper 97.5&lt;sup&gt;th&lt;/sup&gt; percentile) highlighted as points on the plot. I have added notches to the boxplots, which show the confidence interval represeneted by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[median \pm \frac{1.58*IQR}{\sqrt{n}}\]&lt;/span&gt;
Where IQR is the interquartile range. Essentially, if the notch values do not overlap between two boxes, it suggests that there may be a statistically significant difference between them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(y = attack, x = MainType, fill = MainType)) + 
  geom_boxplot(notch = TRUE) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text.x  = element_text(size = 18, angle = 90, hjust = 1),
    axis.text.y  = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/attack_box-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, we can see that ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ type Pokémon have higher attack strength, whilst ‘fairy’ and ‘psychic’ type seem to have slightly lower attack strength. This would make sense based on the names, and we can test this a little more formally by using an ANOVA (analysis of variance) analysis to look for significant effects of the Pokémon type on the attack strength. This can be done quite simply by using linear models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_vs_type1 &amp;lt;- lm(attack ~ type1, data = pokedat)
summary(attack_vs_type1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = attack ~ type1, data = pokedat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -70.16 -22.31  -3.50  19.26 114.88 
## 
## Coefficients:
##               Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)    70.1250     3.6263  19.338 &amp;lt; 0.0000000000000002 ***
## type1dark      17.6681     6.7675   2.611             0.009208 ** 
## type1dragon    36.2824     6.9439   5.225          0.000000223 ***
## type1electric   0.6955     6.1178   0.114             0.909515    
## type1fairy     -8.0139     8.1087  -0.988             0.323308    
## type1fighting  29.0536     6.8531   4.239          0.000025078 ***
## type1fire      11.3750     5.5998   2.031             0.042561 *  
## type1flying    -3.4583    18.1316  -0.191             0.848783    
## type1ghost      2.6157     6.9439   0.377             0.706501    
## type1grass      3.6442     5.0288   0.725             0.468871    
## type1ground    24.6875     6.5375   3.776             0.000171 ***
## type1ice        3.1793     7.3700   0.431             0.666301    
## type1normal     5.0369     4.7082   1.070             0.285036    
## type1poison     2.5313     6.5375   0.387             0.698719    
## type1psychic   -4.5590     5.5691  -0.819             0.413253    
## type1rock      20.5417     5.8473   3.513             0.000468 ***
## type1steel     22.9583     7.2527   3.166             0.001608 ** 
## type1water      3.1820     4.6320   0.687             0.492311    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 30.77 on 783 degrees of freedom
## Multiple R-squared:  0.1039, Adjusted R-squared:  0.08448 
## F-statistic: 5.343 on 17 and 783 DF,  p-value: 0.00000000002374&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is not a great model by any means, as the R-squared values suggest that Pokémon type alone explains only a small proportion of the variance in the data. From the summary of the model, we can see that the fit model has an intercept of 70.1 (which is close to the overall mean of 77.86), with an offset specific to each of the different types. As we saw from the boxplot, ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ all have significant increases in attack (as can be seen by the resulting p-value), but also ‘dark’ and ‘fire’ type which I missed from looking at the boxplot. We see also that ‘fairy’ and ‘psychic’ type Pokémon show a decrease in attack strength, however this difference is not significant. So if you want a strong Pokémon, then these types are your best shot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dark&lt;/li&gt;
&lt;li&gt;dragon&lt;/li&gt;
&lt;li&gt;fighting&lt;/li&gt;
&lt;li&gt;fire&lt;/li&gt;
&lt;li&gt;ground&lt;/li&gt;
&lt;li&gt;rock&lt;/li&gt;
&lt;li&gt;steel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is also worth mentioning that there are a few outliers here, which represent ridiculously powerful Pokémon. In general, these are those with a strength greater than 150, but there are also two fairy Pokémon with higher strength than is typically seen within the ‘fairy’ type Pokémon. Let’s use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cook%27s_distance&#34;&gt;Cook’s Distance&lt;/a&gt; to identify the outliers from the above model. The idea here is to test the influence of each Pokémon by comparing the least-squares regression with and without the sample included. A higher value indicates a possible outlier. So let’s have a look at the outliers, which we are going to class as those with a Cook’s Distance greater than 4 times the mean of the Cook’s Distance over the entire data set (note that this is a commonly used threshold, but is entirely arbitrary):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggrepel&amp;quot;)
library(&amp;quot;RColorBrewer&amp;quot;)
cookdist &amp;lt;- cooks.distance(attack_vs_type1)
cookdist_lim &amp;lt;- 4*mean(cookdist, na.rm=T)#0.012
data.frame(Pokenum   = pokedat[[&amp;quot;pokedex_number&amp;quot;]], 
           Pokename  = pokedat[[&amp;quot;name&amp;quot;]], 
           CooksDist = cookdist,
           Pokelab   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;name&amp;quot;]]), &amp;quot;&amp;quot;),
           Outlier   = ifelse(cookdist &amp;gt; cookdist_lim, TRUE, FALSE),
           PokeCol   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;type1&amp;quot;]]), &amp;quot;&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) +
  geom_point() + 
  scale_shape_manual(values = c(16, 8)) +
  scale_size_manual(values = c(2, 5)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, colorRampPalette(brewer.pal(9, &amp;quot;Set1&amp;quot;))(length(table(pokedat[[&amp;quot;type1&amp;quot;]]))))) +
  geom_text_repel(nudge_x = 0.2, size = 8) +
  geom_hline(yintercept = cookdist_lim, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  xlab(&amp;quot;Pokedex Number&amp;quot;) + ylab(&amp;quot;Cook&amp;#39;s Distance&amp;quot;) +
  theme_bw() +
  theme(#legend.position = &amp;quot;name&amp;quot;,
        axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/strength_cooks-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The flying Pokémon Tornadus and Noibat are the biggest outliers. However, interestingly whilst Tornadus has a particularly high attack strength of 100 for a flying type Pokémon, Noibat has a low attack strength of 30. So both are outliers compared to the flyiong type Pokémon as a whole, with a mean of 66.6666667 ± 35.1188458. It is worth mentioning that Tornadus is a Legendary Pokémon which probably explains why it has a higher attack strength.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/641-incarnate.png&#34; alt=&#34;Tornadus&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Tornadus&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Whilst these two do not show up on the barplots, it is likely that these are responsible for the strange distribution of the notched boxplot. I suspect this odd shape is a result of the calculation of the confidence interval for the notches exceeding the 25&lt;sup&gt;th&lt;/sup&gt; and 75&lt;sup&gt;th&lt;/sup&gt; percentiles due to a wide range of values, but I do enjoy the fact that it looks like it is itself flying!&lt;/p&gt;
&lt;p&gt;The two outlying fairy type Pokémon are Xerneas and Togepi, another favourite of my daughter’s:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/175.png&#34; alt=&#34;Togepi&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Togepi&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;defence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Defence&lt;/h2&gt;
&lt;p&gt;Let’s repeat this to look at the defence values across the different Pokémon types:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(x = defense, fill = MainType)) + 
  geom_density(alpha = 0.2) + 
  xlab(&amp;quot;Defence Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/defence_dist-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be a bit more of a distinction between the different types as compared to the attack distribution, so let’s also look at the boxplots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat %&amp;gt;% mutate(&amp;#39;MainType&amp;#39; = type1) %&amp;gt;%
ggplot(aes(y = defense, x = MainType, fill = MainType)) + 
  geom_boxplot(notch = TRUE) + 
  xlab(&amp;quot;Defence Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(axis.title   = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text.x  = element_text(size = 18, angle = 90, hjust = 1),
    axis.text.y  = element_text(size = 18),
        legend.title = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/defence_box-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see more outliers here as compared to the attack strength and a very clear increase in defence for steel and rock type Pokémon (unsurprisingly), as well as dragon type. There are no clear types with lower defence, similar again to what we saw in the attack strength distribution plots. Let’s once again look at this using linear models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;defence_vs_type1 &amp;lt;- lm(defense ~ type1, data = pokedat)
summary(defence_vs_type1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = defense ~ type1, data = pokedat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -70.208 -20.847  -3.031  16.518 159.153 
## 
## Coefficients:
##                Estimate Std. Error t value             Pr(&amp;gt;|t|)    
## (Intercept)    70.84722    3.37247  21.008 &amp;lt; 0.0000000000000002 ***
## type1dark      -0.32998    6.29376  -0.052               0.9582    
## type1dragon    15.41204    6.45779   2.387               0.0172 *  
## type1electric  -9.02671    5.68954  -1.587               0.1130    
## type1fairy     -2.68056    7.54108  -0.355               0.7223    
## type1fighting  -4.45437    6.37337  -0.699               0.4848    
## type1fire      -3.05876    5.20784  -0.587               0.5571    
## type1flying    -5.84722   16.86236  -0.347               0.7289    
## type1ghost      8.67130    6.45779   1.343               0.1797    
## type1grass      0.02457    4.67678   0.005               0.9958    
## type1ground    13.05903    6.07981   2.148               0.0320 *  
## type1ice        1.06582    6.85403   0.156               0.8765    
## type1normal   -11.15198    4.37865  -2.547               0.0111 *  
## type1poison    -0.81597    6.07981  -0.134               0.8933    
## type1psychic   -1.58307    5.17923  -0.306               0.7599    
## type1rock      25.41944    5.43795   4.674    0.000003469793310 ***
## type1steel     49.36111    6.74494   7.318    0.000000000000623 ***
## type1water      2.63523    4.30777   0.612               0.5409    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 28.62 on 783 degrees of freedom
## Multiple R-squared:  0.1534, Adjusted R-squared:  0.135 
## F-statistic: 8.347 on 17 and 783 DF,  p-value: &amp;lt; 0.00000000000000022&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, we see very clear significant increase in defence strength for rock and steel type Pokémon, with a slight increase seen also for dragon and ground type Pokémon. There is also a slight reduction for normal type Pokémon. Flying Pokémon again show an odd distribution, so I expect again to see outliers for this class. Let’s look at the outliers now, again using the COok’s Distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cookdist &amp;lt;- cooks.distance(defence_vs_type1)
cookdist_lim &amp;lt;- 4*mean(cookdist, na.rm=T)#0.012
data.frame(Pokenum   = pokedat[[&amp;quot;pokedex_number&amp;quot;]], 
           Pokename  = pokedat[[&amp;quot;name&amp;quot;]], 
           CooksDist = cookdist,
           Pokelab   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;name&amp;quot;]]), &amp;quot;&amp;quot;),
           Outlier   = ifelse(cookdist &amp;gt; cookdist_lim, TRUE, FALSE),
           PokeCol   = ifelse(cookdist &amp;gt; cookdist_lim, as.character(pokedat[[&amp;quot;type1&amp;quot;]]), &amp;quot;&amp;quot;)) %&amp;gt;%
  ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) +
  geom_point() + 
  scale_shape_manual(values = c(16, 8)) +
  scale_size_manual(values = c(2, 5)) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, colorRampPalette(brewer.pal(9, &amp;quot;Set1&amp;quot;))(length(table(pokedat[[&amp;quot;type1&amp;quot;]]))))) +
  geom_text_repel(nudge_x = 0.2, size = 8) +
  geom_hline(yintercept = cookdist_lim, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  xlab(&amp;quot;Pokedex Number&amp;quot;) + ylab(&amp;quot;Cook&amp;#39;s Distance&amp;quot;) +
  theme_bw() +
  theme(axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/defence_cooks-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, Noibat and Tornadus are outliers here, and are joined by Noivern (which is an evolution of Noibat). As it turns out, these are the only 3 Pokémon in the flying type, which explains why they show up as outliers and why the boxplot has such a strange distribution. Other outliers include ice-type Pokémon Avalugg (defence 184), steel-type Pokémon Steelix and Aggron (both with defence 230), and bug-type Pokémon Shuckle (defence 230) which doubles up as a rock type Pokémon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/213.png&#34; alt=&#34;Shuckle&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Shuckle&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As seen with Shuckle, we should bear in mind that we have only focussed on the primary Pokémon type here, and have not considered the &lt;code&gt;type2&lt;/code&gt; values. So it is possible that we are missing the full picture when looking only at the first class for some of these Pokémon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Other&lt;/h2&gt;
&lt;p&gt;We could do this for all of the different values, and there are many different ways that we may want to examine these data. We can look at eaxch variable in the data set and examine them for odd distributions, we can look for outliers (as we have done above), we can start to examine relationships between variables, and we can look for correlation between different values (which we will do further below). Indeed a considerable amount of time &lt;em&gt;should&lt;/em&gt; be spent exploring data in this way to ensure it is of good quality – the old saying “garbage in, garbage out” is very true. But to be honest there are more interesting things that I want to do with these data, so let’s crack on…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensuring-we-use-accurate-data-classes-throughout&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.4&lt;/span&gt; Ensuring we use accurate data classes throughout&lt;/h2&gt;
&lt;p&gt;It is important to ensure that we are using the data in the correct way for our analyses. First of all let’s take a look at the abilities that each Pokémon has. As it stands, the &lt;code&gt;abilities&lt;/code&gt; variable is not terribly useful, as it fails to link Pokémon who may share abilities but not exactly. We could spend some time decoding the abilities, and create a “dictionary” of different abilities for each Pokémon, but perhaps a better measure may be to simply look at the &lt;em&gt;number&lt;/em&gt; of abilities that each Pokémon has:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abilities &amp;lt;- strsplit(as.character(pokedat[[&amp;quot;abilities&amp;quot;]]), &amp;quot;, &amp;quot;)
abilities &amp;lt;- sapply(abilities, FUN = function(x) gsub(&amp;quot;\\[|\\&amp;#39;|\\]&amp;quot;, &amp;quot;&amp;quot;, x))
names(abilities) &amp;lt;- rownames(pokedat)
pokedat[[&amp;quot;number_abilities&amp;quot;]] &amp;lt;- sapply(abilities, FUN = length)
table(pokedat[[&amp;quot;number_abilities&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   6 
## 109 245 427   7  13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the majority of Pokémon have only 1, 2 or 3 abilities, with only a small number of Pokémon have more than 3 abilities, and around half having exactly 3.&lt;/p&gt;
&lt;p&gt;I want to now look only at the variables that might theoretically be descriptive of the Pokémon in some kind of model. So we can remove the individual abilties (although the &lt;em&gt;number&lt;/em&gt; of abiltities will likely be of interest), the names and Pokedex number, and the classification which seems to be fairly ambiguous. We also need to ensure that we use the dummy variables for the variables encoded as factors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Normalization&lt;/h1&gt;
&lt;p&gt;Prior to doing this, I want to look at a few different ways to normalize all of the variables to ensure that they all have values that are comparable. Some algorithms are more sensitive to normalization than others, and it becomes quite obvious why this may be something to consider here when you consider that whilst the stats value &lt;code&gt;attack&lt;/code&gt; lies between 5 and 185, the &lt;code&gt;experience_growth&lt;/code&gt; value ranges between 600,000 and 1,640,000. That’s a &amp;gt;10,000-fold increase. This variable will completely dominate the calculations for certain machine-learning algorithms like K-Nearest Neighbours (KNN) and Support Vector Machines (SVM) where the distance between data points is important. SO let’s look at how we might want to normalize these data prior to analysis.&lt;/p&gt;
&lt;div id=&#34;min-max-normalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Min-Max Normalization&lt;/h2&gt;
&lt;p&gt;There are a few different ways to do this. One way is to &lt;em&gt;standardize&lt;/em&gt; the data, so that we bring them all into a common scale of &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;, but we maintain the distribution for each specific variable. So if one variable has a very skewed distriburtion, this will be maintained. A simple way to do this is to use the &lt;em&gt;min-max scaling&lt;/em&gt; approach, where you subtract the lowest value (so that the minimum value is always zero) and then divide by the range of the data so that the data are spread in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x&amp;#39; = \frac{x - x_{min}}{x_{max} - x_{min}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s compare the standardized and non-standardized attack values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_std &amp;lt;- (pokedat[[&amp;quot;attack&amp;quot;]] - min(pokedat[[&amp;quot;attack&amp;quot;]]))/(max(pokedat[[&amp;quot;attack&amp;quot;]]) - min(pokedat[[&amp;quot;attack&amp;quot;]]))
data.frame(Raw          = pokedat[[&amp;quot;attack&amp;quot;]],
           Standardized = attack_std) %&amp;gt;%
  tidyr::gather(&amp;quot;class&amp;quot;, &amp;quot;attack&amp;quot;, Raw, Standardized) %&amp;gt;%
  mutate(class = factor(class, levels = c(&amp;quot;Raw&amp;quot;, &amp;quot;Standardized&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = attack, fill = class)) +
  geom_density(alpha = 0.2) +
  facet_wrap(. ~ class, scales = &amp;quot;free&amp;quot;) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(legend.position = &amp;quot;none&amp;quot;,
    axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    axis.text       = element_text(size = 18),
    legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    legend.text     = element_text(size = 20),
    strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/standardize_attack-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So as you can see here, the distribution of the attack scores remains the same, but the scales over which the distribution is spread are different.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z-score-normalization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Z-Score Normalization&lt;/h2&gt;
&lt;p&gt;However, sometimes it is preferable to instead that every feature has a standard distribution that can be easily described to make them more comparable. In this case, you can &lt;em&gt;normalize&lt;/em&gt; the data so that the distribution of all features is shifted towards that of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34;&gt;normal (Gaussian) distribution&lt;/a&gt;. This is a typical “bell-shaped” curve, which can be exclusively described by the mean and the standard deviation. An example where I use this approach regularly is in gene-expression analysis, where we normalize the data such that the levels of expression of each gene is represented by a normal distribution, so that we can test for samples where the expression is significantly outside of the confines of this distribution. We can then look at all of the genes that seem to show significantly different expression than we would expect using hypothesis testing approaches, and look for common functions of these genes through the use of network analysis, gene ontology analysis and other downstream analyses.&lt;/p&gt;
&lt;p&gt;A simple normalization approach is the z-score normalization, which will transform the data so that the distribution has a mean of 0 and a standard deviation of 1. The transformation is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[x&amp;#39; = \frac{x - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we simply subtract the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (to center the data), and divide by the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Let’s again visualise this by comparing the raw and normalized data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attack_norm &amp;lt;- (pokedat[[&amp;quot;attack&amp;quot;]] - mean(pokedat[[&amp;quot;attack&amp;quot;]]))/sd(pokedat[[&amp;quot;attack&amp;quot;]])
data.frame(Raw          = pokedat[[&amp;quot;attack&amp;quot;]],
           Normalized   = attack_norm) %&amp;gt;%
  tidyr::gather(&amp;quot;class&amp;quot;, &amp;quot;attack&amp;quot;, Raw, Normalized) %&amp;gt;%
  mutate(class = factor(class, levels = c(&amp;quot;Raw&amp;quot;, &amp;quot;Normalized&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = attack, fill = class)) +
  geom_density(alpha = 0.2) +
  facet_wrap(. ~ class, scales = &amp;quot;free&amp;quot;) + 
  xlab(&amp;quot;Attack Strength&amp;quot;) + ylab(&amp;quot;Density&amp;quot;) +
  theme_bw() + 
  theme(legend.position = &amp;quot;none&amp;quot;,
    axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    axis.text       = element_text(size = 18),
    legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
    legend.text     = element_text(size = 20),
    strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/normalize_attack-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the normalized data now has a mean of 0, with the majority of values lying between -2 and 2. Note that since this is a fairly simple normalization technique, the distribution itself is not really changed, but there are other methods such as quantile normalization which can reshape the data to a Gaussian curve, or indeed any other dstribution as required.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-normalization-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Choosing a Normalization Method&lt;/h2&gt;
&lt;p&gt;The choice of which method to use will largely depend on what you are trying to do. The min-max scaling method is a common method used in machine learning, but does not handle outliers very well – a sample with an extreme value in the raw data will still have an extreme value in the scaled data, resulting in bunching up of the remaining data since it is all bounded within the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;. The Z-score normalization is better at dealing with outliers as they will appear far away from the mean value of 0, but the range of data is no longer bounded meaning that different variables will now have different ranges. Other normalization processes may make your data more homogenous, but at the cost of potentially losing aspects of the data that may be useful.&lt;/p&gt;
&lt;p&gt;Many machine learning algorithms use some kind of &lt;em&gt;distance&lt;/em&gt; measure, in order to look for similarity between different items. This can be a simple Euclidean distance, which is a k-dimensional measure of “as the crow flies”. If the scales are very different between your variables, then this will cause a significant issue.&lt;/p&gt;
&lt;p&gt;For the following analyses, I will use Z-score normalization, since I know that there are a few extreme outliers in these data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-for-patterns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Looking for Patterns&lt;/h1&gt;
&lt;div id=&#34;correlation-of-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Correlation of Variables&lt;/h2&gt;
&lt;p&gt;One thing that it is worth doing before heading down a route of data modelling is to look at the correlation structure between the variables in the data set. If there are highly correlated variables (for instance one might imagine height and weight to be highly correlated), then both will offer the same information to the model – adding both will give no additional predictive power than adding only one. There is then a danger of over-fitting the data, which can happen if you create a model so complicated that, whilst it may fit the training data very well, it is so complex as to no longer be applicable to external data sets making it essentially useful. An extreme example is if we create a model where we have one variable for each sample, that is equal to 1 for that specific sample, and 0 for every other sample. This would fit the data perfectly, but would be of absolutely no use for any other data. In addition, we can risk biasing the data by including multiple variables that essentially encode the same information.&lt;/p&gt;
&lt;p&gt;So we can calculate a pairwise correlation matrix, which will give us a measure of similarity between each pair of variables by comparing the two vectors of values for the 801 Pokémon. There are multiple measures of correlation that can be used. The standard is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&#34;&gt;Pearson Correlation&lt;/a&gt;, which is a measure of the linearity of the relationship between two values. A value of 1 represents an entirely linear monotonic relationship (as one value increases, so does the other, but with every unit increase in one variable matching a unit increase in the other in a linear way), a value of 0 represents no linearity between the values (no clear relationship between the two), and a value of -1 represents an inverse monotonic linear relationship (anti-correlated).&lt;/p&gt;
&lt;p&gt;For this analysis, I will be using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient&#34;&gt;Spearman Correlation&lt;/a&gt; coefficient. Instead of using the values themselves, this method first ranks the data, and then looks at the correlation. The idea here is that the unit difference between each successive value is kept constant, meaning that outliers do not have a big impact. This is therefore independent of the distribution of the data, and is therefore a non-parametric method. Since we identified some outliers with some of these variables, this method will remove the effect that these might have.&lt;/p&gt;
&lt;p&gt;We can represent these values by using a heatmap, which is a way of representing 3-dimensional data. The value of the correlation, rather than being represented on an axis as a value, will be represented by a colour. Values of the SPearman correlation closer to 1 will appear more red, whilst those closer to -1 will appear more blue. Those closer to 0 will appear white.&lt;/p&gt;
&lt;p&gt;In addition, I will apply a hierarchical clustering method to ensure that the variables most similar to one another are located close to one another on the figure. Pairwise distances are calculated by looking at the &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34;&gt;Euclidean distance&lt;/a&gt;, and similar variables are clustered together, allowing us to pick out by eye those most similar to one another.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, we first want to ensure that we are looking at numerical values, or at least numerical representations of factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat_vals &amp;lt;- pokedat[, !names(pokedat) %in% c(&amp;quot;abilities&amp;quot;, &amp;quot;classfication&amp;quot;, &amp;quot;japanese_name&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;pokedex_number&amp;quot;)]
pokedat_vals[[&amp;quot;type1&amp;quot;]]        &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;type1&amp;quot;]])
pokedat_vals[[&amp;quot;type2&amp;quot;]]        &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;type2&amp;quot;]])
pokedat_vals[[&amp;quot;generation&amp;quot;]]   &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;generation&amp;quot;]])
pokedat_vals[[&amp;quot;is_legendary&amp;quot;]] &amp;lt;- as.numeric(pokedat_vals[[&amp;quot;is_legendary&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will give us a purely numeric data set for use in numeric calculations. Finally we will normalize the data using Z-score normalization:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokedat_norm &amp;lt;- scale(pokedat_vals, center = TRUE, scale = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally let’s take a look at the correlation plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;pheatmap&amp;quot;)
pokecor_var &amp;lt;- cor(pokedat_norm, method = &amp;quot;spearman&amp;quot;)        
colors  &amp;lt;- colorRampPalette(c(&amp;#39;dark blue&amp;#39;,&amp;#39;white&amp;#39;,&amp;#39;dark red&amp;#39;))(255)#colorRampPalette( rev(brewer.pal(9, &amp;quot;Blues&amp;quot;)) )(255)
pheatmap(pokecor_var, 
         clustering_method = &amp;quot;complete&amp;quot;,
         show_colnames = FALSE,
         show_rownames = TRUE,
         col=colors, 
         fontsize_row = 24)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/correlation_var-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see a few clear cases of correlated variables here, especially between a few of the values giving attack strength against certain Pokémon types. For instance, attack against ghost- and dark-type Pokémon are very similar (which makes sense), as are attacks against electric- and rock-type Pokémon (makes less sense). The clearest aspect of the figure is a block of high positive correlations between a number of variables associated with the Pokémon’s vital statistics. So the most similar variables are those like speed, defence strength, attack strength, height, weight, health points, experience growth, etc. This makes a lot of sense, with bigger Pokémon having better attack and defence, more health points, etc. We also see that these variables are very highly anti-correlated with the capture-rate, which again makes sense – the better Pokémon are harder to catch. However, none of these correlations seem significantly high to require the remobval of any variables prior to analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;height-vs-weight&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; Height vs Weight&lt;/h2&gt;
&lt;p&gt;Let’s take a look at the height vs the weight. We can also include a few additional variables to see how the Pokémon strength and defence affect the relationship. I am going to scale both the x-and y-axes by using a &lt;span class=&#34;math inline&#34;&gt;\(log_{10}\)&lt;/span&gt; transformation to avoid much larger Pokémon drowing out the smaller ones:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = height_m, y = weight_kg, color = attack, size = defense), data = pokedat) +
  geom_point(alpha = 0.2) +
  scale_color_gradient2(name = &amp;quot;Attack&amp;quot;, midpoint = mean(pokedat[[&amp;quot;attack&amp;quot;]]), low = &amp;quot;blue&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;red&amp;quot;) +
  scale_size_continuous(name = &amp;quot;Defence&amp;quot;, range = c(1, 10)) +
  xlab(&amp;quot;Height (m)&amp;quot;) + ylab(&amp;quot;Weight (Kg)&amp;quot;) +
  scale_x_log10() +
  scale_y_log10() +
  #theme_bw() + 
  theme(axis.title      = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        axis.text       = element_text(size = 18),
        legend.title    = element_text(size = 24, face = &amp;quot;bold&amp;quot;),
        legend.text     = element_text(size = 20),
        strip.text      = element_text(size = 24, face = &amp;quot;bold&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/height_vs_weight-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there is a clear relationship, and in general taller Pokémon (such as Walor, the largest of the Pokémon) also weigh more as we might expect:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/321.png&#34; alt=&#34;Walord, the largest Pokemon&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Walord, the largest Pokemon&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We also see that in general the attack strength of the larger Pokémon is higher than the smaller ones, although there are definitely some outliers, such as Cosmeom, a legendary Pokémon only 10 cm tall that weighs nearly 1,000 kg!&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/cosmoem.png&#34; alt=&#34;Cosmoem, the smallest Pokemon&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Cosmoem, the smallest Pokemon&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-between-pokemon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Correlation between Pokémon&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokecor_sample &amp;lt;- cor(t(pokedat_norm), method = &amp;quot;spearman&amp;quot;)        
colors  &amp;lt;- colorRampPalette(c(&amp;#39;dark blue&amp;#39;,&amp;#39;white&amp;#39;,&amp;#39;dark red&amp;#39;))(255)
pheatmap(pokecor_sample, 
         clustering_method = &amp;quot;complete&amp;quot;,
         show_colnames = FALSE,
         show_rownames = FALSE,
         annotation = pokedat[, c(&amp;quot;generation&amp;quot;, &amp;quot;type1&amp;quot;, &amp;quot;type2&amp;quot;, &amp;quot;is_legendary&amp;quot;)],
         col=colors)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/correlation_pokemon-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So here we see the correlation structure between the 801 Pokémon, and it very clearly shows similarities between groups of Pokémon. The red boxes that we see down the diagonal represent highly similar groups, and at the top I have annotated the factor variables generation, type1, type2 and is_legendary. Almost all of the legendary Pokémon cluster together, suggesting that these are all similar to one another across these variables. Similarly there are a number of Pokémon types that clearly cluster together, such as rock type and dark type Pokémon. Interestingly the Generation number seems to be unrelated to Pokémon similarity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;principal-component-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.4&lt;/span&gt; Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;A good way to explore data such as these to look for underlying trends in the data is to use a dimensional-reduction algorithm to reduce these high-dimensional data down into asmaller number of easy to digest chunks. Principal component analysis (PCA) is one such approach, and can be used to look for the largest sources of variation within a high dimensional data set. For a data set with n variables, we can think of these data existing in an n-dimensional space. PCA is a mathematical trick that rotates these axes in n-dimensional space so that the x-axis of the rotation explains the largest possible amount of variation in the data, the y-axis then explains the next largest possible amount of variation, the z-axis the next largest amount, etc. In many cases, a very large amount of the total variation in the original n-dimensional data set can be captured by only a small number of so-called principal components. This can be used to look for underlying trends in the data. So let’s have a look at how this looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- prcomp(pokedat_vals, scale. = TRUE)
pc_plot &amp;lt;- as.data.frame(pc[[&amp;quot;x&amp;quot;]])
pc_plot &amp;lt;- cbind(pc_plot, pokedat[rownames(pc_plot), c(&amp;quot;generation&amp;quot;, &amp;quot;type1&amp;quot;, &amp;quot;type2&amp;quot;, &amp;quot;is_legendary&amp;quot;)])
explained_variance &amp;lt;- 100*((pc[[&amp;quot;sdev&amp;quot;]])^2 / sum(pc[[&amp;quot;sdev&amp;quot;]]^2))
screeplot(pc, type = &amp;quot;line&amp;quot;, main = &amp;quot;Principal Component Loadings&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/PCA-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we see that PC1 and PC2 explain a lot of variance compared to the other PCs, but the drop off is not complete at this point. In a lot of data sets, the first few PCs explain the vast majority of the variance, and so this plot drops off considerably to a flat line by PC3 or PC4. Let’s take a look at how discriminatory these PCs are to these data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = PC1, y = PC2, shape = is_legendary, color = type1), data = pc_plot) +
  geom_point(size = 5, alpha = 0.7) +
  xlab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 1, explained_variance[1])) +
  ylab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 2, explained_variance[2])) +
  theme(axis.title   = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 12),
        legend.title = element_text(size = 18, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/PCA_PC1_vs_PC2-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that this PCA approach clearly discriminates the legendary Pokémon (triangles) from the other Pokémon (circles) in PC1, which is the new axis that explains the most variance in the data (although note that this is still only 17.2%, so not a huge amount really). There is some separation seen between the different Pokémon groups, which seems to represent the second most significant source of variation, accounting for another 10.4%. Let’s look at PC3 as well to see if this is able to discriminate the samples further:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(x = PC2, y = PC3, shape = is_legendary, color = type1), data = pc_plot) +
  geom_point(size = 5, alpha = 0.7) +
  xlab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 2, explained_variance[2])) +
  ylab(sprintf(&amp;quot;PC%d (%.2f%%)&amp;quot;, 3, explained_variance[3])) +
  theme(axis.title   = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
        axis.text    = element_text(size = 12),
        legend.title = element_text(size = 18, face = &amp;quot;bold&amp;quot;),
        legend.text  = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-07-15_Gotta_Catch_em_all_Pokemon/index_files/figure-html/PCA_PC2_vs_PC3-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By looking at the 2&lt;sup&gt;nd&lt;/sup&gt; and 3&lt;sup&gt;rd&lt;/sup&gt; PCs, we do see a slight separation between the Pokémon types, but it is not strong.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-based-recommendation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.5&lt;/span&gt; Correlation-Based Recommendation&lt;/h2&gt;
&lt;p&gt;As a little aside, I want to see whether it is possible to recommend similar Pokémon to any Pokémon that you might suggest. For instance, my daughter’s absolute favourite is Eevee (a cuddly fox-like Pokémon):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/133.png&#34; alt=&#34;Eeevee&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Eeevee&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So let’s see whether there are any other Pokémon that are similar to Eevee, based on a simple correlation match. To do this, I will calculate the Spearman correlation coefficient between Eevee and every other Pokémon, and see which the most similar are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eevee &amp;lt;- as.numeric(pokedat_vals[&amp;quot;Eevee&amp;quot;, ])
eevee_cor &amp;lt;- apply(pokedat_vals, MAR = 1, FUN = function (x) cor(x, eevee, method = &amp;quot;spearman&amp;quot;))
head(sort(eevee_cor, decreasing = TRUE), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Eevee     Aipom   Sentret Teddiursa    Patrat Zigzagoon   Rattata 
## 1.0000000 0.9781018 0.9728923 0.9695460 0.9693680 0.9690263 0.9689643 
##    Meowth  Raticate    Bidoof 
## 0.9687552 0.9676089 0.9669312&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here are a few of the most similar Pokémon:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/190.png&#34; alt=&#34;Aipom&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Aipom&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/161.png&#34; alt=&#34;Sentret&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Sentret&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/216.png&#34; alt=&#34;Teddiursa&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Teddiursa&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/504.png&#34; alt=&#34;Patrat&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Patrat&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Well they all look kind of cuddly like Eevee, except for the really creepy evil beaver Patrat at the end!&lt;/p&gt;
&lt;p&gt;However, what would be even more efficient (maybe not in this case, but in the case of a much higher-dimensional data set like a gene-expression data set of 30,000 genes) is to use the reduced dataset after using PCA. The first 10 PCs explained around two thirds of the variance on the data, so by using these 10 values rather than the 37 values originally, we reduce the computations with a relatively small loss of data. Let’s see what the outputs are in this case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc_df_sub &amp;lt;- as.data.frame(pc[[&amp;quot;x&amp;quot;]])[, 1:10]
eevee &amp;lt;- as.numeric(pc_df_sub[&amp;quot;Eevee&amp;quot;, ])
eevee_cor_pca &amp;lt;- apply(pc_df_sub, MAR = 1, FUN = function (x) cor(x, eevee, method = &amp;quot;spearman&amp;quot;))
head(sort(eevee_cor_pca, decreasing = TRUE), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Eevee    Spinda  Delcatty   Watchog     Aipom    Furret  Smeargle 
## 1.0000000 0.9636364 0.9515152 0.9515152 0.9393939 0.9272727 0.9030303 
##   Herdier Vanillish    Meowth 
## 0.9030303 0.9030303 0.8909091&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/327.png&#34; alt=&#34;Spinda&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Spinda&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/301.png&#34; alt=&#34;Delcatty&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Delcatty&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/505.png&#34; alt=&#34;Watchog&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Watchog&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;pokemon/162.png&#34; alt=&#34;Furret&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Furret&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Similarly cute, but with another creepy one! After checking with the experiment subject (my daughter), it seems that the best hits are definitely Watchog and Furret, so this seems to cope well at picking out similarly cuddly looking Pokémon.&lt;/p&gt;
&lt;p&gt;Obviously here we are simply looking for Pokémon with similar characteristics. This is not as in-depth as a collaboritive filtering method where we have some subjective ranking of items to help us to determine the &lt;em&gt;best&lt;/em&gt; Pokémon to match somebody’s needs. However, by using the reduced PCA data set we are able to find very close matches using only a reduced subset of the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-legendary-pokemon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Predicting Legendary Pokémon&lt;/h1&gt;
&lt;p&gt;Based on our previous analyses, we see that there is a clear discrimination between normal and Legendary Pokémon. These are incredibly rare, and very powerful Pokémon in the game. So is it possible to identify a Legendary Pokémon based on the variables available from this database? And if so, which variables are the most important?&lt;/p&gt;
&lt;p&gt;To do predictive modelling, we need to first split our data up into a training data set to use to train the model, and then a validation data set to use to confirm the accuracy of the predictions that this model makes. There are more robust ways to do this, such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;cross-validation&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&#34;&gt;bootstrapping&lt;/a&gt;, which allow you to assess the accuracy of the model. Cross validation can be applied simply by using the &lt;code&gt;caret&lt;/code&gt; package in R.&lt;/p&gt;
&lt;p&gt;We will also split the data randomly into two data sets – one containing 80% of the Pokémon for training the model, and one containing 20% of the Pokémon for validation purposes. For the following model fitting approaches, we do not need to have the categorical data converted into numbers as we did for the correlation analysis, as the factors will be treated correctly automatically We do however still need to ignore the non-informative variables. In addition, I am going to remove the “against_” columns to avoid overfitting of the data. So let’s generate our two data sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;caret&amp;quot;)
set.seed(0)
split_index &amp;lt;- createDataPartition(y = pokedat[[&amp;quot;is_legendary&amp;quot;]], p = 0.8, list = FALSE)
rm_index       &amp;lt;- which(names(pokedat) %in% c(&amp;quot;abilities&amp;quot;, &amp;quot;classfication&amp;quot;, &amp;quot;japanese_name&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;pokedex_number&amp;quot;))
rm_index       &amp;lt;- c(grep(&amp;quot;against&amp;quot;, names(pokedat)), rm_index)
training_dat   &amp;lt;- pokedat[split_index,  -rm_index]
validation_dat &amp;lt;- pokedat[-split_index, -rm_index]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s try a few different common methods used for classification purposes.&lt;/p&gt;
&lt;div id=&#34;support-vector-machine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; Support-Vector Machine&lt;/h2&gt;
&lt;p&gt;We can then train our SVM model, incorporating a preprocessing step to center and scale the data, and performing 10-fold repeated cross validation which we repeat 3 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_svm &amp;lt;- train(is_legendary ~., 
                   data = training_dat, 
                   method = &amp;quot;svmLinear&amp;quot;,
                   trControl = trctrl,
                   preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                   tuneLength = 10)
model_svm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Support Vector Machines with Linear Kernel 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.9802138  0.8782632
## 
## Tuning parameter &amp;#39;C&amp;#39; was held constant at a value of 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this model is able to predict whether the Pokémon is Legendary based on these 18 predictor variables with 98.02% accuracy (correct predictions). The Kappa value is normalised to account for the fact that we could get pretty good accuracy if we just called everything not Legendary due to the imbalance in the classes.&lt;/p&gt;
&lt;p&gt;This accuracy is based on resampling of the training data. How does it cope with the validation data? Let’s take a look at the confusion matrix for the predicted outcomes compared to the true values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_svm, newdata = validation_dat)
svm_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
svm_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   144    1
##      TRUE      2   13
##                                           
##                Accuracy : 0.9812          
##                  95% CI : (0.9462, 0.9961)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.000314        
##                                           
##                   Kappa : 0.8863          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 1.000000        
##                                           
##             Sensitivity : 0.9863          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9931          
##          Neg Pred Value : 0.8667          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9000          
##    Detection Prevalence : 0.9062          
##       Balanced Accuracy : 0.9574          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SO according to these multiple statistics, of the 14 Legendary Pokémon in the validation data, we correctly identified 13 of them, but missed 1. 2 were identified incorrectly. Our ultimate accuracy is 98.12%, which seems to be pretty good. However, it is possible to further tune this model, by adjusting the tuning parameter &lt;code&gt;C&lt;/code&gt;, by tweaking the parameters included in the model, and moving from a linear SVM model. However, all told, this is a pretty good result.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbour&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; k-Nearest Neighbour&lt;/h2&gt;
&lt;p&gt;Another classification method is the k-Nearest Neighbour (kNN) algorithm. The idea here is that a record is kept of all of the data in the training data, and a new sample is compared to find the k samples “closest” to it. The classification is then calculated based on some average of the classifications of these nearest neighbours. The method of determining the “nearest” neighbour can be one of a number of different methods, including Euclidean distance as described earlier.&lt;/p&gt;
&lt;p&gt;Also, the value of k is very important. Using the &lt;code&gt;caret&lt;/code&gt; package in R, we are able to test using multiple different values of k to find the value that optimises the model accuracy. So let’s train our model, again performing 10-fold repeated cross validation which we repeat 3 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_knn &amp;lt;- train(is_legendary ~., 
                   data = training_dat, 
                   method = &amp;quot;knn&amp;quot;,
                   trControl = trctrl,
                   preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                   tuneLength = 10)
model_knn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## k-Nearest Neighbors 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    5  0.9470304  0.5515501
##    7  0.9444345  0.5158836
##    9  0.9428470  0.4854601
##   11  0.9376461  0.4246775
##   13  0.9350660  0.3903400
##   15  0.9355788  0.3898290
##   17  0.9376542  0.4161790
##   19  0.9376784  0.4063420
##   21  0.9355786  0.3765296
##   23  0.9381750  0.4070014
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 5.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the accuracy is optimised by using k = 5 nearest neighbours, which gives an accuracy of 94.7% – below that of the SVM accuracy. Now let’s test it on the validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_knn, newdata = validation_dat)
knn_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
knn_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   145    4
##      TRUE      1   10
##                                           
##                Accuracy : 0.9688          
##                  95% CI : (0.9286, 0.9898)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.004163        
##                                           
##                   Kappa : 0.7833          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.371093        
##                                           
##             Sensitivity : 0.9932          
##             Specificity : 0.7143          
##          Pos Pred Value : 0.9732          
##          Neg Pred Value : 0.9091          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9062          
##    Detection Prevalence : 0.9313          
##       Balanced Accuracy : 0.8537          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can already see that the results of this model are less positive than the SVM model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; Logistic Regression&lt;/h2&gt;
&lt;p&gt;For classification problems with only two groups, linear regression is often a good first option. This is a generalised linear model, where we require a transformation of the response variable to ensure that it fits a continuous scale. In this case, the response variable is the probability of being in the Legendary class, so we need to map this to the simple yes/no outcome of the input training data. The logit function is often used, which is a transformation between a probability &lt;code&gt;p&lt;/code&gt; and a real number:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[logit(p) = log(\frac{p}{1-p})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So let’s fit a logistic regression model containing all of our data, again using 3 repeats of 10-fold cross validation, and see what we get back:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_logreg &amp;lt;- train(is_legendary ~., 
                      data = training_dat, 
                      method = &amp;quot;glm&amp;quot;,
                      family = &amp;quot;binomial&amp;quot;,
                      trControl = trctrl,
                      preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                      tuneLength = 10)
model_logreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Generalized Linear Model 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results:
## 
##   Accuracy   Kappa   
##   0.9667197  0.802881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we actually get a better accuracy from the cross-validation than for SVM or kNN at 96.67% accuracy. So let’s test it on the validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_logreg, newdata = validation_dat)
logreg_confmat &amp;lt;- confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])
logreg_confmat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   141    1
##      TRUE      5   13
##                                           
##                Accuracy : 0.9625          
##                  95% CI : (0.9202, 0.9861)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.01131         
##                                           
##                   Kappa : 0.792           
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.22067         
##                                           
##             Sensitivity : 0.9658          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9930          
##          Neg Pred Value : 0.7222          
##              Prevalence : 0.9125          
##          Detection Rate : 0.8812          
##    Detection Prevalence : 0.8875          
##       Balanced Accuracy : 0.9472          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The accuracy of this model on the validation data set is 96.25%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; Random Forest&lt;/h2&gt;
&lt;p&gt;As a final model, we will look at using a random forest classifier. A random forest is essentially created by creating a number of decision trees and averaging over them all. I will use the same cross-validation scheme as previously, and will allow &lt;code&gt;caret&lt;/code&gt; to identify the optimum parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(0)
trctrl    &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, number = 10, repeats = 3)
model_rf &amp;lt;- train(is_legendary ~., 
                  data = training_dat, 
                  method = &amp;quot;rf&amp;quot;,
                  trControl = trctrl,
                  preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                  tuneLength = 10)
model_rf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Random Forest 
## 
## 641 samples
##  18 predictor
##   2 classes: &amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 3 times) 
## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9911611  0.9396362
##    8    0.9984292  0.9905178
##   14    0.9984292  0.9905178
##   20    0.9984292  0.9905178
##   26    0.9984292  0.9905178
##   32    0.9984292  0.9905178
##   38    0.9984292  0.9905178
##   44    0.9984292  0.9905178
##   50    0.9984292  0.9905178
##   56    0.9984292  0.9905178
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 8.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach identified an accuracy of 99.84% (with a normalised kappa value of 99.05%) with an &lt;code&gt;mtry&lt;/code&gt; value of 8. This is incredibly good, and is the best outcome so far. However, it is worth noting that this approach is significantly slower than the others. Let’s check against our validation data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_legendary &amp;lt;- predict(model_rf, newdata = validation_dat)
confusionMatrix(predict_legendary, validation_dat[[&amp;quot;is_legendary&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction FALSE TRUE
##      FALSE   145    1
##      TRUE      1   13
##                                           
##                Accuracy : 0.9875          
##                  95% CI : (0.9556, 0.9985)
##     No Information Rate : 0.9125          
##     P-Value [Acc &amp;gt; NIR] : 0.00005782      
##                                           
##                   Kappa : 0.9217          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9932          
##             Specificity : 0.9286          
##          Pos Pred Value : 0.9932          
##          Neg Pred Value : 0.9286          
##              Prevalence : 0.9125          
##          Detection Rate : 0.9062          
##    Detection Prevalence : 0.9125          
##       Balanced Accuracy : 0.9609          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So whilst the training accuracy is nearly 100%, here we see slightly lower accuracy on the test data set. It is still pretty good going, but could still be improved.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;We have managed to explore these data in a number of different ways and have identified some interesting themes and patterns. However, there is a lot of additional work that can be done. Correlation-based recommendation seems to work to identify similar Pokémon, at least in the small number of cases that I looked at. My wife showed me &lt;a href=&#34;https://www.amazon.co.uk/Pokemon-Trainer/dp/B07GYQC8C7&#34;&gt;this&lt;/a&gt; toy that can guess any Pokémon that you think of, which probably uses a similar approach. I’m in the wrong business…&lt;/p&gt;
&lt;p&gt;In particular, whilst it was interesting to play with some of the most commonly used machine learning methods, I have spent no real time tuning the model parameters. The variables themselves used in the data set could be tweaked to identify those variables most associated with the response variable. Given that these are incredibly rare and powerful Pokémon, we will inevitably find that the fighting based variables like attack and defence are highly discriminative, but also those such as the capture rate and experience growth associated. In addition, we have looked only at additive models here and have not considered interaction terms. We can also spend time fine-tuning the parameters of the models themselves to improve the accuracy further.&lt;/p&gt;
&lt;p&gt;However, despite this, we got some very good results from the default model parameters for several commonly used methods. kNN, SVM, logistic regression and random forests gave very similar results, but with SVM and random forest giving the best predictive outcomes.&lt;/p&gt;
&lt;p&gt;Ultimately, the key to developing any machine learning algorithm is to trial multiple different approaches and tune the parameters for the specific data in question. Also, a machine learning algorithm is only as good as the data that it is trained on, so feeding in more good quality cleaned data will help the model to learn. Ultimately, machine learning is aimed at developing a model that is able to predict something from new data, so it needs to be as generalised as possible.&lt;/p&gt;
&lt;p&gt;So hopefully when I come across a new Pokémon not currently found in my Pokedex, I can easily check whether it is Legendary by asking it a few questions about its vital statistics. Very useful.&lt;/p&gt;
&lt;p&gt;Gotta catch ’em all!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] caret_6.0-84       lattice_0.20-38    pheatmap_1.0.12   
## [4] RColorBrewer_1.1-2 ggrepel_0.8.1      dplyr_0.8.3       
## [7] ggplot2_3.2.0     
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.2          lubridate_1.7.4     tidyr_1.0.0        
##  [4] class_7.3-15        assertthat_0.2.1    zeallot_0.1.0      
##  [7] digest_0.6.20       ipred_0.9-9         foreach_1.4.7      
## [10] R6_2.4.0            plyr_1.8.4          backports_1.1.4    
## [13] stats4_3.6.1        evaluate_0.14       e1071_1.7-2        
## [16] blogdown_0.16       pillar_1.4.2        rlang_0.4.0        
## [19] lazyeval_0.2.2      data.table_1.12.2   kernlab_0.9-27     
## [22] rpart_4.1-15        Matrix_1.2-17       rmarkdown_1.14     
## [25] labeling_0.3        splines_3.6.1       gower_0.2.1        
## [28] stringr_1.4.0       munsell_0.5.0       compiler_3.6.1     
## [31] xfun_0.8            pkgconfig_2.0.2     htmltools_0.3.6    
## [34] nnet_7.3-12         tidyselect_0.2.5    tibble_2.1.3       
## [37] prodlim_2018.04.18  bookdown_0.12       codetools_0.2-16   
## [40] randomForest_4.6-14 crayon_1.3.4        withr_2.1.2        
## [43] MASS_7.3-51.4       recipes_0.1.7       ModelMetrics_1.2.2 
## [46] grid_3.6.1          nlme_3.1-140        gtable_0.3.0       
## [49] lifecycle_0.1.0     magrittr_1.5        scales_1.0.0       
## [52] stringi_1.4.3       reshape2_1.4.3      timeDate_3043.102  
## [55] ellipsis_0.2.0.1    generics_0.0.2      vctrs_0.2.0        
## [58] lava_1.6.6          iterators_1.0.12    tools_3.6.1        
## [61] glue_1.3.1          purrr_0.3.3         survival_2.44-1.1  
## [64] yaml_2.2.0          colorspace_1.4-1    knitr_1.23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Strava Data Mining: Assessing Mimi Anderson&#39;s World Record Run Across the USA</title>
      <link>/post/2018-10-19-assessing-mimi-andersons-world_record-run-part-i/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-10-19-assessing-mimi-andersons-world_record-run-part-i/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-are-key&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#get-to-the-point&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#full-disclosure&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#delayed-uploads&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-is-pausing-her-watch&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dodgy-fluctuations-in-cadence-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-running-in-the-185-195-steps-per-minute-range&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spoofed-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I ran most of this analysis one evening at the beginning of the week before the sad news that Mimi was going to give up her World Record attempt due to injury. Whether Mimi’s data is valid to claim the World Record is now moot, but the effect of the damage to her reputation is not. She is understandably devastated at the turn of events, and I can only hope to alleviate some of her grief with what I have written here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;On 7th September 2017, the &lt;a href=&#34;http://www.mimirunsusa.com&#34;&gt;Marvellous Mimi Anderson&lt;/a&gt; began a world record attempt to run across the United States of America. She is planning on running the 2,850 miles in 53 days to beat the current &lt;a href=&#34;http://www.guinnessworldrecords.com/world-records/fastest-crossing-of-america-(usa)-on-foot-(female)/&#34;&gt;Guinness World Record&lt;/a&gt; of 69 days 2 hours 40 minutes by Mavis Hutchinson from South Africa back in 1979.&lt;/p&gt;
&lt;p&gt;Mimi is somewhat of an institution in the UK, and can often be seen both running and crewing at many races in her trademark pink ensemble. I have run with her on many occasions, and have seen first hand her amazing ability at running stupid races, even going so far as to make them more stupid by running there and back again on races such as &lt;a href=&#34;http://www.badwater.com&#34;&gt;Badwater&lt;/a&gt;, the &lt;a href=&#34;http://www.gucr.co.uk&#34;&gt;Grand Union Canal Race&lt;/a&gt;, and &lt;a href=&#34;http://www.spartathlon.gr/en/&#34;&gt;Spartathlon&lt;/a&gt;. She holds records for running across Ireland and across the UK, so going for the USA record is a natural progression for somebody who loves hunting for ever bigger challenges.&lt;/p&gt;
&lt;p&gt;Unfortunately, Over the past year, we have seen some controversial stunt runs from the likes of &lt;a href=&#34;http://www.telegraph.co.uk/news/2016/05/13/chariots-of-fire-or-walter-mitty-doubts-raised-over-runners-incr/&#34;&gt;Mark Vaz&lt;/a&gt; (who drove 860 miles very slowly from Lands End to John O’ Groats to “smash” the previous running record), &lt;a href=&#34;http://www.derehamtimes.co.uk/news/how-controversy-hit-norfolk-man-s-world-record-run-attempt-1-5060552&#34;&gt;Dave Reading&lt;/a&gt; (who tried to do the same, but was “cyber-bullied” into giving up apparently), and Marathon Man UK himself &lt;a href=&#34;https://www.theguardian.com/sport/blog/2016/oct/02/robert-young-marathon-sponsor-stands-tall&#34;&gt;Robert Young&lt;/a&gt; (who sat in the back of an RV and was slowly driven across America until some geezers made him run a bit and he hurt himself).
I confess that when the Mark Vaz thing happened I could genuinely not believe that somebody would actually do that. I mean, what a waste of time - and for what?! I did not believe that somebody would go to that much effort for such a niche accolade, but that was exactly the point. To most people, running across the country is already pretty ridiculous, but they don’t have any baseline for what a good time would be. Is 7 days, 18 hours and 45 minutes good? Of course the running community knew that the time was bloody incredible for the top ultrarunners in the country, never mind an overweight window cleaner in a puffa jacket.&lt;/p&gt;
&lt;p&gt;When Robert Young attempted his transcontinental run, it was a different kettle of fish. Robert had developed somewhat of a following as an inspirational runner, having started running on a whim to complete 370 marathons in a year, beating Dean Karnazes “record” for running without sleeping, and winning the Race Across the USA. So he had a history of running long, but that didn’t stop questions from being asked. In particular, posters at the &lt;a href=&#34;http://www.letsrun.com&#34;&gt;LetsRun&lt;/a&gt; forum smelled a rat very quickly, after one of their own went out to run with him in the middle of the night only to find a slow moving RV with no runner anywhere to be seen. After a lot of vehement denial from the Robert Young fans, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=7355147&#34;&gt;sleuths over at LetsRun&lt;/a&gt; were able to provide enough evidence for Robert’s main sponsor, &lt;a href=&#34;https://www.skins.net&#34;&gt;Skins&lt;/a&gt;, to hire independent experts to prepare a report on whether or not any subterfuge had occurred. The results were not good for the Marathon Man UK brand (although to this day he denies any wrongdoings).&lt;/p&gt;
&lt;p&gt;The main take-home message from the Robert Young (RY) affair was that data transparency is key. RY refused to provide his Strava data for people to check, but to be fair he had a very good reason - he hadn’t got around to doctoring it yet. So anybody looking to take on this record would have to work very hard to ensure that their data was squeaky clean and stood up to scrutiny.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-are-key&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/h1&gt;
&lt;p&gt;In leading up to the challenge, Mimi gave several interviews where the RY affair and, in particular, the importance of data transparency were discussed. And it was pretty clear that this was most definitely clear to Mimi. She was aware of the LetsRun community, and the importance of making her attempt as free from controversy as possible. She had arranged for a &lt;a href=&#34;http://www.racedrone.net&#34;&gt;RaceDrone&lt;/a&gt; tracker to be used to follow her progress in real time, and would be using four different GPS watches (two at any one time for redundancy) uploading the data immediately following every run. In addition, as required by Guinness World Records, they would obtain witness testimonies along the way. Add in social media to provide a running commentary and it would seem to be foolproof.&lt;/p&gt;
&lt;p&gt;Unfortunately it did not work out that way. The LetsRun board was already lighting up with &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8347797&#34;&gt;posts&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8420326&#34;&gt;questioning&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8449925&#34;&gt;her&lt;/a&gt; attempt (EDIT while the posts were skeptical from the start, accusations of foul play did not begin until after the first few weeks with the tracker issues). In addition, a second runner - &lt;a href=&#34;http://www.sandyacrossamerica.com&#34;&gt;Sandra Villines&lt;/a&gt; (aka Sandra Vi) - was joining in the fun, and was planning to also attempt the record (albeit taking a different route) 4 days days later. Suddenly we had a race on.&lt;/p&gt;
&lt;p&gt;Unfortunately, within the first few weeks, questions surrounding Mimi’s attempts had surfaced, largely as a result of failures in the tracker. There are contradicting reports about what happened in this time, and I don’t pretend to understand all of them. Mimi’s crew claim that the issues were due to lack of coverage, whilst RaceDrone’s founder Richard Weremuik claims that the trackers were intentionally turned off. If Richard’s claims are correct, it raises a lot of serious concerns.&lt;/p&gt;
&lt;p&gt;In addition, there have been several other events that have received a lot of criticism, including against the reaction of Mimi’s fans to questions of her integrity (“trolls”, “haterz gonna hate”, etc.), and an incident whereby a stalker presumed (by Mimi’s crew) to be a poster on the LetsRun forum was arrested, despite no record of an arrest being found and no poster admitting to it (which they likely would). Since then, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&#34;&gt;whole run has been torn apart&lt;/a&gt;, and in particular she is now accused of faking the only source of data that is consistently available for review - &lt;a href=&#34;https://www.strava.com/athletes/13566252&#34;&gt;her Strava data&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-the-point&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/h1&gt;
&lt;p&gt;There are definitely things to do with this whole affair that I cannot comment on, such as the Race Drone incident and the arrest report. I also agree with many of the detractors that Sandy’s set up seems to be a far more open approach, and seems to be a good gold standard to use in the future. You will get no arguments from me that mistakes were made and perhaps things should have been done differently. But I genuinely believe that Mimi went out to the USA in the belief that what she had planned was foolproof and would cover all bases and supply the necessary evidence to convince anybody that might choose to look into it. Any mistakes were a result of the fact that Mimi has limited knowledge of technology - by her own omission she is a Luddite. Having said that, it is clear that the focus was on satisfying primarily the requirements of Guinness, which most runners in my experience consider to be very weak.&lt;/p&gt;
&lt;p&gt;However, what does concern me is the insinuation of fabricating her data, so I want to tackle some of these allegations to see if I can help defend her reputation. If I do find evidence of fabrication, so be it. But the idea that “data can be faked therefore we cannot believe any of it” is absurd. All data can be faked. Of course they can. But if we followed this impetus to discount all data out of hand, then scientific research would very quickly stall. What we have here is a peer review process - of course the burden of proof is on Mimi to provide evidence of her claims, but she has done that with daily Strava uploads (already a big improvement over Rob Young). If you subsequently suspect the data are forged, the onus is on you to show evidence of that.&lt;/p&gt;
&lt;p&gt;There are certainly some inconsistencies that need addressing and I am hoping that I can address some of these here. I don’t for one minute believe that I am covering all of the issues here. I am sure there are many more that will be pointed out to me (at over 200 pages, I really don’t feel like wading through the entire thread to pull everything out), but I figured I would make a start. This is all fairly rough and these are basic analyses conducted over a very short period of time, but I hope to look into things using some more statistical methods in a follow-up post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-disclosure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/h1&gt;
&lt;p&gt;In full disclosure, I consider Mimi to be a friend. I have run with her many times, including running over 50 miles together on a recce for the &lt;a href=&#34;http://www.vikingwayultra.com&#34;&gt;Viking Way&lt;/a&gt;, and have seen first hand that she is an incredibly accomplished runner who has achieved amazing things. I don’t expect this to mean anything, and completely understand that plenty of people came out saying similar things for Robert Young, I just wanted to lay my cards in the table. I am, however, typically objective when it comes to data, so I am trying to look at this without letting too many of my biases interfere. For the record, I think that her pedigree and the changes seen in her body over the last few weeks should themselves clearly distinguish her from Rob Young. She is clearly doing &lt;em&gt;something&lt;/em&gt; out there and not just riding in an RV.&lt;/p&gt;
&lt;p&gt;Having said that, I am not of the opinion that anybody that questions these runs (or indeed anything) are haterz and trolls. Extraordinary claims require extraordinary evidence, and if you are not prepared to provide that and have it interrogated then you shouldn’t be doing it. With fame comes a loss of anonymity. I believe that mistakes were made at the start of this run, and I believe that the impulse to fight back against criticism with a similar tact used by Mark Vaz, Robert Young and Dave Reading was the wrong choice. But then I am a naive fool who thinks that everybody is reasonable and open to logical discourse - I suspect I am about to be schooled on this thought.&lt;/p&gt;
&lt;p&gt;In all honesty I have stayed away from this whole debacle for exactly that reason. I do not like the “them vs us” mentality that seems to crop up in all of these discussions. Be that Brits vs Americans, URC vs LR, whatever. I think that the LetsRun community have done an amazing job at routing out cheats over the years, and without them many people would be reaping the benefits of false claims. I have no problem with witch hunts. Witch hunts are only a problem if you don’t live in a world where witches demonstrably exist. I don’t want to feed into that, I don’t want to make enemies here - I just want to help a friend by providing an alternative perspective.&lt;/p&gt;
&lt;p&gt;My aim here then is to look (hopefully) objectively at some of the claims against Mimi in her transcon attempt. I work in a data-heavy scientific discipline, and I believe that I can remain objective in this, although it should be fairly clear already that I know what I am hoping to see. But believe me when I say that if I find something that I do not like I will not hide it. I do not claim anything I say is any more valid than what anybody else says, and I do not want confirmation bias to creep in from Mimi’s fellow supporters. I just want all of the facts to be available to allow people to make an informed assessment. If anyone disagrees with any of my conclusions, or if you identify errors, by all means get in touch and I will try and follow up.&lt;/p&gt;
&lt;p&gt;I decided to write this up as a blog post as the message that I put together for the LetsRun forum got a bit ridiculous, and I thought that this way I could attach and annotate my figures, flesh out my thoughts, and importantly include my code for full transparency.&lt;/p&gt;
&lt;p&gt;A lot of the data that I am showing here is based on runs between 1st October and 7th October. I chose these as these overlap with the faked data generated by &lt;code&gt;Scam_Watcheroo&lt;/code&gt; &lt;del&gt;a user on LetsRun (who also runs the &lt;a href=&#34;https://www.marathoninvestigation.com&#34;&gt;Marathon Investigation&lt;/a&gt; website)&lt;/del&gt; (EDIT) has looked at a lot of these data and made many of the claims of faked data. In addition, he was able to &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8462935&#34;&gt;generate spoofed data files&lt;/a&gt; for a fake world record run to show how easy it is to do, which I will incorporate into these analyses.&lt;/p&gt;
&lt;p&gt;I have also included a short run that Mimi did before beginning her transcon run, which is the only other run available on Strava (presumably this was done to test the data upload). I figured that this could be useful as a baseline of her “normal” running, but of course you could always argue that this was also fabricated.&lt;/p&gt;
&lt;p&gt;I obtained the &lt;code&gt;.gpx&lt;/code&gt; files directly from Strava by using &lt;a href=&#34;https://mapstogpx.com/strava/&#34;&gt;a tool&lt;/a&gt; that is able to slurp the data directly from the Strava session webpage. I can obviously repeat any of this for other data sets, but I had to start somewhere and my free time to look at these things is limited.&lt;/p&gt;
&lt;p&gt;For each day in this period, I downloaded runs for Mimi, Sandra, and the spoofed files. I also downloaded a few of my own runs (much shorter) as a baseline as I am quite confident that these files are genuine. My thesis is that the spoofed data should be identifiable as such, while the other data sets should (hopefully) stand up to scrutiny. Note that Sandra’s data are single runs per day, whereas Mimi uploads two per day.&lt;/p&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.r-project.org&#34;&gt;R&lt;/a&gt;, which is a freely available and well maintained statistical programming language, for these analyses. I haven’t had time to do too much yet, so here I am concentrating on the cadence data as that seems to be the most contentious point at the moment. In a follow up post I will look at the whole run thus far and look at more factors such as the distance traveled and correlations with terrain (which I haven’t even touched so far).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/h1&gt;
&lt;p&gt;First of all let’s load in the packages I will be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(XML)
library(plyr)
library(geosphere)
library(pheatmap)
library(ggplot2)
library(benford.analysis)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I will load in the &lt;code&gt;.gpx&lt;/code&gt; files. They are basically &lt;code&gt;.xml&lt;/code&gt; (Extensible Markup Language) files, so a bit of parsing is required to get them into a usable &lt;code&gt;data.frame&lt;/code&gt; format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## I/O directory
root_dir &amp;lt;- &amp;quot;../../static/post/2017-10-18-assessing-Mimi-Andersons-World_Record-run_files/&amp;quot;

## Load in the gpx data for each run
all_gpx &amp;lt;- list()
for (n in c(&amp;quot;Mine&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;MimiAnderson&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  dirname &amp;lt;- paste0(root_dir, n)
  all_fnames &amp;lt;- list.files(dirname)
  for (fname in all_fnames) {
    
    ## Load .gpx and parse to data.frame
    gpx_raw     &amp;lt;- xmlTreeParse(paste0(dirname, &amp;quot;/&amp;quot;, fname), useInternalNodes = TRUE)
    rootNode    &amp;lt;- xmlRoot(gpx_raw)
    gpx_rawlist &amp;lt;- xmlToList(rootNode)[[&amp;quot;trk&amp;quot;]]
    gpx_list    &amp;lt;- unlist(gpx_rawlist[names(gpx_rawlist) == &amp;quot;trkseg&amp;quot;], recursive = FALSE)
    gpx         &amp;lt;- do.call(rbind.fill, 
                           lapply(gpx_list, function(x) as.data.frame(t(unlist(x)), stringsAsFactors=F)))
    
    ## Convert cadence and GPS coordinates to numeric
    for (i in c(&amp;quot;extensions.cadence&amp;quot;, &amp;quot;.attrs.lat&amp;quot;, &amp;quot;.attrs.lon&amp;quot;)) {
      gpx[[i]] &amp;lt;- as.numeric(gpx[[i]])
    }
    
    ## Convert cadence to steps per minute
    gpx[[&amp;quot;extensions.cadence&amp;quot;]] &amp;lt;- gpx[[&amp;quot;extensions.cadence&amp;quot;]] * 2
    
    ## Convert time to POSIXct date-time format
    gpx[[&amp;quot;time&amp;quot;]] &amp;lt;- as.POSIXct(gpx[[&amp;quot;time&amp;quot;]], format=&amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;)
    
    ## Calculate the time difference between data points
    gpx[[&amp;quot;time.diff&amp;quot;]] &amp;lt;- c(0, (gpx[-1, &amp;quot;time&amp;quot;] - gpx[-nrow(gpx), &amp;quot;time&amp;quot;]))
    
    ## Calculate the shortest distance between successive points (in miles)
    gpx[[&amp;quot;dist.travelled&amp;quot;]] &amp;lt;- c(0, 
                                 distHaversine(gpx[-nrow(gpx), c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               gpx[-1, c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               r = 3959)) # Radius of earth in miles
    
    ## Save to main list
    all_gpx[[n]][[gsub(&amp;quot;\\.gpx&amp;quot;, &amp;quot;&amp;quot;, fname)]] &amp;lt;- gpx
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s consider some of the specific criticisms being made against Mimi.&lt;/p&gt;
&lt;div id=&#34;delayed-uploads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/h2&gt;
&lt;p&gt;One of the criticisms that comes up regarding Mimi’s practice is that it sometimes takes a long time for the data to be uploaded to Strava. I believe that it is typically up within a couple of hours (EDIT - there were also times when the uploads were not made for several days which is obviously more than a delay in tranferring the data), but many people suggest that anything longer than 15 minutes is unacceptable as it provides time to doctor the data. I mean, I guess that this is true, but it is my understanding that, given Mimi’s lack of technological prowess, her crew is using the method that requires the least amount of knowledge; that being syncing to a phone via Bluetooth, which will then upload to Movescount when there is a wifi or mobile data signal. I do this sometimes with my own runs and it takes bloody ages to sync, and that doesn’t take into account the time to then takes to upload it from the phone to Movescount, dealing with blackspots, etc. So it does not surprise me in the least that she rarely uploads things within 15 minutes of stopping. This is not evidence based by any means, just an observation from my own experience (and something others have pointed out). Is this good practice for somebody out to claim a world record? Perhaps not. Is it evidence of subterfuge? I don’t know, but personally I doubt it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-is-pausing-her-watch&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/h2&gt;
&lt;p&gt;In page 201 of the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&amp;amp;page=200&#34;&gt;LetsRun thread&lt;/a&gt;, user &lt;code&gt;So Far Today&lt;/code&gt; says the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elapsed time is total time from start of the day until end of the day. Sandy’s Strava is based on total elapsed time. Mimi’s excludes lunch breaks, and it appears it it also excludes other mini-breaks. If you want to figure out actual running time for Sandy remove the lunch breaks from the total elapsed time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, these guys have been looking into this in a heck of a lot more detail than I have, so I apologise if I have got this wrong here or misunderstood. But the idea that Mimi’s data excludes mini-breaks disagrees with something that I noticed right at the start of this analysis. Let’s look at the time difference between successive data points for Mimi’s data using the &lt;code&gt;table()&lt;/code&gt; function which will simply count the number of occurrences of each time delay between data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;MimiAnderson&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_170828_preUSA&lt;/p&gt;
&lt;p&gt;0 1 2 5
1 4160 4 1&lt;/p&gt;
&lt;p&gt;$MimiAnderson_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 25883    11 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 26174    31 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 29046    44     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 10926    12 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 30896     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3     4 
1 24567    19     1     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28343     6 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 18492    42 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 32400    34     2 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 26461     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     7 
1 27747    42     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 21215    36     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 32114    27 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171012_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28411    17 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So every run has one &lt;code&gt;0&lt;/code&gt; (which is a result of the way that I have calculated the time difference, so the first value is always 0), a few sporadic &lt;code&gt;2&lt;/code&gt;-&lt;code&gt;7&lt;/code&gt; sec intervals (presumably due to brief signal drop out and the like), but the vast majority are &lt;code&gt;1&lt;/code&gt; sec. This is because Mimi has her watch set to 1 sec recording, and leaves it on for the duration of the run. From this I suggest that the assertion made that Mimi stops her watch for lunch breaks etc. is false.&lt;/p&gt;
&lt;p&gt;My watch is set to the same sampling rate, and I see exactly the same thing with my data (albeit with fewer errant digits):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Mine&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$mine_170514&lt;/p&gt;
&lt;p&gt;0 1 2
1 9790 1&lt;/p&gt;
&lt;p&gt;$mine_170521&lt;/p&gt;
&lt;p&gt;0 1 2
1 6860 1&lt;/p&gt;
&lt;p&gt;$mine_170525&lt;/p&gt;
&lt;p&gt;0 1
1 3056&lt;/p&gt;
&lt;p&gt;$mine_170603&lt;/p&gt;
&lt;p&gt;0 1
1 5531&lt;/p&gt;
&lt;p&gt;$mine_170618&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 11297 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, however, when we look at Sandra’s data we see a completely different result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;SandraVi&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$SandraVi_171002&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
1 82 79 77 61 57 48 51 65 126 332 348 335 160 51 14 4 1
18 21 22 29 30 32 35 38 39 43 47 48 51 54 55 61 63 66
1 1 2 1 1 1 1 1 3 1 1 1 2 1 1 1 1 1
86 89 133
1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171003&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 243 238 233 187 192 172 210 305 1459 1424 401 142 54 40
15 16 17 18 19 20 21 22 23 24 25 27 28 30 31
17 7 1 2 4 1 4 2 1 2 2 1 1 1 1
33 34 35 36 37 39 40 41 44 45 46 47 49 50 51
2 1 1 1 1 1 2 1 1 1 1 1 1 1 1
52 53 55 57 58 59 60 63 64 65 66 73 79 82 83
2 3 3 1 1 1 1 1 1 1 1 1 1 1 1
88 89 97 100 102 113 118 119 133 136 162 166 168 183 223
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
249 266
1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171004&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 299 425 356 333 267 311 311 697 1708 1022 327 157 76 33
15 16 17 18 19 20 21 22 23 24 26 27 28 29 32
26 15 6 6 1 1 1 3 1 3 1 1 3 1 1
33 35 36 37 39 41 43 44 49 50 51 52 55 56 57
2 2 1 1 4 1 1 1 1 1 2 1 1 2 1
58 59 61 62 64 67 70 74 75 84 88 94 98 130 149
1 1 2 1 1 2 1 1 1 1 1 3 1 1 1
169
1&lt;/p&gt;
&lt;p&gt;$SandraVi_171005&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 173 167 157 133 156 129 158 400 1891 1393 259 109 53 55
15 16 17 18 19 20 21 22 23 24 26 27 29 30 32
16 6 5 2 1 5 1 1 2 1 1 1 1 1 1
33 34 35 36 37 39 41 44 46 49 50 52 61 68 74
2 1 1 1 2 1 1 1 1 1 3 1 1 2 1
75 76 93 128 138 140 145 153 163 233
1 1 1 1 2 1 1 2 2 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171006&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 112 128 123 132 115 122 101 291 1658 1696 358 85 42 33
15 16 17 18 19 20 24 27 28 32 35 37 42 46 49
8 5 4 2 1 2 1 2 1 2 1 1 1 1 2
51 57 58 60 65 67 76 77 78 86 88 90 96 98 99
1 1 1 1 1 1 1 1 1 1 2 1 1 1 1
106 108 109 126 128 129 147 164 173 196 205
1 1 1 1 1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171007&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 57 85 83 73 81 74 64 197 2038 1707 275 95 64 39
15 16 17 18 22 23 27 28 31 32 38 39 40 42 45
17 11 2 1 2 1 1 5 2 1 1 1 3 3 1
46 49 53 54 55 60 65 67 77 84 109 115 126 128 149
2 1 1 2 1 1 1 1 1 1 1 1 1 1 1
151 154 157 181 182 189 225
1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;There is no one time difference that stands out as the most common. Instead, the time differences between her data points span a large range, with the majority being about &lt;code&gt;8&lt;/code&gt;-&lt;code&gt;11&lt;/code&gt; secs apart. I suspect that this means that Sandra’s watch is set to sample every 10 seconds or so. In addition, there are a lot more longer pauses seen, sometimes up to 4 minutes. Whether this is a result of random fluctuations due to the higher sampling rate, or is the result of pausing the watch at certain times, I do not know. I am most definitely not suggesting there is anything wrong with this, I just think that the better approach is to leave it running the whole time, and it is Mimi who is doing this and not Sandra.&lt;/p&gt;
&lt;p&gt;Note also that this means that Sandra’s data therefore has an order of magnitude fewer data points than Mimi’s does, since Mimi’s data is less smoothed out. This can be seen if we calculate the number of data points for each run and then show the distribution for Mimi and Sandra separately (note that each of Mimi’s runs is actually only half of her distance for the day):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;)) {
  plot_dat[[n]] &amp;lt;- data.frame(Name = n,
                              nPoints = sapply(all_gpx[[n]], FUN = nrow))
}
plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
ggplot(aes(x = nPoints, fill = Name), data = plot_dat) + 
  geom_density(alpha = 0.25)                           +
  ggtitle(&amp;quot;Number of Data Points Per Run&amp;quot;)             +
  xlab(&amp;quot;Number of Data Points&amp;quot;)                        + 
  ylab(&amp;quot;Density&amp;quot;)                                      +
  theme(axis.title = element_text(size = 10), 
        plot.title = element_text(size = 15, 
                                  face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/mimi_vs_sandra_data_points-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’m not sure whether the people on LetsRun have been working on the assumption that both data sets were using the same parameters, but it is pretty clear to me that the sampling rate at least is different between the two runners. There may also be differences in the accuracy - perhaps the crews could confirm one way or another. Whilst the overall approach taken by Sandra is clearly the better of the two, the 1 sec sampling rate used by Mimi is the better option for the Strava data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dodgy-fluctuations-in-cadence-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/h2&gt;
&lt;p&gt;One issue that has been raised is the fact that in most of Mimi’s runs, we occasionally see severe fluctuations in the cadence, which spikes up above 200 at times. This is absolutely true, which can be seen when we plot the cadence values over time. The following function will plot the raw cadence values against the cumulative time (in secs) from the start of each run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time &amp;lt;- function (n, smooth = FALSE) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (smooth) { ## Smooths the data if requested - see below
      cad &amp;lt;- runmed(all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]], 11)
    } else {
      cad &amp;lt;- all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]
    }
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Time    = cumsum(all_gpx[[n]][[r]][[&amp;quot;time.diff&amp;quot;]]),
                                Cadence = cad)
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  ggplot(aes(x = Time, y = Cadence, color = Run), data = plot_dat) + 
    geom_point()                                                   + 
    facet_grid(Run ~ .)                                            + 
    ggtitle(paste(n, &amp;quot;Cadence Over Time&amp;quot;))                         + 
    xlab(&amp;quot;Time (sec)&amp;quot;)                                             + 
    ylab(&amp;quot;Cadence (spm)&amp;quot;)                                          +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))                + 
    ylim(0,500) # Limits the plot to a maximum of 500 which will exclude a small number of outliers for Mimi
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can look at Mimi’s raw cadence over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 120 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/mimi_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there is no denying that these very high cadence values do exist in Mimi’s data. If, however, we look at Sandra’s data we do not see as many of these fluctuations. However, fluctuations are indeed still present, particularly for the shorter day on 2nd October, although they are nowhere near as high as those of Mimi (250 rather than 500):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/sandra_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that here we can make out the lunch breaks in Sandra’s data, whereas Mimi has her runs split into morning and afternoon.&lt;/p&gt;
&lt;p&gt;However, as discussed above, Mimi’s data is much deeper than Sandra’s. Sandra’s data has already undergone some smoothing, so it is likely that these blips are cancelled out by smoothing over a 10 second interval. Indeed, if we smooth Mimi’s data using a running median over an 11 sec window (which replaces the data points with a running average of the data point with the 5 data points either side) to approximate the 10 sec capture, we indeed see a much smoother distribution with these extreme values reduced to be more in keeping with what we see for Sandra.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;, smooth = TRUE)
## Warning: Removed 85 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/mimi_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It would appear that these very high fluctuations are a result of the increased sampling rate, although I do note that I do not see these sorts of fluctuations in my own 1 sec capture data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/my_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This may simply be due to the fact that I am not running through so many different areas, many of which may have different GPS signals that affect the capture. Another possibility is that the accuracy of our watches is set to different modes. Mine is set to “Best”, but I have no idea what Mimi’s is set to. One idea that I had was to ask her to set one of her watches to 10 sec capture and upload both in parallel at the end of one of her runs. However, unfortunately it seems that this is no more an option. I am going to hunt through to find some longer runs in my personal data to see if this crops up in any of my more remote jaunts, but for now I don’t have an answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-running-in-the-185-195-steps-per-minute-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/h2&gt;
&lt;p&gt;Continuing with the cadence data, another issue that has cropped up several times is the fact that Mimi regularly runs in the 185-195 spm. Indeed, if we look at the distribution of the cadence in a histogram rather than looking at it over time, this certainly seems to be the case.&lt;/p&gt;
&lt;p&gt;The following function will plot the above data as a series of overlaid density plots (note that I am smoothing the density estimates here slightly to make the overall distribution clearer and less spiky for the samples with fewer data points):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density &amp;lt;- function (n) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (r == &amp;quot;MimiAnderson_170828_preUSA&amp;quot;) next # Skip the pre-transcon run
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Cadence = all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]])
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  vwalk    &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 100 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 150]) ## Walking
  vrun     &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 150 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 200]) ## Running
  ggplot(aes(x = Cadence, color = Run), data = plot_dat) + 
    geom_density(alpha = 0.25, adjust = 3)               + 
    xlim(0,300)                                          + 
    ggtitle(paste(n, &amp;quot;Cadence Distribution&amp;quot;))            + 
    xlab(&amp;quot;Cadence (spm)&amp;quot;)                                + 
    ylab (&amp;quot;Density&amp;quot;)                                     +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))      + 
    geom_vline(xintercept = vwalk)                       + 
    geom_vline(xintercept = vrun)                        +
    annotate(&amp;quot;text&amp;quot;, x = vwalk, y = 0.05, 
             angle = 90, label = paste(vwalk, &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)                     +
    annotate(&amp;quot;text&amp;quot;, x = vrun,  y = 0.05, 
             angle = 90, label = paste(vrun,  &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am going annotate the peaks of these plots using a very basic method of taking the modal value (the one that occurs the most) over the entire data set for the walking and running distributions (very roughly defined, but as long as the modal value lies ion the range it should give the “correct” answer) . To do this, however, I need a function to calculate the mode since one does not exist in base R (for some odd reason):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mode &amp;lt;- function(v) {
   uniqv &amp;lt;- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s see the distribution for Mimi:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 244 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/mimi_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So Mimi runs with a cadence of around 134 spm for running and 182 spm for running. Now let’s look at Sandra’s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/sandra_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sandra has generally lower cadence of 120 spm for walking, and 170 spm for running. She also appears from this to walk a lot less than Mimi, who seems to do a fairly even split between running and walking in general. In addition, the variation of Mimi’s running cadence is much higher than Sandra’s, so it appears that Sandra tends to run at a relatively constant cadence with a small amount of walking, whereas Mimi’s is much more variable and seems to be split in a 50:50 run/walk. Together with the longer differences between successive time-points, this may indicate that Sandra’s watch is set to pause automatically below a certain speed.&lt;/p&gt;
&lt;p&gt;I also decided to look at my own data to see what that looked like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/my_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I also run with a fairly high cadence (just lower than Mimi’s but not dissimilar), and see more variation than Sandra. Now obviously I am not running across a continent in these runs - I am usually running with a belligerent dog who insists on stopping to sniff every bloody tree on the way. But it is not too dissimilar, and I see the distribution spreads out over 200 for some of the readings just like with Mimi. I’m an okay runner - probably not particularly good compared to many of the posters on LetsRun, but I do okay at shorter stuff and longer stuff. But it’s just a hobby for me, so I’m perfectly happy to self-associate as a hobby jogger. I don’t really know much about cadence, so I’m not sure if averaging 180+ is high or not? If nobody had suggested this was “garbage” and unbelievable, I would just assume that Mimi had a higher than normal cadence, similar to my own. I am a forefoot runner, and I think that Mimi is as well, and I believe that higher cadence tends to go hand in hand, but I am happy to bow to the experience of people more knowledgeable than myself in the matter.&lt;/p&gt;
&lt;p&gt;To get an idea of the level of variation in the data (specifically the running cadence), let’s look at some aspects of that main distribution (excluding the outliers):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;)) {
  cad &amp;lt;- unlist(lapply(all_gpx[[n]], FUN = function (x) x[[&amp;quot;extensions.cadence&amp;quot;]]))
  cad &amp;lt;- cad[cad &amp;gt; 150 &amp;amp; cad &amp;lt; 250]
  cat(sprintf(&amp;quot;%12s: mode   = %d\n&amp;quot;, n, Mode(cad)))
  cat(sprintf(&amp;quot;%12s: mean   = %.2f\n&amp;quot;, n, mean(cad)))
  cat(sprintf(&amp;quot;%12s: median = %d\n&amp;quot;, n, median(cad)))
  cat(sprintf(&amp;quot;%12s: SD     = %.2f\n&amp;quot;, n, sd(cad)))
  cat(sprintf(&amp;quot;%12s: SEM    = %.3f\n&amp;quot;, n, sd(cad)/sqrt(length(cad))))
  cat(&amp;quot;\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MimiAnderson: mode = 182
MimiAnderson: mean = 181.56
MimiAnderson: median = 182
MimiAnderson: SD = 11.07
MimiAnderson: SEM = 0.026&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SandraVi: mode   = 170
SandraVi: mean   = 171.13
SandraVi: median = 170
SandraVi: SD     = 4.54
SandraVi: SEM    = 0.029

    Mine: mode   = 178
    Mine: mean   = 177.96
    Mine: median = 178
    Mine: SD     = 7.53
    Mine: SEM    = 0.042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So clearly the standard deviation (SD) is much higher for Mimi’s data, but the standard error of the mean (SEM) is actually pretty comparable. SD and SEM, whilst both estimates of variability, tell you different things. The standard deviation is simply a measure of how different each individual data point is from the mean. It is descriptive of the data at hand. The SEM on the other hand is a measure of how far the mean of your sample is likely to be from the true population mean (under the assumption that each run is a random sampling of cadence values given Mimi’s true “normal” cadence). As your sample size increases, you more closely estimate the true mean of the population. This tells us that there is high variability in the sampling of cadence values for Mimi, but the precision is comparable with Sandra’s. This suggests nothing of whether the mean itself is actually believable of course, it is just worth noting the benefits of the increased sampling in these data.&lt;/p&gt;
&lt;p&gt;So my overall feeling is that, whilst high, this was just the natural running gait of Mimi. Given recent events, this entire post ended up being highly expedited so that &lt;em&gt;something&lt;/em&gt; was out there to provide a counter point to the accusations that have been made about data forgery and cheating, so in a rushed effort I looked around for some video of Mimi running to get an idea of her natural cadence. I found &lt;a href=&#34;https://www.youtube.com/watch?v=2eGb_goiPfc&#34;&gt;this video of her running at the end of her 7-day treadmill record&lt;/a&gt;. For the 18 seconds between 0:20 (when she begins to run properly) and 0:38 (when the camera pans away) I count 27-28 swings of her left hand/steps with her right foot, which would equate to a cadence of 180-187. Similarly, for the 16 seconds between 0:59 and 1:15, I count 24-25 swings/steps , which also equates to a cadence of 180-187. I’m not saying this is definitive proof, but this is at least evidence of her running with cadence similar to her average cadence across the USA, even at the end of 7 days on a treadmill. Adrenelin and a “sprint finish” mentality may play a role in achieving this as well of course. I would like to see more evidence of her running, and hopefully we will see some of that from the film crew that was with Mimi.&lt;/p&gt;
&lt;p&gt;So here I have shown that, yes Mimi runs with a higher cadence than Sandra, but there is evidence that this is simply her natural gait. As to the fact that she regularly runs in the 185-195 range; well yes she does, but so do I. And I am no elite, particularly for these particular runs which are fairly perambulatory if I am honest. I can assure you that I have not doctored these data to be this mediocre. You can look through and even work out the points where my dog stopped to piss up a tree if you like. It’s not proof, but it is evidence.&lt;/p&gt;
&lt;p&gt;I also wanted to look a bit into how the cadence actually corresponds to the speed at which the women are running. Below is a distribution plot showing the pace at each time point for each of the data sets considered here. Notice that I am excluding the data points where the runners are not moving to avoid divide by 0 errors in the pace calculation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_cor_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  cor_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    cor_dat[[r]]          &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;time.diff&amp;quot;, &amp;quot;dist.travelled&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
    cor_dat[[r]][[&amp;quot;Run&amp;quot;]]  &amp;lt;- r
    cor_dat[[r]][[&amp;quot;Pace&amp;quot;]] &amp;lt;- (cor_dat[[r]][[&amp;quot;time.diff&amp;quot;]]/60)/(cor_dat[[r]][[&amp;quot;dist.travelled&amp;quot;]])
  }
  cor_dat &amp;lt;- do.call(rbind.data.frame, cor_dat)
  cor_dat[[&amp;quot;Name&amp;quot;]] &amp;lt;- n
  cor_dat           &amp;lt;- subset(cor_dat, dist.travelled != 0)
  all_cor_dat[[n]]  &amp;lt;- cor_dat
}
all_cor_dat &amp;lt;- do.call(rbind.data.frame, all_cor_dat)
ggplot(aes(x = Pace, color = Name), data = all_cor_dat) +
  geom_density(alpha = 0.25) +
  xlim(0, 20) +
  xlab(&amp;quot;Pace (min/mile)&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;) +
  ggtitle(&amp;quot;Pace Comparison Between Data Sets&amp;quot;) +
  theme(axis.title = element_text(size = 10),
        plot.title = element_text(size = 15, face = &amp;quot;bold&amp;quot;))
## Warning: Removed 53290 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/cadence_vs_speed-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, we can see that there is a big difference in how the women are approaching the race. As noted before, Mimi runs in a fairly even 50:50 split of running and walking. This graph confirms that with a fairly even split between faster running of about 8.5 mins/mile and slower walking of about 11 mins/mile. Sandra on the other hand appears to be very steady in her approach, moving consistently at a 170 spm cadence run of about 11.5 mins/mile. This was earlier in the run and no doubt changed over time as Sandra began to close in on Mimi over the past week. My runs are predominantly spent jogging at around 8 mins/mile (with the occasional downhill thrown in for fun). The distribution for the fake data however does not follow the same type of distribution as the other runs (with a clearly delineated multimodal distribution for run/walk/sprint segments), and again stands out when compared with the ostensibly genuine data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spoofed-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/h2&gt;
&lt;p&gt;So now I am getting to the nitty gritty of this post. The main accusation that I am attempting to quash is that of doctoring of the data. I have no answers regarding other perceived issues with the run, but the doctoring accusation I believe is a step too far. I have never denied that it would be possible to spoof the data. Of course it would. They are raw text files containing numbers - nothing more impressive than that. I did however think that spoofing it through Movescount would be very difficult, but it seems that I was wrong about that. It’s not simple to do, but it is doable with a little bit of know-how.&lt;/p&gt;
&lt;p&gt;However, I do believe that it would be impossible to generate spoofed data that did not stand out as such when compared with genuine data. Faking data is notoriously hard. That’s not to say that people don’t do it all of the time, and sometimes it takes a while to pick up on. But I think that creating data out of thin air that also matched with what is going on with the tracker (I appreciate people have issues with the tracker, but I’m not getting into that), what is going on with reports from the crew, matched with environmental effects and the terrain that she was running over, what will ultimately come out from the film crew, and importantly what is self consistent, would be near impossible to manage. The cadence and times would have to make sense given the position, terrain and environmental effects into account. LetsRun user &lt;code&gt;Scam_Watcheroo&lt;/code&gt; developed a tool to spoof the data, but he had the benefit of being able to track the things that might give the data away in advance. Mimi would have had to develop her method (or more accurately get somebody else to develop the method) blind, with no idea what sort of things might show it up as being faked. Sounds incredibly risky to me. So in thus section, I wanted to look at a few things to see if the different data sets stand up to scrutiny.&lt;/p&gt;
&lt;p&gt;I am only touching the surface here, and I am looking into some more in depth methods to run statistical tests over the entire data set so far to check that the data are consistent and show the patterns one would expect. My hope in advance was that doing this would highlight the faked data as such. So to start with, I am not looking at consistency between the data sets, I am merely looking at the raw cadence data to look at a few potential things that might highlight anything that looked incongruous.&lt;/p&gt;
&lt;p&gt;First of all I went back to simply looking at the time differences between the data points. For my data and Mimi’s data, they are almost all 1s differences, but there is the occasional blip (presumably when it is not able to update with the satellite straight away) leading to a few counts of around 2-7 secs. The spoofed data has none of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Fake&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;25895 25894&lt;/p&gt;
&lt;p&gt;$Fake_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;26206 26205&lt;/p&gt;
&lt;p&gt;$Fake_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;29092 29091&lt;/p&gt;
&lt;p&gt;$Fake_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 19626 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 27499 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most recent ones (7th October) which were I think generated from scratch are ALL 1s differences, whilst the 2nd October ones (which were generated based on fiddling with Mimi’s uploads) were split half and half between 0s and 1s (a 0.5s sampling rate perhaps?). Either way, they do not have these little blips – the faked data appear to be too perfect. Being able to account for this and other such data imperfections heuristically (especially without knowing ahead of time that one would need to) would be bloody difficult and very very risky in my opinion.&lt;/p&gt;
&lt;p&gt;I am also looking at how well the spoofed data stand up to scrutiny using some other methods. One obvious test would be to see whether the cadence data obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Benford%27s_law&#34;&gt;Benford’s Law&lt;/a&gt;, which shows that the first digits (and indeed second digits, third digits, etc.) have a unique logarithmic distribution such that smaller numbers are more likely than bigger numbers. Notice here that I am looking at &lt;em&gt;half&lt;/em&gt; of the cadence, since the actual reported data in the &lt;code&gt;.gpx&lt;/code&gt; file was doubled to give the cadence in spm. However, in this case the first digits are somewhat constrained, since the cadence is typically in a very narrow range resulting in a huge proportion of 8s and 9s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/benford-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about if we look at the second digit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(as.numeric(substr(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, 2, 2)), number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/benford_2digit-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is now approaching a more standardised distribution. It does not appear to follow Benford’s Law, but instead these data appear to be somewhat uniformly distributed. One idea that I have looked at is whether the trailing (and thus least significant) digits of the cadence data follow any particular distribution. Perhaps one might imagine that they should follow &lt;em&gt;some&lt;/em&gt; distribution, such as the more uniform distribution seen above. Again remember here that I am plotting the raw cadence data, which is &lt;em&gt;half&lt;/em&gt; of the cadence values reported in the distribution plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Look at the final digit of the cadence data
all_digit_counts &amp;lt;- list()
all_digit_percent &amp;lt;- list()
for (n in names(all_gpx)) {
  all_digit &amp;lt;- matrix(0, ncol = 10, nrow = length(all_gpx[[n]]), dimnames = list(names(all_gpx[[n]]), as.character(0:9)))
  for (r in names(all_gpx[[n]])) {
      digit &amp;lt;- gsub(&amp;quot;^\\d*(\\d)$&amp;quot;, &amp;quot;\\1&amp;quot;, all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]/2)
      all_digit[r, ] &amp;lt;- table(digit)[as.character(0:9)]
  }
  all_digit_counts[[n]]  &amp;lt;- all_digit
  all_digit_percent[[n]] &amp;lt;- 100*all_digit/rowSums(all_digit)
}
  
## Plot heatmap
hm_dat &amp;lt;- do.call(rbind.data.frame, all_digit_percent)
rownames(hm_dat) &amp;lt;- gsub(&amp;quot;^.*\\.&amp;quot;, &amp;quot;&amp;quot;, rownames(hm_dat))
pheatmap(hm_dat, cluster_rows = FALSE, cluster_cols = FALSE, main = &amp;quot;Cadence - trailing digit percentage&amp;quot;)#, display.numbers = FALSE,  annotation_legend = TRUE, cluster_cols = TRUE, show_colnames = TRUE, show_rownames = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-assessing-Mimi-Andersons-World_Record-run-part-I/index_files/figure-html/final_digit_dist-1.png&#34; width=&#34;1800&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows the distribution of the percentage of trailing digits seen amongst the different data sets – Mimi’s, Sandra’s, the spoofed data, and some of my own runs. The colour of the heatmap indicates the percentage of times that digit is seen in the final position, with blue being less often and red being more often. In general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mine seem to be quite uniformly distributed (mainly blue)&lt;/li&gt;
&lt;li&gt;Mimi’s are pretty uniform (except for the pre-USA run that I put in as well) with the exception of a regular depletion of 2s and 5s&lt;/li&gt;
&lt;li&gt;Sandra’s seem to show a depletion of 1s and 9s, and enrichment of several digits in most runs (but nothing consistent)&lt;/li&gt;
&lt;li&gt;The fake data however seem very consistent in their prevalence of 6s and 7s (and to a lesser extent 4s and 8s).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is not really enough data here to identify a pattern, but from what is here the spoofed data stands out with a distribution that is different from that seen with my own data. Mimi’s is actually the most alike to data that I know to be real, although the depletion of certain digits is quite odd. But then the same is true for Sandra’s data as well to a greater degree (albeit different numbers). I plan to look into this in more detail using more data, including all of Sandra’s and Mimi’s runs, and a lot more genuine data taken from Strava as a base line to see if this distribution holds.&lt;/p&gt;
&lt;p&gt;Obviously none of this “proves” these data are not fabricated. That is impossible. I do however thus far see no sufficient evidence to suggest that these data are not real (for both runners, although Sandra was never on “trial” here). And really I have only scratched the surface on how to test the validity of these data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;I am very saddened about what has happened to Mimi on this journey. There are questions that I hope will be answered regarding certain aspects of the run, and I’m sure that these by themselves are enough to convince some people of wrong-doings. But I truly do not believe that Mimi has set out to flim flam, bamboozle or otherwise beffudle people into believing that she ran across America when she didn’t. I wrote this post before Mimi announced her intentions to stop, but the damage that she has done to herself must surely be evidence that she is doing it. And if you accept that the Strava data are genuine, there is no way to deny what she has done. Perhaps what I have introduced here will help a little to bring more people to that way of thinking, but others likely need more convincing. I will continue to try to provide reasoned explanations for some of the remaining inconsistencies where I can.&lt;/p&gt;
&lt;p&gt;So my feeling is that there is a zebra hunt going on here. What is more likely; that a 55 year old grandma is running across the country at world record pace, or that she has convinced several people to go on a month-long trip to fake a run, and in the process developed an incredibly sophisticated piece of software (which accounts for specific nuances) to spoof the data (even though she is clearly doing &lt;em&gt;something&lt;/em&gt; out there as she is losing weight and suffering exactly as one might expect for somebody running across America)? I’m going with the world record grandma. I do not think that Mimi is a witch.&lt;/p&gt;
&lt;p&gt;There are likely many questions outstanding which I have not addressed here. This is a fairly rudimentary piece of work compared to the amount of time and effort that others have put into looking into this. I am interested to look into &lt;code&gt;Scam_Watcheroo&#39;s&lt;/code&gt; blog post about this to see what other issues he addresses. I would also like to look at how Mimi’s performance changed over time, and in particular how it changed following the LetsRun forum taking off and her ultimate switch from Race Drone to the Garmin tracker. In addition, something that I have not considered is whether or not the data are modified in the move from MovesCount to Strava. Although the overal trends would not change drastically, the raw data themselves (and therefore the digit distributions) might. This is probably worth considering in due time.&lt;/p&gt;
&lt;p&gt;I genuinely hope that this is useful to some of you in addressing some of the concerns. Nothing I can do can change people’s opinions on what happened in the first 2 weeks, but I hope that I can at least start to alleviate fears that there is any duplicity in the Strava data. It is all academic now following Mimi’s recent announcement that she will be pulling out from the event, but hopefully this should also help to assure people of the validity of Sandra’s run as well (although her regular updating and constant tracking have allayed any such fears already). All I can do now is wish Sandra good luck in getting the record (she looks to be on excellent form), and wish Mimi a very speedy recovery. It is sad that it had to go like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There are aspects of the spoofed data that make it stand out when compared to Mimi’s and Sandra’s (and my own) data&lt;/li&gt;
&lt;li&gt;I just do not think that it would be possible to create a forged data set that stands up to intense scrutiny - this is fairly basic scrutiny and it stands out&lt;/li&gt;
&lt;li&gt;Mimi is using 1s capture mode on constant capture for her runs, with the very occasional 3 or 4 sec delay&lt;/li&gt;
&lt;li&gt;Sandra is using 10s capture and has longer pauses in data retrieval of several minutes at a time (auto-pause?)&lt;/li&gt;
&lt;li&gt;Mimi’s data sets are therefore an order of magnitude denser than Sandra’s&lt;/li&gt;
&lt;li&gt;Mimi’s cadence blips of 200+ spm are likely just random c*ck ups in data capture - they disappear if you smooth out to a 10s capture rate (I was trying to contact her crew to ask her to set a second watch to 10s capture for one of her runs to confirm this, but unfortunately it was too late)&lt;/li&gt;
&lt;li&gt;You probably don’t see them for Sandra because they get averaged out&lt;/li&gt;
&lt;li&gt;Mimi is running with a high cadence but the average seems to fit with previous evidence (albeit very limited and definitely open to scrutiny) of her running gait (evidence from the film crew videos in the future will also help if/when released)&lt;/li&gt;
&lt;li&gt;Mimi is often running in 185-195 range, but then so do I - granted I am not running across a continent on a day by day basis, but I am also nowhere near elite&lt;/li&gt;
&lt;li&gt;Mimi’s cadence is about 10 spm quicker than Sandra’s for both walking and running&lt;/li&gt;
&lt;li&gt;Mimi seems to have a fairly even split of walking and running, whilst Sandra seems to consistently run but at a slower overall pace&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How Predictable Are Ultra Runners?</title>
      <link>/post/2018-04-24-how-predictable-are-ultrarunners/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-24-how-predictable-are-ultrarunners/</guid>
      <description>

&lt;h1 id=&#34;note&#34;&gt;Note&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This post is a continuation from my &lt;a href=&#34;/post/2018-04-18-Suunto-Or-Garmin/index.html&#34;&gt;previous post&lt;/a&gt;, looking at various aspects of the posting habits of the Ultra Running Community (URC). This was originally intended to be just an additional section in that blog post, but it was getting a little unwieldy so I decided to split it off into its own post to stop it from getting too crazy. This post is probably a lot less interesting than the last post, as it is really looking at one central question; can I predict which group member is posting based on the content of the message? Spoiler alert, you can&amp;rsquo;t! The posters on this forum are apparently &lt;em&gt;all&lt;/em&gt; sarcastic bastards, so it is difficult to pick them apart. But it was quite interesting trying.&lt;/p&gt;

&lt;p&gt;So as a bit of an experiment, I decided to play around with these data to see if the language that people use in their posts is specific enough to allow us to predict who has written something based on what the post says. This is a job for &lt;em&gt;machine-learning&lt;/em&gt;, which is really a lot less grandiose than it sounds. Essentially what we are doing here is using data to train a model of some description that can be applied to a new set of data to make predictions. In this case, we are looking to fit a model that is able to classify posts into one of a number of groups, where each group represents a single user. As an example of a classification problem, think of the spam filter in your email client. This is essentially a model that has been trained to look at the email message content and determine whether it is spam or not (e.g. if it is full of words like &lt;em&gt;viagra&lt;/em&gt;, &lt;em&gt;Nigerian Prince&lt;/em&gt;, &lt;em&gt;penis enlargement&lt;/em&gt;, &lt;em&gt;make money today&lt;/em&gt;, etc. then it is clearly all kosher). This would be a 2-class classification problem.&lt;/p&gt;

&lt;p&gt;For classification problems such as this, we require a training set on which to fit our model, and a validation set to determine the quality of the model. The validation set must be independent of the training set, as we want to test how the model will generalize to new data. The idea of &lt;em&gt;cross validation&lt;/em&gt; is essentially to split your training data into a training set and a validation set such that the validation is independent of the model fitting (to avoid the effects of over-fitting in the training set). There are various ways to split your data in this way. For now I will simply randomly select a subset for training and a smaller subset for validation (the &lt;em&gt;Holdout Method&lt;/em&gt;), but for true cross-validation this should then be repeated several times so that the average over several validation sets is used. For example, in &lt;em&gt;k-fold cross validation&lt;/em&gt; you would randomly distribute the data into &lt;code&gt;k&lt;/code&gt; equally sized subsets, and use exactly one of these as the validation set and &lt;code&gt;k-1&lt;/code&gt; as the training set. This is then repeated &lt;code&gt;k&lt;/code&gt; times, each time using a different subset as the validation set.&lt;/p&gt;

&lt;p&gt;It makes sense to restrict this analysis to the most active posters, and so I will limit the analysis to only users who have contributed 50 or more posts to the forum. This gives us 5,233 posts, from 48 users. I will randomly select 4,000 posts for the training set, and use the remainder for validation:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
posts50 &amp;lt;- URC                 %&amp;gt;%
           group_by(from_name) %&amp;gt;%    ## Group by poster
           filter(n() &amp;gt;= 50)   %&amp;gt;%    ## Select only posters with &amp;gt;50 posts
           select(from_name, message) ## Keep poster name and message content
set.seed(0) ## Set seed for random number generation for reproducibility
ids   &amp;lt;- sample(1:nrow(posts50), 4000) ## Randomly select 4000
train &amp;lt;- posts50[ids,]  ## Keep random ids as training set
test  &amp;lt;- posts50[-ids,] ## Use remaining ids as validation
{% endhighlight %}&lt;/p&gt;

&lt;h1 id=&#34;natural-language-programming&#34;&gt;Natural Language Programming&lt;/h1&gt;

&lt;p&gt;The model that we will be using is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bag-of-words_model&#34; target=&#34;_blank&#34;&gt;Bag Of Words&lt;/a&gt; model, which is a natural language programming technique that aims to represent text based on the frequency of words within it. There are some things that we can do to reduce the vector space of available terms, such as removing capital letters and removing so called &amp;ldquo;stop words&amp;rdquo; (common words like &amp;ldquo;is&amp;rdquo;, &amp;ldquo;and&amp;rdquo;, &amp;ldquo;but&amp;rdquo;, &amp;ldquo;the&amp;rdquo;, etc.). We can also limit the analysis to only words that occur frequently in the text, although there is a possibility of missing specific terms used by only one or two individuals, say, that may help the predictiveness of the model.&lt;/p&gt;

&lt;p&gt;I will be using the &lt;a href=&#34;https://cran.r-project.org/web/packages/text2vec&#34; target=&#34;_blank&#34;&gt;text2vec&lt;/a&gt; package in R which is efficient at generating the required document-term matrix (DTM) for fitting our model. In particular, it generates unique tokens for each term rather than using the terms themselves, which reduces computational overheads. An iterative function can then be applied to generate the DTM. So let&amp;rsquo;s generate such an iterator over the term tokens:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(text2vec)
train_tokens &amp;lt;- train$message                      %&amp;gt;%
                iconv(&amp;ldquo;latin1&amp;rdquo;, &amp;ldquo;ASCII&amp;rdquo;, sub = &amp;ldquo;&amp;rdquo;) %&amp;gt;% # Convert to ASCII format
                tolower                            %&amp;gt;% # Make lower case
                word_tokenizer                         # Break terms into tokens
it_train &amp;lt;- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;itoken&#34;&gt;&lt;itoken&gt;&lt;/h2&gt;

&lt;h2 id=&#34;inherits-from-iterator&#34;&gt;Inherits from: &lt;iterator&gt;&lt;/h2&gt;

&lt;h2 id=&#34;public&#34;&gt;Public:&lt;/h2&gt;

&lt;h2 id=&#34;chunk-size-400&#34;&gt;chunk_size: 400&lt;/h2&gt;

&lt;h2 id=&#34;clone-function-deep-false&#34;&gt;clone: function (deep = FALSE)&lt;/h2&gt;

&lt;h2 id=&#34;counter-0&#34;&gt;counter: 0&lt;/h2&gt;

&lt;h2 id=&#34;ids-nici-griffin-tremayne-dill-cowdry-iain-edward-smuts-neil&#34;&gt;ids: Nici Griffin Tremayne Dill Cowdry Iain Edward Smuts Neil &amp;hellip;&lt;/h2&gt;

&lt;h2 id=&#34;initialize-function-iterable-ids-null-chunks-number-10-progress-interactive&#34;&gt;initialize: function (iterable, ids = NULL, chunks&lt;em&gt;number = 10, progress&lt;/em&gt; = interactive(),&lt;/h2&gt;

&lt;h2 id=&#34;is-complete-active-binding&#34;&gt;is_complete: active binding&lt;/h2&gt;

&lt;h2 id=&#34;iterable-list&#34;&gt;iterable: list&lt;/h2&gt;

&lt;h2 id=&#34;length-active-binding&#34;&gt;length: active binding&lt;/h2&gt;

&lt;h2 id=&#34;nextelem-function&#34;&gt;nextElem: function ()&lt;/h2&gt;

&lt;h2 id=&#34;preprocessor-list&#34;&gt;preprocessor: list&lt;/h2&gt;

&lt;h2 id=&#34;progress-false&#34;&gt;progress: FALSE&lt;/h2&gt;

&lt;h2 id=&#34;progressbar-null&#34;&gt;progressbar: NULL&lt;/h2&gt;

&lt;h2 id=&#34;tokenizer-list&#34;&gt;tokenizer: list&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Next we use this iterator to create a vocabulary DTM for fitting the model. To start with, I will use all of the words, but later we could look at filtering out stop words and less frequent terms:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
vocab      &amp;lt;- create_vocabulary(it_train)
vectorizer &amp;lt;- vocab_vectorizer(vocab)
train_dtm  &amp;lt;- create_dtm(it_train, vectorizer)
dim(train_dtm)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;1-4000-13209&#34;&gt;[1]  4000 13209&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;The result is a matrix with 4,000 rows (the number of messages in the training set) and 13,209 columns (the number of unique terms in the training set). So each message is now represented as a vector of counts for all possible terms in the search space. The hope now is that we will be able to fit a model that is able to discriminate different users based on their word usage. Unlikely, but hey let&amp;rsquo;s give it a shot.&lt;/p&gt;

&lt;h1 id=&#34;random-forest&#34;&gt;Random Forest&lt;/h1&gt;

&lt;p&gt;In this case, our dependent variable is the name of the user who posted the message which is a categorical variable. The independent variables are the counts for each of the 13,209 terms across the data set. I am going to start by using a random forest model, which is one of the more popular classification models available. A decision tree is a quite simple (although incredibly powerful) stepwise model that you can think of like a flow chart. The model fitting will create a series of nodes where your independent variables are used to discrimate between one choice and another, eventually leading to a certain prediction depending on the values of the variables in your model. A random forest essentially fits a whole load of these classification decision trees and outputs the &lt;em&gt;modal&lt;/em&gt; (most common) class across all of them.&lt;/p&gt;

&lt;p&gt;One benefit of using random forests over something like generalised linear models (see later) is that, since they rely on fairly independent tests at each stage in the tree, they are more robust to correlated variables in the model. With such a large set of term variables there is undoubtedly correlation between many of these terms, particularly as many of these variables are likely to be largely made of zeroes. Of course, this sparsity itself causes somewhat of a problem, and should be taken into account in the analysis. But for now I will ignore it and just hope that it isn&amp;rsquo;t a problem&amp;hellip;&lt;/p&gt;

&lt;p&gt;To begin with,let&amp;rsquo;s fit a simple random forest model and see how it looks:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;randomForest&amp;rdquo;)
rf_model &amp;lt;- randomForest(x = as.matrix(train_dtm), y = as.factor(rownames(train_dtm)))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Note that I set the &lt;code&gt;y&lt;/code&gt; parameter to be a factor so that it is clear that this is a classification model. Now we can test our model by seeing how it performs at predicting the user for our test data set. First we generate a similar DTM for the test data set. Note that we use the same &lt;code&gt;vectorizer&lt;/code&gt; as we used for the training set:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
test_tokens &amp;lt;- test$message                       %&amp;gt;%
               iconv(&amp;ldquo;latin1&amp;rdquo;, &amp;ldquo;ASCII&amp;rdquo;, sub = &amp;ldquo;&amp;rdquo;) %&amp;gt;% # Convert to ASCII format
               tolower                            %&amp;gt;% # Make lower case
               word_tokenizer                         # Break terms into tokens
it_test     &amp;lt;- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    &amp;lt;- create_dtm(it_test, vectorizer)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;And then we use our model to predict the user for each of the posts in our test data set. To do this we use the &lt;code&gt;predict()&lt;/code&gt; method for &lt;code&gt;randomForest&lt;/code&gt; objects, and output the response class with the majority vote amongst all of the decision trees:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
test_predict &amp;lt;- predict(rf_model, as.matrix(test_dtm), type = &amp;ldquo;response&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;So, how did we do? Let&amp;rsquo;s see how many of these were correctly predicted:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
table(test_predict == rownames(test_dtm))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;false-true&#34;&gt;FALSE  TRUE&lt;/h2&gt;

&lt;h2 id=&#34;760-473&#34;&gt;760   473&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;This model predicts the poster only 38.4 % of the time, which isn&amp;rsquo;t particularly good.&lt;/p&gt;

&lt;h1 id=&#34;model-improvements&#34;&gt;Model Improvements&lt;/h1&gt;

&lt;p&gt;So can we improve this? Yes, probably. The first thing that I can try is to be a little more clever in the way that I parameterise the data. So rather than simply counting words, I will instead use &lt;em&gt;n-grams&lt;/em&gt; &amp;ndash; combinations of &lt;code&gt;n&lt;/code&gt; words that will be more sensitive to the types of phrases that different people typically use. Obviously increasing &lt;code&gt;n&lt;/code&gt; in this case will also increase the memory and run time considerably, so there are limits to what we can feasibly do. Also, it is probably worth noting that removal of stop words is less likely to be the best way to go about this, as this will affect the structure of the n-grams. So this time let us leave the stop words in, but parameterise with &lt;code&gt;3-grams&lt;/code&gt;. I will also limit the count to those n-grams used at least 10 times:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
vocab &amp;lt;- create_vocabulary(it_train, ngram = c(1L, 3L)) ## use 1-, 2- and 3-grams
vocab &amp;lt;- vocab %&amp;gt;%
         prune_vocabulary(term_count_min = 10) ## Only keep n-grams with count greater than 10
vectorizer &amp;lt;- vocab_vectorizer(vocab)
dtm_train  &amp;lt;- create_dtm(it_train, vectorizer)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Note here that we used the notation &lt;code&gt;1L, 3L&lt;/code&gt;, which tells &lt;code&gt;R&lt;/code&gt; to explicitly use integer values rather than numeric values. In many cases this has little to no effect, but in programming an integer variable will take up much less memory (4 bytes per element) than a double precision floating point number (8 bytes per element).&lt;/p&gt;

&lt;p&gt;Another thing that we can do to improve the model fit is that we can attempt to normalise our DTM to account for the fact that different Facebook messages may be longer or shorter than others. Typically the &amp;ldquo;documents&amp;rdquo; in this case (the messages) are very small so I imagine this will have only a minimal effect. Here I will use the &lt;em&gt;term frequency-inverse document frequency&lt;/em&gt; (TF-IDF) transformation. The idea here is to not only normalise the data, but also to scale the terms such that terms that are more common (i.e. those used regularly in all posts) are down-weighted, whilst those that are more specific to a small number of users (and will thus be more predictive) are up-weighted.&lt;/p&gt;

&lt;p&gt;{% highlight r %}
tfidf           &amp;lt;- TfIdf$new()
train_dtm_tfidf &amp;lt;- fit_transform(train_dtm, tfidf)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Finally there is some fine tunning that can be made to the model fitting procudure. Here we are dealing with a very sparse set of data, since most of the counts are zero in this matrix (not everybody uses every word or set of words). This can cause issues with the random forest model. In addition, there may be some imbalance in the classes (for instance as we saw above different individuals post more often than others).&lt;/p&gt;

&lt;p&gt;Now I don&amp;rsquo;t claim to be an expert in machine learning, and random forests in particular are not my forte. However, different selections for the parameters can have big effects on the quality of the model. The two main parameters for a random forest are the number of trees (&lt;code&gt;ntree&lt;/code&gt;) and the number of features that are evaluated at each branch in the trees (&lt;code&gt;mtry&lt;/code&gt;). The higher the better for the number of trees, although run-time can be a hindrance on this. For the second parameter, I have seen it suggested that the square root of the number of features is a good place to start, and this is the default for classification anyway. So let&amp;rsquo;s try increasing the number of trees, and running this on the TF-IDF transformed 3-gram data:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
rf_model_tfidf &amp;lt;- randomForest(x = as.matrix(train_dtm_tfidf),
                               y = as.factor(rownames(train_dtm_tfidf)),
                               ntree = 1000)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;One note to make here is that this is slllllllloooooooooowwwwwwwwww! This needed to be run overnight to finish. Using something like python is probably a better bet when running machine learning algorithms like this, and I will probably do another post later in the future to look at some alternative ways to do this.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s take a look at whether or not this model is more effective at predicting the user:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
dtm_test       &amp;lt;- create_dtm(it_test, vectorizer)
test_dtm_tfidf &amp;lt;- fit_transform(test_dtm, tfidf)
test_predict &amp;lt;- predict(rf_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;ldquo;response&amp;rdquo;)
table(test_predict == rownames(test_dtm))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;false-true-1&#34;&gt;FALSE  TRUE&lt;/h2&gt;

&lt;h2 id=&#34;751-482&#34;&gt;751   482&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Wow, so now we have improved the prediction to a whopping 39.1%. Hmm. An improvement of 0.7% was not &lt;em&gt;quite&lt;/em&gt; as much as I was hoping for.&lt;/p&gt;

&lt;h1 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h1&gt;

&lt;p&gt;Okay, so let&amp;rsquo;s try a different model to see if that has any effect. I am going to fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34; target=&#34;_blank&#34;&gt;logistic regression&lt;/a&gt;. Regression is simply an attempt to fit a linear approximation to a set of data that minimises the difference between the modeled value and the true value (the &lt;em&gt;residuals&lt;/em&gt;). I will do a more thorough post on statistical modelling in the future, but for now think of regression models as being an attempt to fit a line of best fit between some variable $y$ that you suspect is dependent on some other variables $x_1, x_2, &amp;hellip;, x_n$. The idea then is to use this model to predict $y$ based on new measurements of $x_1, x_2, &amp;hellip;, x_n$. So here we are trying to fit a model that will provide us with an estimate of the user based on the words used in the post.&lt;/p&gt;

&lt;p&gt;Here I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/index.html&#34; target=&#34;_blank&#34;&gt;glmnet&lt;/a&gt; package to fit the logistic regression. Logistic regression is a subset of Generalised Linear Models (GLM), which are an extension of ordinary linear regression allowing for errors that are not normally distributed through the use of a link function. Since we have multiple possible classes in the dependent variable, this will be a multinomial logistic regression:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;glmnet&amp;rdquo;)
glm_model_tfidf &amp;lt;- cv.glmnet(x = train_dtm_tfidf, y = as.factor(train$from_name),
                              family = &amp;lsquo;multinomial&amp;rsquo;,
                              alpha = 1,
                              type.measure = &amp;ldquo;deviance&amp;rdquo;,
                              nfolds = 5,
                              thresh = 1e-3,
                              maxit = 1e3)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;This is an n-fold cross-validated GLM (hence &lt;code&gt;cv.glmnet&lt;/code&gt;), which is a method of validation for the model that splits the data into &lt;code&gt;n&lt;/code&gt; equally sized subsets, then uses &lt;code&gt;n-1&lt;/code&gt; subsets as training data and the remaining subset as the validation data to test the accuracy of the model. This is repeated &lt;code&gt;n&lt;/code&gt; times, and the average is used. This is actually a better method than I have used in these data (selecting a test data set and running the model on the remaining subset) as every sample is used in the validation, which avoids over-fitting.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;family&lt;/code&gt; parameter gives the model family that defines the error model, which in turn determines the link function to be used. In this case we are using multinomial logistic regression, so the predicted response is a vector of probabilities between 0 and 1 &amp;ndash; one for each potential response &amp;ndash; all adding to 1. The link function, which defines the relationship between the linear predictor and the mean of the distribution function, is the &lt;code&gt;logit&lt;/code&gt; function, which in the binary case gives the log odds of the prediction ($X\beta = ln (\mu/(1-\mu)$).&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;alpha&lt;/code&gt; value will utilise an L1 regularisation of the data to account for the sparsity that we see in the data. The &lt;code&gt;type.measure&lt;/code&gt; value determines the measurement to use to determine the cross validation, in this case the misclassification error. &lt;code&gt;nfolds&lt;/code&gt; gives the value of &lt;code&gt;k&lt;/code&gt; for the k-fold cross validation, &lt;code&gt;thresh&lt;/code&gt; gives the threshold for the convergence of the coordinate descent loop, and &lt;code&gt;maxit&lt;/code&gt; gives the maximum number of iterations to perform.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s see if this is any better:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
test_predict &amp;lt;- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;ldquo;response&amp;rdquo;)
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;false-true-2&#34;&gt;FALSE  TRUE&lt;/h2&gt;

&lt;h2 id=&#34;752-481&#34;&gt;752   481&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Nope. We still only see about 39% accurately assigned.&lt;/p&gt;

&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;Okay, so it is possible (highly probable?) that I have made some mistakes in this analysis, and that I could vastly improve the creation of the DTM, but I think it is more propbable that these posts are simply not distinct enough to determine individuals writing styles. I guess in a group with such a narrow focus, it is inevitable that people will be posting very similar content to one another. There is after all only so many ways to ask &amp;ldquo;Suunto or Garmin&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s examine why we are struggling to distinguish these posts in a little more detail. Below is a heatmap showing the probability for each of the 48 potential posters, predicted for all 1,233 of the posts in the validation data set. A heatmap is a kind of 3-dimensional plot, where colour is used to represent the third dimension. So the 48 potential posters are shown on the x-axis, the 1,233 posts are shown on the y-axis, and the magnitude of the estimated probability for user &lt;code&gt;i&lt;/code&gt; based on post &lt;code&gt;j&lt;/code&gt; is represented by a colour from red (0% probability) to white (100% probability). Note that here I have scaled the data using a square root so that smaller probabilities (which we expect to see) are more visible. The rows and columns are arranged such that more similar values are closer together.&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;gplots&amp;rdquo;)
heatmap.2(sqrt(test_predict[,,1]), trace = &amp;ldquo;none&amp;rdquo;, margins = c(10,0), labRow = FALSE)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;similar_posts.png&#34; alt=&#34;Heatmap showing similarity between posters on the URC Facebook group&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So the biggest problem here is that the vast majority of the posts are estimated as most likely coming from either Neil Bryant, Stef Schuermans or James Adams. And actually, the ones that it gets correct are almost all from one of these posters:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
pie(sort(table(rownames(test_dtm)[colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm)])))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;similar_posts_pie.png&#34; alt=&#34;Pie chart showing the most active posters within the URC Facebook group&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wonder whether these guys are skewing the model because of their, ahem, above average posting habits. But frankly at this stage I&amp;rsquo;m kind of bored, so I think that I will leave it there. Another time maybe. Ultimately I believe that these posts are simply too short and bereft of salient information to be useful for making predictions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Suunto Or Garmin? The Age Old Question.</title>
      <link>/post/2018-04-18-suunto-or-garmin/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-18-suunto-or-garmin/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rfacebook&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#temporary-token&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fboauth&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ultra-running-community&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likes-comments-and-shares&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-contributors&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-are-people-posting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-popular-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-often-do-people-actually-talk-about-ultras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suunto-or-garmin&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summing-up&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;In &lt;a href=&#34;/post/2018-04-15-ultrarunner-or-ultra-runner/&#34;&gt;my last post&lt;/a&gt;, I took a look at ways to pull data down from Twitter and analyse some specific trends. It was quite interesting for me to see how easy it is to access these data, and there is a huge amount to be gleened from these sorts of data. The idea of this post is to use a similar approach to pull data from the Ultra Running Community page on &lt;a href=&#34;https://www.facebook.com&#34;&gt;Facebook&lt;/a&gt;, and then to use these data to play around further with the &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; R packages. Just for funsies, I’m also going to have a bit of a crack at some machine learning concepts. In particular, a question that seems to comes up pretty regularly is whether the best GPS watch for running is from &lt;a href=&#34;http://www.suunto.com&#34;&gt;Suunto&lt;/a&gt; or &lt;a href=&#34;https://www.garmin.com&#34;&gt;Garmin&lt;/a&gt;. I figured I could save us all some time and answer the question once and for all…&lt;/p&gt;
&lt;p&gt;Just a little aside here; I think that some people missed the point last time. I honestly don’t care much about these questions, they are just a jumping off point for me to practice some of the data analysis techniques that come up in my work. The best way to get better at something is to practice, so these posts are just a way of combining something I love with a more practical purpose. The idea of this blog is for me to practice these things until they become second nature. Of course in the process, I may just find something interesting along the way.&lt;/p&gt;
&lt;p&gt;Probably not though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rfacebook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/h1&gt;
&lt;p&gt;Following on from my experiences playing around with the &lt;a href=&#34;https://twitter.com&#34;&gt;Twitter&lt;/a&gt; API, I decided to have a look to see if there were similar programmatic ways to access Facebook data. This can be accomplished using the &lt;a href=&#34;https://cran.r-project.org/web/packages/Rfacebook/&#34;&gt;Rfacebook&lt;/a&gt; package in R, which is very similar to the &lt;code&gt;TwitteR&lt;/code&gt; package that I used previously. This package accesses the Facebook &lt;a href=&#34;https://developers.facebook.com/docs/graph-api&#34;&gt;Graph API Explorer&lt;/a&gt;, allowing access to a huge amount of data from the Facebook social &lt;em&gt;graph&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So first of all, let’s install the &lt;code&gt;Rfacebook&lt;/code&gt; package. We can install the stable version from the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or install the more up-to-date but less stable developmental version from Github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;devtools&amp;quot;)
install_github(&amp;quot;pablobarbera/Rfacebook/Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am using the developmental version here. There are several additional packages that also need to be installed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;httr&amp;quot;, &amp;quot;rjson&amp;quot;, &amp;quot;httpuv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;code&gt;TwitteR&lt;/code&gt;, access to the API is controlled through the use of API tokens. There are two ways of doing this - either by registering as a developer and generating an app as I did with &lt;code&gt;TwitteR&lt;/code&gt;, or through the use of a temporary token which gives you access for a limited period of 2 hours. Let’s look at each of these in turn:&lt;/p&gt;
&lt;div id=&#34;temporary-token&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/h2&gt;
&lt;p&gt;To generate a temporary token, just go to the &lt;a href=&#34;https://developers.facebook.com/tools/explorer/&#34;&gt;Graph API Explorer&lt;/a&gt; page and generate a new token by clicking on &lt;code&gt;Get Token&lt;/code&gt; -&amp;gt; &lt;code&gt;Get Access Token&lt;/code&gt;. You need to select the permissions that you want to grant access for, which will depend on what you are looking do:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Graph-API-Explorer.png&#34; alt=&#34;Create Temporary Access Token&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Create Temporary Access Token&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I just granted permission to everything for this analysis. Once you have an access token, this can be used as the &lt;code&gt;token&lt;/code&gt; parameter when using functions such as &lt;code&gt;getUsers()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fboauth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/h2&gt;
&lt;p&gt;The above is the most simple method, but this access token will only last for 2 hours, at which point you will need to generate a new one. If you want a longer term solution, you can set up &lt;a href=&#34;https://en.wikipedia.org/wiki/OAuth&#34;&gt;Open Authorization&lt;/a&gt; access in a similar way to for the &lt;code&gt;TwitteR&lt;/code&gt; package. The downside is that you lose the ability to search friend networks unless your friends are also using the app that you generate - and I don’t want to inflict that on people just so that I can &lt;del&gt;steal their identity&lt;/del&gt; analyse their networks.&lt;/p&gt;
&lt;p&gt;This method is almost identical to the process used for generating the OAuth tokens in the &lt;code&gt;TwitteR&lt;/code&gt; app, and a good description of how to do it can be found in this &lt;a href=&#34;http://thinktostart.com/analyzing-facebook-with-r/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, I am feeling pretty lazy today and so I will just use the temporary method.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ultra-running-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/h1&gt;
&lt;p&gt;With nearly 18,000 members, the Ultra Running Community Facebook page is a very active community of ultrarunners from around the world. Runners are able to ask questions, share blogs, and generally speak with like-minded individuals about everything from gear selection to how best to prevent chaffing when running. It’s been going since June 2012, so there are a fair few posts available to look through.&lt;/p&gt;
&lt;p&gt;So let’s load in all of the posts from the URC page:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Rfacebook&amp;quot;)
token &amp;lt;- &amp;quot;XXXXXX&amp;quot;  ## Insert your temporary token from Graph API Explorer
URC &amp;lt;- getGroup(&amp;quot;259647654139161&amp;quot;, token, n=50000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will create an object of class &lt;code&gt;data.frame&lt;/code&gt; containing the most recent 50,000 posts available in the Facebook Group with ID &lt;code&gt;259647654139161&lt;/code&gt; (which is the internal ID for the Ultra Running Community page). The page was set up in June 2012 By Neil Bryant, and currently (as of 20th March 2017) contains a total of 24,836 posts. So this command will actually capture every single post.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;data.frame&lt;/code&gt; is somewhat of the workhorse of R, and looks to the user like a spreadsheet like you would expect to see in Excel. Behind the scene it is actually a list of lists, with each column representing a particular measurement or descriptive annotation of that particular datum. The ideal situation is to design your data frame such that every row is an individual measurement, and every column is some aspect relating to that measurement. This can sometimes go against the instinctual way that you might store data, but makes downstream analyses much simpler.&lt;/p&gt;
&lt;p&gt;As an example, suppose that you were measuring something (blood glucose levels, weight, lung capacity, VO2 max, etc.) at three times of the day for 2 individuals. Your natural inclination may be to design your table in this way:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement 1&lt;/th&gt;
&lt;th&gt;Measurement 2&lt;/th&gt;
&lt;th&gt;Measurement 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But actually the optimum way to represent this is to treat each measurement as a different row in your data table, and use a descriptive categorical variable to represent the repeated measurements:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement&lt;/th&gt;
&lt;th&gt;Replicate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can then add additional information relating to each individual measurement, which can be factored into your model down the line.&lt;/p&gt;
&lt;p&gt;In this case, we have a data frame where each row is a post on the URC feed, and each column gives you information on the post such as who wrote it, what the post says, when it was written, any links involved, and how many likes, comments and shares each post has. We can take a quick look at what the data.frame looks like by using the &lt;code&gt;str()&lt;/code&gt; function. This will tell us a little about each column, such as the data format (character, numeric, logical, factor, etc.) and the first few entries in each column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(URC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    24836 obs. of  11 variables:
##  $ from_id       : chr  &amp;quot;10203232759527000&amp;quot; &amp;quot;10154582131480554&amp;quot; &amp;quot;10206266800967107&amp;quot; &amp;quot;10153499987966664&amp;quot; ...
##  $ from_name     : chr  &amp;quot;Steph Wade&amp;quot; &amp;quot;Esther Bramley&amp;quot; &amp;quot;Tom Chapman&amp;quot; &amp;quot;Polat Dede&amp;quot; ...
##  $ message       : chr  &amp;quot;Anyone who runs in Speedcross 3s tried Sportiva? I&amp;#39;ve had several pairs of Speedcross  but thinking about tryin&amp;quot;| __truncated__ &amp;quot;Hi guys server pain in knee two weeks after 41miler. Ran 3 miles Tuesday no problem. Pain started at 5m and got&amp;quot;| __truncated__ &amp;quot;mega depressed; need advice. Running really well over xmas, since then, painful hip &amp;amp; groin, chasing itb, glute&amp;quot;| __truncated__ NA ...
##  $ created_time  : chr  &amp;quot;2017-03-19T08:38:15+0000&amp;quot; &amp;quot;2017-03-18T12:19:26+0000&amp;quot; &amp;quot;2017-03-18T16:30:54+0000&amp;quot; &amp;quot;2017-03-19T08:42:38+0000&amp;quot; ...
##  $ type          : chr  &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;photo&amp;quot; ...
##  $ link          : chr  NA NA NA &amp;quot;https://www.facebook.com/tahtaliruntosky/photos/a.659614340816057.1073741827.659609490816542/1145262965584523/?type=3&amp;quot; ...
##  $ id            : chr  &amp;quot;259647654139161_1064067560363829&amp;quot; &amp;quot;259647654139161_1063418100428775&amp;quot; &amp;quot;259647654139161_1063562803747638&amp;quot; &amp;quot;259647654139161_1064068937030358&amp;quot; ...
##  $ story         : logi  NA NA NA NA NA NA ...
##  $ likes_count   : num  0 0 0 0 0 2 2 58 7 1 ...
##  $ comments_count: num  5 23 9 0 25 9 4 64 77 12 ...
##  $ shares_count  : num  0 1 0 0 0 0 0 0 3 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likes-comments-and-shares&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/h1&gt;
&lt;p&gt;The first step in any data analysis is to check that the data make sense. You’ve probably heard the old adage “garbage in; garbage out”, so data cleaning is an essential first step to ensure that we are not basing our conclusions on erroneous information from the beginning. There are far too many posts here to look at them all by hand, but there are a few things we can certainly have a look at to check that the values make sense.&lt;/p&gt;
&lt;p&gt;For instance, let’s take a look at the number of likes, comments and shares. We would expect all of these values to be positive whole numbers, so this is something that is easy to check. To do this, I will be making use of the &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;ggplot2&lt;/a&gt; package, which allows for some incredibly powerful plotting in R. The idea is to define the plot in terms of &lt;em&gt;aesthetics&lt;/em&gt;, where different elements of the plot (x and y values, colour, size, shape, etc.) can be mapped to elements of your data.&lt;/p&gt;
&lt;p&gt;In this case I want to plot a distribution plot where the colour of the plot is mapped to whether we are looking at likes, comments or shares. To do this, I need to rearrange the data such that the &lt;code&gt;likes_count&lt;/code&gt;, &lt;code&gt;comments_count&lt;/code&gt; and &lt;code&gt;shares_count&lt;/code&gt; columns are in a single column &lt;code&gt;counts&lt;/code&gt;, with an additional column defining whether it is a like, a comment, or a share count (as described in the example above).&lt;/p&gt;
&lt;p&gt;I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/tidyr.pdf&#34;&gt;tidyr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html&#34;&gt;stringr&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html&#34;&gt;dplyr&lt;/a&gt; packages to rearrange the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyr&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
library(&amp;quot;stringr&amp;quot;)
like_comment_share &amp;lt;- URC                                                 %&amp;gt;% 
                      gather(count_type, count, likes_count:shares_count) %&amp;gt;%
                      select(count_type, count)
head(like_comment_share)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    count_type count
## 1 likes_count     0
## 2 likes_count     0
## 3 likes_count     0
## 4 likes_count     0
## 5 likes_count     0
## 6 likes_count     2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;stringr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; are incredibly powerful packages written by Hadley Wickham, which provide a simple to understand &lt;em&gt;grammar&lt;/em&gt; to apply to the filtering and tweaking of data frames in R. In particular, these can be used to convert the data into the &lt;em&gt;tidy&lt;/em&gt; format described above, allowing very simple and intuitive plotting with &lt;code&gt;ggplot2&lt;/code&gt;. One particularly useful feature is the ability to use the &lt;code&gt;%&amp;gt;%&lt;/code&gt; command to pipe the output to perform multiple data frame modifications.&lt;/p&gt;
&lt;p&gt;In the above code, we pipe the raw data &lt;code&gt;URC&lt;/code&gt; into the &lt;code&gt;gather()&lt;/code&gt; function, which will take the three columns from &lt;code&gt;likes_count&lt;/code&gt; through to &lt;code&gt;shares_count&lt;/code&gt; and split them into two new columns: &lt;code&gt;count_type&lt;/code&gt; which will be one of &lt;code&gt;shares_count&lt;/code&gt;, &lt;code&gt;likes_count&lt;/code&gt; and &lt;code&gt;comments_count&lt;/code&gt; , and &lt;code&gt;count&lt;/code&gt; which will take the value from the specified column. So essentially this produces a new data set with 3 times as many rows. This is then piped into &lt;code&gt;select()&lt;/code&gt; which will select the relevant columns.&lt;/p&gt;
&lt;p&gt;First let’s just check that they are are all positive integers as expected:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count == as.integer(like_comment_share$count))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count &amp;gt;= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;p&gt;Annoyingly there is no easy way to check that a vector of numbers is made up of integers, so this line will check that the numbers do not change after converting to integers. Similarly, we use &lt;code&gt;all()&lt;/code&gt; to check that all of the counts are greater than or equal to zero. They are as we would hope.&lt;/p&gt;
&lt;p&gt;Then we can use &lt;code&gt;ggplot2&lt;/code&gt; for the plotting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
ggplot(like_comment_share, aes(x = log10(count+1), col = count_type, fill = count_type)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Count (log10)&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-like_comment_share-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we use this output to plot a distribution plot showing the distribution of counts for the three different metrics – shares, comments and likes. We use the &lt;code&gt;count&lt;/code&gt; as the &lt;code&gt;x&lt;/code&gt; aesthetic, and &lt;code&gt;count_type&lt;/code&gt; as both the &lt;code&gt;col&lt;/code&gt; and &lt;code&gt;fill&lt;/code&gt; aesthetics to colour them. The main function &lt;code&gt;ggplot()&lt;/code&gt; will specify the aesthetics, and then we add additional elements to the plot by using the &lt;code&gt;+&lt;/code&gt; command. Here we add the &lt;code&gt;geom_density()&lt;/code&gt; element to plot the data in a density plot (the &lt;code&gt;alpha&lt;/code&gt; value will make the colours transparent for overplotting), the &lt;code&gt;labs()&lt;/code&gt; function will change the plot labels, and the &lt;code&gt;theme()&lt;/code&gt; function let’s you change aspects of the figure text, such as the size.&lt;/p&gt;
&lt;p&gt;Note that here I have plotted the &lt;span class=&#34;math inline&#34;&gt;\(log_{10}\)&lt;/span&gt; of the counts, which reduces the effects of outliers. Also note that I have added 1 to the counts. This is because &lt;span class=&#34;math inline&#34;&gt;\(log_{10}(0)\)&lt;/span&gt; is undefined. The idea here is that a count of 1 will get a value of 0, a count of 10 gets a value of 1, a count of 100 gets a value of 2, etc.&lt;/p&gt;
&lt;p&gt;So what does this tell us? Well not too much really. Not many people share posts from the page, but there aren’t too many that don’t get comments or likes. So it is a very active community. Posts tend to have more comments than likes, which makes sense because you can only like something once, but can comment as many times as you want. But the main thing that this shows is that these counts all seem to be in the right sort of expected range.&lt;/p&gt;
&lt;p&gt;Often exploratory plots like this can be useful to highlight problems in the raw data. One such example might be if a negative count existed in these data, which could happen due to input errors but quite clearly does not represent a valid count. As it happens, since these data are not manually curated, it is highly unlikely that such errors will be present, but you should never assume anything about any given data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-contributors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the all-time most common contributors to the page, again using the &lt;code&gt;ggplot2&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_contributors &amp;lt;- URC              %&amp;gt;% 
                    count(from_name) %&amp;gt;%
                    top_n(50, n)     %&amp;gt;%
                    arrange(desc(n))
ggplot(top_contributors, 
       aes(x = factor(from_name, levels = top_contributors$from_name), 
           y = n,
           fill = n)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_gradient(low=&amp;quot;blue&amp;quot;, high=&amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) +
  theme(axis.title  = element_text(size = 18),
        axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 14),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-contributors-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have first used &lt;code&gt;dplyr&lt;/code&gt; to count up the number of posts per user and select the top 50 contributors, then used &lt;code&gt;ggplot2&lt;/code&gt; to plot a barplot showing the number of posts per person. I have used the &lt;code&gt;scale_fill_gradient()&lt;/code&gt; element to colour the bars based on their height, such that those with the highest number of posts are coloured red, whilst those with the lowest are coloured blue.&lt;/p&gt;
&lt;p&gt;The top contributor to the page is Neil Bryant (757 posts), who is the founder member so this makes sense. James Adams is the second biggest contributor (489 posts), and he has less of an excuse really.&lt;/p&gt;
&lt;p&gt;Let’s take a look at James’ posting habits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;xts&amp;quot;)
jamesadams &amp;lt;- URC                                                        %&amp;gt;%
              filter(from_name == &amp;quot;James Adams&amp;quot;)                         %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
jamesadams_xts &amp;lt;- xts(jamesadams$n, order.by = jamesadams$created_time)
jamesadams_month &amp;lt;- apply.monthly(jamesadams_xts, FUN = sum)
plot(jamesadams_month, ylab = &amp;quot;Number of Posts&amp;quot;, main = &amp;quot;&amp;quot;, cex.lab = 1.7, cex.axis = 1.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-JamesAdams-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have used the package &lt;code&gt;xts&lt;/code&gt; to deal with the &lt;code&gt;POSIXct&lt;/code&gt; date format. In particular this will deal correctly with months with zero counts. James has been pretty active since summer 2013 (probably publicising &lt;a href=&#34;https://www.amazon.com/Running-Stuff-James-Adams/dp/1784622621&#34;&gt;his book&lt;/a&gt;), but his activity has been on the decline throughout 2016 – the price you pay when your family size doubles I guess.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-are-people-posting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/h1&gt;
&lt;p&gt;Next we can break the posts down by the day on which they are posted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;URC      &amp;lt;- URC %&amp;gt;%
            mutate(dow = factor(weekdays(as.POSIXct(created_time)), labels = c(&amp;quot;Monday&amp;quot;, &amp;quot;Tuesday&amp;quot;, &amp;quot;Wednesday&amp;quot;, &amp;quot;Thursday&amp;quot;, &amp;quot;Friday&amp;quot;, &amp;quot;Saturday&amp;quot;, &amp;quot;Sunday&amp;quot;)))
post_day &amp;lt;- URC %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-posting_day-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly to previously, here I have used &lt;code&gt;dplyr&lt;/code&gt; to reduce the full data down to a table of counts of posts per day of the week, then plotted them using &lt;code&gt;ggplot2&lt;/code&gt;. Surprisingly (to me anyway) there is no increase in activity during the weekend. I guess most of us are checking Facebook during working hours and busy running at the weekend…&lt;/p&gt;
&lt;p&gt;Wednesdays are interestingly bereft of posts though for some reason. Could this be people checking URC at work on Monday and Tuesday through boredom, only to find themselves told off and having to catch up on work by Wedesday? Then by the time the weekend rolls around we’re all back liking away with impunity ready to go through the whole process again the next week.&lt;/p&gt;
&lt;p&gt;Let’s look at the same plot for the 1,000 most popular posts (based on likes):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_day &amp;lt;- URC                      %&amp;gt;%
            top_n(1000, likes_count) %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-posting_day_most_likes-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So from this it is clear that if you want people to like your post, you should post on a Tuesday or a Thursday. Quite why people might be feeling so much more likely to click that all important like button on these dayas, I have no idea. But hey, stats don’t lie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-popular-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/h1&gt;
&lt;p&gt;So looking at the popular posts above got me thinking about how best to actually define a “popular” post. Is it a post with a lot of likes, or a post that everybody is commenting on? Let’s take a look at the top 5 posts based on each criteria:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(likes_count)) %&amp;gt;% top_n(5, likes_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                       message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 Thinking how far I’ve come and getting a bit emotional. .3 year ago i was a size 20/22 and couldnt run to end of the street. Yesterday i ran 30 mile as a training run and wasn’t even aching afterwards. Now nearly 45 and a size 10 and never felt better. I love my life!!!!
2 I saw this picture couple years ago and I found it very inspiring so I thought I’d share it. is a 12-year-old childhood cancer survivor who loves to run with his dad.
3 Not sure if swear words are accepted. 088208820882
4 “What you doing up here?” said the sheep.Pike last night, not a soul to be seen…
5 0880&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(comments_count)) %&amp;gt;% top_n(5, comments_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 For me claiming something you haven’t earned is not only immoral it’s fraudulent. And it’s a huge insult to all who’ve attempted the feat before you and legitimately fallen short. What do you guys think?
2 Anyone else do race to the stones and found it a rip off? I was not impressed with most things. were good such as medics and lots of water but who wants fudge bars and cadburies at aid stations. like a money making race to me, especially considering all the sponsorship they had. ’ve got lots of other rants about it but let’s hear anyone else’s thoughts first
3 New Game.am trying to convince some new ultra runners that you do not need to spend a load of money on kit to put one foot in front of the other a few more times. This is difficult given that half the posts in forums seems to be asking for recommendations or giving recommendations as to how one might waste money on kit.out of interest, what was the value of the kit you wore in your last ultra? Including nutrition. Obviously you will have to guess if you had them as a gift or can’t remember. Surely someone is going to have less than £100?
4 I had a small sabre rattling session last night with someone on this group. Nothing major by any stretch of the imagination - we just have opposing views on DNF. But it got me curios to what the opinions of others are on this subject. Is failing to finish something that you would risk your life to avoid? Is it something to fear? Is it something that will eventually happen to us all? Is it something that we can learn from? Etc,etc. Your thoughts please
5 i cannot wait to watch the EPSN footage - amazing stuff. It is a shame Robert has his doubters though.: was described as “trolling”, which was an over the top description (agree with the comments there)&lt;/p&gt;
&lt;p&gt;It seems to me that the posts with more likes tend to be posts with a much more positive message than those with most comments. The top liked posts are those from people who have overcome some form of adversity (such as the top liked post with 1,369 likes from Mandy Norris who had awesomely run 30 miles after losing half her body weight), whilst the top commented posts tend to be more controversial posts (such as the top commented post with 287 comments about Mark Vaz’s fraudulent JOGLE “World Record”).&lt;/p&gt;
&lt;p&gt;Let’s take a look at how closely these two poularity measures are correlated in the Ultra Running Community:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(URC, aes(x = comments_count, y = likes_count)) +
  geom_point(shape = 19, alpha = 0.2, size = 5) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  geom_point(data = URC %&amp;gt;% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = &amp;quot;red&amp;quot;, size = 5)) +
  geom_point(data = URC %&amp;gt;% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = &amp;quot;blue&amp;quot;, size = 5)) +
  labs(x = &amp;quot;Number of Comments&amp;quot;, y = &amp;quot;Number of Likes&amp;quot;) + 
  theme(axis.text   = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-likes_vs_comments-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are plotting a correlation scatterplot between the number of likes and the number of comments for each post. I have a set an alpha value of 0.2 for the scatterplot so that the individual points are made more see-through. That way, overplotting can be seen by darker coloured plots. I have also added in a line of best fit generated by fitting a linear model (&lt;code&gt;method= lm&lt;/code&gt;), together with an estimate of the standard error shown by the grey surrounding of the blue line (&lt;code&gt;se = TRUE&lt;/code&gt;). Finally I have highlighted the top 5 commented posts in blue, and the top 5 liked posts in red.&lt;/p&gt;
&lt;p&gt;It is pretty clear from this plot that there is virtually no correlation between the number of comments and the number of likes, particularly for those with more likes or comments. In general the posts with more likes do not have the most comments (and vice versa), suggesting that in general we like the nice posts, but comment on the ones that upset us. In fact, it looks as if Mandy’s post is the only exception, with both the highest number of likes but also a high number of comments (220).&lt;/p&gt;
&lt;p&gt;We can calculate the correlation between these measures, which is a measure of the linear relationship between two variables. A value of 1 indicates that they are entirely dependent on one another, whilst a value of 0 indicates that the two are entirely independent of one another. A value of -1 indicates an inverse depdnedancy, such that an increase in one variable is associated with a similar decrease in the other variable. Given the difference in the scales between likes and comments, I will use the &lt;em&gt;Spearman correlation&lt;/em&gt;, which looks at correlation between the ranks of the data and therefore ensures that each unit increment is 1 for both variables meaning that it is robust to outliers. The Spearman correlation between these two variables is &lt;code&gt;0.27&lt;/code&gt;, so there is virtually no correlation between likes and comments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-often-do-people-actually-talk-about-ultras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/h1&gt;
&lt;p&gt;It seems recently that there is more talk of charlatans and frauds than races, and a lot of people have commented on the fact that there seems to be less and less actual discussion of ultras recently. So let’s see if this is the case, by tracking how often the term &lt;em&gt;ultra&lt;/em&gt; is actually used over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ultraposts &amp;lt;- URC                                                        %&amp;gt;%
              filter(str_detect(message, &amp;quot;ultra&amp;quot;))                       %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
ultraposts_xts &amp;lt;- xts(ultraposts$n, order.by = ultraposts$created_time)
ultraposts_month &amp;lt;- apply.monthly(ultraposts_xts, FUN = sum)
plot(ultraposts_month, ylab = &amp;quot;Number of Ultra Posts&amp;quot;, main = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-ultra_usage-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Over the last year or so, the number of people in the group has risen dramatically, and yet it certainly seems that fewer people are actually discussing ultras these days. I guess read into that what you will – perhaps the feed is indeed dominated by Suunto vs Garmin questions?&lt;/p&gt;
&lt;p&gt;Hell, let’s find out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suunto-or-garmin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/h1&gt;
&lt;p&gt;So let’s take a look at the real question that everybody cares about – which is more popular, Suunto or Garmin. All my cards on the table; I have a Suunto Peak Ambit 3, but if it helps I had to Google that because I really don’t keep up on these things. I’m really not a gear not, and prefer to make do. The only reason that I got this is that my previous watch died a death, and I like to use a watch for navigation. I didn’t pay for it – at that price I couldn’t bring myself to fork out the money. But it was a gift, and I am very pleased with it. It has a great battery life, and is pretty simple when loading data to my computer. Despite being a stats guy, I don’t normally focus much on my own data, but actually it has been interesting to see how slow I have become recently due to an ongoing injury. Perhaps it will help me to push myself in training onece it is sorted.&lt;/p&gt;
&lt;p&gt;But as I understand it, the Garmin Fenix 3 does exactly the same stuff. Is one better than the other? I couldn’t possibly say. Many people have tried, but I suspect that it comes down to personal preference rather than there being some objective difference between the two.&lt;/p&gt;
&lt;p&gt;But just for the hell of it, let’s see how often people talk about the two. I will be simply using fuzzy matching to look for any use of the terms “suunto” or “garmin” in the post. Fuzzy matching is able to spot slight misspellings, such as “Sunto” or “Garmin”, and is carried out using the base &lt;code&gt;agrep()&lt;/code&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suunto &amp;lt;- agrep(&amp;quot;suunto&amp;quot;, URC$message, ignore.case = TRUE)
garmin &amp;lt;- agrep(&amp;quot;garmin&amp;quot;, URC$message, ignore.case = TRUE)
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c(&amp;quot;Suunto&amp;quot;, &amp;quot;Garmin&amp;quot;, &amp;quot;Both&amp;quot;), cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So of the 24,836 posts on the URC page, 237 (0.95 %) mention Suunto, whilst 552 (2.22 %) mention Garmin. Only 77 (0.3 %) mention both, which I assume are the posts specifically asking which of the two is best. Given the way some people moan about how often this question comes up, these numbers are actually surprisingly small I think. But based on this it seems that Garmin is more popular, although it would be interesting to look at the actual responses on those “VS” posts to see what the outcome actually was in each case.&lt;/p&gt;
&lt;p&gt;Having said that, there is nothing to suggest what these posts about Garmin’s are actually saying. They may be generally saying that they hate Garmins. So I am going to play around with a bit of sentiment analysis using the &lt;code&gt;qdap&lt;/code&gt; package. Essentially this is a machine learning algorithm that has been trained to identify the sentiment underlying a post, with positive values representing a generally positive sentiment (and vice versa). So let’s break down the Garmin and Suunto posts to see how they stack up:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;qdap&amp;quot;)

## Convert to ASCII and get rid of upper case
suunto_msg &amp;lt;- URC$message[suunto]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                         
garmin_msg &amp;lt;- URC$message[garmin]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                            

## Calculate the sentiment polarity
suunto_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, suunto_msg))
garmin_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, garmin_msg))

## Plot in a stacked barplot
sent_dat &amp;lt;- data.frame(Watch     = c(rep(&amp;quot;Suunto&amp;quot;, length(suunto)), 
                                     rep(&amp;quot;Garmin&amp;quot;, length(garmin))),
                       Sentiment = c(suunto_sentiment$all$polarity,
                                     garmin_sentiment$all$polarity))
ggplot(sent_dat, aes(x = Sentiment, col = Watch, fill = Watch)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Sentiment Polarity&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-Suunto-Or-Garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin_sentiment-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So a value of zero on this distribution plot suggests a neutral sentiment to the post, a positive number suggests a positive sentiment, and a negative number suggests a negative sentiment. While the majority of the posts seem to be fairly neutral in both cases, it certainly seems that the majority of the Garmin posts are positive whilst the Suunto posts are more neutral with many positive &lt;em&gt;and&lt;/em&gt; negative posts.&lt;/p&gt;
&lt;p&gt;We can actually put a number on this, for whether or not there is truly a statistically significant difference between the distribution of sentiment scores for the two watches. To do this, we will us a statistical test that checks how likely it is that we would see a difference of the magnitude seen here given that there is no difference between what people actually think of the watch. This is the so-called “null-hypothesis”, and essentially says that there is no difference, and any differences that we do see are purely random errors. We can test this hypothesis using one of a number of different tests, with the aim to see whether there is evidence that we can reject this null hypothesis, thus suggesting that there is indeed a true difference between the distributions. So we never really “prove” that there is a difference, but instead show that there is suitable evidence to disprove the null hypothesis.&lt;/p&gt;
&lt;p&gt;To do this some test statistic is calculated and is tested to see if it is significantly different than what you would expect to see by chance. Typically this is assessed using a “p-value”, which is one of the most misunderstood measurements in statistics. This value represents the probability that you would see a test statistic &lt;em&gt;at least as high&lt;/em&gt; as that measured purely by chance, even if both sets of data were drawn from the same distribution. So both the Garmin and Suunto scores are a tiny subset of all possible opinions of people in the world, the population distribution. Our two sample populations are either drawn from an overall population where there is no difference, or from two distinct populations for people who have a view one way or the other.&lt;/p&gt;
&lt;p&gt;It is pretty clear from the above figure that these values are not normally distributed (a so-called “bell-curve” distribution), so we cannot use a simple t-test which basically tests the difference in the means between two distributions (after taking the variance into account). Instead we would be better off using a non-parametric test which does not require the data to be parameterised into some fixed probability density function. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test&#34;&gt;Kolmogorov Smirnov test&lt;/a&gt; is one method that can be used, and works by looking at how the cummulative distribution functions of two distinct samples differ:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]],
## subset(sent_dat, : p-value will be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## D = 0.093653, p-value = 0.1091
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we run a two-sided test, which simply means that we have no &lt;em&gt;a priori&lt;/em&gt; reason to suspect one distribution to be greater than the other. We could do a one-sided test where the alternative hypothesis that we are testing is “A is greater than B”, rather than the two-sided test where we are testing the alternative hypothesis that “A is not equal to B”. &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the maximum distance between the empirical distribution functions, and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the p-value. Typically, a threshold used to reject the null hypothesis is for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be less than 0.05 (5 %), although it is fairly arbitrary. But in this case, we would conclude that there is not sufficient evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;As an alternative, we can instead use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann–Whitney_U_test&#34;&gt;Wilcoxon rank sum test&lt;/a&gt; (also called the Mann-Whitney &lt;em&gt;U&lt;/em&gt; test). The idea is to rank all of the data, sum up the ranks from one of the samples, and use this to calculate the test statistic &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; (which takes into account the sample sizes). So if the distributions are pretty similar, the sum of the ranks will be similar for sample 1 and sample 2. If there is a big difference, one sample will have more higher ranked values than the other, resulting in a higher value for &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. Let’s take a look at this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## W = 59157, p-value = 0.03066
## alternative hypothesis: true location shift is not equal to 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the test statistic here is actually &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, but in this case &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are equivalent. So this test would result in us rejecting the null hypothesis with the same threshold as above. So which one is correct? Well, this is a great example of why you should never trust statistics. Both of these are perfectly reasonable tests to perform in this case but give different results. Many people would simply choose the one that gives the lowest p-value, but this is pretty naughty and is often called “p-hacking”. At the end of the day, a p-value higher than 0.05 does not mean that there is &lt;em&gt;not&lt;/em&gt; a true difference between the distributions, just that the current data does not provide enough evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;So my conclusion from this is that I made the wrong decision, and will from now on look upon my useless Suunto watch with hatred and resentment. I can only hope that this post will save anyone else from making such an awful mistake.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/h1&gt;
&lt;p&gt;It has been quite &lt;em&gt;fun&lt;/em&gt; playing around with these data tonight, and I have had an opportunity to try out a few new techniques that I have wanted to play with for a while. As ever, there is loads more that can be gleaned from these data, but I should probably do something a little more productive right now. Like sleeping. I have actually done a while load more playing with machine learning algorithms of my own, but this post has already become a little too unruly so I will post this later as a separate post.&lt;/p&gt;
&lt;p&gt;But in summary, people on the Ultra Running Community page spend far too much time posting during working hours, seem to be talking less and less about ultra running, and definitely prefer Garmins to Suuntos. So this has all been completely worth it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] qdap_2.3.2             RColorBrewer_1.1-2     qdapTools_1.3.3       
##  [4] qdapRegex_0.7.2        qdapDictionaries_1.0.7 xts_0.11-2            
##  [7] zoo_1.8-6              ggplot2_3.2.0          stringr_1.4.0         
## [10] dplyr_0.8.3            tidyr_1.0.0            randomForest_4.6-14   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.2          lattice_0.20-38     xlsxjars_0.6.1     
##  [4] gtools_3.8.1        assertthat_0.2.1    zeallot_0.1.0      
##  [7] digest_0.6.20       slam_0.1-45         R6_2.4.0           
## [10] plyr_1.8.4          chron_2.3-54        backports_1.1.4    
## [13] evaluate_0.14       blogdown_0.16       pillar_1.4.2       
## [16] rlang_0.4.0         lazyeval_0.2.2      data.table_1.12.2  
## [19] gdata_2.18.0        rmarkdown_1.14      gender_0.5.2       
## [22] labeling_0.3        igraph_1.2.4.1      RCurl_1.95-4.12    
## [25] munsell_0.5.0       compiler_3.6.1      xfun_0.8           
## [28] pkgconfig_2.0.2     htmltools_0.3.6     reports_0.1.4      
## [31] tidyselect_0.2.5    tibble_2.1.3        gridExtra_2.3      
## [34] bookdown_0.12       codetools_0.2-16    XML_3.98-1.20      
## [37] crayon_1.3.4        withr_2.1.2         bitops_1.0-6       
## [40] openNLP_0.2-6       grid_3.6.1          gtable_0.3.0       
## [43] lifecycle_0.1.0     magrittr_1.5        scales_1.0.0       
## [46] xlsx_0.6.1          stringi_1.4.3       reshape2_1.4.3     
## [49] NLP_0.2-0           openNLPdata_1.5.3-4 xml2_1.2.0         
## [52] venneuler_1.1-0     ellipsis_0.2.0.1    vctrs_0.2.0        
## [55] wordcloud_2.6       tools_3.6.1         glue_1.3.1         
## [58] purrr_0.3.3         plotrix_3.7-6       parallel_3.6.1     
## [61] yaml_2.2.0          tm_0.7-6            colorspace_1.4-1   
## [64] rJava_0.9-11        knitr_1.23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Ultramarathon, Ultra Marathon or Ultra-Marathon?</title>
      <link>/post/2018-04-15-ultrarunner-or-ultra-runner/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-15-ultrarunner-or-ultra-runner/</guid>
      <description>

&lt;h1 id=&#34;note&#34;&gt;Note&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;welcome&#34;&gt;Welcome!&lt;/h1&gt;

&lt;p&gt;Well, welcome to my new blog I guess. I have had a bit of a hiatus from writing recently but I am trying to get back into the habit over on my running-related blog &lt;a href=&#34;http://constantforwardmotion.blogspot.com&#34; target=&#34;_blank&#34;&gt;http://constantforwardmotion.blogspot.com&lt;/a&gt;. Over there I will be mainly moaning about my injury woes (and maybe talking about some of my more ridiculous races if I ever get back to being able to run again), but this blog is a little bit different. In the past I have played around with various bits of data for some quite interesting (I think anyway) posts (e.g. &lt;a href=&#34;http://constantforwardmotion.blogspot.co.uk/2013/08/thames-path-100-2013-race-analysis.html&#34; target=&#34;_blank&#34;&gt;this post looking at the 2013 Centurion Running Thames Path 100 mile race&lt;/a&gt;). I am a data analyst by trade, and I am about to start a whole new stage of my career, working as a Senior Post Doc at the &lt;a href=&#34;http://www.port.ac.uk&#34; target=&#34;_blank&#34;&gt;University of Portsmouth&lt;/a&gt; where I will be building my own Bioinformatcs lab. Scary stuff.&lt;/p&gt;

&lt;p&gt;Anyway, I decided to set this blog up as a more technical place to play around with various data analysis techniques, new algorithms, new packages, etc. Since it is something that I am pretty passionate about, there is likely to be a bit of a running theme throughout, but really I will be looking at data from a whole load of different sources. I often play around with &amp;ldquo;fun&amp;rdquo; challenges like those set by &lt;a href=&#34;https://projecteuler.net&#34; target=&#34;_blank&#34;&gt;Project Euler&lt;/a&gt; and &lt;a href=&#34;https://www.kaggle.com&#34; target=&#34;_blank&#34;&gt;Kaggle&lt;/a&gt;, so I figured that it may be useful for me to put some of these out there in case my dumb mistakes when learning these things can help somebody else in the future. As the great philosopher Jake from &lt;a href=&#34;https://en.wikipedia.org/wiki/Adventure_Time&#34; target=&#34;_blank&#34;&gt;Adventure Time&lt;/a&gt;, once said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dude, sucking at something is the first step towards being sorta good at something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, here is the first post in what I hope will become a regular source of potentially interesting data wrangling. I just wanted to do something simple to start with, and one thing that I have always wanted to play with is the Twitter API for accessing the myriad completely valid and interesting opinions of the millions of Twitter users out there&amp;hellip; Hopefully I will keep away from the shadier parts of the interwebz, but in all seriousness there is a huge amount of useful stuff floating around out there.&lt;/p&gt;

&lt;p&gt;So quite why I picked this particular question as my first post I have no idea. With billions of opinions and social interactions available to me, I have chosen to answer the following rather inconsequential question:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Is it an &amp;ldquo;Ultramarathon&amp;rdquo;, &amp;ldquo;Ultra Marathon&amp;rdquo; or &amp;ldquo;Ultra-Marathon&amp;rdquo;?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My idea of a good time is running a really, really long way, usually for a whole day or sometimes even more. Any race over a marathon in distance is called an &amp;ldquo;ultramarathon&amp;rdquo; - i.e. it is beyond the marathon. Now, there is a huge amount of baggage attached to this, and some people seem to really get their panties in a bunch over the term. Does it &amp;ldquo;count&amp;rdquo; as an ultramarathon if I run back to the car after a marathon? Does it &amp;ldquo;count&amp;rdquo; as an ultramarathon if I walk most of it? Does it &amp;ldquo;count&amp;rdquo; as an ultramarathon if I run a marathon a day for a whole week? There&amp;rsquo;s a lot of questions about &amp;ldquo;counting&amp;rdquo;, but I&amp;rsquo;ve never been very good at counting personally (says the mathematician&amp;hellip;). I actually really dislike the word myself as it smacks a little of elitism, and I prefer to just think of it as running. A 10K is a running race, a marathon is a running race, and a 100 miler is a running race. Let&amp;rsquo;s just leave it at that.&lt;/p&gt;

&lt;p&gt;Anyway, one thing that I have never seen definitively answered is what the correct nomenclature should actually be, and I find myself switching willy nilly between the three possible spellings as the winds change. I&amp;rsquo;ve probably used all three in this post. So I thought that I would let the people speak, and see what the general consensus is of the Twitterati. And let&amp;rsquo;s face it, no ultrarunner worth their salt would run without Tweeting about it. So let&amp;rsquo;s take a look at which term is used most often on Twitter and settle this mass debate that I am having with myself (chortle) once and for all.&lt;/p&gt;

&lt;h1 id=&#34;setting-up-the-twitter-api&#34;&gt;Setting up the twitter API&lt;/h1&gt;

&lt;p&gt;Twitter uses &lt;a href=&#34;https://apps.twitter.com&#34; target=&#34;_blank&#34;&gt;OAuth&lt;/a&gt; as a way to control programmatic access to its information without requiring passwords. Essentially, Twitter grants you an access token which is used to grant you access to the client information via a web service without actually giving you direct access to the client machine. It&amp;rsquo;s pretty easy to set up. Once you have a Twitter account set up, go to &lt;a href=&#34;https://apps.twitter.com&#34; target=&#34;_blank&#34;&gt;https://apps.twitter.com&lt;/a&gt; and click on the &amp;ldquo;Create New App&amp;rdquo; button. This will bring up the following page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;Twitter_create_app.png&#34; alt=&#34;Create a new app in Twitter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I filled this in with details of this blog, and created a new app. Note that to avoid errors further down the line, you need to set the &amp;ldquo;Callback URL&amp;rdquo; field to &lt;code&gt;http://127.0.0.1:1410&lt;/code&gt; which will return to your localhost on port 1410 following authentication. Press &amp;ldquo;Create your Twitter application&amp;rdquo; to create your app. This will take you to a page with information about your new app, including a tab at the top of the page called &amp;ldquo;Keys and Access Tokens&amp;rdquo;. The &amp;ldquo;API Key&amp;rdquo; and &amp;ldquo;API Secret&amp;rdquo; can then be used to access the API.&lt;/p&gt;

&lt;p&gt;One important change to make to the basic settings is to click on the &amp;ldquo;Permissions&amp;rdquo; tab and make sure that your app is set up to have permissions to &amp;ldquo;Read, Write and Access direct messages&amp;rdquo;. Be sure to regenerate your access tokens after making any changes.&lt;/p&gt;

&lt;h1 id=&#34;twitter&#34;&gt;TwitteR&lt;/h1&gt;

&lt;p&gt;Since R is my go-to analysis package, I will be using the &lt;code&gt;TwitteR&lt;/code&gt; package from Jeff Gentry to access the API. You can also access through scripting languages like perl and python, which I will likely explore in the future. You can install &lt;code&gt;TwitteR&lt;/code&gt; from the &lt;a href=&#34;https://cran.r-project.org&#34; target=&#34;_blank&#34;&gt;Comprehensive R Archive Network&lt;/a&gt; by doing the following:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
install.packages(&amp;ldquo;twitteR&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Or alternatively you can install the less stable, but more up-to-date, development version from GitHub:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
install_github(&amp;ldquo;geoffjentry/twitteR&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;This requires the &lt;code&gt;devtools&lt;/code&gt; package to be installed as well.&lt;/p&gt;

&lt;p&gt;We should now be set up, but actually I found that I also needed to install some additional packages so that OAuth credentials can be correctly captured in the browser-based authentication:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
install.packages(&amp;ldquo;httpuv&amp;rdquo;)
install.packages(&amp;ldquo;httr&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Now we need to set up our authorisation (put your API key and secret in place of these placeholders):&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;twitteR&amp;rdquo;)
setup_twitter_oauth(&amp;ldquo;API key&amp;rdquo;, &amp;ldquo;API secret&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;This will open up a browser session where you can authenticate your API app. After this, close the browser and you are ready to go!&lt;/p&gt;

&lt;h1 id=&#34;ultramarathon-ultra-marathon-or-ultra-marathon&#34;&gt;Ultramarathon, Ultra Marathon, or Ultra-Marathon?&lt;/h1&gt;

&lt;p&gt;So now we are all set up and we can take a look at how to access the API. The workhorse of the &lt;code&gt;twitteR&lt;/code&gt; package is the &lt;code&gt;searchTwitter()&lt;/code&gt; function. This can search for something like a hashtag or key word, and can use basic boolean logic such as &lt;code&gt;AND&lt;/code&gt; (&lt;code&gt;+&lt;/code&gt;) and &lt;code&gt;OR&lt;/code&gt; (&lt;code&gt;-&lt;/code&gt;). The API actually only allows you to access information from a short time in the past, so we can only get Tweets from the last week or so. So let&amp;rsquo;s get the most recent Tweets relating to ultrarunning and count how many Tweets over the last few days have used the three different terms:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;twitteR&amp;rdquo;)
numtweets      &amp;lt;- 500000
um_tweets_all  &amp;lt;- searchTwitter(&amp;ldquo;ultramarathon|ultra marathon&amp;rdquo;, n = numtweets)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;warning-in-dorppapicall-search-tweets-n-params-params&#34;&gt;Warning in doRppAPICall(&amp;ldquo;search/tweets&amp;rdquo;, n, params = params,&lt;/h2&gt;

&lt;h2 id=&#34;retryonratelimit-retryonratelimit-500000-tweets-were-requested-but-the&#34;&gt;retryOnRateLimit = retryOnRateLimit, : 500000 tweets were requested but the&lt;/h2&gt;

&lt;h2 id=&#34;api-can-only-return-961&#34;&gt;API can only return 961&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight r %}
um_tweets_trim &amp;lt;- strip_retweets(um_tweets_all, strip_manual=TRUE, strip_mt=TRUE)
um_tweets_text &amp;lt;- sapply(um_tweets_trim, function (x) x$getText())
um_tweets_text &amp;lt;- um_tweets_text[grep(&amp;ldquo;ultramarathon|ultra marathon|ultra-marathon&amp;rdquo;, um_tweets_text, ignore.case = TRUE)]
um_count &amp;lt;- NULL
for (t in c(&amp;ldquo;ultramarathon&amp;rdquo;, &amp;ldquo;ultra-marathon&amp;rdquo;, &amp;ldquo;ultra marathon&amp;rdquo;)) {
  um_count[[t]] &amp;lt;- length(grep(t, um_tweets_text, ignore.case = TRUE))
}
par(mar = c(0,0,0,0))
pie(um_count, col = c(&amp;ldquo;grey90&amp;rdquo;,&amp;ldquo;grey70&amp;rdquo;,&amp;ldquo;grey50&amp;rdquo;))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ultramarathon_tweet_pie.png&#34; alt=&#34;plot of chunk 2017-03-15_ultramarathon_tweet_pie&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at this code. After loading the &lt;code&gt;twitteR&lt;/code&gt; package and specifying the number of Tweets to load, we access the Twitter API using &lt;code&gt;searchTwitter&lt;/code&gt; and load in the 500000 most recent Tweets with the terms &amp;ldquo;ultra&amp;rdquo;, &amp;ldquo;running&amp;rdquo;, or &amp;ldquo;ultrarunning&amp;rdquo; in them (there will be many fewer than this, but I want to capture every Tweet possible). This produces a list of 961 objects of class &lt;code&gt;status&lt;/code&gt;, which is a specially defined reference class as a container for Twitter statuses. Next we strip out all of the retweets to leave 585 Tweets, then pull out the text from the list objects (&lt;code&gt;sapply()&lt;/code&gt; applies the accessor function &lt;code&gt;getText()&lt;/code&gt; to all elements of the list). Since I am only interested in three specific terms, I use regular expressions to look only at Tweets containing one of these terms (giving 316 Tweets), and then count how many Tweets contain each of the three specific terms. Finally I generate a pie chart of the results (yes, yes, I know - I hate pie charts as well).&lt;/p&gt;

&lt;p&gt;A couple of things here. First of all, there are a lot of retweets. Of the 961 Tweets originally analysed, only 585 remain after removing the retweets. This means that 39.13% of these Tweets were retweets. Man, we ultrarunners aren&amp;rsquo;t very original are we? Unfortunately this therefore drastically reduces the number of Tweets that I am analysing. Secondly, because of the way the pattern matching is done we end up with a lot of Tweets with &amp;ldquo;ultra&amp;rdquo; or &amp;ldquo;running&amp;rdquo; in them that don&amp;rsquo;t match any of the three specific terms that I am looking at here. Also, this counting may be double counting some Tweets if both versions are used in a single Tweet. But I can&amp;rsquo;t be bothered taking such stupidity into account right now! ;)&lt;/p&gt;

&lt;p&gt;So with these caveats in place, it seems pretty clear that the correct term is most definitely &amp;ldquo;Ultra Marathon&amp;rdquo;. So there you go.&lt;/p&gt;

&lt;h1 id=&#34;ultrarunning-ultra-running-or-ultra-running&#34;&gt;Ultrarunning, Ultra Running or Ultra-Running?&lt;/h1&gt;

&lt;p&gt;Okay cool. So we know how to define the event. How about the act of running an ultra marathon? So let&amp;rsquo;s do the same again, this time looking at whether I should be saying &amp;ldquo;ultrarunning&amp;rdquo;, &amp;ldquo;ultra running&amp;rdquo;, or &amp;ldquo;ultra-running&amp;rdquo;. The code is practically identical, just using slightly different words in the regular expression:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
ur_tweets_all  &amp;lt;- searchTwitter(&amp;ldquo;ultrarunning|ultra running&amp;rdquo;,  n = numtweets)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight text %}&lt;/p&gt;

&lt;h2 id=&#34;warning-in-dorppapicall-search-tweets-n-params-params-1&#34;&gt;Warning in doRppAPICall(&amp;ldquo;search/tweets&amp;rdquo;, n, params = params,&lt;/h2&gt;

&lt;h2 id=&#34;retryonratelimit-retryonratelimit-500000-tweets-were-requested-but-the-1&#34;&gt;retryOnRateLimit = retryOnRateLimit, : 500000 tweets were requested but the&lt;/h2&gt;

&lt;h2 id=&#34;api-can-only-return-11984&#34;&gt;API can only return 11984&lt;/h2&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;{% highlight r %}
ur_tweets_trim &amp;lt;- strip_retweets(ur_tweets_all, strip_manual=TRUE, strip_mt=TRUE)
ur_tweets_text &amp;lt;- sapply(ur_tweets_trim, function (x) x$getText())
ur_tweets_text &amp;lt;- ur_tweets_text[grep(&amp;ldquo;ultrarunning|ultra running|ultra-running&amp;rdquo;, ur_tweets_text, ignore.case = TRUE)]
ur_count &amp;lt;- NULL
for (t in c(&amp;ldquo;ultrarunning&amp;rdquo;, &amp;ldquo;ultra-running&amp;rdquo;, &amp;ldquo;ultra running&amp;rdquo;)) {
  ur_count[[t]] &amp;lt;- length(grep(t, ur_tweets_text, ignore.case = TRUE))
}
par(mar = c(0,0,0,0))
pie(ur_count, col = c(&amp;ldquo;grey90&amp;rdquo;,&amp;ldquo;grey70&amp;rdquo;,&amp;ldquo;grey50&amp;rdquo;))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ultrarunning_tweet_pie.png&#34; alt=&#34;plot of chunk 2017-03-15_ultrarunning_tweet_pie&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are a lot more Tweets relating to ultra &amp;ldquo;running&amp;rdquo; compared to ultra &amp;ldquo;marathon&amp;rdquo;, with 11984 Tweets in the starting data set. However, again we lose a lot of Tweets through retweets leaving us with only 2112 Tweets to play with. After trimming out Tweets that don&amp;rsquo;t follow the format that I am looking at here, we are left with only 179 &amp;ndash; even less than in the last analysis.&lt;/p&gt;

&lt;p&gt;In this case, it is less clear cut, and whilst the single word term &amp;ldquo;ultrarunning&amp;rdquo; is used most often, the two word &amp;ldquo;ultra running&amp;rdquo; is not far behind. Damn, I wanted a clearly defined outcome, but I guess I will let you off whichever one you choose to use. But god help anybody who chooses to hyphenate either term&amp;hellip;&lt;/p&gt;

&lt;h1 id=&#34;word-cloud&#34;&gt;Word Cloud&lt;/h1&gt;

&lt;p&gt;Since we have these Tweets available, let&amp;rsquo;s generate a word cloud to see what other terms are being talked about in relation to ultra marathons and ultrarunning. A word cloud takes some text and works out the most common words within it, then represents them in a cloud of words (funnily enough) with more common words being more prominent. Here we use the text mining package &lt;code&gt;tm&lt;/code&gt; for identifying and processing unique words from these Tweets, and the &lt;code&gt;wordcloud&lt;/code&gt; package for plotting them. The Tweets are loaded into a &lt;code&gt;Corpus&lt;/code&gt; object, and various mappings are performed to remove irrelevant text like punctuation, as well as commonly used words in English like &lt;em&gt;I&lt;/em&gt;, &lt;em&gt;We&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt;, &lt;em&gt;the&lt;/em&gt;, etc. Note that I have converted the encoding of all of these Tweets into UTF-8 encoding as I was having issues using the &lt;code&gt;tolower()&lt;/code&gt; function when some Tweets contained non-UTF-8 characters. I have coloured the plot using the &lt;code&gt;brewer.pal()&lt;/code&gt; function from the &lt;code&gt;RColorBrewer&lt;/code&gt; package, which in this case generates a palette of 9 equally spaced colours ranging from Red to blue (via white). The parameters here will plot a maximum of 1,000 words, and will only consider a word if it is present more than 5 times. By not using a random order, the most prominent words are plotted at the center of the cloud:&lt;/p&gt;

&lt;p&gt;{% highlight r %}
library(&amp;ldquo;tm&amp;rdquo;)
library(&amp;ldquo;wordcloud&amp;rdquo;)
library(&amp;ldquo;SnowballC&amp;rdquo;)
all_tweets &amp;lt;- c(ur_tweets_text, um_tweets_text)
all_tweets &amp;lt;- iconv(all_tweets, &amp;ldquo;latin1&amp;rdquo;, &amp;ldquo;ASCII&amp;rdquo;, sub = &amp;ldquo;&amp;rdquo;)         ## Convert encodings
cloud_dat &amp;lt;- Corpus(VectorSource(all_tweets))                        ## Create Corpus
cloud_dat &amp;lt;- tm_map(cloud_dat, PlainTextDocument)                    ## Make plain text
cloud_dat &amp;lt;- tm_map(cloud_dat, content_transformer(tolower))         ## Convert to lower case
cloud_dat &amp;lt;- tm_map(cloud_dat, removePunctuation)                    ## Remove punctuation
cloud_dat &amp;lt;- tm_map(cloud_dat, removeWords, stopwords(&amp;ldquo;english&amp;rdquo;))    ## Remove common English words
par(mar = c(0,0,0,0))
wordcloud(cloud_dat, max.words = 1000, min.freq = 5, random.order = FALSE, colors = brewer.pal(9, &amp;ldquo;RdBu&amp;rdquo;))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ultramarathon_tweet_wordcloud.png&#34; alt=&#34;plot of chunk 2017-03-15_ultramarathon_tweet_wordcloud&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kind of what you would expect, with &lt;em&gt;ultra&lt;/em&gt;, &lt;em&gt;marathon&lt;/em&gt;, &lt;em&gt;running&lt;/em&gt;, &lt;em&gt;ultramarathon&lt;/em&gt;, and &lt;em&gt;ultrarunning&lt;/em&gt; being most prominent. &lt;em&gt;training&lt;/em&gt; is also pretty common, so it&amp;rsquo;s good to know that there is some of that going on between Tweets. I&amp;rsquo;m also pleased to see that &lt;em&gt;tom&lt;/em&gt; is quite enriched, which I can only assume to be Tom from Bognor&amp;rsquo;s take-over of the internet. It&amp;rsquo;s also nice to see people talking about &lt;em&gt;shorts&lt;/em&gt;, but come on people &amp;ndash; it&amp;rsquo;s always shorts weather!&lt;/p&gt;

&lt;h1 id=&#34;final-word&#34;&gt;Final Word&lt;/h1&gt;

&lt;p&gt;This has been quite useful for me to get the hang of using the Twitter API, and I hope it has been a little interesting or useful for some of you. I am hoping that this will kick off regular use of this blog, and I will try and update it more regularly along with my less technical &lt;a href=&#34;http://constantforwardmotion.blogspot.com&#34; target=&#34;_blank&#34;&gt;running blog&lt;/a&gt; as I start using it to play with new toys in my work. And hey, at least now you know that they are called &lt;strong&gt;Ultra Marathons&lt;/strong&gt; and that I love &lt;strong&gt;Ultrarunning&lt;/strong&gt;. So we&amp;rsquo;ve all learned something today. And knowing is half the battle.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Use R</title>
      <link>/resources/rtutorial/</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      <guid>/resources/rtutorial/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Installing R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basics-of-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Basics of R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-1&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-classes&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; Data Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.3&lt;/span&gt; Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lists&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.4&lt;/span&gt; Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.5&lt;/span&gt; Matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.6&lt;/span&gt; Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#printing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-packages&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Installing Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Data Frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-and-writing-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Reading and Writing Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#control-sequences&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Control Sequences&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#if-else&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.1&lt;/span&gt; IF ELSE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.2&lt;/span&gt; FOR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#while&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.3&lt;/span&gt; WHILE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loop-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7.4&lt;/span&gt; Loop Control&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#writing-functions-in-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Writing Functions in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-simple-statistics&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Some Simple Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plotting-with-r&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Plotting With R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.1&lt;/span&gt; Scatterplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.2&lt;/span&gt; Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.3&lt;/span&gt; Quantile-Quantile Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#line-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.4&lt;/span&gt; Line Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.5&lt;/span&gt; Density Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.6&lt;/span&gt; Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bar-plots-and-pie-charts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.7&lt;/span&gt; Bar Plots and Pie Charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graphical-control&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.8&lt;/span&gt; Graphical Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subplots&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.9&lt;/span&gt; Subplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-figures&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10.10&lt;/span&gt; Saving Figures&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Example Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-2&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.1&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#load-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.2&lt;/span&gt; Load Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculate-fold-change&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.3&lt;/span&gt; Calculate Fold Change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11.4&lt;/span&gt; Compare Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;This tutorial is a basic introduction to R that was originally written for biologists to get a basic understanding of how R functions. R is a software package that is a free to use open-source version of the S programming language. It is designed mainly for running statistical analyses and is very powerful in this regard. Follow through the tutorial and run the example commands by typing them into the command line as you go to see what happens. Don’t be afraid to play around with things as you go – it’s the best way to find out what certain functions do.&lt;/p&gt;
&lt;p&gt;You will notice that I have added comments to some of the code using the &lt;code&gt;#&lt;/code&gt; comment character. Everything to the right of this character is ignored by R. This can be used to add comments to your code, for instance to explain what a particular code chunk does. You can NEVER have too many comments!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Installing R&lt;/h1&gt;
&lt;p&gt;First of all, you will need to download and install R. The R website can be found at &lt;a href=&#34;http://www.r-project.org&#34;&gt;r-project.org&lt;/a&gt;. R is updated quite regularly – there is an updated release roughly every 6 months, with various developmental versions released between the official versions. The functions in R are actively maintained to ensure that they run as they should, and new functionality is added all of the time.&lt;/p&gt;
&lt;p&gt;The current version is 3.3.2. To download it, go to the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org&#34;&gt;cran.r-project.org&lt;/a&gt;). There are ready-made binaries available for MAC, windows, and most Linux distributions, so follow the links and download as instructed. You can also download the source code in a tarball, and can compile and install it using &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is also worth taking a look at the Integrated Development Environment &lt;a href=&#34;https://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;, which is a great open-source interface for R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basics-of-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Basics of R&lt;/h1&gt;
&lt;div id=&#34;introduction-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;Open the R environment. This is a command line version allowing you to see the results of the commands that you enter as you run them.&lt;/p&gt;
&lt;p&gt;The command line is shown by the &lt;code&gt;&amp;gt;&lt;/code&gt; character. Simply type your command here and press return to see the results. If your command is not complete, then the command line character will change to a &lt;code&gt;+&lt;/code&gt; to indicate that more input is required, for instance a missing parenthesis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print (&amp;quot;Hello World!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error: &amp;lt;text&amp;gt;:2:0: unexpected end of input
## 1: print (&amp;quot;Hello World!&amp;quot;
##    ^&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R stores “variables” using names made up of characters and numbers. A variable, as the name suggests, is a data “object” that can take any value that you want, and can be changed.&lt;/p&gt;
&lt;p&gt;The variable name can be anything that you like, although it must begin with a character. Whilst it is perfectly acceptable to use simple variable names such as &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;i&lt;/code&gt;, I recommend using a more descriptive name (e.g. &lt;code&gt;patient_height&lt;/code&gt; instead of &lt;code&gt;x&lt;/code&gt;). There are lots of different variable naming conventions to choose from (e.g. see &lt;a href=&#34;https://en.wikipedia.org/wiki/Naming_convention_(programming)&#34;&gt;here&lt;/a&gt;), but once you have chosen one try and stick to it.&lt;/p&gt;
&lt;p&gt;To assign a value to the variable, use the &lt;code&gt;&amp;lt;-&lt;/code&gt; command (less-than symbol followed by minus symbol). You can also use the &lt;code&gt;=&lt;/code&gt; symbol, but this has other uses (for instance using &lt;code&gt;==&lt;/code&gt; to test for equality) so I prefer to use the &lt;code&gt;&amp;lt;-&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
x # Returns the value stored in &amp;#39;x&amp;#39; - currently 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 5
x # Returns the value stored in &amp;#39;x&amp;#39; - now 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simple arithmetic can be performed using the standard arithmetic operators (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;), as well as the exponent operator (&lt;code&gt;^&lt;/code&gt;). There is a level of precedence to these functions – the exponent will be calculated first, followed by multiplication and division, followed by plus and minus. For this reason, you must be careful that your arithmetic is doing what you expect it to do. You can get around this by encapsulating subsets of the sum in parentheses, which will be calculated from the inside out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1+2*3 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1 + 2) * 3 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + (2 * 3) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Personally I think that you can NEVER have too many parentheses – they ensure that your equations are doing what they should, and they can help improve the readability of things making it easier to see what a calculation is trying to achieve.&lt;/p&gt;
&lt;p&gt;Another operator that you may not have seen before is the “modulo” operator (&lt;code&gt;%%&lt;/code&gt;), which gives you the remainder left after dividing by the number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6%%2 # 6 is divisible by 2 exactly three times&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6%%4 # 6 is divisible by 4 one time with a remainder of 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use other variables in these assignments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1
y &amp;lt;- x
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- x + y 
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-classes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Data Classes&lt;/h2&gt;
&lt;p&gt;Variables can take many forms, or “classes”. The most common are “numeric” (which you can do numerical calculations on), character (can contain letters, numbers, symbols etc., but cannot run numerical calculations), and logical (TRUE or FALSE). The speech marks character &lt;code&gt;&amp;quot;&lt;/code&gt; is used to show that the class of y is “character”. You can also use the apostrophe &lt;code&gt;&#39;&lt;/code&gt;. There &lt;em&gt;is&lt;/em&gt; a difference between these, but for now this is not important. You can check the class of a variable by using the &lt;code&gt;class()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 12345
class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- &amp;quot;12345&amp;quot;
class(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Addition is a well-defined operation on numerical objects, but is not defined on character class objects. Attempting to use a function which has not been defined for the object in question will throw an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x + 1 # x is numeric, so addition is well defined&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12346&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y + 1 # y is a character, so addition is not defined - produces an error&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in y + 1: non-numeric argument to binary operator&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see which objects are currently present in the R environment, use the &lt;code&gt;ls()&lt;/code&gt; command. To remove a particular object, use the &lt;code&gt;rm()&lt;/code&gt; command. &lt;em&gt;BE CAREFUL&lt;/em&gt; – once you have removed an object, it is gone forever!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 5
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(x)
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;y&amp;quot; &amp;quot;z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls()) # Removes all objects in the current R session
ls ()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also change the class of a variable by assigning to the &lt;code&gt;class()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- &amp;quot;12345&amp;quot;
x+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in x + 1: non-numeric argument to binary operator&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x) &amp;lt;- &amp;quot;numeric&amp;quot; 
class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12346&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other important data class is “logical”, which is simply a binary TRUE or FALSE value. There are certain operators that are used to compare two variables. The obvious ones are “is less than” (&lt;code&gt;&amp;lt;&lt;/code&gt;), “is greater than” (&lt;code&gt;&amp;gt;&lt;/code&gt;), “is equal to”&amp;quot; (&lt;code&gt;==&lt;/code&gt;). You can also combine these to see “is less than or equal to” (&lt;code&gt;&amp;lt;=&lt;/code&gt;) or “is greater than or equal to” (&lt;code&gt;&amp;gt;=&lt;/code&gt;). If the statement is true, then it will return the output “TRUE”. Otherwise it will return “FALSE”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 2
y &amp;lt;- 3
x &amp;lt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;gt;= y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also combine these logical tests to ask complex questions by using the “AND” (&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;) or the “OR” (&lt;code&gt;||&lt;/code&gt;) operators. You can also negate the output of a logical test by using the “NOT” (&lt;code&gt;!&lt;/code&gt;) operator. This lets you test for very specific events in your data. Again, I recommend using parentheses to break up your tests to ensure that the tests occur in the order which you expect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
y &amp;lt;- 7
z &amp;lt;- 6
(x &amp;lt;= 3 &amp;amp;&amp;amp; y &amp;gt;= 8) &amp;amp;&amp;amp; z == 6  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(x &amp;lt;= 3 &amp;amp;&amp;amp; y &amp;gt;= 8) || z == 6 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One important set of functions are the log and exponential functions. The exponential function is the function &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt;, such that &lt;span class=&#34;math inline&#34;&gt;\(e^x\)&lt;/span&gt; is its own derivative (&lt;span class=&#34;math inline&#34;&gt;\(\frac{d}{dx} e^x = e^x\)&lt;/span&gt;). The value e is the constant 2.718281828…, which is the limit &lt;span class=&#34;math inline&#34;&gt;\(\lim_{n \to \infty} (1+\frac{1}{n})^n\)&lt;/span&gt;. It is a very important value in mathematics (hence why it has its own constant). Logarithms are the inverse of exponents, with natural log being log base &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. Here are some examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log (8)     ## Natural logarithm - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.079442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log2 (8)    ## Log base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp (1)     ## e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.718282&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp (5)     ## e^5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 148.4132&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(exp(8)) ## log and exponential cancel out - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(log(8)) ## log and exponential cancel out - base e&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2^(log2(8)) ## log and exponential cancel out - base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log2 (2^8)  ## log and exponential cancel out - base 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Vectors&lt;/h2&gt;
&lt;p&gt;Single values are all well and good, but R has a number of ways to store multiple values in a single data structure. The simplest one of these is as a “vector” – simply a list of values of the same class. You create a vector by using the &lt;code&gt;c()&lt;/code&gt; (concatenate) function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(1,2,3,4,5) 
my_vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a very useful way of storing linked data together. You can access the individual elements of the vector by using square brackets (&lt;code&gt;[&lt;/code&gt;) to take a subset of the data. The elements in the vector are numbered from 1 upwards, so to take the first and last values we do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50)
my_vector[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, a value &lt;code&gt;NA&lt;/code&gt; (Not Applicable) is returned if you try to take an element that does not exist. The subset can be as long as you like, as long as it’s not longer than the full set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50)
my_vector[1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 20 30 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the &lt;code&gt;:&lt;/code&gt; in the brackets simply means to take all of the numbers from 1 through to 4, so this returns the first 4 elements of the vector. For instance, this is a simple way to take the numbers from 1 to 20:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:20&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To drop elements from an array, you use the minus symbol:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(10,20,30,40,50) 
my_vector[-1] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20 30 40 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[-length(my_vector)] &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 20 30 40&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[-c(1,3,5)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to generate a regular sequence in R is to use the &lt;code&gt;seq()&lt;/code&gt; command. You supply the start number and the end number, and then either supply the parameter &lt;code&gt;by&lt;/code&gt; to define the regular interval between values, or the parameter &lt;code&gt;length&lt;/code&gt; to specify the total number of values to return between the start and end value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, by = 1) # Returns the same as 1:20&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, by = 2) # Just the even numbers between 1 and 20 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  3  5  7  9 11 13 15 17 19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = 1, to = 20, length = 10) # Slightly different to above&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1.000000  3.111111  5.222222  7.333333  9.444444 11.555556 13.666667
##  [8] 15.777778 17.888889 20.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the &lt;code&gt;rep()&lt;/code&gt; function to give a vector of the specified length containing repeated values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(10, times = 5)      # Returns vector containing five copies of the number 10 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 10 10 10 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(1,2,3), each = 5) # Returns five 1s, then five 2s, then five 3s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the most powerful features of R is the fact that arithmetic can be conducted on entire vectors rather than having to loop through all values in the vector. Vectorisation of calculations in this way can give huge improvements in performance. For instance, if you sum two vectors (of equal size), the result will be a vector where the i&lt;sup&gt;th&lt;/sup&gt; entry is the sum of the i&lt;sup&gt;th&lt;/sup&gt; entries from the input vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(2,3,2,4,5) 
y &amp;lt;- c(4,1,1,2,3) 
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6 4 3 6 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x*y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  8  3  2  8 15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Lists&lt;/h2&gt;
&lt;p&gt;Another data structure that is very useful is the “list”. A list contains a number of things in a similar way to the vector, but the things that it contains can all be completely different classes. They can even be vectors and other lists (a list of lists):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 12345
## 
## [[2]]
## [1] &amp;quot;12345&amp;quot;
## 
## [[3]]
## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To subset a list, the syntax is slightly different and you use double square brackets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your list contains lists or vectors, you can subset these as well by using multiple sets of square brackets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[3]][5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can you see the difference between subsetting using &lt;code&gt;[[&lt;/code&gt; and using &lt;code&gt;[&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(12345, &amp;quot;12345&amp;quot;, c(1,2,3,4,5)) 
my_list[[3]]  ## Returns a vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[3]    ## Returns a list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list[3][5] ## Not defined!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can give names to the values in a vector or in a list by using the &lt;code&gt;names()&lt;/code&gt; function to make it easier to follow what the values are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector        &amp;lt;- c(1:5)
names(my_vector) &amp;lt;- c(&amp;quot;length&amp;quot;, &amp;quot;width&amp;quot;, &amp;quot;height&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;age&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use these names instead of the reference number to subset lists and vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector        &amp;lt;- c(1:5)
names(my_vector) &amp;lt;- c(&amp;quot;length&amp;quot;, &amp;quot;width&amp;quot;, &amp;quot;height&amp;quot;, &amp;quot;weight&amp;quot;, &amp;quot;age&amp;quot;) 
my_vector[&amp;quot;age&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## age 
##   5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The number of values in a vector or list can be found by using the &lt;code&gt;length()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- 1:5 
length(my_vector)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also sort the data simply using the &lt;code&gt;sort()&lt;/code&gt; function. If we want to get the indeces of the sorted vector (for instance to order a second vector based on the values in the first), we can use the &lt;code&gt;order()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Some values and their corresponding names
my_vals  &amp;lt;- c( 0.2, 1.7, 0.5, 3.4, 2.7 ) 
my_names &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;, &amp;quot;val5&amp;quot;)

## Sort the data
my_sorted &amp;lt;- sort(my_vals)  ## Returns the values in sorted order 
my_order  &amp;lt;- order(my_vals) ## Returns the indeces of the sorted values

## What is the difference between the two?
my_sorted &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2 0.5 1.7 2.7 3.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_order&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 2 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Get the sorted value names
sort(my_names)     ## This won&amp;#39;t work as this will order names alphabetically &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;val1&amp;quot; &amp;quot;val2&amp;quot; &amp;quot;val3&amp;quot; &amp;quot;val4&amp;quot; &amp;quot;val5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_names[my_order] ## This gives us the order based on the values themselves&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;val1&amp;quot; &amp;quot;val3&amp;quot; &amp;quot;val2&amp;quot; &amp;quot;val5&amp;quot; &amp;quot;val4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default the sort functions sort from lowest to highest. You can sort in decreasing by order by using the decreasing parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(my_vals , decreasing = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.4 2.7 1.7 0.5 0.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.5&lt;/span&gt; Matrices&lt;/h2&gt;
&lt;p&gt;Another data format is a “matrix”&amp;quot; (also known as an “array” in R). This is simply a table of values, and can be thought of as a multidimensional vector. To access specific values in the matrix, you again use the square bracket accessor function, but this time must specify both the row (first value) and column (second value):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    6   11   16
## [2,]    2    7   12   17
## [3,]    3    8   13   18
## [4,]    4    9   14   19
## [5,]    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix[3,4] &amp;lt;- 99999
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]  [,4]
## [1,]    1    6   11    16
## [2,]    2    7   12    17
## [3,]    3    8   13 99999
## [4,]    4    9   14    19
## [5,]    5   10   15    20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, the values are added to the matrix in a column-wise fashion (from top to bottom for column 1, then the same for column 2, etc.). To fill the matrix in a row-wise fashion, use the byrow parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the square bracket accessor function to extract subsets of the matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    6   11   16
## [2,]    2    7   12   17
## [3,]    3    8   13   18
## [4,]    4    9   14   19
## [5,]    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sub_matrix &amp;lt;- my_matrix[1:2, 3:4]
sub_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   11   16
## [2,]   12   17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;cbind()&lt;/code&gt; (column bind) and &lt;code&gt;rbind()&lt;/code&gt; (row bind) functions can also be used to concatenate vectors together by row or by column to give a matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cbind(c(1,2,3), c(4,5,6), c(7,8,9)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(c(1,2,3), c(4,5,6), c(7,8,9)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## [3,]    7    8    9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the names of both the rows and the columns by using the &lt;code&gt;rownames()&lt;/code&gt; and &lt;code&gt;colnames()&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix           &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
rownames(my_matrix) &amp;lt;- c(&amp;quot;row1&amp;quot;, &amp;quot;row2&amp;quot;, &amp;quot;row3&amp;quot;, &amp;quot;row4&amp;quot;, &amp;quot;row5&amp;quot;) 
colnames(my_matrix) &amp;lt;- c(&amp;quot;col1&amp;quot;, &amp;quot;col2&amp;quot;, &amp;quot;col3&amp;quot;, &amp;quot;col4&amp;quot;) 
my_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      col1 col2 col3 col4
## row1    1    6   11   16
## row2    2    7   12   17
## row3    3    8   13   18
## row4    4    9   14   19
## row5    5   10   15   20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dimensions of the matrix can be found by using the &lt;code&gt;dim()&lt;/code&gt; function, which gives the number of rows (first value) and the number of columns (second value) of the matrix. You can access the number of rows or columns directly by using the &lt;code&gt;nrows()&lt;/code&gt; or &lt;code&gt;ncols()&lt;/code&gt; functions respectively:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_matrix &amp;lt;- matrix(1:20, nrow = 5, ncol = 4) 
dim(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(my_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.6&lt;/span&gt; Functions&lt;/h2&gt;
&lt;p&gt;R also uses functions (also known as methods, subroutines, and procedures) which simply take in one or more values, do something to them, and return a result. A simple example is the &lt;code&gt;sum()&lt;/code&gt; function, which takes in two or more values in the form of a vector, and returns the sum of all of the values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- 1:5
sum(my_vector) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, the &lt;code&gt;sum()&lt;/code&gt; function takes only one variable (in this case a numeric vector). Sometimes functions take more than one variable (also known as “arguments”). These are named values that must be specified for the function to run. For example, the &lt;code&gt;cor()&lt;/code&gt; function returns the correlation between two vectors. This requires several variables to be supplied – two vectors, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, of equal length – and you can also supply a number of additional arguments to control how the function works, including the &lt;code&gt;method&lt;/code&gt; argument, which lets you specify which method to use to calculate the correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample1 &amp;lt;- c(0.9, 1.2, 8.9, -0.3, 6.4)
sample2 &amp;lt;- c(0.6, 1.3, 9.0, -0.5, 6.2)
cor(sample1, sample2 , method = &amp;quot;pearson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9991263&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(sample1, sample2 , method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we gave a name to the third argument (“method”), but not the first two. If you do not name arguments, they will be taken and assigned to the arguments in the order in which they are input. The first two arguments required by the function are &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; – the two vectors to compare. So there is no problem with not naming these (although you could, if you wanted to, say &lt;code&gt;x=sample1, y=sample2&lt;/code&gt;). Any arguments not submitted will use their default value. For instance, the Pearson correlation is the default for &lt;code&gt;method&lt;/code&gt;, so you could get this by simply typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pearson_cor &amp;lt;- cor(sample1 , sample2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, there is another argument for &lt;code&gt;cor()&lt;/code&gt;, &lt;code&gt;use&lt;/code&gt;, for which we are happy to use the default value before we get to &lt;code&gt;method&lt;/code&gt;. We therefore need to name &lt;code&gt;method&lt;/code&gt; to make sure that “pearson”&amp;quot; is not assigned to the &lt;code&gt;use&lt;/code&gt; argument in the function. It is always safer to name the arguments if you are unsure of the order. You can check the arguments using the &lt;code&gt;args()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(cor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, y = NULL, use = &amp;quot;everything&amp;quot;, method = c(&amp;quot;pearson&amp;quot;, 
##     &amp;quot;kendall&amp;quot;, &amp;quot;spearman&amp;quot;)) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to find out what a function does, there is a lot of very helpful documentation available in R. To see the documentation for a specific function, use the &lt;code&gt;help()&lt;/code&gt; function. If you want to try and find a function, you can search using a keyword by using the &lt;code&gt;help.search()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(cor)
?cor # Alternative for help() 
help.search(&amp;quot;correlation&amp;quot;)
??correlation # Alternative for help.search()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;printing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.7&lt;/span&gt; Printing&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;print()&lt;/code&gt; can be used to print whatever is stored in the variable the function is called on. Print is what is known as an “overloaded”&amp;quot; function, which means that there are many functions named &lt;code&gt;print()&lt;/code&gt;, each written to deal with a variable of a different class. The correct one is used based on the variable that you supply. So calling &lt;code&gt;print()&lt;/code&gt; on a numeric variable will print the value stored in the variable. Calling it on a vector prints all of the values stored in the vector. Calling it on a list will print the contents of the list split into an easily identifiable way. There are also many more classes in R for which print is defined, but there are too many to describe here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1
print (x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- 1:5
print (y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- list(val1 = 1:5, val2 = 6:10)
print (z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you notice, using &lt;code&gt;print()&lt;/code&gt; is the default when you just call the variable itself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- list(val1 = 1:5, val2 = 6:10) 
print (z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 1 2 3 4 5
## 
## $val2
## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cat()&lt;/code&gt; is similar to print in that the results of calling it are that text is printed to the console. The main use for &lt;code&gt;cat()&lt;/code&gt; is to con&lt;strong&gt;CAT&lt;/strong&gt;enate two or more variables together and instantly print them to the console. The additional argument &lt;code&gt;sep&lt;/code&gt; specifies the character to use to separate the different variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;hello&amp;quot;, &amp;quot;world&amp;quot;, sep = &amp;quot; &amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## hello world&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:5
y &amp;lt;- &amp;quot;bottles of beer&amp;quot;
cat(x, y, sep = &amp;quot;\t&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1    2   3   4   5   bottles of beer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;\t&lt;/code&gt; is a special printing character that you can use with the &lt;code&gt;cat()&lt;/code&gt; function that prints a tab character. Another similar special character that you may need to use is &lt;code&gt;\n&lt;/code&gt; which prints a new line.&lt;/p&gt;
&lt;p&gt;Another similar function is the &lt;code&gt;paste()&lt;/code&gt; function, which also concatenates multiple values together. The differences between this and &lt;code&gt;cat()&lt;/code&gt; are that the results of &lt;code&gt;paste()&lt;/code&gt; can be saved to a different variable which requires a call to &lt;code&gt;print()&lt;/code&gt; to see the results, and &lt;code&gt;paste()&lt;/code&gt; can be used to concatenate individual elements of a vector by using the additional &lt;code&gt;collapse&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;hello&amp;quot;, &amp;quot;world&amp;quot;, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;hello\tworld&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;sample&amp;quot;, 1:5, sep=&amp;quot;_&amp;quot;)) # Returns a vector of values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample_1&amp;quot; &amp;quot;sample_2&amp;quot; &amp;quot;sample_3&amp;quot; &amp;quot;sample_4&amp;quot; &amp;quot;sample_5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;quot;sample&amp;quot;, 1:5, sep=&amp;quot;_&amp;quot;, collapse=&amp;quot;\n&amp;quot;)) # Prints values separated by new lines&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sample_1\nsample_2\nsample_3\nsample_4\nsample_5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do you notice the difference between &lt;code&gt;print()&lt;/code&gt; and &lt;code&gt;cat()&lt;/code&gt;? While &lt;code&gt;print()&lt;/code&gt; prints the &lt;code&gt;\t&lt;/code&gt; character as is, &lt;code&gt;cat()&lt;/code&gt; prints the actual tab space. This is a process known as “interpolation”. In many programming languages, using double quotes in strings results in special characters being interpolated, whilst single quotes will print as is. However, in R the two can be used relatively interchangeably.&lt;/p&gt;
&lt;p&gt;There are also other characters, such as &lt;code&gt;&#39;&lt;/code&gt;, &lt;code&gt;&amp;quot;&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;\&lt;/code&gt;, which may require “escaping” with a backslash to avoid R interpreting the character in a different context. For instance, if you have a string containing an apostrophe within a string defined using apostrophes, the string will be interpreted as terminating earlier, and the code will not do what you expect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;It&amp;#39;s very annoying when this happens...&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error: &amp;lt;text&amp;gt;:1:9: unexpected symbol
## 1: cat(&amp;#39;It&amp;#39;s
##             ^&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the string submitted to &lt;code&gt;cat()&lt;/code&gt; is actual “It” rather than the intended “It’s very annoying when this happens…”. The function will not know what to do about the remainder of the string, so an error will occur. However, by escaping the apostrophe, the string will be interpreted correctly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;#39;It\&amp;#39;s easily fixed though!&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another alternative is to use double apostrophes as the delimiter, which will avoid the single apostrophe being misinterpreted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;It&amp;#39;s easily fixed though!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## It&amp;#39;s easily fixed though!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One function that gives you slightly more control over the formatting of your data is the &lt;code&gt;sprintf()&lt;/code&gt; function. This function allows you to specify things like the width in which to print each variable, which is useful for arranging output in a table format (note that you need to use &lt;code&gt;cat()&lt;/code&gt; to actual print to the screen):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10s\t%5s\n&amp;quot;, &amp;quot;Hello&amp;quot;, &amp;quot;World&amp;quot;), 
    sprintf(&amp;quot;%10s\t%5s\n&amp;quot;, &amp;quot;Helloooooo&amp;quot;, &amp;quot;World&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Hello   World
##  Helloooooo  World&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sprintf()&lt;/code&gt; function takes as input a string telling R how you want your inputs to be formatted, followed by a list of the inputs. Within the formatting string, placeholders of the form &lt;code&gt;%10s&lt;/code&gt; are replaced by the given inputs, with the first being replaced by the first argument in the list, and so on (so the number of additional arguments to &lt;code&gt;sprintf&lt;/code&gt; must match the number of placeholders). The number in the placeholder defines the width to allocate for printing that argument (positive is right aligned, negative is left aligned), decimal numbers in the placeholder define precision of floating point numbers, and the letter defines the type of argument to print (e.g. &lt;code&gt;s&lt;/code&gt; for string, &lt;code&gt;i&lt;/code&gt; for integer, &lt;code&gt;f&lt;/code&gt; for fixed point decimal, &lt;code&gt;e&lt;/code&gt; for exponential decimal). Note that special characters are interpolated by &lt;code&gt;cat()&lt;/code&gt; as before. Here are some examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%20s\n&amp;quot;, &amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%-20s\n&amp;quot;, &amp;quot;Hello&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hello&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10i\n&amp;quot;, 12345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      12345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10f\n&amp;quot;, 12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  12.345000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(sprintf(&amp;quot;%10e\n&amp;quot;, 12.345)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1.234500e+01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Installing Packages&lt;/h1&gt;
&lt;p&gt;The main R package contains a large number of commonly used functions. There are also additional functions available in other “packages” that you can get hold of from the Comprehensive R Archive Network, or &lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;. To load in a package, first download and install the package from CRAN using the &lt;code&gt;install.packages()&lt;/code&gt; function (if it is not already downloaded), and then use the “library” command to make the libraries available to your current R session:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?xtable &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## No documentation for &amp;#39;xtable&amp;#39; in specified packages and libraries:
## you could try &amp;#39;??xtable&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;xtable&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in contrib.url(repos, &amp;quot;source&amp;quot;): trying to use CRAN without setting a mirror&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;xtable&amp;quot;) 
?xtable&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;xtable()&lt;/code&gt; is not available in the R environment until you have loaded the package. Only the most commonly used functions are made available in the R environment by default (for example the package “stats” is loaded by default, which contains all commonly used statistical fuctions). There are also a number of commonly used packages that are part of the R installation, but which are not automatically loaded when you start a new R session. There are also thousands of additional packages available, some written by users, which can perform most of the things that you would ever want to do. Chances are, if you want to do something it’s already available from somewhere. Don’t re-invent the wheel if you can help it.&lt;/p&gt;
&lt;p&gt;Since R is so useful for analysing biological data, the &lt;code&gt;bioconductor&lt;/code&gt; project was set up to bring together packages used for the analysis of high-throughput data (it started with microarrays, but now there are packages available for analysis of sequencing data). Bioconductor packages can be downloaded from &lt;a href=&#34;http://www.bioconductor.org&#34;&gt;bioconductor.org&lt;/a&gt;. However, there is also a simple way to install bioconductor packages directly from within R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;http://bioconductor.org/biocLite.R&amp;quot;) # Load the biocLite() script 
biocLite() # Installs the basic packages required to use bioconductor 
biocLite(&amp;quot;DESeq&amp;quot;) # Installs a specific bioconductor package&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Data Frames&lt;/h1&gt;
&lt;p&gt;Data frames are the most powerful data types in R. They look similar to matrices, but the data structure is actually more similar to a list of vectors (all of the same length). The simplest way to think of them is as being similar to spreadsheets in Excel.&lt;/p&gt;
&lt;p&gt;You can create data frames either in a similar way to how you create a list, or also by converting a matrix object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(matrix(1:12, nrow = 3, ncol = 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3 V4
## 1  1  4  7 10
## 2  2  5  8 11
## 3  3  6  9 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how, in the second data frame, no column names are specified so R sets the defaults as &lt;code&gt;V1&lt;/code&gt;, &lt;code&gt;V2&lt;/code&gt;, &lt;code&gt;V3&lt;/code&gt;, etc. Whilst data frames do have row names, it is the column names that are the most important. As with lists, these can be changed by using the &lt;code&gt;names()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df        &amp;lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) 
names(my_df) &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;)
my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You access the elements of a data frame either using single square bracket notation in the same way as for a matrix, or you can access the individual columns using double square bracket notation in the same way as for lists. You can also access the individual columns by using the special &lt;code&gt;$&lt;/code&gt; operator which is specifically used for data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df &amp;lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) 
names(my_df) &amp;lt;- c(&amp;quot;val1&amp;quot;, &amp;quot;val2&amp;quot;, &amp;quot;val3&amp;quot;, &amp;quot;val4&amp;quot;)
my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sub_df &amp;lt;- my_df[1:2, 3:4]
sub_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val3 val4
## 1    7   10
## 2    8   11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val1 &amp;lt;- my_df[[1]]
val1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val2 &amp;lt;- my_df[[&amp;quot;val2&amp;quot;]] 
val2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4 5 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val3 &amp;lt;- my_df$val3
val3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7 8 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The beauty of data frames is that the data frame columns can be dealt with as if they were individual variables. For this reason, the column names must be suitable variable names (i.e. alphanumeric and not starting with a number) and must be unique. If you attach a data frame, you can access the columns as if they were variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df &amp;lt;- data.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12))
attach(my_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked _by_ .GlobalEnv:
## 
##     val1, val2, val3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   val1 val2 val3 val4
## 1    1    4    7   10
## 2    2    5    8   11
## 3    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;val1 + 1000 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1001 1002 1003&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detach(my_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a slight aside, I dislike using the attach/detach approach to accessing elements of a data frame, as it can make it difficult when reading through your code to tell which variable is being accessed. For instance, if you have a variable named &lt;code&gt;myname&lt;/code&gt;, and a data frame with a column &lt;code&gt;myname&lt;/code&gt;, then using &lt;code&gt;df$myname&lt;/code&gt; in your code makes it much clearer where you are accessing your data from than simply using &lt;code&gt;myname&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that to make changes to the data frame itself, we need to use the &lt;code&gt;$&lt;/code&gt; accessor function (or double square brackets), otherwise a new variable &lt;code&gt;val1&lt;/code&gt; will be created. Data frames should be set up in such a way that every row represents an independent observation, and the columns represent the independent variables that you may be interested in. For instance, if you have taken a measurement of say the weight of each sample in triplicate, you would not represent the data like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;SampleName&lt;/th&gt;
&lt;th&gt;Weight1&lt;/th&gt;
&lt;th&gt;Weight2&lt;/th&gt;
&lt;th&gt;Weight3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;td&gt;66.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;80.3&lt;/td&gt;
&lt;td&gt;79.8&lt;/td&gt;
&lt;td&gt;79.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But instead you would ensure that the two independent variables (weight and replicate number) were in their own columns:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;SampleName&lt;/th&gt;
&lt;th&gt;Replicate&lt;/th&gt;
&lt;th&gt;Weight&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;67.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;66.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;80.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;79.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;79.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now all of the weights are in a single column that can be analysed.&lt;/p&gt;
&lt;p&gt;Subsetting a data frame is also very powerful. The subset command allows you to look for the rows of a data frame that fit certain criteria. For instance, to pull out the genes that show more than 2-fold expression and a p-value less than 0.05, you would do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp &amp;lt;- data.frame(geneName   = paste(&amp;quot;gene&amp;quot;, 1:10, sep = &amp;quot;&amp;quot;), 
                       foldChange = rnorm(10, mean = 2, sd = 1),
                       pVal       = rnorm(10, mean = 0.05, sd = 0.05)) 
signif_genes &amp;lt;- subset(gene_exp, foldChange &amp;gt; 2 &amp;amp; pVal &amp;lt;= 0.05)
signif_genes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   geneName foldChange        pVal
## 1    gene1   2.234659 0.038236225
## 3    gene3   2.713056 0.047058009
## 5    gene5   2.982536 0.003146303&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice here that we use a single &lt;code&gt;&amp;amp;&lt;/code&gt; rather than the double &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; that we used earlier. This is because we are doing a vector-based logical test (that is performing the test on each element of the vector to get a vector of logical values at the end). It is very easy to forget this and accidentally use the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;, which will not give you what you want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fake_signif_genes &amp;lt;- subset(gene_exp, foldChange &amp;gt; 2 &amp;amp;&amp;amp; pVal &amp;lt;= 0.05) 
fake_signif_genes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    geneName foldChange        pVal
## 1     gene1  2.2346589 0.038236225
## 2     gene2  1.1171569 0.010953464
## 3     gene3  2.7130557 0.047058009
## 4     gene4  1.9352067 0.046603706
## 5     gene5  2.9825355 0.003146303
## 6     gene6  2.2315724 0.138186853
## 7     gene7  1.6141956 0.093650234
## 8     gene8  1.8556861 0.032846692
## 9     gene9  0.7700426 0.063940521
## 10   gene10  1.3124471 0.081082591&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another form of data that comes in very handy, particularly with data frames, is the “factor”. Factors are a way of dealing with categorical data, and simply encode the possible levels with numberic dummy values 0, 1, 2, etc. (which are used in modelling procedures such as ANOVA):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector &amp;lt;- c(&amp;quot;apples&amp;quot;, &amp;quot;pears&amp;quot;, &amp;quot;apples&amp;quot;, &amp;quot;oranges&amp;quot;, &amp;quot;pears&amp;quot;) 
my_vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;apples&amp;quot;  &amp;quot;pears&amp;quot;   &amp;quot;apples&amp;quot;  &amp;quot;oranges&amp;quot; &amp;quot;pears&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_factor &amp;lt;- as.factor(my_vector)
my_factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] apples  pears   apples  oranges pears  
## Levels: apples oranges pears&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(my_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;apples&amp;quot;  &amp;quot;oranges&amp;quot; &amp;quot;pears&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since data frames can hold data of different classes within its columns (a data frame is essentially a glorified list), it is very important to ensure that each column is assigned the correct class so that R functions that you use later do the correct thing with the data. For instance, R will automatically convert character entries to factors with all possible values as the factor levels. You can quickly see the class of all of your columns by using the &lt;code&gt;str()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(gene_exp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    10 obs. of  3 variables:
##  $ geneName  : Factor w/ 10 levels &amp;quot;gene1&amp;quot;,&amp;quot;gene10&amp;quot;,..: 1 3 4 5 6 7 8 9 10 2
##  $ foldChange: num  2.23 1.12 2.71 1.94 2.98 ...
##  $ pVal      : num  0.03824 0.01095 0.04706 0.0466 0.00315 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Whilst factors are incredibly useful in modelling and plotting, they may not necessarily be exactly what you want. For instance, in this case the column &lt;code&gt;geneName&lt;/code&gt; has been converted into a factor, with levels &lt;code&gt;gene1&lt;/code&gt;, …, &lt;code&gt;gene10&lt;/code&gt;. If we try and add in a new gene, &lt;code&gt;gene11&lt;/code&gt;, this will not work as all entries of a factor must be one of the specified levels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp_plus &amp;lt;- rbind(gene_exp, c(&amp;quot;gene11&amp;quot;, 1.789, 0.0034))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in `[&amp;lt;-.factor`(`*tmp*`, ri, value = &amp;quot;gene11&amp;quot;): invalid factor
## level, NA generated&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gene_exp_plus&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    geneName        foldChange                pVal
## 1     gene1  2.23465888817809  0.0382362251121874
## 2     gene2  1.11715691372872  0.0109534636149735
## 3     gene3  2.71305571811009  0.0470580093671668
## 4     gene4  1.93520669112966  0.0466037061884548
## 5     gene5  2.98253553079485 0.00314630349135952
## 6     gene6  2.23157241340974   0.138186853004448
## 7     gene7  1.61419556341739   0.093650234491183
## 8     gene8  1.85568611921072  0.0328466921261732
## 9     gene9 0.770042566261392  0.0639405210499306
## 10   gene10  1.31244706165258  0.0810825912438105
## 11     &amp;lt;NA&amp;gt;             1.789              0.0034&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead we would be better off treating &lt;code&gt;geneName&lt;/code&gt; as a character vector, since we are unlikely to treat it as a categorical variable in later model fitting analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-and-writing-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Reading and Writing Data&lt;/h1&gt;
&lt;p&gt;Reading and writing data in R is quite simple, and is most easily done by using pure text files. Functions exist for reading other formats as well (e.g. Excel tables), but for now we will concentrate on raw text. There are some very basic example files available from &lt;a href=&#34;/files/RTutorial/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unless you give the complete path for a file, R will look in it’s current working directory for any files that you want to load in. By default, R will use your system’s home directory, but you can set this by using the setwd() function. You can check that the correct working directory is set by using the getwd() function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;/path/to/mydir/&amp;quot;) 
getwd ()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have a list of data in a file (e.g. a list of gene names separated by new lines), then the simplest method to use is &lt;code&gt;scan()&lt;/code&gt;. You must tell &lt;code&gt;scan()&lt;/code&gt; where to find the data file (either the full path, or a relative path from the current working directory), as well as the format that the data should be read in as (generally either “character” or “numeric”):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_file &amp;lt;- &amp;quot;gene_list.txt&amp;quot;
gene_list &amp;lt;- scan(my_file, what = &amp;quot;character&amp;quot;, sep = &amp;quot;\n&amp;quot;) 
gene_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;gene1&amp;quot;  &amp;quot;gene2&amp;quot;  &amp;quot;gene3&amp;quot;  &amp;quot;gene4&amp;quot;  &amp;quot;gene5&amp;quot;  &amp;quot;gene6&amp;quot;  &amp;quot;gene7&amp;quot; 
##  [8] &amp;quot;gene8&amp;quot;  &amp;quot;gene9&amp;quot;  &amp;quot;gene10&amp;quot; &amp;quot;gene11&amp;quot; &amp;quot;gene12&amp;quot; &amp;quot;gene13&amp;quot; &amp;quot;gene14&amp;quot;
## [15] &amp;quot;gene15&amp;quot; &amp;quot;gene16&amp;quot; &amp;quot;gene17&amp;quot; &amp;quot;gene18&amp;quot; &amp;quot;gene19&amp;quot; &amp;quot;gene20&amp;quot; &amp;quot;gene21&amp;quot;
## [22] &amp;quot;gene22&amp;quot; &amp;quot;gene23&amp;quot; &amp;quot;gene24&amp;quot; &amp;quot;gene25&amp;quot; &amp;quot;gene26&amp;quot; &amp;quot;gene27&amp;quot; &amp;quot;gene28&amp;quot;
## [29] &amp;quot;gene29&amp;quot; &amp;quot;gene30&amp;quot; &amp;quot;gene31&amp;quot; &amp;quot;gene32&amp;quot; &amp;quot;gene33&amp;quot; &amp;quot;gene34&amp;quot; &amp;quot;gene35&amp;quot;
## [36] &amp;quot;gene36&amp;quot; &amp;quot;gene37&amp;quot; &amp;quot;gene38&amp;quot; &amp;quot;gene39&amp;quot; &amp;quot;gene40&amp;quot; &amp;quot;gene41&amp;quot; &amp;quot;gene42&amp;quot;
## [43] &amp;quot;gene43&amp;quot; &amp;quot;gene44&amp;quot; &amp;quot;gene45&amp;quot; &amp;quot;gene46&amp;quot; &amp;quot;gene47&amp;quot; &amp;quot;gene48&amp;quot; &amp;quot;gene49&amp;quot;
## [50] &amp;quot;gene50&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For tables (for instance tab-delimited files saved from Excel), the easiest way is to use the &lt;code&gt;read.table()&lt;/code&gt; function. This works by using &lt;code&gt;scan()&lt;/code&gt; to read in each line from the table, then splitting the line by the specified delimiter. It is easier (or at least you are less prone to mistakes) to read such files when there are no empty cells, so try to fill empty data with a missing data character, such as &lt;code&gt;NA&lt;/code&gt; (the default):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  4 variables:
##  $ SampleName: Factor w/ 6 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;,..: 1 2 3 4 5 6
##  $ Treatment : Factor w/ 2 levels &amp;quot;Control&amp;quot;,&amp;quot;Drug&amp;quot;: 1 1 1 2 2 2
##  $ Replicate : int  1 2 3 1 2 3
##  $ CellType  : Factor w/ 1 level &amp;quot;HeLa&amp;quot;: 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are lots of additional arguments to the &lt;code&gt;read.table()&lt;/code&gt; function; &lt;code&gt;header&lt;/code&gt; is a boolean value that says whether or not the first row should be used to name the columns of the data frame, &lt;code&gt;sep&lt;/code&gt; gives the delimiter between column entries (e.g. &lt;code&gt;\t&lt;/code&gt; for tab-delimited files, or &lt;code&gt;,&lt;/code&gt; for comma-separated files), &lt;code&gt;skip&lt;/code&gt; tells R to skip the first &lt;code&gt;n&lt;/code&gt; rows of the input, and &lt;code&gt;nrow&lt;/code&gt; tells R to only load the first &lt;code&gt;n&lt;/code&gt; rows that it sees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;, nrow = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2 obs. of  4 variables:
##  $ SampleName: Factor w/ 2 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;: 1 2
##  $ Treatment : Factor w/ 1 level &amp;quot;Control&amp;quot;: 1 1
##  $ Replicate : int  1 2
##  $ CellType  : Factor w/ 1 level &amp;quot;HeLa&amp;quot;: 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = FALSE, sep = &amp;quot;\t&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    7 obs. of  4 variables:
##  $ V1: Factor w/ 7 levels &amp;quot;sample1&amp;quot;,&amp;quot;sample2&amp;quot;,..: 7 1 2 3 4 5 6
##  $ V2: Factor w/ 3 levels &amp;quot;Control&amp;quot;,&amp;quot;Drug&amp;quot;,..: 3 1 1 1 2 2 2
##  $ V3: Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;Replicate&amp;quot;: 4 1 2 3 1 2 3
##  $ V4: Factor w/ 2 levels &amp;quot;CellType&amp;quot;,&amp;quot;HeLa&amp;quot;: 1 2 2 2 2 2 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that when the header is not used, the numeric column &lt;code&gt;Replicate&lt;/code&gt; is now interpreted in the same way as the character columns, because now the first entry is non-numeric. By default, &lt;code&gt;read.table()&lt;/code&gt; converts character columns into factors, which can be avoided by setting the &lt;code&gt;stringsAsFactors&lt;/code&gt; argument to &lt;code&gt;FALSE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(read.table(&amp;quot;sample_annotation.txt&amp;quot;, header = TRUE, sep = &amp;quot;\t&amp;quot;, stringsAsFactors = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    6 obs. of  4 variables:
##  $ SampleName: chr  &amp;quot;sample1&amp;quot; &amp;quot;sample2&amp;quot; &amp;quot;sample3&amp;quot; &amp;quot;sample4&amp;quot; ...
##  $ Treatment : chr  &amp;quot;Control&amp;quot; &amp;quot;Control&amp;quot; &amp;quot;Control&amp;quot; &amp;quot;Drug&amp;quot; ...
##  $ Replicate : int  1 2 3 1 2 3
##  $ CellType  : chr  &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; &amp;quot;HeLa&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;control-sequences&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Control Sequences&lt;/h1&gt;
&lt;p&gt;One of the most useful things to be able to do with computers is to repeat the same command multiple times without having to do it by hand each time. For this, control sequences can be used to give you close control over the progress of your program.&lt;/p&gt;
&lt;div id=&#34;if-else&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.1&lt;/span&gt; IF ELSE&lt;/h2&gt;
&lt;p&gt;The first control sequence to look at is the “if else” command, which acts as a switch to run one of a selection of possible commands given a switch that you specify. For instance, you may want to do something different depending on whether a value is odd or even:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_val &amp;lt;- 3
if (my_val%%2 == 0) { # If it is even (exactly divisible by 2)
  cat (&amp;quot;Value is even\n&amp;quot;)
} else {              # Otherwise it must be odd
  cat (&amp;quot;Value is odd\n&amp;quot;) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Value is odd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the expression in the parentheses following “if” is evaluated, and if it evaluates to TRUE then the block of code contained within the following curly braces is evaluated. Otherwise, the block of code following the “else” statement is evaluated. You can add additional tests by using the “else if” statement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_val &amp;lt;- 27
if (my_val%%2 == 0) {
  cat(&amp;quot;Value is divisible by 2\n&amp;quot;)
} else if (my_val%%3 == 0) { 
  cat(&amp;quot;Value is divisible by 3\n&amp;quot;)
} else if (my_val%%4 == 0) {
  ...
} else if (my_val%%n == 0) {
  cat(&amp;quot;Value is divisible by n\n&amp;quot;)
} else {
  cat(&amp;quot;Value is not divisible by 1:n\n&amp;quot;) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each switch is followed by a block of code surrounded by curly braces, and the conditional statements are evaluated until one evaluates to TRUE, at which point R avaluates this code bloack then exits. If none of them evaluate to TRUE, then the default code block following “else” is evaluated instead. If no “else” block is present, then the default is to just do nothing. These blocks can be as complicated as you like, and you can have “if else” statements within the blocks to create a hierarchical structure. Note that this ifelse block will ony return the smallest factor of &lt;code&gt;myval&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.2&lt;/span&gt; FOR&lt;/h2&gt;
&lt;p&gt;Another control structure is the “for loop”, which will conduct the code in the block multiple times for a variety of values that you specify at the start. For instance, here is a simple countdown script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 10:1) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
  if (i == 1) {
    cat(&amp;quot;Blastoff!\n&amp;quot;) 
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the index &lt;code&gt;i&lt;/code&gt; is taken from the set of numbers (10, 9, …, 1), starting at the first value 10, and each time prints out the number followed by a newline. Then an &lt;code&gt;if&lt;/code&gt; statement checks to see if we have reached the final number, at which point it is time for blast off! At this point, it returns to the start of the block, updates the number to the second value 9, and repeats. It does this until there are no more values to use.&lt;/p&gt;
&lt;p&gt;As a small aside, this is slightly inefficient. Evaluation of the &lt;code&gt;if&lt;/code&gt; statement is conducted every single time the loop is traversed (10 times in this example). It will only ever be true at the end of the loop, so we could always take this out of the loop and evaluate the final printout after the loop is finished and save ourselves 10 calculations. Whilst the difference here is negligible, thinking of things like this may save you time in the future:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 10:1) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;)
} 
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;while&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.3&lt;/span&gt; WHILE&lt;/h2&gt;
&lt;p&gt;The final main control structure is the “while loop”. This is similar to the “for loop”, and will continue to evaluate the code chunk as long as the specified expression evaluates to TRUE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i &amp;lt;- 10
while (i &amp;gt; 0) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
  i &amp;lt;- i - 1
} 
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10...
## 9...
## 8...
## 7...
## 6...
## 5...
## 4...
## 3...
## 2...
## 1...
## Blastoff!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This does exactly the same as the “for loop” above. In general, either can be used for a given purpose, but there are times when one would be more “elegant” than the other. For instance, here the for loop is better as you do not need to manually subtract 1 from the index each time.&lt;/p&gt;
&lt;p&gt;However, if you did not know how many iterations were required before finding what you are looking for (for instance searching through a number of files), a “while loop” may be more suitable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HOWEVER&lt;/strong&gt;: Be aware that it is possible to get caught up in an “infinite loop”. This happens if the conditional statement never evaluates to FALSE. If this happens, press either ESCAPE or press the &lt;code&gt;CONROL&lt;/code&gt; key and the letter &lt;code&gt;c&lt;/code&gt; (&lt;code&gt;CTRL+c&lt;/code&gt;) to quit out of the current function (&lt;code&gt;CMD+c&lt;/code&gt;) for Mac). For instance, if we forget to decrement the index, &lt;code&gt;i&lt;/code&gt; will always be 10 and will therefore never be less than 0. This loop will therefore run forever:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i &amp;lt;- 10
while (i &amp;gt; 0) {
  cat(i, &amp;quot;...\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Blastoff!\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loop-control&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;7.4&lt;/span&gt; Loop Control&lt;/h2&gt;
&lt;p&gt;You can leave control loops early by using flow control constructs. &lt;code&gt;next&lt;/code&gt; skips out of the current loop and moves onto the next in the sequence:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:10) { 
  if (i == 5) {
    next 
  }
  cat (i, &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Finished loop\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4
## 6
## 7
## 8
## 9
## 10
## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;break&lt;/code&gt; will leave the loop entirely, and will return to the function after the last curly brace in the code chunk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:10) { 
  if (i == 5) {
    break 
  }
  cat (i, &amp;quot;\n&amp;quot;, sep = &amp;quot;&amp;quot;) 
}
cat(&amp;quot;Finished loop\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1
## 2
## 3
## 4
## Finished loop&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-functions-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Writing Functions in R&lt;/h1&gt;
&lt;p&gt;There are many functions available in R, and chances are if you want to do something somebody has already written the function to do it. It is best to not re-invent the wheel if possible (or at least it is more efficient – sometimes it is good to reinvent the wheel to understand how it works), but very often you will want to create your own functions to save replicating code.&lt;/p&gt;
&lt;p&gt;A function takes in one or more variables, does something with them, and returns something (e.g. a value or a plot). For instance, calculating the mean of a number of values is simply a case of adding them together and dividing by the number of values. Let’s write a function to do this and check that it matches the &lt;code&gt;mean()&lt;/code&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_mean &amp;lt;- function (x) { # Here, x is a numeric vector 
  nvals &amp;lt;- length(x)
  valsum &amp;lt;- sum(x)
  return (valsum/nvals)
}
my_vals &amp;lt;- c(3,5,6,3,4,3,4,7) 
my_mean(my_vals) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, as with the loops earlier, the function is contained within a block of curly braces. A numeric vector is given to the function, the mean is calculated, and this value is returned to the user using the &lt;code&gt;return()&lt;/code&gt; function. This value can be captured into a variable of your choosing in the same way as with any function.&lt;/p&gt;
&lt;p&gt;You can also add further arguments to the function call. If you want an argument to have a default value, you can specify this in the function declaration. This is the value that will be used if no argument value is specified. Any arguments that do not have a default value must be specified when calling the function, or an error will be thrown:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- function(x, arg) { 
  return(paste(x, arg, sep = &amp;quot; &amp;quot;))
}
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in paste(x, arg, sep = &amp;quot; &amp;quot;): argument &amp;quot;arg&amp;quot; is missing, with no default&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s try and add a default value for &lt;code&gt;arg&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;foo &amp;lt;- function(x, arg = &amp;quot;World!&amp;quot;) { 
  return(paste(x, arg, sep = &amp;quot; &amp;quot;))
}
foo (&amp;quot;Hello&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hello World!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a good point to mention an idea known as “scope”. After running the following function, have a look at the value &lt;code&gt;valsum&lt;/code&gt; calculated within the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_mean &amp;lt;- function (x) { # Here, x is a numeric vector 
  nvals &amp;lt;- length(x)
  valsum &amp;lt;- sum(x)
  return (valsum/nvals)
}
my_vals &amp;lt;- c(3,5,6,3,4,3,4,7)
my_mean(my_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(valsum) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in print(valsum): object &amp;#39;valsum&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what went wrong? The error message says that R cannot find the object &lt;code&gt;valsum&lt;/code&gt;. So where is it? The “scope” of an object is the environment where it can be found. Up until now, we have been using what are known as “global variables”. That is we have created all of our objects within the “global environment”, which is the top level where R searches for objects. These objects are available at all times.&lt;/p&gt;
&lt;p&gt;However, when we call a function, a new environment, or “scope”, is created, and all variables created within the function become “local variables” that can only be accessed from within the function itself. As soon as we leave the function, these local variables are deleted. If you think about it, this makes sense – otherwise, every time we called a function, memory would fill up with a whole load of temporary objects. Also, how many functions do you think create an object called &lt;code&gt;x&lt;/code&gt;? Pretty much all of them (it’s generally the name of the first argument, as in my example). If we created an object &lt;code&gt;x&lt;/code&gt;, then ran a couple of functions, and then went to use &lt;code&gt;x&lt;/code&gt; again, chances are it would no longer be what we thought it was.&lt;/p&gt;
&lt;p&gt;So, the function itself is completely self-contained. A copy of the input variable is stored in a new local variable called &lt;code&gt;x&lt;/code&gt;, something is done to this object (possibly creating additional objects along the way), something is returned, and then all of these objects in the scope of the function are removed, and we move back into the global environment.&lt;/p&gt;
&lt;p&gt;Functions are incredibly useful when we want to repeat the same set of actions on multiple sets of data. The “apply”&amp;quot; set of functions are very useful for running a single function multiple times on input data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;apply()&lt;/code&gt; works on a matrix or data frame, and applies the function named by the argument &lt;code&gt;FUN&lt;/code&gt; across either the rows or the columns of the table, as specified with the &lt;code&gt;MAR&lt;/code&gt; (margin) argument (&lt;code&gt;MAR=1&lt;/code&gt; for rows, &lt;code&gt;MAR=2&lt;/code&gt; for columns). For instance, suppose that you had a matrix of expression values from a microarray, where each row was a different gene, and each column is the signal from a different probe on the array. We may want to calculate the mean value across these probes for each gene:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probe_file  &amp;lt;- &amp;quot;probe_values.txt&amp;quot;
probe_dat   &amp;lt;- read.table(probe_file, header = TRUE, sep = &amp;quot;\t&amp;quot;) 
probe_means &amp;lt;- apply(probe_dat[, -1], MAR = 1, FUN = mean) 
probe_means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 21.2 29.8 85.8 63.6 25.6 44.8 70.8 88.4 47.6 35.8 90.2 57.8 24.2 23.2
## [15] 25.0 57.6 83.0 62.8 33.8 28.4 13.2 58.4 24.6 28.2 47.2  6.4 94.6 14.2
## [29] 39.6 53.4 80.2 47.8  9.8 58.8 59.8  0.4 63.8 33.0 22.4 53.8 37.8 68.8
## [43] 99.6 97.6  5.0 59.8 95.4 -0.2  1.4 52.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, apply can be used to apply a function to all values by using &lt;code&gt;MAR=c(1,2)&lt;/code&gt; to run across rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;probe_file    &amp;lt;- &amp;quot;probe_values.txt&amp;quot;
probe_dat     &amp;lt;- read.table(probe_file, header = TRUE, sep = &amp;quot;\t&amp;quot;) 
probe_dat_log &amp;lt;- apply(probe_dat[, -1], MAR = c(1,2), FUN = exp) 
probe_dat_log&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Probe1       Probe2       Probe3       Probe4       Probe5
##  [1,] 4.424134e+05 7.896296e+13 2.648912e+10 2.980958e+03 3.931334e+12
##  [2,] 3.931334e+12 1.907347e+21 8.886111e+06 5.987414e+04 1.285160e+19
##  [3,] 2.235247e+37 5.052394e+31 1.811239e+41 1.220403e+39 8.223013e+36
##  [4,] 3.404276e+29 4.607187e+28 5.685720e+24 1.373383e+32 1.041376e+23
##  [5,] 3.584913e+09 1.068647e+13 1.484132e+02 7.896296e+13 8.659340e+16
##  [6,] 3.493427e+19 2.581313e+20 1.142007e+26 3.185593e+16 5.834617e+14
##  [7,] 2.758513e+33 1.545539e+25 6.837671e+30 1.373383e+32 1.373383e+32
##  [8,] 2.451246e+40 1.373383e+32 4.375039e+48 3.025077e+36 2.038281e+34
##  [9,] 3.185593e+16 1.907347e+21 1.041376e+23 1.545539e+25 2.353853e+17
## [10,] 2.146436e+14 6.565997e+07 1.285160e+19 2.581313e+20 1.171914e+16
## [11,] 2.235247e+37 6.076030e+37 5.399228e+44 6.837671e+30 1.467662e+45
## [12,] 1.142007e+26 8.438357e+26 4.201210e+25 7.016736e+20 1.142007e+26
## [13,] 1.784823e+08 1.784823e+08 4.311232e+15 5.987414e+04 4.311232e+15
## [14,] 9.744803e+09 1.627548e+05 9.744803e+09 2.353853e+17 6.565997e+07
## [15,] 1.957296e+11 7.200490e+10 4.851652e+08 2.146436e+14 1.318816e+09
## [16,] 2.091659e+24 3.493427e+19 1.858672e+31 1.014800e+33 8.659340e+16
## [17,] 2.038281e+34 2.451246e+40 4.923458e+41 5.540622e+34 1.252363e+29
## [18,] 9.253782e+29 3.831008e+22 4.093997e+35 1.545539e+25 1.041376e+23
## [19,] 4.311232e+15 1.907347e+21 1.957296e+11 2.146436e+14 7.200490e+10
## [20,] 1.446257e+12 1.318816e+09 5.834617e+14 5.320482e+11 7.896296e+13
## [21,] 8.103084e+03 2.648912e+10 3.584913e+09 5.459815e+01 1.096633e+03
## [22,] 6.235149e+27 5.685720e+24 7.694785e+23 2.515439e+30 9.496119e+19
## [23,] 3.584913e+09 6.398435e+17 2.415495e+07 1.318816e+09 3.584913e+09
## [24,] 3.584913e+09 5.987414e+04 4.311232e+15 8.659340e+16 2.146436e+14
## [25,] 1.409349e+22 8.659340e+16 1.285160e+19 4.201210e+25 4.727839e+18
## [26,] 1.096633e+03 7.389056e+00 1.957296e+11 1.096633e+03 4.539993e-05
## [27,] 1.811239e+41 1.112864e+36 4.093997e+35 7.307060e+43 4.375039e+48
## [28,] 1.627548e+05 3.931334e+12 1.353353e-01 1.627548e+05 4.851652e+08
## [29,] 1.739275e+18 1.285160e+19 1.907347e+21 8.659340e+16 2.648912e+10
## [30,] 7.016736e+20 4.201210e+25 2.091659e+24 1.739275e+18 8.438357e+26
## [31,] 4.093997e+35 1.858672e+31 6.663176e+40 4.093997e+35 6.837671e+30
## [32,] 1.409349e+22 2.830753e+23 3.831008e+22 8.659340e+16 4.727839e+18
## [33,] 2.415495e+07 1.484132e+02 2.648912e+10 1.000000e+00 2.008554e+01
## [34,] 6.837671e+30 1.409349e+22 1.041376e+23 1.694889e+28 2.830753e+23
## [35,] 1.252363e+29 1.041376e+23 6.837671e+30 3.104298e+26 2.581313e+20
## [36,] 5.459815e+01 3.059023e-07 4.424134e+05 8.315287e-07 1.202604e+06
## [37,] 9.253782e+29 1.142007e+26 4.093997e+35 1.041376e+23 7.694785e+23
## [38,] 1.068647e+13 3.185593e+16 5.320482e+11 1.586013e+15 1.586013e+15
## [39,] 2.415495e+07 9.744803e+09 1.586013e+15 3.931334e+12 2.980958e+03
## [40,] 7.694785e+23 6.398435e+17 5.685720e+24 1.041376e+23 2.293783e+27
## [41,] 5.834617e+14 1.068647e+13 4.727839e+18 1.446257e+12 2.830753e+23
## [42,] 9.253782e+29 1.858672e+31 5.540622e+34 7.694785e+23 3.404276e+29
## [43,] 1.338335e+42 6.493134e+50 1.084464e+46 1.220403e+39 1.651636e+38
## [44,] 1.811239e+41 4.093997e+35 3.989520e+45 1.986265e+44 1.467662e+45
## [45,] 5.459815e+01 5.459815e+01 6.565997e+07 6.144212e-06 5.987414e+04
## [46,] 4.607187e+28 4.201210e+25 7.694785e+23 3.831008e+22 1.252363e+29
## [47,] 4.489613e+38 2.178204e+47 2.758513e+33 3.989520e+45 1.338335e+42
## [48,] 7.389056e+00 3.678794e-01 1.831564e-02 1.125352e-07 6.565997e+07
## [49,] 5.459815e+01 1.234098e-04 6.565997e+07 9.118820e-04 2.718282e+00
## [50,] 9.496119e+19 7.694785e+23 1.142007e+26 6.398435e+17 4.201210e+25&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same results can be generated by using a &lt;code&gt;for&lt;/code&gt; loop to loop over all entries, but this is much slower.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;lapply()&lt;/code&gt; (“list apply”) is similar but runs over a list of values, and returns the output as a list of values. In this example, the mean is calculated for a number of vectors, but these vectors can be different sizes (unlike for a matrix or data frame):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
lapply(my_list, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 3.142857
## 
## $val2
## [1] 1.5
## 
## $val3
## [1] 6.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, since the output is a list, the output could also be a list of vectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
lapply(my_list, FUN = sort)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $val1
## [1] 2 2 3 3 4 4 4
## 
## $val2
## [1] 1 2
## 
## $val3
## [1]  2  5  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sapply()&lt;/code&gt; (“simple apply”) is similar to &lt;code&gt;lapply()&lt;/code&gt;, but returns the results as a vector rather than a list. This is a better method to use when returning a single value for each list entry:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_list &amp;lt;- list(val1 = c(2,4,2,3,4,3,4), 
                val2 = c(1,2), 
                val3 = c(10,2,5,9)) 
sapply(my_list, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     val1     val2     val3 
## 3.142857 1.500000 6.500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mapply()&lt;/code&gt; (“multivariate apply”) is very useful for vectorization, and works by applying the function &lt;code&gt;FUN&lt;/code&gt; to the first elements of each object, then to the second element, and so on. The following example will replicate the number &lt;code&gt;n&lt;/code&gt; n-times for numbers 1 to 5. This could also be done using loops, but loops do not scale as well as vectorised functions such as this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapply(rep, 1:5, 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2 2
## 
## [[3]]
## [1] 3 3 3
## 
## [[4]]
## [1] 4 4 4 4
## 
## [[5]]
## [1] 5 5 5 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tapply()&lt;/code&gt; is a little more complicated, but essentially applies a function after breaking data up based on some index variable. It is useful for calculating summary statistics of different groups of data, and uses a factor parameter &lt;code&gt;INDEX&lt;/code&gt; to define the groups over which to apply the function &lt;code&gt;FUN&lt;/code&gt;. So in the following code, &lt;code&gt;tapply&lt;/code&gt; will apply the function &lt;code&gt;mean()&lt;/code&gt; on the values of &lt;code&gt;Expression&lt;/code&gt; for the two different treatment classes defined in the &lt;code&gt;INDEX&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_dat &amp;lt;- data.frame(Treatment = c(&amp;quot;Control&amp;quot;, &amp;quot;Control&amp;quot;, &amp;quot;Control&amp;quot;, 
                                   &amp;quot;Treated&amp;quot;, &amp;quot;Treated&amp;quot;, &amp;quot;Treated&amp;quot;),
                     Expression = c(13, 17, 9,
                                    28, 37, 34))
tapply(my_dat$Expression, INDEX = my_dat$Treatment, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Control Treated 
##      13      33&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;some-simple-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Some Simple Statistics&lt;/h1&gt;
&lt;p&gt;R is mainly designed for easy computation of statistics and there are many in-built functions and additional libraries that allow you to carry out most tasks. Most simple statistics can be easily calculated using in-built functions. The following example creates two vectors of 100 random values sampled from a normal distribution with mean 0 and standard deviation 1, then calculates various basic summary statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sort(rnorm(100, mean = 0, sd = 1))
min(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.336472&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(x)   &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.939707&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07717416&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02808852&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum and maximum values are the smallest and largest values respectively. The mean is what most people would think of when you asked for the average, and is calculated by summing the values and dividing by the total number of values. The median is another way of looking at the average, and is essentially the middle value (&lt;code&gt;50^th^&lt;/code&gt; percentile). Other percentiles can be calculated, which can give you an idea of where the majority of your data lie:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = 0.25) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        25% 
## -0.6394031&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = 0.75) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       75% 
## 0.6994683&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(x, probs = seq(0, 1, 0.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          0%         10%         20%         30%         40%         50% 
## -2.33647209 -1.31947437 -0.87549325 -0.55566797 -0.15779236  0.02808852 
##         60%         70%         80%         90%        100% 
##  0.27175879  0.56818018  1.05753511  1.62287076  2.93970745&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summary()&lt;/code&gt; function will calculate many of these basic statistics for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -2.33647 -0.63940  0.02809  0.07717  0.69947  2.93971&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variance is the average of the squared distances from the mean, and is a measure of how spread out the data are from the average. The standard deviation is simply the square root of this value &lt;span class=&#34;math inline&#34;&gt;\(var(x) = sd(x)^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.07332&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.152016&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(sd(x)^2, var(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum((x-mean(x))^2)/(length(x)-1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.152016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The covariance is a measure of how much two sets of data vary together:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- sort(rnorm(100, mean = 0, sd = 1))
var(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9048439&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.006295&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The covariance is related to the correlation between two data sets, which is a number between -1 and 1 indicating the level of dependance between the two variables. A value of 1 indicates perfect correlation, so that as one value increases so does the other. A value of -1 indicates perfect anti-correlation, so that as one value increases the other decreases. A value of 0 indicates that the two values change independently of one another:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9856189&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(x, y)/(sd(x) * sd(y)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9856189&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This value is known as the Pearson correlation. An alternative method for calculating the correlation between two sets of values is to use the Spearman correlation, which is essentially the same as the Pearson correlation but is calculated on the ranks of the data rather than the values themselves. In this way, each value increases by only one unit at a time, meaning that the correlation score is more robust to the presence of outliers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y, method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So these values are pretty highly dependent on one another – not surprising considering that they are both drawn randomly from the same distribution. We can calculate the line of best fit between the two vectors by using linear regression, which searches for the best straight line model &lt;span class=&#34;math inline&#34;&gt;\(y = a + bx\)&lt;/span&gt; that minimises the squared distances between the line (estimated values) and the observed data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_lin_mod &amp;lt;- lm(y ~ x)
summary(my_lin_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.75519 -0.06913  0.03729  0.10091  0.29603 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.26517    0.01620  -16.37   &amp;lt;2e-16 ***
## x            0.87351    0.01513   57.74   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1616 on 98 degrees of freedom
## Multiple R-squared:  0.9714, Adjusted R-squared:  0.9712 
## F-statistic:  3334 on 1 and 98 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explaining this output is beyond the scope of this short tutorial, but the coefficient estimates give us the values for &lt;code&gt;a&lt;/code&gt; (&lt;code&gt;-0.2651651&lt;/code&gt;) and &lt;code&gt;b&lt;/code&gt; (&lt;code&gt;0.8735073&lt;/code&gt;) in the linear model. The p-value tells us how significant these estimates are. In statistical terms, we are testing the null hypothesis that the coefficient is actually equal to zero (i.e. there is not an association between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;). The p-value gives the probability of detecting a coefficient at least as large as the one that we calculated in our model given that the null hypothesis is actually true. If this probability is low enough, we can safely reject the null hypothesis and say that this variable is statistically significant. Often a value of 0.05 (5%) is used as the cutoff for rejection of the null hypothesis.&lt;/p&gt;
&lt;p&gt;Hypothesis testing is a large part of statistics. The t-test is a commonly used test for comparing the means of two sets of data. In simple terms we are looking to see if they are significantly different (e.g. does the expression of a particular gene change significantly following treatment with a drug). In statistical terms, we are testing to see if the change that we see in the means is greater than we would expect by chance alone.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, y) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and y
## t = 1.917, df = 195.18, p-value = 0.0567
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.007920033  0.557774135
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 -0.19775289&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are drawn from the same distribution, the test shows there is no evidence that there is a difference between the mean. Let’s try again with a different data set, drawn from a different distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- rnorm(100, mean = 10, sd = 1) 
t.test(x, z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.239515  -9.660461
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the p-value is much less than 0.05, so we can make the claim that the mean of &lt;code&gt;z&lt;/code&gt; is significantly different from that of &lt;code&gt;x&lt;/code&gt;. By default, the &lt;code&gt;t.test()&lt;/code&gt; function is 2-sided, meaning that it does not distinguish between whether or not the difference in the means is an increase or a decrease in &lt;code&gt;z&lt;/code&gt;. We can specify the &lt;code&gt;alternative&lt;/code&gt; parameter to define the alternative hypothesis that we want to test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, z, alternative = &amp;quot;less&amp;quot;)    ## Tests if mean(x) &amp;lt; mean(z) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##       -Inf -9.707361
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, z, alternative = &amp;quot;greater&amp;quot;) ## Tests if mean(x) &amp;gt; mean(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  x and z
## t = -67.773, df = 197.06, p-value = 1
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -10.19262       Inf
## sample estimates:
##   mean of x   mean of y 
##  0.07717416 10.02716244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us that the difference in the means between &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt; is less than 0, or that the mean of &lt;code&gt;z&lt;/code&gt; is greater than that of &lt;code&gt;x&lt;/code&gt; (as we expect).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-with-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Plotting With R&lt;/h1&gt;
&lt;p&gt;One of the most useful functions of R is the ability to plot publication-quality figures simply and easily. The vast number of tools available to users for plotting figures is beyond the scope of this tutorial, but I will mention a few of the most commonly used plotting functions to allow you to have a quick look at your data. These functions are all part of the &lt;code&gt;base&lt;/code&gt; plotting package, but I also recommend looking into the &lt;a href=&#34;http://ggplot2.org&#34;&gt;ggplot2()&lt;/a&gt; package for an incredibly intuative appraoch to plotting data.&lt;/p&gt;
&lt;div id=&#34;scatterplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.1&lt;/span&gt; Scatterplots&lt;/h2&gt;
&lt;p&gt;Scatterplots are probably the simplest plot that we can look at. Here we take two sets of values and plot one against the other to see how they correlate. This means that the two data sets are paired, such that the first element of each data set represents one event, the second represents another, and so on. For instance, for every student in a class, we may have scores from tests taken at the start and at the end of the year, and we want to compare them against one another to see how they compare. Here is how to plot a simple scatterplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, 
     pch = 19,                  ## Plot each point as a filled circle
     col = &amp;quot;red&amp;quot;,               ## Colour each point red
     xlab = &amp;quot;This is x&amp;quot;,        ## Add a label to the x-axis
     ylab = &amp;quot;This is y&amp;quot;,        ## Add a label to the y-axis
     main = &amp;quot;This is y vs. x&amp;quot;,  ## Add a main title to the plot
     cex.main = 1.4,            ## Change the size of the title
     cex.lab  = 1.2,            ## Change the size of the axis labels
     cex.axis = 1.1             ## Change the size of the axis values
     )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are lots of additional plotting arguments that can be set in the &lt;code&gt;plot()&lt;/code&gt; command. These are just a few. These arguments will typically work for any plotting function that you may want to use.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plot()&lt;/code&gt; is the standard plotting function, and works differently depending on the type of data on which it is called. Most of the following plots use this function in some way, even though it may not be obvious.&lt;/p&gt;
&lt;p&gt;Here we have coloured all of our points a single colour by using the &lt;code&gt;col = &amp;quot;red&amp;quot;&lt;/code&gt; argument. However, we can assign colours to each point separately by supplying a vector of colours that is the same length as &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. This means that we can set colours based on the data themselves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_cols &amp;lt;- rep(&amp;quot;black&amp;quot;, length(x)) 
my_cols[x &amp;gt; 1 &amp;amp; y &amp;gt;  1] &amp;lt;- &amp;quot;red&amp;quot;
my_cols[x &amp;gt; 1 &amp;amp; y &amp;lt; -1] &amp;lt;- &amp;quot;green&amp;quot; 
my_cols[x &amp;lt; 0 &amp;amp; y &amp;gt;  0] &amp;lt;- &amp;quot;blue&amp;quot;
plot(x, y, col = my_cols, pch = 19)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since this plot is useful for observing the level of correlation between two data sets, it may be useful to add a couple of lines in to the plot to help us determine if there is a trend indicating that x is well correlated with y. First of all we will add lines in through the origin, and then we will add in a dotted line along the x = y line (since, if the two datasets were exactly correlated, the points would lie on this line). To do this, we use the &lt;code&gt;abline()&lt;/code&gt; function. This plots a straight line in one of three ways. We can either specify a horizontal line by specifying the &lt;code&gt;h&lt;/code&gt; argument, or we can specify a vertical line by using the &lt;code&gt;v&lt;/code&gt; argument, or we can specify a straight line in the format &lt;span class=&#34;math inline&#34;&gt;\(y = a + bx\)&lt;/span&gt; (where &lt;code&gt;a&lt;/code&gt; is the intercept term and &lt;code&gt;b&lt;/code&gt; is the gradient term):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, ylim = c(-3,3), xlim = c(-3,3))
abline(h = 0)
abline(v = 0)
abline(a = 0, b = 1, lty = 2) ## lty gives the line type - in this case dotted&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;abline()&lt;/code&gt; does not create a new plot, but instead adds to the plot that we already have. This is because it does not call the &lt;code&gt;plot.new()&lt;/code&gt; function, which would otherwise create a new plotting region.&lt;/p&gt;
&lt;p&gt;We may be particularly interested in how the line of best fit looks as compared to the &lt;span class=&#34;math inline&#34;&gt;\(x = y\)&lt;/span&gt; line, as this will show us if there is a general trend in the data or not. To do this we can use a linear model to predict &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; from the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x, y, ylim = c(-3,3), xlim = c(-3,3))
my_lin_model &amp;lt;- lm(y ~ x) 
abline(my_lin_model, lty = 2, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/scatterplot4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want to explicitly pull out &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, use the &lt;code&gt;coef()&lt;/code&gt; function to get the coefficients:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(my_lin_model)[1] ## Get the intercept from the coefficients of the model &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##  -0.2651651&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(my_lin_model)[2] ## Get the gradient from the coefficients of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         x 
## 0.8735073&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.2&lt;/span&gt; Histograms&lt;/h2&gt;
&lt;p&gt;Now let’s look at the distribution of the data. A histogram is useful for this. Here we count up the number of values that fall into discrete bins. The size of the bins (or the number of bins) can be specified by using the &lt;code&gt;breaks&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- rnorm (1000)
par(mfrow=c(1,2))
hist(x) ## Shows a nice bell shape curve about mean 0 
hist(x, breaks = 200) ## More fine-grained&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.3&lt;/span&gt; Quantile-Quantile Plots&lt;/h2&gt;
&lt;p&gt;Quantile-quantile plots are a particular type of scatterplot that are used to see if two data sets are drawn from the same distribution. To do this, it plots the quantiles of each data set against each other. That is it plots the 0&lt;sup&gt;th&lt;/sup&gt; percentile of data set A (the minimum value) against the 0th percentile of data set B, the 50&lt;sup&gt;th&lt;/sup&gt; percentiles (the medians) against each other, the 100&lt;sup&gt;th&lt;/sup&gt; percentiles (the maximum values) against each other, etc. Simply, it sorts both data sets, and makes them both the same length by estimating any missing values, then plots a scatterplot of the sorted data. If the two data sets are drawn from the same distribution, this plot should follow the &lt;span class=&#34;math inline&#34;&gt;\(x = y\)&lt;/span&gt; identity line at all but the most extreme point.&lt;/p&gt;
&lt;p&gt;Here is a QQ plot for two data sets drawn from the same normal distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- rnorm(100, mean = 0, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 0, sd = 1) 
qqplot(x1, x2)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/QQplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here is a QQ plot for two data sets drawn from different normal distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x1 &amp;lt;- rnorm(100, mean = 0, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 1, sd = 3) 
qqplot(x1, x2)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/QQplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;line-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.4&lt;/span&gt; Line Plots&lt;/h2&gt;
&lt;p&gt;Scatterplots are useful for generating correlation plots for pairs of data. Another form of data is a set of values along a continuum, for instance we may have the read count along the length of the genome. For this, a scatterplot may not be the most sensible way of viewing these data. Instead, a line plot may be a more fitting way of viewing the data. To do this we simply specify the &lt;code&gt;type&lt;/code&gt; argument to be &lt;code&gt;line&lt;/code&gt; (or simply &lt;code&gt;l&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;One thing to be careful of with data such as this is that you must make sure that the data are ordered from left to right (or right to left) on the x axis so that connecting the points makes sense on the continuum. For instance, the following plot is not terribly useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x = c(2,4,5,3,1,7,9,8,6,10)
y = c(4,2,5,4,10,6,6,5,6,9)
plot(x = x, y = y, type = &amp;#39;l&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot1-1.png&#34; width=&#34;672&#34; /&gt;
But if we order the data from left to right then it will be a lot more useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = x[order(x)], y = y[order(x)], type = &amp;#39;l&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot2-1.png&#34; width=&#34;672&#34; /&gt;
You can also plot both points and lines by setting the &lt;code&gt;type&lt;/code&gt; argument to &lt;code&gt;both&lt;/code&gt; (or &lt;code&gt;b&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(x = x[order(x)], y = y[order(x)], type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/lineplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.5&lt;/span&gt; Density Plots&lt;/h2&gt;
&lt;p&gt;We can use a line plot like this to plot the density of the data, which gives us a similar plot to the histogram. The benefit of this type of plot over a histogram is that you can overlay the distribution of multiple data sets. The &lt;code&gt;density()&lt;/code&gt; function is a kernal density estimator function that basically calculates the density of the data within each bin such that the total area under the resulting curve is 1. This makes these plots useful for comparing data sets of different sizes as they are essentially normalised. We can add a legend to this plot to make it clear which line represents which sample. Again, this does not call &lt;code&gt;plot.new()&lt;/code&gt; so will appear on top of the current plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Create 2 random normal distributions about 5 and 10 respectively
x1 &amp;lt;- rnorm(100, mean = 5, sd = 1) 
x2 &amp;lt;- rnorm(1000, mean = 10, sd = 1)

## Calculate the density of each
x1dens &amp;lt;- density(x1) 
x2dens &amp;lt;- density(x2)

## Set up a plotting region explicitly
plot.new()
plot.window(xlim = c(0,15), 
            ylim = c(0,0.5))
range&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (..., na.rm = FALSE)  .Primitive(&amp;quot;range&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title(xlab = &amp;quot;Value&amp;quot;, ylab = &amp;quot;Density&amp;quot;, main = &amp;quot;Density Plot&amp;quot;) 
axis(1)
axis(2)

## Add the data (notice that these do not call plot.new() so will add onto the current figure
lines(x1dens , col = &amp;quot;red&amp;quot;) 
lines(x2dens , col = &amp;quot;blue&amp;quot;)

## Add a legend
legend(&amp;quot;topleft&amp;quot;, legend = c(&amp;quot;Mean = 5&amp;quot;, &amp;quot;Mean = 10&amp;quot;), col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), lty = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/density1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.6&lt;/span&gt; Boxplots&lt;/h2&gt;
&lt;p&gt;Another way to compare the distribution of two (or more) data sets is by using a boxplot. A boxplot shows the overal distribution by plotting a box bounded by the first and third quartiles, with the median highlighted. This shows where the majority of the data lie. Additional values are plotted as whiskers coming out from the main box:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(x1, x2, names = c(&amp;quot;Mean = 5&amp;quot;, &amp;quot;Mean = 10&amp;quot;), ylab = &amp;quot;Value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/boxplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;boxplot()&lt;/code&gt; can also take the data in the form of a data frame, which is useful for instance if you want to compare the distribution of expression values over all genes for a number of different samples. This will automatically label the boxes with the column names from the data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_data &amp;lt;- data.frame(Sample1 = rnorm(100), 
                      Sample2 = rnorm(100), 
                      Sample3 = rnorm(100), 
                      Sample4 = rnorm(100), 
                      Sample5 = rnorm(100))
boxplot(my_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/boxplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bar-plots-and-pie-charts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.7&lt;/span&gt; Bar Plots and Pie Charts&lt;/h2&gt;
&lt;p&gt;Now let’s say that we have a data set that shows the number of called peaks from a ChIPseq data set that fall into distinct genomic features (exons, introns, promoters and intergenic regions). One way to look at how the peaks fall would be to look at a pie graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_peak_nums &amp;lt;- c(&amp;quot;exon&amp;quot;       = 1400, 
                  &amp;quot;intron&amp;quot;     = 900, 
                  &amp;quot;promoter&amp;quot;   = 200, 
                  &amp;quot;intergenic&amp;quot; = 150) 
pie(my_peak_nums)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/pie1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the majority of the peaks fall into exons. However, pie graphs are typically discouraged by statisticians, because your eyes can often misjudge estimates of the area taken up by each feature. A better way of looking at data such as this would be in the form of a barplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(my_peak_nums, 
        ylab = &amp;quot;Number of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s suppose that we had data showing the number of peaks in different genomic features for multiple samples. We could plot multiple pie charts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_peak_nums &amp;lt;- data.frame(GeneFeature = c(&amp;quot;exon&amp;quot;, &amp;quot;intron&amp;quot;, &amp;quot;promoter&amp;quot;, &amp;quot;intergenic&amp;quot;),
                           Sample1 = c( 1400, 900, 200, 150 ),
                           Sample2 = c( 2400, 1000, 230,250 ),
                           Sample3 = c( 40,30, 5,7 )
                           )
par(mfrow = c(1,3))
pie(my_peak_nums[[2]], main = &amp;quot;Sample1&amp;quot;, labels = my_peak_nums[[1]])
pie(my_peak_nums[[3]], main = &amp;quot;Sample2&amp;quot;, labels = my_peak_nums[[1]])
pie(my_peak_nums[[4]], main = &amp;quot;Sample3&amp;quot;, labels = my_peak_nums[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/pie2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1,1)) ## Reset the plotting region&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However comparing across multiple pie charts is very difficult. Instead, a single barplot will work better. Note that here the number of peaks is different for each sample, so it makes more sense to convert the data into a format whereby the bar height represents the percentage of peaks within a particular feature:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Convert to percentages so that the samples are comparable
my_peak_percent &amp;lt;- my_peak_nums[, 2:4] 
for (i in 1:3) {
  my_peak_percent[[i]] &amp;lt;- 100*my_peak_percent[[i]]/sum(my_peak_percent[[i]]) 
}

## Convert to a matrix to satisfy requirements for barplot()
my_peak_percent &amp;lt;- as.matrix(my_peak_percent)

## Plot the bar plot
barplot(my_peak_percent ,
        ylab = &amp;quot;Percentage of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;, 
        legend.text = my_peak_nums[[&amp;quot;GeneFeature&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot2-1.png&#34; width=&#34;672&#34; /&gt;
Notice that the default way that &lt;code&gt;barplot()&lt;/code&gt; works is to plot the bars in a single stack for each sample. This is fine for comparing the exons, but trying to compare the other classes is much harder. A better way to plot these data would be to plot the bars side by side for each sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(my_peak_percent ,
        ylab = &amp;quot;Percentage of Peaks in Feature&amp;quot;, 
        main = &amp;quot;Peaks in Gene Features&amp;quot;, 
        legend.text = my_peak_nums[[&amp;quot;GeneFeature&amp;quot;]], 
        beside = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;graphical-control&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.8&lt;/span&gt; Graphical Control&lt;/h2&gt;
&lt;p&gt;That covers the majority of the basic plotting functions that you may want to use. You can change the standard plotting arguments by using the par() command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(5,10,0,3))  ## Sets the figure margins (in &amp;#39;number of lines&amp;#39;) - b,l,t,r
par(las = 1)            ## Changes axis labels to always be horizontal
par(tcl = -0.2)         ## Change the size of the axis ticks
plot(x = rnorm(100), y = rnorm(100))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/barplot4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.9&lt;/span&gt; Subplots&lt;/h2&gt;
&lt;p&gt;By default, the graphics device will plot a single figure only. There are several ways to create subfigures within this region. The first is to set the &lt;code&gt;mfrow&lt;/code&gt; argument in &lt;code&gt;par()&lt;/code&gt;. This will split the graphics region into equally sized subplots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(3, 2)) ## Creates a figure region with 3 rows and 2 columns 
for (i in 1:6) {
  plot(x = rnorm(100), y = rnorm(100)) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/subplot1-1.png&#34; width=&#34;672&#34; /&gt;
However, if you want more control over your plotting, you can use the &lt;code&gt;layout()&lt;/code&gt; function which allows you to specify the size and layout of the subplots. This function takes a matrix specifying where in the grid of subplots each plot should be drawn to. So the first call to &lt;code&gt;plot()&lt;/code&gt; will put its figure in the grid regions labelled &lt;code&gt;1&lt;/code&gt;, the scond call will put its figure anywhere that there is a &lt;code&gt;2&lt;/code&gt;, etc. Anywhere that you do not want a figure should have a &lt;code&gt;0&lt;/code&gt;. The heights and widths arguments allow you to specify the size of each grid region. You can check what the resulting figure layout will look like by using &lt;code&gt;layout.show(n)&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is the number of subplots in your figure. With a bit of work, you can get some very good layouts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_layout &amp;lt;- matrix(c(1,1,1,1,2,2,3,4,2,2,3,4,0,0,3,4,0,0,5,5), nrow = 5, ncol = 4, byrow = TRUE)
layout(my_layout, widths = c(10,10,2,2), heights = c(1,5,5,5,2)) 
my_layout&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    1    1    1
## [2,]    2    2    3    4
## [3,]    2    2    3    4
## [4,]    0    0    3    4
## [5,]    0    0    5    5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;layout.show(5) ## Can you see how this matrix leads to this layout? &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/subplot2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-figures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;10.10&lt;/span&gt; Saving Figures&lt;/h2&gt;
&lt;p&gt;By default, figures are generated in a seperate window from R. However, you can save the figure to an external file by using one of the functions &lt;code&gt;png()&lt;/code&gt;, &lt;code&gt;pdf()&lt;/code&gt;, &lt;code&gt;jpeg()&lt;/code&gt;, etc. These functions open a new “device”, which R can use to plot to. After the figure has been plotted, the device must be turned off again using the &lt;code&gt;dev.off()&lt;/code&gt; function. There are many arguments that can be used for these functions. In general, these define the dimensions and resolution of the resulting figure. It can be difficult to get these right, so play around to see how they affect things:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;png(&amp;quot;figures/test_figure.png&amp;quot;, height = 10, width = 10, unit = &amp;quot;in&amp;quot;, res = 300)
plot(1:10, 1:10, type = &amp;quot;l&amp;quot;, main = &amp;quot;My Test Figure&amp;quot;, xlab = &amp;quot;x axis&amp;quot;, ylab = &amp;quot;y axis&amp;quot;) 
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Example Analysis&lt;/h1&gt;
&lt;div id=&#34;introduction-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;p&gt;This is just a simple example analysis to give you an idea of the sort of things that we can do with R. Suppose that we have two experiments, each looking at the effects on gene expression of using a particular drug (“Drug A” and “Drug B”). For each experiment we have two samples; one showing the gene expression when treated with the drug, and the other showing the gene expression when treated with some control agent. Obviously in a real experiment, we would have many replicates, but here we have &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;. We want to do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For each drug, we want to get the fold change for each gene&lt;/li&gt;
&lt;li&gt;For each drug, we want to identify the genes that are significantly changed when using the drug&lt;/li&gt;
&lt;li&gt;We want to compare the results for Drug A with those from Drug B to find genes that are affected similarly by both drugs&lt;/li&gt;
&lt;li&gt;We want to plot the correlation between the fold change values for the two drugs to see how similar they are&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this, we will need four files. These files are in a tab-delimited text format. They are tables of values where each row is separated by a new line, and each column is separated by a tab character (&lt;code&gt;\t&lt;/code&gt;). These files can be created by and read into Excel for ease of use. To avoid errors when reading in files from text, it is good practice to ensure that there are no missing cells in your data. Instead try to get into the habit of using some “missing”&amp;quot; character (e.g. &lt;code&gt;NA&lt;/code&gt;).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;File Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_control.txt&#34;&gt;experiment1_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for ctrl in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment1_drug.txt&#34;&gt;experiment1_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_control.txt&#34;&gt;experiment2_control.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for control in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;experiment2_drug.txt&#34;&gt;experiment2_drug.txt&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Expression levels for drug A in expt 2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.2&lt;/span&gt; Load Data&lt;/h2&gt;
&lt;p&gt;First let’s load in the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expt1_ctrl &amp;lt;- read.table(&amp;quot;experiment1_control.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt1_drug &amp;lt;- read.table(&amp;quot;experiment1_drug.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt2_ctrl &amp;lt;- read.table(&amp;quot;experiment2_control.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)
expt2_drug &amp;lt;- read.table(&amp;quot;experiment2_drug.txt&amp;quot;, 
                         header = TRUE, sep = &amp;quot;\t&amp;quot;, 
                         stringsAsFactors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;head()&lt;/code&gt; to look at the data. Each of these files contains two columns; the gene name and some value that represents the expression level for that gene (assume that these values have been calculated after pre-processing, normalisation, etc.).&lt;/p&gt;
&lt;p&gt;In all of these cases, the list of gene names is identical, and in the same order which means that we could compare row 1 from the control-treated file with row 2 from the drug-treated file to get all of the comparisons. However, in a real data set you will not know for sure that the gene names match so I recommend merging the files together into a single data frame to ensure that all analyses are conducted on a gene by gene basis on the correct values.&lt;/p&gt;
&lt;p&gt;We therefore create a single data frame for both experiments using the merge() command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expt1 &amp;lt;- merge(expt1_ctrl, expt1_drug, 
               by = &amp;quot;GeneName&amp;quot;) ## The &amp;#39;by&amp;#39; variable tells merge which column to merge
names(expt1)[2] &amp;lt;- &amp;quot;Control&amp;quot; 
names(expt1)[3] &amp;lt;- &amp;quot;Drug&amp;quot; 
expt2 &amp;lt;- merge(expt2_ctrl, expt2_drug, 
               by = &amp;quot;GeneName&amp;quot;) 
names(expt2)[2] &amp;lt;- &amp;quot;Control&amp;quot;
names(expt2)[3] &amp;lt;- &amp;quot;Drug&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-fold-change&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.3&lt;/span&gt; Calculate Fold Change&lt;/h2&gt;
&lt;p&gt;Now we calculate the fold change for each gene by dividing the drug-treated expression by the control expression. To avoid divide by zero errors, we can set a minimum expression value. This will also ensure that we are only looking at expression changes between significant expression values. Since we want to do the same thing to both the experiment 1 and the experiment 2 data sets, it makes sense to write a single function to use for both:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_fold_change &amp;lt;- function (x, min_expression = 10) {
  ctrl_val &amp;lt;- as.numeric(x[&amp;quot;Control&amp;quot;]) 
  drug_val &amp;lt;- as.numeric(x[&amp;quot;Drug&amp;quot;])
  ctrl_val &amp;lt;- ifelse(ctrl_val &amp;lt;= min_expression, min_expression, ctrl_val)
  drug_val &amp;lt;- ifelse(drug_val &amp;lt;= min_expression, min_expression, drug_val)
  return(drug_val/ctrl_val) 
}
expt1[[&amp;quot;FoldChange&amp;quot;]] &amp;lt;- apply(expt1, MAR = 1, FUN = get_fold_change) 
expt2[[&amp;quot;FoldChange&amp;quot;]] &amp;lt;- apply(expt2, MAR = 1, FUN = get_fold_change)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;11.4&lt;/span&gt; Compare Data&lt;/h2&gt;
&lt;p&gt;Now let’s find the genes that are upregulated and downregulated in each experiment. Due to the lack of replicates, we do not have any estimate for the variance of these genes, so we will have to make do with using a threshold on the fold change:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fold_change_threshold &amp;lt;- 1.5
expt1_up   &amp;lt;- subset(expt1, FoldChange &amp;gt;= fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt1_down &amp;lt;- subset(expt1, FoldChange &amp;lt;= 1/fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt2_up   &amp;lt;- subset(expt2, FoldChange &amp;gt;= fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
expt2_down &amp;lt;- subset(expt2, FoldChange &amp;lt;= 1/fold_change_threshold)[[&amp;quot;GeneName&amp;quot;]]
cat(&amp;quot;Upregulated in Experiment 1:&amp;quot;,   paste(expt1_up,   collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1:
## gene12
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 1:&amp;quot;, paste(expt1_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1:
## gene32
## gene46&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Upregulated in Experiment 2:&amp;quot;,   paste(expt2_up,   collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 2:
## gene18
## gene50
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 2:&amp;quot;, paste(expt2_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 2:
## gene22
## gene43&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we now have the genes that change when each of the drugs is used. But now we want to compare the two drugs together. First, let’s see if there are any genes similarly affected by both drugs. We can do this using the &lt;code&gt;intersect()&lt;/code&gt; function which gives the intersect of two lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_up   &amp;lt;- intersect(expt1_up, expt2_up) 
common_down &amp;lt;- intersect(expt1_down, expt2_down)
cat(&amp;quot;Upregulated in Experiment 1 and Experiment 2:&amp;quot;, paste(common_up, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Upregulated in Experiment 1 and Experiment 2:
## gene8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;Downregulated in Experiment 1 and Experiment 2:&amp;quot;, paste(common_down, collapse = &amp;quot;\n&amp;quot;), sep = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Downregulated in Experiment 1 and Experiment 2:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can see that only one gene is similarly affected by both drugs (“gene8”). Now let’s plot a figure to see how the fold change differs between the two drugs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fold_change_data &amp;lt;- merge(expt1[, c(&amp;quot;GeneName&amp;quot;, &amp;quot;FoldChange&amp;quot;)], 
                          expt2[, c(&amp;quot;GeneName&amp;quot;, &amp;quot;FoldChange&amp;quot;)], 
                          by = &amp;quot;GeneName&amp;quot;)
names(fold_change_data)[2] &amp;lt;- &amp;quot;Experiment1&amp;quot;
names(fold_change_data)[3] &amp;lt;- &amp;quot;Experiment2&amp;quot;
plot(x = log2(fold_change_data[[&amp;quot;Experiment1&amp;quot;]]), 
     y = log2(fold_change_data[[&amp;quot;Experiment2&amp;quot;]]), 
     pch = 19,
     xlab = &amp;quot;log2(Experiment1 Fold Change)&amp;quot;,
     ylab = &amp;quot;log2(Experiment2 Fold Change)&amp;quot;,
     main = &amp;quot;Experiment1 Fold Change vs Experiment2 Fold Change&amp;quot;, 
     cex.lab = 1.3,
     cex.axis = 1.2,
     cex.main = 1.4,
     xlim = c(-2,2),
     ylim = c(-2,2)
     )
abline(h = 0) 
abline(v = 0)
abline(a = 0, b = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/resources/RTutorial/index_files/figure-html/foldchange-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows that the effect on the gene expression is actually quite different for the two drugs. We can also see this by looking at the correlation between the two experiments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x = fold_change_data[[&amp;quot;Experiment1&amp;quot;]], 
    y = fold_change_data[[&amp;quot;Experiment2&amp;quot;]], 
    method = &amp;quot;pearson&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08381614&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x = fold_change_data[[&amp;quot;Experiment1&amp;quot;]], 
    y = fold_change_data[[&amp;quot;Experiment2&amp;quot;]], 
    method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.02618115&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
