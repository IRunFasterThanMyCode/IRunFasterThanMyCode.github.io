<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ultrarunning | IRunFasterThanMyCode</title>
    <link>/tags/ultrarunning/</link>
      <atom:link href="/tags/ultrarunning/index.xml" rel="self" type="application/rss+xml" />
    <description>ultrarunning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2017 Sam Robson</copyright><lastBuildDate>Fri, 19 Oct 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/irunfasterthanmycode.jpg</url>
      <title>ultrarunning</title>
      <link>/tags/ultrarunning/</link>
    </image>
    
    <item>
      <title>Strava Data Mining: Assessing Mimi Anderson&#39;s World Record Run Across the USA</title>
      <link>/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-are-key&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#get-to-the-point&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#full-disclosure&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#delayed-uploads&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-is-pausing-her-watch&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dodgy-fluctuations-in-cadence-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mimi-running-in-the-185-195-steps-per-minute-range&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spoofed-data&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I ran most of this analysis one evening at the beginning of the week before the sad news that Mimi was going to give up her World Record attempt due to injury. Whether Mimi’s data is valid to claim the World Record is now moot, but the effect of the damage to her reputation is not. She is understandably devastated at the turn of events, and I can only hope to alleviate some of her grief with what I have written here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;On 7th September 2017, the &lt;a href=&#34;http://www.mimirunsusa.com&#34;&gt;Marvellous Mimi Anderson&lt;/a&gt; began a world record attempt to run across the United States of America. She is planning on running the 2,850 miles in 53 days to beat the current &lt;a href=&#34;http://www.guinnessworldrecords.com/world-records/fastest-crossing-of-america-(usa)-on-foot-(female)/&#34;&gt;Guinness World Record&lt;/a&gt; of 69 days 2 hours 40 minutes by Mavis Hutchinson from South Africa back in 1979.&lt;/p&gt;
&lt;p&gt;Mimi is somewhat of an institution in the UK, and can often be seen both running and crewing at many races in her trademark pink ensemble. I have run with her on many occasions, and have seen first hand her amazing ability at running stupid races, even going so far as to make them more stupid by running there and back again on races such as &lt;a href=&#34;http://www.badwater.com&#34;&gt;Badwater&lt;/a&gt;, the &lt;a href=&#34;http://www.gucr.co.uk&#34;&gt;Grand Union Canal Race&lt;/a&gt;, and &lt;a href=&#34;http://www.spartathlon.gr/en/&#34;&gt;Spartathlon&lt;/a&gt;. She holds records for running across Ireland and across the UK, so going for the USA record is a natural progression for somebody who loves hunting for ever bigger challenges.&lt;/p&gt;
&lt;p&gt;Unfortunately, Over the past year, we have seen some controversial stunt runs from the likes of &lt;a href=&#34;http://www.telegraph.co.uk/news/2016/05/13/chariots-of-fire-or-walter-mitty-doubts-raised-over-runners-incr/&#34;&gt;Mark Vaz&lt;/a&gt; (who drove 860 miles very slowly from Lands End to John O’ Groats to “smash” the previous running record), &lt;a href=&#34;http://www.derehamtimes.co.uk/news/how-controversy-hit-norfolk-man-s-world-record-run-attempt-1-5060552&#34;&gt;Dave Reading&lt;/a&gt; (who tried to do the same, but was “cyber-bullied” into giving up apparently), and Marathon Man UK himself &lt;a href=&#34;https://www.theguardian.com/sport/blog/2016/oct/02/robert-young-marathon-sponsor-stands-tall&#34;&gt;Robert Young&lt;/a&gt; (who sat in the back of an RV and was slowly driven across America until some geezers made him run a bit and he hurt himself).
I confess that when the Mark Vaz thing happened I could genuinely not believe that somebody would actually do that. I mean, what a waste of time - and for what?! I did not believe that somebody would go to that much effort for such a niche accolade, but that was exactly the point. To most people, running across the country is already pretty ridiculous, but they don’t have any baseline for what a good time would be. Is 7 days, 18 hours and 45 minutes good? Of course the running community knew that the time was bloody incredible for the top ultrarunners in the country, never mind an overweight window cleaner in a puffa jacket.&lt;/p&gt;
&lt;p&gt;When Robert Young attempted his transcontinental run, it was a different kettle of fish. Robert had developed somewhat of a following as an inspirational runner, having started running on a whim to complete 370 marathons in a year, beating Dean Karnazes “record” for running without sleeping, and winning the Race Across the USA. So he had a history of running long, but that didn’t stop questions from being asked. In particular, posters at the &lt;a href=&#34;http://www.letsrun.com&#34;&gt;LetsRun&lt;/a&gt; forum smelled a rat very quickly, after one of their own went out to run with him in the middle of the night only to find a slow moving RV with no runner anywhere to be seen. After a lot of vehement denial from the Robert Young fans, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=7355147&#34;&gt;sleuths over at LetsRun&lt;/a&gt; were able to provide enough evidence for Robert’s main sponsor, &lt;a href=&#34;https://www.skins.net&#34;&gt;Skins&lt;/a&gt;, to hire independent experts to prepare a report on whether or not any subterfuge had occurred. The results were not good for the Marathon Man UK brand (although to this day he denies any wrongdoings).&lt;/p&gt;
&lt;p&gt;The main take-home message from the Robert Young (RY) affair was that data transparency is key. RY refused to provide his Strava data for people to check, but to be fair he had a very good reason - he hadn’t got around to doctoring it yet. So anybody looking to take on this record would have to work very hard to ensure that their data was squeaky clean and stood up to scrutiny.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-are-key&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Data Are Key&lt;/h1&gt;
&lt;p&gt;In leading up to the challenge, Mimi gave several interviews where the RY affair and, in particular, the importance of data transparency were discussed. And it was pretty clear that this was most definitely clear to Mimi. She was aware of the LetsRun community, and the importance of making her attempt as free from controversy as possible. She had arranged for a &lt;a href=&#34;http://www.racedrone.net&#34;&gt;RaceDrone&lt;/a&gt; tracker to be used to follow her progress in real time, and would be using four different GPS watches (two at any one time for redundancy) uploading the data immediately following every run. In addition, as required by Guinness World Records, they would obtain witness testimonies along the way. Add in social media to provide a running commentary and it would seem to be foolproof.&lt;/p&gt;
&lt;p&gt;Unfortunately it did not work out that way. The LetsRun board was already lighting up with &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8347797&#34;&gt;posts&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8420326&#34;&gt;questioning&lt;/a&gt; &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8449925&#34;&gt;her&lt;/a&gt; attempt (EDIT while the posts were skeptical from the start, accusations of foul play did not begin until after the first few weeks with the tracker issues). In addition, a second runner - &lt;a href=&#34;http://www.sandyacrossamerica.com&#34;&gt;Sandra Villines&lt;/a&gt; (aka Sandra Vi) - was joining in the fun, and was planning to also attempt the record (albeit taking a different route) 4 days days later. Suddenly we had a race on.&lt;/p&gt;
&lt;p&gt;Unfortunately, within the first few weeks, questions surrounding Mimi’s attempts had surfaced, largely as a result of failures in the tracker. There are contradicting reports about what happened in this time, and I don’t pretend to understand all of them. Mimi’s crew claim that the issues were due to lack of coverage, whilst RaceDrone’s founder Richard Weremuik claims that the trackers were intentionally turned off. If Richard’s claims are correct, it raises a lot of serious concerns.&lt;/p&gt;
&lt;p&gt;In addition, there have been several other events that have received a lot of criticism, including against the reaction of Mimi’s fans to questions of her integrity (“trolls”, “haterz gonna hate”, etc.), and an incident whereby a stalker presumed (by Mimi’s crew) to be a poster on the LetsRun forum was arrested, despite no record of an arrest being found and no poster admitting to it (which they likely would). Since then, the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&#34;&gt;whole run has been torn apart&lt;/a&gt;, and in particular she is now accused of faking the only source of data that is consistently available for review - &lt;a href=&#34;https://www.strava.com/athletes/13566252&#34;&gt;her Strava data&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-the-point&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Get to the point&lt;/h1&gt;
&lt;p&gt;There are definitely things to do with this whole affair that I cannot comment on, such as the Race Drone incident and the arrest report. I also agree with many of the detractors that Sandy’s set up seems to be a far more open approach, and seems to be a good gold standard to use in the future. You will get no arguments from me that mistakes were made and perhaps things should have been done differently. But I genuinely believe that Mimi went out to the USA in the belief that what she had planned was foolproof and would cover all bases and supply the necessary evidence to convince anybody that might choose to look into it. Any mistakes were a result of the fact that Mimi has limited knowledge of technology - by her own omission she is a Luddite. Having said that, it is clear that the focus was on satisfying primarily the requirements of Guinness, which most runners in my experience consider to be very weak.&lt;/p&gt;
&lt;p&gt;However, what does concern me is the insinuation of fabricating her data, so I want to tackle some of these allegations to see if I can help defend her reputation. If I do find evidence of fabrication, so be it. But the idea that “data can be faked therefore we cannot believe any of it” is absurd. All data can be faked. Of course they can. But if we followed this impetus to discount all data out of hand, then scientific research would very quickly stall. What we have here is a peer review process - of course the burden of proof is on Mimi to provide evidence of her claims, but she has done that with daily Strava uploads (already a big improvement over Rob Young). If you subsequently suspect the data are forged, the onus is on you to show evidence of that.&lt;/p&gt;
&lt;p&gt;There are certainly some inconsistencies that need addressing and I am hoping that I can address some of these here. I don’t for one minute believe that I am covering all of the issues here. I am sure there are many more that will be pointed out to me (at over 200 pages, I really don’t feel like wading through the entire thread to pull everything out), but I figured I would make a start. This is all fairly rough and these are basic analyses conducted over a very short period of time, but I hope to look into things using some more statistical methods in a follow-up post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-disclosure&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Full disclosure&lt;/h1&gt;
&lt;p&gt;In full disclosure, I consider Mimi to be a friend. I have run with her many times, including running over 50 miles together on a recce for the &lt;a href=&#34;http://www.vikingwayultra.com&#34;&gt;Viking Way&lt;/a&gt;, and have seen first hand that she is an incredibly accomplished runner who has achieved amazing things. I don’t expect this to mean anything, and completely understand that plenty of people came out saying similar things for Robert Young, I just wanted to lay my cards in the table. I am, however, typically objective when it comes to data, so I am trying to look at this without letting too many of my biases interfere. For the record, I think that her pedigree and the changes seen in her body over the last few weeks should themselves clearly distinguish her from Rob Young. She is clearly doing &lt;em&gt;something&lt;/em&gt; out there and not just riding in an RV.&lt;/p&gt;
&lt;p&gt;Having said that, I am not of the opinion that anybody that questions these runs (or indeed anything) are haterz and trolls. Extraordinary claims require extraordinary evidence, and if you are not prepared to provide that and have it interrogated then you shouldn’t be doing it. With fame comes a loss of anonymity. I believe that mistakes were made at the start of this run, and I believe that the impulse to fight back against criticism with a similar tact used by Mark Vaz, Robert Young and Dave Reading was the wrong choice. But then I am a naive fool who thinks that everybody is reasonable and open to logical discourse - I suspect I am about to be schooled on this thought.&lt;/p&gt;
&lt;p&gt;In all honesty I have stayed away from this whole debacle for exactly that reason. I do not like the “them vs us” mentality that seems to crop up in all of these discussions. Be that Brits vs Americans, URC vs LR, whatever. I think that the LetsRun community have done an amazing job at routing out cheats over the years, and without them many people would be reaping the benefits of false claims. I have no problem with witch hunts. Witch hunts are only a problem if you don’t live in a world where witches demonstrably exist. I don’t want to feed into that, I don’t want to make enemies here - I just want to help a friend by providing an alternative perspective.&lt;/p&gt;
&lt;p&gt;My aim here then is to look (hopefully) objectively at some of the claims against Mimi in her transcon attempt. I work in a data-heavy scientific discipline, and I believe that I can remain objective in this, although it should be fairly clear already that I know what I am hoping to see. But believe me when I say that if I find something that I do not like I will not hide it. I do not claim anything I say is any more valid than what anybody else says, and I do not want confirmation bias to creep in from Mimi’s fellow supporters. I just want all of the facts to be available to allow people to make an informed assessment. If anyone disagrees with any of my conclusions, or if you identify errors, by all means get in touch and I will try and follow up.&lt;/p&gt;
&lt;p&gt;I decided to write this up as a blog post as the message that I put together for the LetsRun forum got a bit ridiculous, and I thought that this way I could attach and annotate my figures, flesh out my thoughts, and importantly include my code for full transparency.&lt;/p&gt;
&lt;p&gt;A lot of the data that I am showing here is based on runs between 1st October and 7th October. I chose these as these overlap with the faked data generated by &lt;code&gt;Scam_Watcheroo&lt;/code&gt; &lt;del&gt;a user on LetsRun (who also runs the &lt;a href=&#34;https://www.marathoninvestigation.com&#34;&gt;Marathon Investigation&lt;/a&gt; website)&lt;/del&gt; (EDIT) has looked at a lot of these data and made many of the claims of faked data. In addition, he was able to &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8462935&#34;&gt;generate spoofed data files&lt;/a&gt; for a fake world record run to show how easy it is to do, which I will incorporate into these analyses.&lt;/p&gt;
&lt;p&gt;I have also included a short run that Mimi did before beginning her transcon run, which is the only other run available on Strava (presumably this was done to test the data upload). I figured that this could be useful as a baseline of her “normal” running, but of course you could always argue that this was also fabricated.&lt;/p&gt;
&lt;p&gt;I obtained the &lt;code&gt;.gpx&lt;/code&gt; files directly from Strava by using &lt;a href=&#34;https://mapstogpx.com/strava/&#34;&gt;a tool&lt;/a&gt; that is able to slurp the data directly from the Strava session webpage. I can obviously repeat any of this for other data sets, but I had to start somewhere and my free time to look at these things is limited.&lt;/p&gt;
&lt;p&gt;For each day in this period, I downloaded runs for Mimi, Sandra, and the spoofed files. I also downloaded a few of my own runs (much shorter) as a baseline as I am quite confident that these files are genuine. My thesis is that the spoofed data should be identifiable as such, while the other data sets should (hopefully) stand up to scrutiny. Note that Sandra’s data are single runs per day, whereas Mimi uploads two per day.&lt;/p&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.r-project.org&#34;&gt;R&lt;/a&gt;, which is a freely available and well maintained statistical programming language, for these analyses. I haven’t had time to do too much yet, so here I am concentrating on the cadence data as that seems to be the most contentious point at the moment. In a follow up post I will look at the whole run thus far and look at more factors such as the distance traveled and correlations with terrain (which I haven’t even touched so far).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Analysis&lt;/h1&gt;
&lt;p&gt;First of all let’s load in the packages I will be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(XML)
library(plyr)
library(geosphere)
library(pheatmap)
library(ggplot2)
library(benford.analysis)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I will load in the &lt;code&gt;.gpx&lt;/code&gt; files. They are basically &lt;code&gt;.xml&lt;/code&gt; (Extensible Markup Language) files, so a bit of parsing is required to get them into a usable &lt;code&gt;data.frame&lt;/code&gt; format:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## I/O directory
root_dir &amp;lt;- &amp;quot;../../static/post/2017-10-18-assessing-Mimi-Andersons-World_Record-run_files/&amp;quot;

## Load in the gpx data for each run
all_gpx &amp;lt;- list()
for (n in c(&amp;quot;Mine&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;MimiAnderson&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  dirname &amp;lt;- paste0(root_dir, n)
  all_fnames &amp;lt;- list.files(dirname)
  for (fname in all_fnames) {
    
    ## Load .gpx and parse to data.frame
    gpx_raw     &amp;lt;- xmlTreeParse(paste0(dirname, &amp;quot;/&amp;quot;, fname), useInternalNodes = TRUE)
    rootNode    &amp;lt;- xmlRoot(gpx_raw)
    gpx_rawlist &amp;lt;- xmlToList(rootNode)[[&amp;quot;trk&amp;quot;]]
    gpx_list    &amp;lt;- unlist(gpx_rawlist[names(gpx_rawlist) == &amp;quot;trkseg&amp;quot;], recursive = FALSE)
    gpx         &amp;lt;- do.call(rbind.fill, 
                           lapply(gpx_list, function(x) as.data.frame(t(unlist(x)), stringsAsFactors=F)))
    
    ## Convert cadence and GPS coordinates to numeric
    for (i in c(&amp;quot;extensions.cadence&amp;quot;, &amp;quot;.attrs.lat&amp;quot;, &amp;quot;.attrs.lon&amp;quot;)) {
      gpx[[i]] &amp;lt;- as.numeric(gpx[[i]])
    }
    
    ## Convert cadence to steps per minute
    gpx[[&amp;quot;extensions.cadence&amp;quot;]] &amp;lt;- gpx[[&amp;quot;extensions.cadence&amp;quot;]] * 2
    
    ## Convert time to POSIXct date-time format
    gpx[[&amp;quot;time&amp;quot;]] &amp;lt;- as.POSIXct(gpx[[&amp;quot;time&amp;quot;]], format=&amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;)
    
    ## Calculate the time difference between data points
    gpx[[&amp;quot;time.diff&amp;quot;]] &amp;lt;- c(0, (gpx[-1, &amp;quot;time&amp;quot;] - gpx[-nrow(gpx), &amp;quot;time&amp;quot;]))
    
    ## Calculate the shortest distance between successive points (in miles)
    gpx[[&amp;quot;dist.travelled&amp;quot;]] &amp;lt;- c(0, 
                                 distHaversine(gpx[-nrow(gpx), c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               gpx[-1, c(&amp;quot;.attrs.lon&amp;quot;, &amp;quot;.attrs.lat&amp;quot;)], 
                                               r = 3959)) # Radius of earth in miles
    
    ## Save to main list
    all_gpx[[n]][[gsub(&amp;quot;\\.gpx&amp;quot;, &amp;quot;&amp;quot;, fname)]] &amp;lt;- gpx
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s consider some of the specific criticisms being made against Mimi.&lt;/p&gt;
&lt;div id=&#34;delayed-uploads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.1&lt;/span&gt; Delayed uploads&lt;/h2&gt;
&lt;p&gt;One of the criticisms that comes up regarding Mimi’s practice is that it sometimes takes a long time for the data to be uploaded to Strava. I believe that it is typically up within a couple of hours (EDIT - there were also times when the uploads were not made for several days which is obviously more than a delay in tranferring the data), but many people suggest that anything longer than 15 minutes is unacceptable as it provides time to doctor the data. I mean, I guess that this is true, but it is my understanding that, given Mimi’s lack of technological prowess, her crew is using the method that requires the least amount of knowledge; that being syncing to a phone via Bluetooth, which will then upload to Movescount when there is a wifi or mobile data signal. I do this sometimes with my own runs and it takes bloody ages to sync, and that doesn’t take into account the time to then takes to upload it from the phone to Movescount, dealing with blackspots, etc. So it does not surprise me in the least that she rarely uploads things within 15 minutes of stopping. This is not evidence based by any means, just an observation from my own experience (and something others have pointed out). Is this good practice for somebody out to claim a world record? Perhaps not. Is it evidence of subterfuge? I don’t know, but personally I doubt it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-is-pausing-her-watch&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.2&lt;/span&gt; “Mimi is pausing her watch”&lt;/h2&gt;
&lt;p&gt;In page 201 of the &lt;a href=&#34;http://www.letsrun.com/forum/flat_read.php?thread=8390477&amp;amp;page=200&#34;&gt;LetsRun thread&lt;/a&gt;, user &lt;code&gt;So Far Today&lt;/code&gt; says the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elapsed time is total time from start of the day until end of the day. Sandy’s Strava is based on total elapsed time. Mimi’s excludes lunch breaks, and it appears it it also excludes other mini-breaks. If you want to figure out actual running time for Sandy remove the lunch breaks from the total elapsed time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, these guys have been looking into this in a heck of a lot more detail than I have, so I apologise if I have got this wrong here or misunderstood. But the idea that Mimi’s data excludes mini-breaks disagrees with something that I noticed right at the start of this analysis. Let’s look at the time difference between successive data points for Mimi’s data using the &lt;code&gt;table()&lt;/code&gt; function which will simply count the number of occurrences of each time delay between data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;MimiAnderson&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_170828_preUSA&lt;/p&gt;
&lt;p&gt;0 1 2 5
1 4160 4 1&lt;/p&gt;
&lt;p&gt;$MimiAnderson_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 25883    11 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 26174    31 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 29046    44     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 10926    12 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171003_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 30896     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3     4 
1 24567    19     1     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171004_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28343     6 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 18492    42 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171005_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 32400    34     2 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 26461     7     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171006_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     7 
1 27747    42     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2     3 
1 21215    36     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 32114    27 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$MimiAnderson_171012_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1     2 
1 28411    17 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So every run has one &lt;code&gt;0&lt;/code&gt; (which is a result of the way that I have calculated the time difference, so the first value is always 0), a few sporadic &lt;code&gt;2&lt;/code&gt;-&lt;code&gt;7&lt;/code&gt; sec intervals (presumably due to brief signal drop out and the like), but the vast majority are &lt;code&gt;1&lt;/code&gt; sec. This is because Mimi has her watch set to 1 sec recording, and leaves it on for the duration of the run. From this I suggest that the assertion made that Mimi stops her watch for lunch breaks etc. is false.&lt;/p&gt;
&lt;p&gt;My watch is set to the same sampling rate, and I see exactly the same thing with my data (albeit with fewer errant digits):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Mine&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$mine_170514&lt;/p&gt;
&lt;p&gt;0 1 2
1 9790 1&lt;/p&gt;
&lt;p&gt;$mine_170521&lt;/p&gt;
&lt;p&gt;0 1 2
1 6860 1&lt;/p&gt;
&lt;p&gt;$mine_170525&lt;/p&gt;
&lt;p&gt;0 1
1 3056&lt;/p&gt;
&lt;p&gt;$mine_170603&lt;/p&gt;
&lt;p&gt;0 1
1 5531&lt;/p&gt;
&lt;p&gt;$mine_170618&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 11297 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interestingly, however, when we look at Sandra’s data we see a completely different result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;SandraVi&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$SandraVi_171002&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
1 82 79 77 61 57 48 51 65 126 332 348 335 160 51 14 4 1
18 21 22 29 30 32 35 38 39 43 47 48 51 54 55 61 63 66
1 1 2 1 1 1 1 1 3 1 1 1 2 1 1 1 1 1
86 89 133
1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171003&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 243 238 233 187 192 172 210 305 1459 1424 401 142 54 40
15 16 17 18 19 20 21 22 23 24 25 27 28 30 31
17 7 1 2 4 1 4 2 1 2 2 1 1 1 1
33 34 35 36 37 39 40 41 44 45 46 47 49 50 51
2 1 1 1 1 1 2 1 1 1 1 1 1 1 1
52 53 55 57 58 59 60 63 64 65 66 73 79 82 83
2 3 3 1 1 1 1 1 1 1 1 1 1 1 1
88 89 97 100 102 113 118 119 133 136 162 166 168 183 223
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
249 266
1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171004&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 299 425 356 333 267 311 311 697 1708 1022 327 157 76 33
15 16 17 18 19 20 21 22 23 24 26 27 28 29 32
26 15 6 6 1 1 1 3 1 3 1 1 3 1 1
33 35 36 37 39 41 43 44 49 50 51 52 55 56 57
2 2 1 1 4 1 1 1 1 1 2 1 1 2 1
58 59 61 62 64 67 70 74 75 84 88 94 98 130 149
1 1 2 1 1 2 1 1 1 1 1 3 1 1 1
169
1&lt;/p&gt;
&lt;p&gt;$SandraVi_171005&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 173 167 157 133 156 129 158 400 1891 1393 259 109 53 55
15 16 17 18 19 20 21 22 23 24 26 27 29 30 32
16 6 5 2 1 5 1 1 2 1 1 1 1 1 1
33 34 35 36 37 39 41 44 46 49 50 52 61 68 74
2 1 1 1 2 1 1 1 1 1 3 1 1 2 1
75 76 93 128 138 140 145 153 163 233
1 1 1 1 2 1 1 2 2 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171006&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 112 128 123 132 115 122 101 291 1658 1696 358 85 42 33
15 16 17 18 19 20 24 27 28 32 35 37 42 46 49
8 5 4 2 1 2 1 2 1 2 1 1 1 1 2
51 57 58 60 65 67 76 77 78 86 88 90 96 98 99
1 1 1 1 1 1 1 1 1 1 2 1 1 1 1
106 108 109 126 128 129 147 164 173 196 205
1 1 1 1 1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;$SandraVi_171007&lt;/p&gt;
&lt;p&gt;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 57 85 83 73 81 74 64 197 2038 1707 275 95 64 39
15 16 17 18 22 23 27 28 31 32 38 39 40 42 45
17 11 2 1 2 1 1 5 2 1 1 1 3 3 1
46 49 53 54 55 60 65 67 77 84 109 115 126 128 149
2 1 1 2 1 1 1 1 1 1 1 1 1 1 1
151 154 157 181 182 189 225
1 1 1 1 1 1 1&lt;/p&gt;
&lt;p&gt;There is no one time difference that stands out as the most common. Instead, the time differences between her data points span a large range, with the majority being about &lt;code&gt;8&lt;/code&gt;-&lt;code&gt;11&lt;/code&gt; secs apart. I suspect that this means that Sandra’s watch is set to sample every 10 seconds or so. In addition, there are a lot more longer pauses seen, sometimes up to 4 minutes. Whether this is a result of random fluctuations due to the higher sampling rate, or is the result of pausing the watch at certain times, I do not know. I am most definitely not suggesting there is anything wrong with this, I just think that the better approach is to leave it running the whole time, and it is Mimi who is doing this and not Sandra.&lt;/p&gt;
&lt;p&gt;Note also that this means that Sandra’s data therefore has an order of magnitude fewer data points than Mimi’s does, since Mimi’s data is less smoothed out. This can be seen if we calculate the number of data points for each run and then show the distribution for Mimi and Sandra separately (note that each of Mimi’s runs is actually only half of her distance for the day):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;)) {
  plot_dat[[n]] &amp;lt;- data.frame(Name = n,
                              nPoints = sapply(all_gpx[[n]], FUN = nrow))
}
plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
ggplot(aes(x = nPoints, fill = Name), data = plot_dat) + 
  geom_density(alpha = 0.25)                           +
  ggtitle(&amp;quot;Number of Data Points Per Run&amp;quot;)             +
  xlab(&amp;quot;Number of Data Points&amp;quot;)                        + 
  ylab(&amp;quot;Density&amp;quot;)                                      +
  theme(axis.title = element_text(size = 10), 
        plot.title = element_text(size = 15, 
                                  face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_vs_sandra_data_points-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’m not sure whether the people on LetsRun have been working on the assumption that both data sets were using the same parameters, but it is pretty clear to me that the sampling rate at least is different between the two runners. There may also be differences in the accuracy - perhaps the crews could confirm one way or another. Whilst the overall approach taken by Sandra is clearly the better of the two, the 1 sec sampling rate used by Mimi is the better option for the Strava data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dodgy-fluctuations-in-cadence-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.3&lt;/span&gt; Dodgy fluctuations in cadence data&lt;/h2&gt;
&lt;p&gt;One issue that has been raised is the fact that in most of Mimi’s runs, we occasionally see severe fluctuations in the cadence, which spikes up above 200 at times. This is absolutely true, which can be seen when we plot the cadence values over time. The following function will plot the raw cadence values against the cumulative time (in secs) from the start of each run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time &amp;lt;- function (n, smooth = FALSE) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (smooth) { ## Smooths the data if requested - see below
      cad &amp;lt;- runmed(all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]], 11)
    } else {
      cad &amp;lt;- all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]
    }
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Time    = cumsum(all_gpx[[n]][[r]][[&amp;quot;time.diff&amp;quot;]]),
                                Cadence = cad)
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  ggplot(aes(x = Time, y = Cadence, color = Run), data = plot_dat) + 
    geom_point()                                                   + 
    facet_grid(Run ~ .)                                            + 
    ggtitle(paste(n, &amp;quot;Cadence Over Time&amp;quot;))                         + 
    xlab(&amp;quot;Time (sec)&amp;quot;)                                             + 
    ylab(&amp;quot;Cadence (spm)&amp;quot;)                                          +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))                + 
    ylim(0,500) # Limits the plot to a maximum of 500 which will exclude a small number of outliers for Mimi
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can look at Mimi’s raw cadence over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 120 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there is no denying that these very high cadence values do exist in Mimi’s data. If, however, we look at Sandra’s data we do not see as many of these fluctuations. However, fluctuations are indeed still present, particularly for the shorter day on 2nd October, although they are nowhere near as high as those of Mimi (250 rather than 500):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/sandra_cadence_over_time-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that here we can make out the lunch breaks in Sandra’s data, whereas Mimi has her runs split into morning and afternoon.&lt;/p&gt;
&lt;p&gt;However, as discussed above, Mimi’s data is much deeper than Sandra’s. Sandra’s data has already undergone some smoothing, so it is likely that these blips are cancelled out by smoothing over a 10 second interval. Indeed, if we smooth Mimi’s data using a running median over an 11 sec window (which replaces the data points with a running average of the data point with the 5 data points either side) to approximate the 10 sec capture, we indeed see a much smoother distribution with these extreme values reduced to be more in keeping with what we see for Sandra.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;MimiAnderson&amp;quot;, smooth = TRUE)
## Warning: Removed 85 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It would appear that these very high fluctuations are a result of the increased sampling rate, although I do note that I do not see these sorts of fluctuations in my own 1 sec capture data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_over_time(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/my_cadence_over_time_smooth-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This may simply be due to the fact that I am not running through so many different areas, many of which may have different GPS signals that affect the capture. Another possibility is that the accuracy of our watches is set to different modes. Mine is set to “Best”, but I have no idea what Mimi’s is set to. One idea that I had was to ask her to set one of her watches to 10 sec capture and upload both in parallel at the end of one of her runs. However, unfortunately it seems that this is no more an option. I am going to hunt through to find some longer runs in my personal data to see if this crops up in any of my more remote jaunts, but for now I don’t have an answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mimi-running-in-the-185-195-steps-per-minute-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.4&lt;/span&gt; Mimi running in the 185-195 steps per minute range:&lt;/h2&gt;
&lt;p&gt;Continuing with the cadence data, another issue that has cropped up several times is the fact that Mimi regularly runs in the 185-195 spm. Indeed, if we look at the distribution of the cadence in a histogram rather than looking at it over time, this certainly seems to be the case.&lt;/p&gt;
&lt;p&gt;The following function will plot the above data as a series of overlaid density plots (note that I am smoothing the density estimates here slightly to make the overall distribution clearer and less spiky for the samples with fewer data points):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density &amp;lt;- function (n) {
  plot_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    if (r == &amp;quot;MimiAnderson_170828_preUSA&amp;quot;) next # Skip the pre-transcon run
    plot_dat[[r]] &amp;lt;- data.frame(Run     = r,
                                Cadence = all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]])
  }
  plot_dat &amp;lt;- do.call(rbind.data.frame, plot_dat)
  vwalk    &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 100 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 150]) ## Walking
  vrun     &amp;lt;- Mode(plot_dat[[&amp;quot;Cadence&amp;quot;]][plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;gt; 150 &amp;amp; plot_dat[[&amp;quot;Cadence&amp;quot;]] &amp;lt; 200]) ## Running
  ggplot(aes(x = Cadence, color = Run), data = plot_dat) + 
    geom_density(alpha = 0.25, adjust = 3)               + 
    xlim(0,300)                                          + 
    ggtitle(paste(n, &amp;quot;Cadence Distribution&amp;quot;))            + 
    xlab(&amp;quot;Cadence (spm)&amp;quot;)                                + 
    ylab (&amp;quot;Density&amp;quot;)                                     +
    theme(axis.title = element_text(size = 10), 
          plot.title = element_text(size = 15, 
                                    face = &amp;quot;bold&amp;quot;))      + 
    geom_vline(xintercept = vwalk)                       + 
    geom_vline(xintercept = vrun)                        +
    annotate(&amp;quot;text&amp;quot;, x = vwalk, y = 0.05, 
             angle = 90, label = paste(vwalk, &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)                     +
    annotate(&amp;quot;text&amp;quot;, x = vrun,  y = 0.05, 
             angle = 90, label = paste(vrun,  &amp;quot;spm&amp;quot;), 
             vjust = 1.2, size = 10)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am going annotate the peaks of these plots using a very basic method of taking the modal value (the one that occurs the most) over the entire data set for the walking and running distributions (very roughly defined, but as long as the modal value lies ion the range it should give the “correct” answer) . To do this, however, I need a function to calculate the mode since one does not exist in base R (for some odd reason):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mode &amp;lt;- function(v) {
   uniqv &amp;lt;- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So let’s see the distribution for Mimi:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;MimiAnderson&amp;quot;)
## Warning: Removed 244 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/mimi_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So Mimi runs with a cadence of around 134 spm for running and 182 spm for running. Now let’s look at Sandra’s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;SandraVi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/sandra_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sandra has generally lower cadence of 120 spm for walking, and 170 spm for running. She also appears from this to walk a lot less than Mimi, who seems to do a fairly even split between running and walking in general. In addition, the variation of Mimi’s running cadence is much higher than Sandra’s, so it appears that Sandra tends to run at a relatively constant cadence with a small amount of walking, whereas Mimi’s is much more variable and seems to be split in a 50:50 run/walk. Together with the longer differences between successive time-points, this may indicate that Sandra’s watch is set to pause automatically below a certain speed.&lt;/p&gt;
&lt;p&gt;I also decided to look at my own data to see what that looked like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_cadence_density(&amp;quot;Mine&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/my_cadence_density-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I also run with a fairly high cadence (just lower than Mimi’s but not dissimilar), and see more variation than Sandra. Now obviously I am not running across a continent in these runs - I am usually running with a belligerent dog who insists on stopping to sniff every bloody tree on the way. But it is not too dissimilar, and I see the distribution spreads out over 200 for some of the readings just like with Mimi. I’m an okay runner - probably not particularly good compared to many of the posters on LetsRun, but I do okay at shorter stuff and longer stuff. But it’s just a hobby for me, so I’m perfectly happy to self-associate as a hobby jogger. I don’t really know much about cadence, so I’m not sure if averaging 180+ is high or not? If nobody had suggested this was “garbage” and unbelievable, I would just assume that Mimi had a higher than normal cadence, similar to my own. I am a forefoot runner, and I think that Mimi is as well, and I believe that higher cadence tends to go hand in hand, but I am happy to bow to the experience of people more knowledgeable than myself in the matter.&lt;/p&gt;
&lt;p&gt;To get an idea of the level of variation in the data (specifically the running cadence), let’s look at some aspects of that main distribution (excluding the outliers):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;)) {
  cad &amp;lt;- unlist(lapply(all_gpx[[n]], FUN = function (x) x[[&amp;quot;extensions.cadence&amp;quot;]]))
  cad &amp;lt;- cad[cad &amp;gt; 150 &amp;amp; cad &amp;lt; 250]
  cat(sprintf(&amp;quot;%12s: mode   = %d\n&amp;quot;, n, Mode(cad)))
  cat(sprintf(&amp;quot;%12s: mean   = %.2f\n&amp;quot;, n, mean(cad)))
  cat(sprintf(&amp;quot;%12s: median = %d\n&amp;quot;, n, median(cad)))
  cat(sprintf(&amp;quot;%12s: SD     = %.2f\n&amp;quot;, n, sd(cad)))
  cat(sprintf(&amp;quot;%12s: SEM    = %.3f\n&amp;quot;, n, sd(cad)/sqrt(length(cad))))
  cat(&amp;quot;\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;MimiAnderson: mode = 182
MimiAnderson: mean = 181.56
MimiAnderson: median = 182
MimiAnderson: SD = 11.07
MimiAnderson: SEM = 0.026&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SandraVi: mode   = 170
SandraVi: mean   = 171.13
SandraVi: median = 170
SandraVi: SD     = 4.54
SandraVi: SEM    = 0.029

    Mine: mode   = 178
    Mine: mean   = 177.96
    Mine: median = 178
    Mine: SD     = 7.53
    Mine: SEM    = 0.042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So clearly the standard deviation (SD) is much higher for Mimi’s data, but the standard error of the mean (SEM) is actually pretty comparable. SD and SEM, whilst both estimates of variability, tell you different things. The standard deviation is simply a measure of how different each individual data point is from the mean. It is descriptive of the data at hand. The SEM on the other hand is a measure of how far the mean of your sample is likely to be from the true population mean (under the assumption that each run is a random sampling of cadence values given Mimi’s true “normal” cadence). As your sample size increases, you more closely estimate the true mean of the population. This tells us that there is high variability in the sampling of cadence values for Mimi, but the precision is comparable with Sandra’s. This suggests nothing of whether the mean itself is actually believable of course, it is just worth noting the benefits of the increased sampling in these data.&lt;/p&gt;
&lt;p&gt;So my overall feeling is that, whilst high, this was just the natural running gait of Mimi. Given recent events, this entire post ended up being highly expedited so that &lt;em&gt;something&lt;/em&gt; was out there to provide a counter point to the accusations that have been made about data forgery and cheating, so in a rushed effort I looked around for some video of Mimi running to get an idea of her natural cadence. I found &lt;a href=&#34;https://www.youtube.com/watch?v=2eGb_goiPfc&#34;&gt;this video of her running at the end of her 7-day treadmill record&lt;/a&gt;. For the 18 seconds between 0:20 (when she begins to run properly) and 0:38 (when the camera pans away) I count 27-28 swings of her left hand/steps with her right foot, which would equate to a cadence of 180-187. Similarly, for the 16 seconds between 0:59 and 1:15, I count 24-25 swings/steps , which also equates to a cadence of 180-187. I’m not saying this is definitive proof, but this is at least evidence of her running with cadence similar to her average cadence across the USA, even at the end of 7 days on a treadmill. Adrenelin and a “sprint finish” mentality may play a role in achieving this as well of course. I would like to see more evidence of her running, and hopefully we will see some of that from the film crew that was with Mimi.&lt;/p&gt;
&lt;p&gt;So here I have shown that, yes Mimi runs with a higher cadence than Sandra, but there is evidence that this is simply her natural gait. As to the fact that she regularly runs in the 185-195 range; well yes she does, but so do I. And I am no elite, particularly for these particular runs which are fairly perambulatory if I am honest. I can assure you that I have not doctored these data to be this mediocre. You can look through and even work out the points where my dog stopped to piss up a tree if you like. It’s not proof, but it is evidence.&lt;/p&gt;
&lt;p&gt;I also wanted to look a bit into how the cadence actually corresponds to the speed at which the women are running. Below is a distribution plot showing the pace at each time point for each of the data sets considered here. Notice that I am excluding the data points where the runners are not moving to avoid divide by 0 errors in the pace calculation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_cor_dat &amp;lt;- list()
for (n in c(&amp;quot;MimiAnderson&amp;quot;, &amp;quot;SandraVi&amp;quot;, &amp;quot;Mine&amp;quot;, &amp;quot;Fake&amp;quot;)) {
  cor_dat &amp;lt;- list()
  for (r in names(all_gpx[[n]])) {
    cor_dat[[r]]          &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;time.diff&amp;quot;, &amp;quot;dist.travelled&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
    cor_dat[[r]][[&amp;quot;Run&amp;quot;]]  &amp;lt;- r
    cor_dat[[r]][[&amp;quot;Pace&amp;quot;]] &amp;lt;- (cor_dat[[r]][[&amp;quot;time.diff&amp;quot;]]/60)/(cor_dat[[r]][[&amp;quot;dist.travelled&amp;quot;]])
  }
  cor_dat &amp;lt;- do.call(rbind.data.frame, cor_dat)
  cor_dat[[&amp;quot;Name&amp;quot;]] &amp;lt;- n
  cor_dat           &amp;lt;- subset(cor_dat, dist.travelled != 0)
  all_cor_dat[[n]]  &amp;lt;- cor_dat
}
all_cor_dat &amp;lt;- do.call(rbind.data.frame, all_cor_dat)
ggplot(aes(x = Pace, color = Name), data = all_cor_dat) +
  geom_density(alpha = 0.25) +
  xlim(0, 20) +
  xlab(&amp;quot;Pace (min/mile)&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;) +
  ggtitle(&amp;quot;Pace Comparison Between Data Sets&amp;quot;) +
  theme(axis.title = element_text(size = 10),
        plot.title = element_text(size = 15, face = &amp;quot;bold&amp;quot;))
## Warning: Removed 53290 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/cadence_vs_speed-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, we can see that there is a big difference in how the women are approaching the race. As noted before, Mimi runs in a fairly even 50:50 split of running and walking. This graph confirms that with a fairly even split between faster running of about 8.5 mins/mile and slower walking of about 11 mins/mile. Sandra on the other hand appears to be very steady in her approach, moving consistently at a 170 spm cadence run of about 11.5 mins/mile. This was earlier in the run and no doubt changed over time as Sandra began to close in on Mimi over the past week. My runs are predominantly spent jogging at around 8 mins/mile (with the occasional downhill thrown in for fun). The distribution for the fake data however does not follow the same type of distribution as the other runs (with a clearly delineated multimodal distribution for run/walk/sprint segments), and again stands out when compared with the ostensibly genuine data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spoofed-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;6.5&lt;/span&gt; Spoofed data&lt;/h2&gt;
&lt;p&gt;So now I am getting to the nitty gritty of this post. The main accusation that I am attempting to quash is that of doctoring of the data. I have no answers regarding other perceived issues with the run, but the doctoring accusation I believe is a step too far. I have never denied that it would be possible to spoof the data. Of course it would. They are raw text files containing numbers - nothing more impressive than that. I did however think that spoofing it through Movescount would be very difficult, but it seems that I was wrong about that. It’s not simple to do, but it is doable with a little bit of know-how.&lt;/p&gt;
&lt;p&gt;However, I do believe that it would be impossible to generate spoofed data that did not stand out as such when compared with genuine data. Faking data is notoriously hard. That’s not to say that people don’t do it all of the time, and sometimes it takes a while to pick up on. But I think that creating data out of thin air that also matched with what is going on with the tracker (I appreciate people have issues with the tracker, but I’m not getting into that), what is going on with reports from the crew, matched with environmental effects and the terrain that she was running over, what will ultimately come out from the film crew, and importantly what is self consistent, would be near impossible to manage. The cadence and times would have to make sense given the position, terrain and environmental effects into account. LetsRun user &lt;code&gt;Scam_Watcheroo&lt;/code&gt; developed a tool to spoof the data, but he had the benefit of being able to track the things that might give the data away in advance. Mimi would have had to develop her method (or more accurately get somebody else to develop the method) blind, with no idea what sort of things might show it up as being faked. Sounds incredibly risky to me. So in thus section, I wanted to look at a few things to see if the different data sets stand up to scrutiny.&lt;/p&gt;
&lt;p&gt;I am only touching the surface here, and I am looking into some more in depth methods to run statistical tests over the entire data set so far to check that the data are consistent and show the patterns one would expect. My hope in advance was that doing this would highlight the faked data as such. So to start with, I am not looking at consistency between the data sets, I am merely looking at the raw cadence data to look at a few potential things that might highlight anything that looked incongruous.&lt;/p&gt;
&lt;p&gt;First of all I went back to simply looking at the time differences between the data points. For my data and Mimi’s data, they are almost all 1s differences, but there is the occasional blip (presumably when it is not able to update with the satellite straight away) leading to a few counts of around 2-7 secs. The spoofed data has none of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(all_gpx[[&amp;quot;Fake&amp;quot;]], FUN = function (x) table(x[[&amp;quot;time.diff&amp;quot;]]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171001_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;25895 25894&lt;/p&gt;
&lt;p&gt;$Fake_171002_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;26206 26205&lt;/p&gt;
&lt;p&gt;$Fake_171002_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;29092 29091&lt;/p&gt;
&lt;p&gt;$Fake_171007_afternoon&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 19626 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$Fake_171007_morning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0     1 
1 27499 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most recent ones (7th October) which were I think generated from scratch are ALL 1s differences, whilst the 2nd October ones (which were generated based on fiddling with Mimi’s uploads) were split half and half between 0s and 1s (a 0.5s sampling rate perhaps?). Either way, they do not have these little blips – the faked data appear to be too perfect. Being able to account for this and other such data imperfections heuristically (especially without knowing ahead of time that one would need to) would be bloody difficult and very very risky in my opinion.&lt;/p&gt;
&lt;p&gt;I am also looking at how well the spoofed data stand up to scrutiny using some other methods. One obvious test would be to see whether the cadence data obey &lt;a href=&#34;https://en.wikipedia.org/wiki/Benford%27s_law&#34;&gt;Benford’s Law&lt;/a&gt;, which shows that the first digits (and indeed second digits, third digits, etc.) have a unique logarithmic distribution such that smaller numbers are more likely than bigger numbers. Notice here that I am looking at &lt;em&gt;half&lt;/em&gt; of the cadence, since the actual reported data in the &lt;code&gt;.gpx&lt;/code&gt; file was doubled to give the cadence in spm. However, in this case the first digits are somewhat constrained, since the cadence is typically in a very narrow range resulting in a huge proportion of 8s and 9s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/benford-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about if we look at the second digit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- &amp;quot;Mine&amp;quot;
cad_dat &amp;lt;- list()
for (r in names(all_gpx[[n]])) {
  cad_dat[[r]] &amp;lt;- all_gpx[[n]][[r]][,c(&amp;quot;name&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;extensions.cadence&amp;quot;)]
  cad_dat[[r]][[&amp;quot;Run&amp;quot;]] &amp;lt;- r
}
cad_dat &amp;lt;- do.call(rbind.data.frame, cad_dat)
cad_bentest &amp;lt;- benford(as.numeric(substr(cad_dat[[&amp;quot;extensions.cadence&amp;quot;]]/2, 2, 2)), number.of.digits = 1)
plot(cad_bentest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/benford_2digit-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is now approaching a more standardised distribution. It does not appear to follow Benford’s Law, but instead these data appear to be somewhat uniformly distributed. One idea that I have looked at is whether the trailing (and thus least significant) digits of the cadence data follow any particular distribution. Perhaps one might imagine that they should follow &lt;em&gt;some&lt;/em&gt; distribution, such as the more uniform distribution seen above. Again remember here that I am plotting the raw cadence data, which is &lt;em&gt;half&lt;/em&gt; of the cadence values reported in the distribution plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Look at the final digit of the cadence data
all_digit_counts &amp;lt;- list()
all_digit_percent &amp;lt;- list()
for (n in names(all_gpx)) {
  all_digit &amp;lt;- matrix(0, ncol = 10, nrow = length(all_gpx[[n]]), dimnames = list(names(all_gpx[[n]]), as.character(0:9)))
  for (r in names(all_gpx[[n]])) {
      digit &amp;lt;- gsub(&amp;quot;^\\d*(\\d)$&amp;quot;, &amp;quot;\\1&amp;quot;, all_gpx[[n]][[r]][[&amp;quot;extensions.cadence&amp;quot;]]/2)
      all_digit[r, ] &amp;lt;- table(digit)[as.character(0:9)]
  }
  all_digit_counts[[n]]  &amp;lt;- all_digit
  all_digit_percent[[n]] &amp;lt;- 100*all_digit/rowSums(all_digit)
}
  
## Plot heatmap
hm_dat &amp;lt;- do.call(rbind.data.frame, all_digit_percent)
rownames(hm_dat) &amp;lt;- gsub(&amp;quot;^.*\\.&amp;quot;, &amp;quot;&amp;quot;, rownames(hm_dat))
pheatmap(hm_dat, cluster_rows = FALSE, cluster_cols = FALSE, main = &amp;quot;Cadence - trailing digit percentage&amp;quot;)#, display.numbers = FALSE,  annotation_legend = TRUE, cluster_cols = TRUE, show_colnames = TRUE, show_rownames = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/index_files/figure-html/final_digit_dist-1.png&#34; width=&#34;1800&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure shows the distribution of the percentage of trailing digits seen amongst the different data sets – Mimi’s, Sandra’s, the spoofed data, and some of my own runs. The colour of the heatmap indicates the percentage of times that digit is seen in the final position, with blue being less often and red being more often. In general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mine seem to be quite uniformly distributed (mainly blue)&lt;/li&gt;
&lt;li&gt;Mimi’s are pretty uniform (except for the pre-USA run that I put in as well) with the exception of a regular depletion of 2s and 5s&lt;/li&gt;
&lt;li&gt;Sandra’s seem to show a depletion of 1s and 9s, and enrichment of several digits in most runs (but nothing consistent)&lt;/li&gt;
&lt;li&gt;The fake data however seem very consistent in their prevalence of 6s and 7s (and to a lesser extent 4s and 8s).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is not really enough data here to identify a pattern, but from what is here the spoofed data stands out with a distribution that is different from that seen with my own data. Mimi’s is actually the most alike to data that I know to be real, although the depletion of certain digits is quite odd. But then the same is true for Sandra’s data as well to a greater degree (albeit different numbers). I plan to look into this in more detail using more data, including all of Sandra’s and Mimi’s runs, and a lot more genuine data taken from Strava as a base line to see if this distribution holds.&lt;/p&gt;
&lt;p&gt;Obviously none of this “proves” these data are not fabricated. That is impossible. I do however thus far see no sufficient evidence to suggest that these data are not real (for both runners, although Sandra was never on “trial” here). And really I have only scratched the surface on how to test the validity of these data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;I am very saddened about what has happened to Mimi on this journey. There are questions that I hope will be answered regarding certain aspects of the run, and I’m sure that these by themselves are enough to convince some people of wrong-doings. But I truly do not believe that Mimi has set out to flim flam, bamboozle or otherwise beffudle people into believing that she ran across America when she didn’t. I wrote this post before Mimi announced her intentions to stop, but the damage that she has done to herself must surely be evidence that she is doing it. And if you accept that the Strava data are genuine, there is no way to deny what she has done. Perhaps what I have introduced here will help a little to bring more people to that way of thinking, but others likely need more convincing. I will continue to try to provide reasoned explanations for some of the remaining inconsistencies where I can.&lt;/p&gt;
&lt;p&gt;So my feeling is that there is a zebra hunt going on here. What is more likely; that a 55 year old grandma is running across the country at world record pace, or that she has convinced several people to go on a month-long trip to fake a run, and in the process developed an incredibly sophisticated piece of software (which accounts for specific nuances) to spoof the data (even though she is clearly doing &lt;em&gt;something&lt;/em&gt; out there as she is losing weight and suffering exactly as one might expect for somebody running across America)? I’m going with the world record grandma. I do not think that Mimi is a witch.&lt;/p&gt;
&lt;p&gt;There are likely many questions outstanding which I have not addressed here. This is a fairly rudimentary piece of work compared to the amount of time and effort that others have put into looking into this. I am interested to look into &lt;code&gt;Scam_Watcheroo&#39;s&lt;/code&gt; blog post about this to see what other issues he addresses. I would also like to look at how Mimi’s performance changed over time, and in particular how it changed following the LetsRun forum taking off and her ultimate switch from Race Drone to the Garmin tracker. In addition, something that I have not considered is whether or not the data are modified in the move from MovesCount to Strava. Although the overal trends would not change drastically, the raw data themselves (and therefore the digit distributions) might. This is probably worth considering in due time.&lt;/p&gt;
&lt;p&gt;I genuinely hope that this is useful to some of you in addressing some of the concerns. Nothing I can do can change people’s opinions on what happened in the first 2 weeks, but I hope that I can at least start to alleviate fears that there is any duplicity in the Strava data. It is all academic now following Mimi’s recent announcement that she will be pulling out from the event, but hopefully this should also help to assure people of the validity of Sandra’s run as well (although her regular updating and constant tracking have allayed any such fears already). All I can do now is wish Sandra good luck in getting the record (she looks to be on excellent form), and wish Mimi a very speedy recovery. It is sad that it had to go like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There are aspects of the spoofed data that make it stand out when compared to Mimi’s and Sandra’s (and my own) data&lt;/li&gt;
&lt;li&gt;I just do not think that it would be possible to create a forged data set that stands up to intense scrutiny - this is fairly basic scrutiny and it stands out&lt;/li&gt;
&lt;li&gt;Mimi is using 1s capture mode on constant capture for her runs, with the very occasional 3 or 4 sec delay&lt;/li&gt;
&lt;li&gt;Sandra is using 10s capture and has longer pauses in data retrieval of several minutes at a time (auto-pause?)&lt;/li&gt;
&lt;li&gt;Mimi’s data sets are therefore an order of magnitude denser than Sandra’s&lt;/li&gt;
&lt;li&gt;Mimi’s cadence blips of 200+ spm are likely just random c*ck ups in data capture - they disappear if you smooth out to a 10s capture rate (I was trying to contact her crew to ask her to set a second watch to 10s capture for one of her runs to confirm this, but unfortunately it was too late)&lt;/li&gt;
&lt;li&gt;You probably don’t see them for Sandra because they get averaged out&lt;/li&gt;
&lt;li&gt;Mimi is running with a high cadence but the average seems to fit with previous evidence (albeit very limited and definitely open to scrutiny) of her running gait (evidence from the film crew videos in the future will also help if/when released)&lt;/li&gt;
&lt;li&gt;Mimi is often running in 185-195 range, but then so do I - granted I am not running across a continent on a day by day basis, but I am also nowhere near elite&lt;/li&gt;
&lt;li&gt;Mimi’s cadence is about 10 spm quicker than Sandra’s for both walking and running&lt;/li&gt;
&lt;li&gt;Mimi seems to have a fairly even split of walking and running, whilst Sandra seems to consistently run but at a slower overall pace&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How Predictable Are Ultra Runners?</title>
      <link>/post/2018-04-24-how-predictable-are-ultrarunners/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-24-how-predictable-are-ultrarunners/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#natural-language-processing&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-forest&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Random Forest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-improvements&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Model Improvements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;This post is a continuation from my &lt;a href=&#34;/post/2018-04-18-Suunto-Or-Garmin/index.html&#34;&gt;previous post&lt;/a&gt;, looking at various aspects of the posting habits of the Ultra Running Community (URC). This was originally intended to be just an additional section in that blog post, but it was getting a little unwieldy so I decided to split it off into its own post to stop it from getting too crazy. This post is probably a lot less interesting than the last post, as it is really looking at one central question; can I predict which group member is posting based on the content of the message? Spoiler alert, you can’t! The posters on this forum are apparently &lt;em&gt;all&lt;/em&gt; sarcastic SOBs, so it is difficult to pick them apart. But it was quite interesting trying.&lt;/p&gt;
&lt;p&gt;So as a bit of an experiment, I decided to play around with these data to see if the language that people use in their posts is specific enough to allow us to predict who has written something based on what the post says. This is a job for &lt;em&gt;machine-learning&lt;/em&gt;, which is really a lot less grandiose than it sounds. Essentially what we are doing here is using data to train a model of some description that can be applied to a new set of data to make predictions. In this case, we are looking to fit a model that is able to classify posts into one of a number of groups, where each group represents a single user. As an example of a classification problem, think of the spam filter in your email client. This is essentially a model that has been trained to look at the email message content and determine whether it is spam or not (e.g. if it is full of words like &lt;em&gt;viagra&lt;/em&gt;, &lt;em&gt;Nigerian Prince&lt;/em&gt;, &lt;em&gt;penis enlargement&lt;/em&gt;, &lt;em&gt;make money today&lt;/em&gt;, etc. then it is clearly all kosher). This would be a 2-class classification problem.&lt;/p&gt;
&lt;p&gt;For classification problems such as this, we require a training set on which to fit our model, and a validation set to determine the quality of the model. The validation set must be independent of the training set, as we want to test how the model will generalize to new data. The idea of &lt;em&gt;cross validation&lt;/em&gt; is essentially to split your training data into a training set and a validation set such that the validation is independent of the model fitting (to avoid the effects of over-fitting in the training set). There are various ways to split your data in this way. For now I will simply randomly select a subset for training and a smaller subset for validation (the &lt;em&gt;Holdout Method&lt;/em&gt;), but for true cross-validation this should then be repeated several times so that the average over several validation sets is used. For example, in &lt;em&gt;k-fold cross validation&lt;/em&gt; you would randomly distribute the data into &lt;code&gt;k&lt;/code&gt; equally sized subsets, and use exactly one of these as the validation set and &lt;code&gt;k-1&lt;/code&gt; as the training set. This is then repeated &lt;code&gt;k&lt;/code&gt; times, each time using a different subset as the validation set.&lt;/p&gt;
&lt;p&gt;It makes sense to restrict this analysis to the most active posters, and so I will limit the analysis to only users who have contributed 50 or more posts to the forum. This gives us 5,233 posts, from 48 users. I will randomly select 4,000 posts for the training set, and use the remainder for validation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posts50 &amp;lt;- URC                 %&amp;gt;% 
           group_by(from_name) %&amp;gt;%    ## Group by poster
           filter(n() &amp;gt;= 50)   %&amp;gt;%    ## Select only posters with &amp;gt;50 posts
           select(from_name, message) ## Keep poster name and message content
set.seed(0) ## Set seed for random number generation for reproducibility
ids   &amp;lt;- sample(1:nrow(posts50), 4000) ## Randomly select 4000
train &amp;lt;- posts50[ids,]  ## Keep random ids as training set
test  &amp;lt;- posts50[-ids,] ## Use remaining ids as validation&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;natural-language-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Natural Language Processing&lt;/h1&gt;
&lt;p&gt;The model that we will be using is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Bag-of-words_model&#34;&gt;Bag Of Words&lt;/a&gt; model, which is a natural language processing technique that aims to represent text based on the frequency of words within it. There are some things that we can do to reduce the vector space of available terms, such as removing capital letters and removing so called “stop words” (common words like “is”, “and”, “but”, “the”, etc.). We can also limit the analysis to only words that occur frequently in the text, although there is a possibility of missing specific terms used by only one or two individuals, say, that may help the predictiveness of the model.&lt;/p&gt;
&lt;p&gt;I will be using the &lt;a href=&#34;https://cran.r-project.org/web/packages/text2vec&#34;&gt;text2vec&lt;/a&gt; package in R which is efficient at generating the required document-term matrix (DTM) for fitting our model. In particular, it generates unique tokens for each term rather than using the terms themselves, which reduces computational overheads. An iterative function can then be applied to generate the DTM. So let’s generate such an iterator over the term tokens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(text2vec)
train_tokens &amp;lt;- train$message                      %&amp;gt;%
                iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;% # Convert to ASCII format
                tolower                            %&amp;gt;% # Make lower case
                word_tokenizer                         # Break terms into tokens
it_train &amp;lt;- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;itoken&amp;gt;
##   Inherits from: &amp;lt;iterator&amp;gt;
##   Public:
##     chunk_size: 400
##     clone: function (deep = FALSE) 
##     counter: 0
##     ids: Jean-François Tantin Gary Kiernan Richard Lendon Iain Ed ...
##     initialize: function (iterable, ids = NULL, n_chunks = 10, progress_ = interactive(), 
##     is_complete: active binding
##     iterable: list
##     length: active binding
##     nextElem: function () 
##     preprocessor: list
##     progress: FALSE
##     progressbar: NULL
##     tokenizer: list&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we use this iterator to create a vocabulary DTM for fitting the model. To start with, I will use all of the words, but later we could look at filtering out stop words and less frequent terms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vocab      &amp;lt;- create_vocabulary(it_train)
vectorizer &amp;lt;- vocab_vectorizer(vocab)
train_dtm  &amp;lt;- create_dtm(it_train, vectorizer)
dim(train_dtm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4000 13922&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a matrix with 4,000 rows (the number of messages in the training set) and 13,922 columns (the number of unique terms in the training set). So each message is now represented as a vector of counts for all possible terms in the search space. The hope now is that we will be able to fit a model that is able to discriminate different users based on their word usage. Unlikely, but hey let’s give it a shot.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Random Forest&lt;/h1&gt;
&lt;p&gt;In this case, our dependent variable is the name of the user who posted the message which is a categorical variable. The independent variables are the counts for each of the 13,922 terms across the data set. I am going to start by using a random forest model, which is one of the more popular classification models available. A decision tree is a quite simple (although incredibly powerful) stepwise model that you can think of like a flow chart. The model fitting will create a series of nodes where your independent variables are used to discrimate between one choice and another, eventually leading to a certain prediction depending on the values of the variables in your model. A random forest essentially fits a whole load of these classification decision trees and outputs the &lt;em&gt;modal&lt;/em&gt; (most common) class across all of them.&lt;/p&gt;
&lt;p&gt;One benefit of using random forests over something like generalised linear models (see later) is that, since they rely on fairly independent tests at each stage in the tree, they are more robust to correlated variables in the model. With such a large set of term variables there is undoubtedly correlation between many of these terms, particularly as many of these variables are likely to be largely made of zeroes. Of course, this sparsity itself causes somewhat of a problem, and should be taken into account in the analysis. But for now I will ignore it and just hope that it isn’t a problem…&lt;/p&gt;
&lt;p&gt;To begin with,let’s fit a simple random forest model and see how it looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;randomForest&amp;quot;)
library(&amp;quot;caret&amp;quot;)
rf_model &amp;lt;- randomForest(x = as.matrix(train_dtm), y = as.factor(rownames(train_dtm)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I set the &lt;code&gt;y&lt;/code&gt; parameter to be a factor so that it is clear that this is a classification model. Now we can test our model by seeing how it performs at predicting the user for our test data set. First we generate a similar DTM for the test data set. Note that we use the same &lt;code&gt;vectorizer&lt;/code&gt; as we used for the training set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_tokens &amp;lt;- test$message                       %&amp;gt;%
               iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;% # Convert to ASCII format
               tolower                            %&amp;gt;% # Make lower case
               word_tokenizer                         # Break terms into tokens
it_test     &amp;lt;- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    &amp;lt;- create_dtm(it_test, vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we use our model to predict the user for each of the posts in our test data set. To do this we use the &lt;code&gt;predict()&lt;/code&gt; method for &lt;code&gt;randomForest&lt;/code&gt; objects, and output the response class with the majority vote amongst all of the decision trees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_predict &amp;lt;- predict.train(rf_model, test_dtm, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, how did we do? Let’s see how many of these were correctly predicted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test_predict == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   760   473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this model predicts the poster only 38.4 % of the time, which isn’t particularly good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-improvements&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Model Improvements&lt;/h1&gt;
&lt;p&gt;So can we improve this? Yes, probably. The first thing that I can try is to be a little more clever in the way that I parameterise the data. So rather than simply counting words, I will instead use &lt;em&gt;n-grams&lt;/em&gt; – combinations of &lt;code&gt;n&lt;/code&gt; words that will be more sensitive to the types of phrases that different people typically use. Obviously increasing &lt;code&gt;n&lt;/code&gt; in this case will also increase the memory and run time considerably, so there are limits to what we can feasibly do. Also, it is probably worth noting that removal of stop words is less likely to be the best way to go about this, as this will affect the structure of the n-grams. So this time let us leave the stop words in, but parameterise with &lt;code&gt;3-grams&lt;/code&gt;. I will also limit the count to those n-grams used at least 10 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vocab &amp;lt;- create_vocabulary(it_train, ngram = c(1L, 3L)) ## use 1-, 2- and 3-grams
vocab &amp;lt;- vocab %&amp;gt;% 
         prune_vocabulary(term_count_min = 10) ## Only keep n-grams with count greater than 10
vectorizer &amp;lt;- vocab_vectorizer(vocab)
dtm_train  &amp;lt;- create_dtm(it_train, vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that we used the notation &lt;code&gt;1L, 3L&lt;/code&gt;, which tells &lt;code&gt;R&lt;/code&gt; to explicitly use integer values rather than numeric values. In many cases this has little to no effect, but in programming an integer variable will take up much less memory (4 bytes per element) than a double precision floating point number (8 bytes per element).&lt;/p&gt;
&lt;p&gt;Another thing that we can do to improve the model fit is that we can attempt to normalise our DTM to account for the fact that different Facebook messages may be longer or shorter than others. Typically the “documents” in this case (the messages) are very small so I imagine this will have only a minimal effect. Here I will use the &lt;em&gt;term frequency-inverse document frequency&lt;/em&gt; (TF-IDF) transformation. The idea here is to not only normalise the data, but also to scale the terms such that terms that are more common (i.e. those used regularly in all posts) are down-weighted, whilst those that are more specific to a small number of users (and will thus be more predictive) are up-weighted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tfidf           &amp;lt;- TfIdf$new()
train_dtm_tfidf &amp;lt;- fit_transform(train_dtm, tfidf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally there is some fine tunning that can be made to the model fitting procudure. Here we are dealing with a very sparse set of data, since most of the counts are zero in this matrix (not everybody uses every word or set of words). This can cause issues with the random forest model. In addition, there may be some imbalance in the classes (for instance as we saw above different individuals post more often than others).&lt;/p&gt;
&lt;p&gt;One particular aspect to explore is that different selections for the parameters can have big effects on the quality of the model. The two main parameters for a random forest are the number of trees (&lt;code&gt;ntree&lt;/code&gt;) and the number of features that are evaluated at each branch in the trees (&lt;code&gt;mtry&lt;/code&gt;). The higher the better for the number of trees, although run-time can be a hindrance on this. For the second parameter, I have seen it suggested that the square root of the number of features is a good place to start, and this is the default for classification anyway. So let’s try increasing the number of trees, and running this on the TF-IDF transformed 3-gram data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_model_tfidf &amp;lt;- randomForest(x = as.matrix(train_dtm_tfidf), 
                               y = as.factor(rownames(train_dtm_tfidf)),
                               ntree = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One note to make here is that this is &lt;em&gt;slllllllloooooooooowwwwwwwwww&lt;/em&gt;! This needed to be run overnight to finish. Using something like python is probably a better bet when running machine learning algorithms like this, and I will probably do another post later in the future to look at some alternative ways to do this.&lt;/p&gt;
&lt;p&gt;So let’s take a look at whether or not this model is more effective at predicting the user:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm_test       &amp;lt;- create_dtm(it_test, vectorizer)
test_dtm_tfidf &amp;lt;- fit_transform(test_dtm, tfidf)
test_predict &amp;lt;- predict(rf_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;quot;response&amp;quot;)
table(test_predict == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   751   482&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, so now we have improved the prediction to a whopping 39.1%. Hmm. An improvement of 0.7% was not &lt;em&gt;quite&lt;/em&gt; as much as I was hoping for.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Logistic Regression&lt;/h1&gt;
&lt;p&gt;Okay, so let’s try a different model to see if that has any effect. I am going to fit a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;logistic regression&lt;/a&gt;. Regression is simply an attempt to fit a linear approximation to a set of data that minimises the difference between the modeled value and the true value (the &lt;em&gt;residuals&lt;/em&gt;). I will do a more thorough post on statistical modelling in the future, but for now think of regression models as being an attempt to fit a line of best fit between some variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; that you suspect is dependent on some other variables &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, ..., x_n\)&lt;/span&gt;. The idea then is to use this model to predict &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; based on new measurements of &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2, ..., x_n\)&lt;/span&gt;. So here we are trying to fit a model that will provide us with an estimate of the user based on the words used in the post.&lt;/p&gt;
&lt;p&gt;Here I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/index.html&#34;&gt;glmnet&lt;/a&gt; package to fit the logistic regression. Logistic regression is a subset of Generalised Linear Models (GLM), which are an extension of ordinary linear regression allowing for errors that are not normally distributed through the use of a link function. Since we have multiple possible classes in the dependent variable, this will be a multinomial logistic regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;glmnet&amp;quot;)
glm_model_tfidf &amp;lt;- cv.glmnet(x = train_dtm_tfidf, y = as.factor(train$from_name), 
                              family = &amp;#39;multinomial&amp;#39;, 
                              alpha = 1,
                              type.measure = &amp;quot;deviance&amp;quot;,
                              nfolds = 5,
                              thresh = 1e-3,
                              maxit = 1e3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is an n-fold cross-validated GLM (hence &lt;code&gt;cv.glmnet&lt;/code&gt;), which is a method of validation for the model that splits the data into &lt;code&gt;n&lt;/code&gt; equally sized subsets, then uses &lt;code&gt;n-1&lt;/code&gt; subsets as training data and the remaining subset as the validation data to test the accuracy of the model. This is repeated &lt;code&gt;n&lt;/code&gt; times, and the average is used. This is actually a better method than I have used in these data (selecting a test data set and running the model on the remaining subset) as every sample is used in the validation, which avoids over-fitting.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;family&lt;/code&gt; parameter gives the model family that defines the error model, which in turn determines the link function to be used. In this case we are using multinomial logistic regression, so the predicted response is a vector of probabilities between 0 and 1 – one for each potential response – all adding to 1. The link function, which defines the relationship between the linear predictor and the mean of the distribution function, is the &lt;code&gt;logit&lt;/code&gt; function, which in the binary case gives the log odds of the prediction:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X\beta = ln \frac{\mu}{(1-\mu)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;alpha&lt;/code&gt; value will utilise an L1 regularisation of the data to account for the sparsity that we see in the data. The &lt;code&gt;type.measure&lt;/code&gt; value determines the measurement to use to determine the cross validation, in this case the misclassification error. &lt;code&gt;nfolds&lt;/code&gt; gives the value of &lt;code&gt;k&lt;/code&gt; for the k-fold cross validation, &lt;code&gt;thresh&lt;/code&gt; gives the threshold for the convergence of the coordinate descent loop, and &lt;code&gt;maxit&lt;/code&gt; gives the maximum number of iterations to perform.&lt;/p&gt;
&lt;p&gt;So let’s see if this is any better:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_predict &amp;lt;- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = &amp;quot;response&amp;quot;)
table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## FALSE  TRUE 
##   752   481&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope. We still only see about 39% accurately assigned.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Discussion&lt;/h1&gt;
&lt;p&gt;Okay, so it is possible (highly probable?) that I have made some mistakes in this analysis, and that I could vastly improve the creation of the DTM, but I think it is more propbable that these posts are simply not distinct enough to determine individuals writing styles. I guess in a group with such a narrow focus, it is inevitable that people will be posting very similar content to one another. There is after all only so many ways to ask “Suunto or Garmin”.&lt;/p&gt;
&lt;p&gt;Let’s examine why we are struggling to distinguish these posts in a little more detail. Below is a heatmap showing the probability for each of the 48 potential posters, predicted for all 1,233 of the posts in the validation data set. A heatmap is a kind of 3-dimensional plot, where colour is used to represent the third dimension. So the 48 potential posters are shown on the x-axis, the 1,233 posts are shown on the y-axis, and the magnitude of the estimated probability for user &lt;code&gt;i&lt;/code&gt; based on post &lt;code&gt;j&lt;/code&gt; is represented by a colour from red (0% probability) to white (100% probability). Note that here I have scaled the data using a square root so that smaller probabilities (which we expect to see) are more visible. The rows and columns are arranged such that more similar values are closer together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;gplots&amp;quot;)
heatmap.2(sqrt(test_predict[,,1]), trace = &amp;quot;none&amp;quot;, margins = c(10,0), labRow = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;similar_posts.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So the biggest problem here is that the vast majority of the posts are estimated as most likely coming from either Neil Bryant, Stef Schuermans or James Adams. And actually, the ones that it gets correct are almost all from one of these posters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pie(sort(table(rownames(test_dtm)[colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;similar_posts_pie.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I wonder whether these guys are skewing the model because of their, ahem, above average posting habits. But frankly at this stage I’m kind of bored, so I think that I will leave it there. Another time maybe. Ultimately I believe that these posts are simply too short and bereft of salient information to be useful for making predictions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Suunto Or Garmin? The Age Old Question.</title>
      <link>/post/2018-04-18-suunto-or-garmin/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-04-18-suunto-or-garmin/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#note&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rfacebook&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#temporary-token&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fboauth&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ultra-running-community&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likes-comments-and-shares&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#top-contributors&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-are-people-posting&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-popular-posts&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-often-do-people-actually-talk-about-ultras&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suunto-or-garmin&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summing-up&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#session-info&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Note&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Introduction&lt;/h1&gt;
&lt;p&gt;In &lt;a href=&#34;/post/2018-04-15-ultrarunner-or-ultra-runner/&#34;&gt;my last post&lt;/a&gt;, I took a look at ways to pull data down from Twitter and analyse some specific trends. It was quite interesting for me to see how easy it is to access these data, and there is a huge amount to be gleened from these sorts of data. The idea of this post is to use a similar approach to pull data from the Ultra Running Community page on &lt;a href=&#34;https://www.facebook.com&#34;&gt;Facebook&lt;/a&gt;, and then to use these data to play around further with the &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; R packages. Just for funsies, I’m also going to have a bit of a crack at some machine learning concepts. In particular, a question that seems to comes up pretty regularly is whether the best GPS watch for running is from &lt;a href=&#34;http://www.suunto.com&#34;&gt;Suunto&lt;/a&gt; or &lt;a href=&#34;https://www.garmin.com&#34;&gt;Garmin&lt;/a&gt;. I figured I could save us all some time and answer the question once and for all…&lt;/p&gt;
&lt;p&gt;Just a little aside here; I think that some people missed the point last time. I honestly don’t care much about these questions, they are just a jumping off point for me to practice some of the data analysis techniques that come up in my work. The best way to get better at something is to practice, so these posts are just a way of combining something I love with a more practical purpose. The idea of this blog is for me to practice these things until they become second nature. Of course in the process, I may just find something interesting along the way.&lt;/p&gt;
&lt;p&gt;Probably not though.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rfacebook&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Rfacebook&lt;/h1&gt;
&lt;p&gt;Following on from my experiences playing around with the &lt;a href=&#34;https://twitter.com&#34;&gt;Twitter&lt;/a&gt; API, I decided to have a look to see if there were similar programmatic ways to access Facebook data. This can be accomplished using the &lt;a href=&#34;https://cran.r-project.org/web/packages/Rfacebook/&#34;&gt;Rfacebook&lt;/a&gt; package in R, which is very similar to the &lt;code&gt;TwitteR&lt;/code&gt; package that I used previously. This package accesses the Facebook &lt;a href=&#34;https://developers.facebook.com/docs/graph-api&#34;&gt;Graph API Explorer&lt;/a&gt;, allowing access to a huge amount of data from the Facebook social &lt;em&gt;graph&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So first of all, let’s install the &lt;code&gt;Rfacebook&lt;/code&gt; package. We can install the stable version from the Comprehensive R Archive Network (&lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or install the more up-to-date but less stable developmental version from Github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;devtools&amp;quot;)
install_github(&amp;quot;pablobarbera/Rfacebook/Rfacebook&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am using the developmental version here. There are several additional packages that also need to be installed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;httr&amp;quot;, &amp;quot;rjson&amp;quot;, &amp;quot;httpuv&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with &lt;code&gt;TwitteR&lt;/code&gt;, access to the API is controlled through the use of API tokens. There are two ways of doing this - either by registering as a developer and generating an app as I did with &lt;code&gt;TwitteR&lt;/code&gt;, or through the use of a temporary token which gives you access for a limited period of 2 hours. Let’s look at each of these in turn:&lt;/p&gt;
&lt;div id=&#34;temporary-token&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Temporary Token&lt;/h2&gt;
&lt;p&gt;To generate a temporary token, just go to the &lt;a href=&#34;https://developers.facebook.com/tools/explorer/&#34;&gt;Graph API Explorer&lt;/a&gt; page and generate a new token by clicking on &lt;code&gt;Get Token&lt;/code&gt; -&amp;gt; &lt;code&gt;Get Access Token&lt;/code&gt;. You need to select the permissions that you want to grant access for, which will depend on what you are looking do:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;Graph-API-Explorer.png&#34; alt=&#34;Create Temporary Access Token&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Create Temporary Access Token&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I just granted permission to everything for this analysis. Once you have an access token, this can be used as the &lt;code&gt;token&lt;/code&gt; parameter when using functions such as &lt;code&gt;getUsers()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fboauth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; fbOAuth&lt;/h2&gt;
&lt;p&gt;The above is the most simple method, but this access token will only last for 2 hours, at which point you will need to generate a new one. If you want a longer term solution, you can set up &lt;a href=&#34;https://en.wikipedia.org/wiki/OAuth&#34;&gt;Open Authorization&lt;/a&gt; access in a similar way to for the &lt;code&gt;TwitteR&lt;/code&gt; package. The downside is that you lose the ability to search friend networks unless your friends are also using the app that you generate - and I don’t want to inflict that on people just so that I can &lt;del&gt;steal their identity&lt;/del&gt; analyse their networks.&lt;/p&gt;
&lt;p&gt;This method is almost identical to the process used for generating the OAuth tokens in the &lt;code&gt;TwitteR&lt;/code&gt; app, and a good description of how to do it can be found in this &lt;a href=&#34;http://thinktostart.com/analyzing-facebook-with-r/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, I am feeling pretty lazy today and so I will just use the temporary method.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ultra-running-community&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Ultra Running Community&lt;/h1&gt;
&lt;p&gt;With nearly 18,000 members, the Ultra Running Community Facebook page is a very active community of ultrarunners from around the world. Runners are able to ask questions, share blogs, and generally speak with like-minded individuals about everything from gear selection to how best to prevent chaffing when running. It’s been going since June 2012, so there are a fair few posts available to look through.&lt;/p&gt;
&lt;p&gt;So let’s load in all of the posts from the URC page:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;Rfacebook&amp;quot;)
token &amp;lt;- &amp;quot;XXXXXX&amp;quot;  ## Insert your temporary token from Graph API Explorer
URC &amp;lt;- getGroup(&amp;quot;259647654139161&amp;quot;, token, n=50000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will create an object of class &lt;code&gt;data.frame&lt;/code&gt; containing the most recent 50,000 posts available in the Facebook Group with ID &lt;code&gt;259647654139161&lt;/code&gt; (which is the internal ID for the Ultra Running Community page). The page was set up in June 2012 By Neil Bryant, and currently (as of 20th March 2017) contains a total of 24,836 posts. So this command will actually capture every single post.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;data.frame&lt;/code&gt; is somewhat of the workhorse of R, and looks to the user like a spreadsheet like you would expect to see in Excel. Behind the scene it is actually a list of lists, with each column representing a particular measurement or descriptive annotation of that particular datum. The ideal situation is to design your data frame such that every row is an individual measurement, and every column is some aspect relating to that measurement. This can sometimes go against the instinctual way that you might store data, but makes downstream analyses much simpler.&lt;/p&gt;
&lt;p&gt;As an example, suppose that you were measuring something (blood glucose levels, weight, lung capacity, VO2 max, etc.) at three times of the day for 2 individuals. Your natural inclination may be to design your table in this way:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement 1&lt;/th&gt;
&lt;th&gt;Measurement 2&lt;/th&gt;
&lt;th&gt;Measurement 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But actually the optimum way to represent this is to treat each measurement as a different row in your data table, and use a descriptive categorical variable to represent the repeated measurements:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Sample&lt;/th&gt;
&lt;th&gt;Measurement&lt;/th&gt;
&lt;th&gt;Replicate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Sample2&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can then add additional information relating to each individual measurement, which can be factored into your model down the line.&lt;/p&gt;
&lt;p&gt;In this case, we have a data frame where each row is a post on the URC feed, and each column gives you information on the post such as who wrote it, what the post says, when it was written, any links involved, and how many likes, comments and shares each post has. We can take a quick look at what the data.frame looks like by using the &lt;code&gt;str()&lt;/code&gt; function. This will tell us a little about each column, such as the data format (character, numeric, logical, factor, etc.) and the first few entries in each column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(URC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    24836 obs. of  11 variables:
##  $ from_id       : chr  &amp;quot;10203232759527000&amp;quot; &amp;quot;10154582131480554&amp;quot; &amp;quot;10206266800967107&amp;quot; &amp;quot;10153499987966664&amp;quot; ...
##  $ from_name     : chr  &amp;quot;Steph Wade&amp;quot; &amp;quot;Esther Bramley&amp;quot; &amp;quot;Tom Chapman&amp;quot; &amp;quot;Polat Dede&amp;quot; ...
##  $ message       : chr  &amp;quot;Anyone who runs in Speedcross 3s tried Sportiva? I&amp;#39;ve had several pairs of Speedcross  but thinking about tryin&amp;quot;| __truncated__ &amp;quot;Hi guys server pain in knee two weeks after 41miler. Ran 3 miles Tuesday no problem. Pain started at 5m and got&amp;quot;| __truncated__ &amp;quot;mega depressed; need advice. Running really well over xmas, since then, painful hip &amp;amp; groin, chasing itb, glute&amp;quot;| __truncated__ NA ...
##  $ created_time  : chr  &amp;quot;2017-03-19T08:38:15+0000&amp;quot; &amp;quot;2017-03-18T12:19:26+0000&amp;quot; &amp;quot;2017-03-18T16:30:54+0000&amp;quot; &amp;quot;2017-03-19T08:42:38+0000&amp;quot; ...
##  $ type          : chr  &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;status&amp;quot; &amp;quot;photo&amp;quot; ...
##  $ link          : chr  NA NA NA &amp;quot;https://www.facebook.com/tahtaliruntosky/photos/a.659614340816057.1073741827.659609490816542/1145262965584523/?type=3&amp;quot; ...
##  $ id            : chr  &amp;quot;259647654139161_1064067560363829&amp;quot; &amp;quot;259647654139161_1063418100428775&amp;quot; &amp;quot;259647654139161_1063562803747638&amp;quot; &amp;quot;259647654139161_1064068937030358&amp;quot; ...
##  $ story         : logi  NA NA NA NA NA NA ...
##  $ likes_count   : num  0 0 0 0 0 2 2 58 7 1 ...
##  $ comments_count: num  5 23 9 0 25 9 4 64 77 12 ...
##  $ shares_count  : num  0 1 0 0 0 0 0 0 3 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likes-comments-and-shares&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Likes, Comments and Shares&lt;/h1&gt;
&lt;p&gt;The first step in any data analysis is to check that the data make sense. You’ve probably heard the old adage “garbage in; garbage out”, so data cleaning is an essential first step to ensure that we are not basing our conclusions on erroneous information from the beginning. There are far too many posts here to look at them all by hand, but there are a few things we can certainly have a look at to check that the values make sense.&lt;/p&gt;
&lt;p&gt;For instance, let’s take a look at the number of likes, comments and shares. We would expect all of these values to be positive whole numbers, so this is something that is easy to check. To do this, I will be making use of the &lt;a href=&#34;https://ggplot2.tidyverse.org&#34;&gt;ggplot2&lt;/a&gt; package, which allows for some incredibly powerful plotting in R. The idea is to define the plot in terms of &lt;em&gt;aesthetics&lt;/em&gt;, where different elements of the plot (x and y values, colour, size, shape, etc.) can be mapped to elements of your data.&lt;/p&gt;
&lt;p&gt;In this case I want to plot a distribution plot where the colour of the plot is mapped to whether we are looking at likes, comments or shares. To do this, I need to rearrange the data such that the &lt;code&gt;likes_count&lt;/code&gt;, &lt;code&gt;comments_count&lt;/code&gt; and &lt;code&gt;shares_count&lt;/code&gt; columns are in a single column &lt;code&gt;counts&lt;/code&gt;, with an additional column defining whether it is a like, a comment, or a share count (as described in the example above).&lt;/p&gt;
&lt;p&gt;I will use the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyr/tidyr.pdf&#34;&gt;tidyr&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html&#34;&gt;stringr&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html&#34;&gt;dplyr&lt;/a&gt; packages to rearrange the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyr&amp;quot;)
library(&amp;quot;dplyr&amp;quot;)
library(&amp;quot;stringr&amp;quot;)
like_comment_share &amp;lt;- URC                                                 %&amp;gt;% 
                      gather(count_type, count, likes_count:shares_count) %&amp;gt;%
                      select(count_type, count)
head(like_comment_share)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    count_type count
## 1 likes_count     0
## 2 likes_count     0
## 3 likes_count     0
## 4 likes_count     0
## 5 likes_count     0
## 6 likes_count     2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;stringr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; are incredibly powerful packages written by Hadley Wickham, which provide a simple to understand &lt;em&gt;grammar&lt;/em&gt; to apply to the filtering and tweaking of data frames in R. In particular, these can be used to convert the data into the &lt;em&gt;tidy&lt;/em&gt; format described above, allowing very simple and intuitive plotting with &lt;code&gt;ggplot2&lt;/code&gt;. One particularly useful feature is the ability to use the &lt;code&gt;%&amp;gt;%&lt;/code&gt; command to pipe the output to perform multiple data frame modifications.&lt;/p&gt;
&lt;p&gt;In the above code, we pipe the raw data &lt;code&gt;URC&lt;/code&gt; into the &lt;code&gt;gather()&lt;/code&gt; function, which will take the three columns from &lt;code&gt;likes_count&lt;/code&gt; through to &lt;code&gt;shares_count&lt;/code&gt; and split them into two new columns: &lt;code&gt;count_type&lt;/code&gt; which will be one of &lt;code&gt;shares_count&lt;/code&gt;, &lt;code&gt;likes_count&lt;/code&gt; and &lt;code&gt;comments_count&lt;/code&gt; , and &lt;code&gt;count&lt;/code&gt; which will take the value from the specified column. So essentially this produces a new data set with 3 times as many rows. This is then piped into &lt;code&gt;select()&lt;/code&gt; which will select the relevant columns.&lt;/p&gt;
&lt;p&gt;First let’s just check that they are are all positive integers as expected:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count == as.integer(like_comment_share$count))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(like_comment_share$count &amp;gt;= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;[1] TRUE&lt;/p&gt;
&lt;p&gt;Annoyingly there is no easy way to check that a vector of numbers is made up of integers, so this line will check that the numbers do not change after converting to integers. Similarly, we use &lt;code&gt;all()&lt;/code&gt; to check that all of the counts are greater than or equal to zero. They are as we would hope.&lt;/p&gt;
&lt;p&gt;Then we can use &lt;code&gt;ggplot2&lt;/code&gt; for the plotting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggplot2&amp;quot;)
ggplot(like_comment_share, aes(x = log10(count+1), col = count_type, fill = count_type)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Count (log10)&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-like_comment_share-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we use this output to plot a distribution plot showing the distribution of counts for the three different metrics – shares, comments and likes. We use the &lt;code&gt;count&lt;/code&gt; as the &lt;code&gt;x&lt;/code&gt; aesthetic, and &lt;code&gt;count_type&lt;/code&gt; as both the &lt;code&gt;col&lt;/code&gt; and &lt;code&gt;fill&lt;/code&gt; aesthetics to colour them. The main function &lt;code&gt;ggplot()&lt;/code&gt; will specify the aesthetics, and then we add additional elements to the plot by using the &lt;code&gt;+&lt;/code&gt; command. Here we add the &lt;code&gt;geom_density()&lt;/code&gt; element to plot the data in a density plot (the &lt;code&gt;alpha&lt;/code&gt; value will make the colours transparent for overplotting), the &lt;code&gt;labs()&lt;/code&gt; function will change the plot labels, and the &lt;code&gt;theme()&lt;/code&gt; function let’s you change aspects of the figure text, such as the size.&lt;/p&gt;
&lt;p&gt;Note that here I have plotted the &lt;span class=&#34;math inline&#34;&gt;\(log_{10}\)&lt;/span&gt; of the counts, which reduces the effects of outliers. Also note that I have added 1 to the counts. This is because &lt;span class=&#34;math inline&#34;&gt;\(log_{10}(0)\)&lt;/span&gt; is undefined. The idea here is that a count of 1 will get a value of 0, a count of 10 gets a value of 1, a count of 100 gets a value of 2, etc.&lt;/p&gt;
&lt;p&gt;So what does this tell us? Well not too much really. Not many people share posts from the page, but there aren’t too many that don’t get comments or likes. So it is a very active community. Posts tend to have more comments than likes, which makes sense because you can only like something once, but can comment as many times as you want. But the main thing that this shows is that these counts all seem to be in the right sort of expected range.&lt;/p&gt;
&lt;p&gt;Often exploratory plots like this can be useful to highlight problems in the raw data. One such example might be if a negative count existed in these data, which could happen due to input errors but quite clearly does not represent a valid count. As it happens, since these data are not manually curated, it is highly unlikely that such errors will be present, but you should never assume anything about any given data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;top-contributors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Top Contributors&lt;/h1&gt;
&lt;p&gt;Let’s take a look at the all-time most common contributors to the page, again using the &lt;code&gt;ggplot2&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_contributors &amp;lt;- URC              %&amp;gt;% 
                    count(from_name) %&amp;gt;%
                    top_n(50, n)     %&amp;gt;%
                    arrange(desc(n))
ggplot(top_contributors, 
       aes(x = factor(from_name, levels = top_contributors$from_name), 
           y = n,
           fill = n)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  scale_fill_gradient(low=&amp;quot;blue&amp;quot;, high=&amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) +
  theme(axis.title  = element_text(size = 18),
        axis.text.x = element_text(size = 12, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 14),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-contributors-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have first used &lt;code&gt;dplyr&lt;/code&gt; to count up the number of posts per user and select the top 50 contributors, then used &lt;code&gt;ggplot2&lt;/code&gt; to plot a barplot showing the number of posts per person. I have used the &lt;code&gt;scale_fill_gradient()&lt;/code&gt; element to colour the bars based on their height, such that those with the highest number of posts are coloured red, whilst those with the lowest are coloured blue.&lt;/p&gt;
&lt;p&gt;The top contributor to the page is Neil Bryant (757 posts), who is the founder member so this makes sense. James Adams is the second biggest contributor (489 posts), and he has less of an excuse really.&lt;/p&gt;
&lt;p&gt;Let’s take a look at James’ posting habits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;xts&amp;quot;)
jamesadams &amp;lt;- URC                                                        %&amp;gt;%
              filter(from_name == &amp;quot;James Adams&amp;quot;)                         %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
jamesadams_xts &amp;lt;- xts(jamesadams$n, order.by = jamesadams$created_time)
jamesadams_month &amp;lt;- apply.monthly(jamesadams_xts, FUN = sum)
plot(jamesadams_month, ylab = &amp;quot;Number of Posts&amp;quot;, main = &amp;quot;&amp;quot;, cex.lab = 1.7, cex.axis = 1.4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-JamesAdams-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here I have used the package &lt;code&gt;xts&lt;/code&gt; to deal with the &lt;code&gt;POSIXct&lt;/code&gt; date format. In particular this will deal correctly with months with zero counts. James has been pretty active since summer 2013 (probably publicising &lt;a href=&#34;https://www.amazon.com/Running-Stuff-James-Adams/dp/1784622621&#34;&gt;his book&lt;/a&gt;), but his activity has been on the decline throughout 2016 – the price you pay when your family size doubles I guess.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-are-people-posting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; When are people posting?&lt;/h1&gt;
&lt;p&gt;Next we can break the posts down by the day on which they are posted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;URC      &amp;lt;- URC %&amp;gt;%
            mutate(dow = factor(weekdays(as.POSIXct(created_time)), labels = c(&amp;quot;Monday&amp;quot;, &amp;quot;Tuesday&amp;quot;, &amp;quot;Wednesday&amp;quot;, &amp;quot;Thursday&amp;quot;, &amp;quot;Friday&amp;quot;, &amp;quot;Saturday&amp;quot;, &amp;quot;Sunday&amp;quot;)))
post_day &amp;lt;- URC %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-posting_day-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly to previously, here I have used &lt;code&gt;dplyr&lt;/code&gt; to reduce the full data down to a table of counts of posts per day of the week, then plotted them using &lt;code&gt;ggplot2&lt;/code&gt;. Surprisingly (to me anyway) there is no increase in activity during the weekend. I guess most of us are checking Facebook during working hours and busy running at the weekend…&lt;/p&gt;
&lt;p&gt;Wednesdays are interestingly bereft of posts though for some reason. Could this be people checking URC at work on Monday and Tuesday through boredom, only to find themselves told off and having to catch up on work by Wedesday? Then by the time the weekend rolls around we’re all back liking away with impunity ready to go through the whole process again the next week.&lt;/p&gt;
&lt;p&gt;Let’s look at the same plot for the 1,000 most popular posts (based on likes):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_day &amp;lt;- URC                      %&amp;gt;%
            top_n(1000, likes_count) %&amp;gt;%
            count(dow)
ggplot(post_day, aes(x = dow, y = n, fill = dow)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Number of Posts&amp;quot;) + 
  theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-posting_day_most_likes-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So from this it is clear that if you want people to like your post, you should post on a Tuesday or a Thursday. Quite why people might be feeling so much more likely to click that all important like button on these dayas, I have no idea. But hey, stats don’t lie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-popular-posts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;8&lt;/span&gt; Most Popular Posts&lt;/h1&gt;
&lt;p&gt;So looking at the popular posts above got me thinking about how best to actually define a “popular” post. Is it a post with a lot of likes, or a post that everybody is commenting on? Let’s take a look at the top 5 posts based on each criteria:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(likes_count)) %&amp;gt;% top_n(5, likes_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                       message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 Thinking how far I’ve come and getting a bit emotional. .3 year ago i was a size 20/22 and couldnt run to end of the street. Yesterday i ran 30 mile as a training run and wasn’t even aching afterwards. Now nearly 45 and a size 10 and never felt better. I love my life!!!!
2 I saw this picture couple years ago and I found it very inspiring so I thought I’d share it. is a 12-year-old childhood cancer survivor who loves to run with his dad.
3 Not sure if swear words are accepted. 088208820882
4 “What you doing up here?” said the sheep.Pike last night, not a soul to be seen…
5 0880&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print.AsIs(URC %&amp;gt;% arrange(desc(comments_count)) %&amp;gt;% top_n(5, comments_count) %&amp;gt;% select(message))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   message&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1 For me claiming something you haven’t earned is not only immoral it’s fraudulent. And it’s a huge insult to all who’ve attempted the feat before you and legitimately fallen short. What do you guys think?
2 Anyone else do race to the stones and found it a rip off? I was not impressed with most things. were good such as medics and lots of water but who wants fudge bars and cadburies at aid stations. like a money making race to me, especially considering all the sponsorship they had. ’ve got lots of other rants about it but let’s hear anyone else’s thoughts first
3 New Game.am trying to convince some new ultra runners that you do not need to spend a load of money on kit to put one foot in front of the other a few more times. This is difficult given that half the posts in forums seems to be asking for recommendations or giving recommendations as to how one might waste money on kit.out of interest, what was the value of the kit you wore in your last ultra? Including nutrition. Obviously you will have to guess if you had them as a gift or can’t remember. Surely someone is going to have less than £100?
4 I had a small sabre rattling session last night with someone on this group. Nothing major by any stretch of the imagination - we just have opposing views on DNF. But it got me curios to what the opinions of others are on this subject. Is failing to finish something that you would risk your life to avoid? Is it something to fear? Is it something that will eventually happen to us all? Is it something that we can learn from? Etc,etc. Your thoughts please
5 i cannot wait to watch the EPSN footage - amazing stuff. It is a shame Robert has his doubters though.: was described as “trolling”, which was an over the top description (agree with the comments there)&lt;/p&gt;
&lt;p&gt;It seems to me that the posts with more likes tend to be posts with a much more positive message than those with most comments. The top liked posts are those from people who have overcome some form of adversity (such as the top liked post with 1,369 likes from Mandy Norris who had awesomely run 30 miles after losing half her body weight), whilst the top commented posts tend to be more controversial posts (such as the top commented post with 287 comments about Mark Vaz’s fraudulent JOGLE “World Record”).&lt;/p&gt;
&lt;p&gt;Let’s take a look at how closely these two poularity measures are correlated in the Ultra Running Community:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(URC, aes(x = comments_count, y = likes_count)) +
  geom_point(shape = 19, alpha = 0.2, size = 5) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  geom_point(data = URC %&amp;gt;% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = &amp;quot;red&amp;quot;, size = 5)) +
  geom_point(data = URC %&amp;gt;% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = &amp;quot;blue&amp;quot;, size = 5)) +
  labs(x = &amp;quot;Number of Comments&amp;quot;, y = &amp;quot;Number of Likes&amp;quot;) + 
  theme(axis.text   = element_text(size = 16),
        axis.title  = element_text(size = 20),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-likes_vs_comments-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are plotting a correlation scatterplot between the number of likes and the number of comments for each post. I have a set an alpha value of 0.2 for the scatterplot so that the individual points are made more see-through. That way, overplotting can be seen by darker coloured plots. I have also added in a line of best fit generated by fitting a linear model (&lt;code&gt;method= lm&lt;/code&gt;), together with an estimate of the standard error shown by the grey surrounding of the blue line (&lt;code&gt;se = TRUE&lt;/code&gt;). Finally I have highlighted the top 5 commented posts in blue, and the top 5 liked posts in red.&lt;/p&gt;
&lt;p&gt;It is pretty clear from this plot that there is virtually no correlation between the number of comments and the number of likes, particularly for those with more likes or comments. In general the posts with more likes do not have the most comments (and vice versa), suggesting that in general we like the nice posts, but comment on the ones that upset us. In fact, it looks as if Mandy’s post is the only exception, with both the highest number of likes but also a high number of comments (220).&lt;/p&gt;
&lt;p&gt;We can calculate the correlation between these measures, which is a measure of the linear relationship between two variables. A value of 1 indicates that they are entirely dependent on one another, whilst a value of 0 indicates that the two are entirely independent of one another. A value of -1 indicates an inverse depdnedancy, such that an increase in one variable is associated with a similar decrease in the other variable. Given the difference in the scales between likes and comments, I will use the &lt;em&gt;Spearman correlation&lt;/em&gt;, which looks at correlation between the ranks of the data and therefore ensures that each unit increment is 1 for both variables meaning that it is robust to outliers. The Spearman correlation between these two variables is &lt;code&gt;0.27&lt;/code&gt;, so there is virtually no correlation between likes and comments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-often-do-people-actually-talk-about-ultras&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;9&lt;/span&gt; How Often Do People Actually Talk About Ultras?&lt;/h1&gt;
&lt;p&gt;It seems recently that there is more talk of charlatans and frauds than races, and a lot of people have commented on the fact that there seems to be less and less actual discussion of ultras recently. So let’s see if this is the case, by tracking how often the term &lt;em&gt;ultra&lt;/em&gt; is actually used over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ultraposts &amp;lt;- URC                                                        %&amp;gt;%
              filter(str_detect(message, &amp;quot;ultra&amp;quot;))                       %&amp;gt;% 
              mutate(created_time = as.POSIXct(created_time))            %&amp;gt;%
              count(created_time)
ultraposts_xts &amp;lt;- xts(ultraposts$n, order.by = ultraposts$created_time)
ultraposts_month &amp;lt;- apply.monthly(ultraposts_xts, FUN = sum)
plot(ultraposts_month, ylab = &amp;quot;Number of Ultra Posts&amp;quot;, main = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-ultra_usage-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Over the last year or so, the number of people in the group has risen dramatically, and yet it certainly seems that fewer people are actually discussing ultras these days. I guess read into that what you will – perhaps the feed is indeed dominated by Suunto vs Garmin questions?&lt;/p&gt;
&lt;p&gt;Hell, let’s find out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suunto-or-garmin&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;10&lt;/span&gt; Suunto or Garmin?&lt;/h1&gt;
&lt;p&gt;So let’s take a look at the real question that everybody cares about – which is more popular, Suunto or Garmin. All my cards on the table; I have a Suunto Peak Ambit 3, but if it helps I had to Google that because I really don’t keep up on these things. I’m really not a gear not, and prefer to make do. The only reason that I got this is that my previous watch died a death, and I like to use a watch for navigation. I didn’t pay for it – at that price I couldn’t bring myself to fork out the money. But it was a gift, and I am very pleased with it. It has a great battery life, and is pretty simple when loading data to my computer. Despite being a stats guy, I don’t normally focus much on my own data, but actually it has been interesting to see how slow I have become recently due to an ongoing injury. Perhaps it will help me to push myself in training onece it is sorted.&lt;/p&gt;
&lt;p&gt;But as I understand it, the Garmin Fenix 3 does exactly the same stuff. Is one better than the other? I couldn’t possibly say. Many people have tried, but I suspect that it comes down to personal preference rather than there being some objective difference between the two.&lt;/p&gt;
&lt;p&gt;But just for the hell of it, let’s see how often people talk about the two. I will be simply using fuzzy matching to look for any use of the terms “suunto” or “garmin” in the post. Fuzzy matching is able to spot slight misspellings, such as “Sunto” or “Garmin”, and is carried out using the base &lt;code&gt;agrep()&lt;/code&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suunto &amp;lt;- agrep(&amp;quot;suunto&amp;quot;, URC$message, ignore.case = TRUE)
garmin &amp;lt;- agrep(&amp;quot;garmin&amp;quot;, URC$message, ignore.case = TRUE)
pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c(&amp;quot;Suunto&amp;quot;, &amp;quot;Garmin&amp;quot;, &amp;quot;Both&amp;quot;), cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So of the 24,836 posts on the URC page, 237 (0.95 %) mention Suunto, whilst 552 (2.22 %) mention Garmin. Only 77 (0.3 %) mention both, which I assume are the posts specifically asking which of the two is best. Given the way some people moan about how often this question comes up, these numbers are actually surprisingly small I think. But based on this it seems that Garmin is more popular, although it would be interesting to look at the actual responses on those “VS” posts to see what the outcome actually was in each case.&lt;/p&gt;
&lt;p&gt;Having said that, there is nothing to suggest what these posts about Garmin’s are actually saying. They may be generally saying that they hate Garmins. So I am going to play around with a bit of sentiment analysis using the &lt;code&gt;qdap&lt;/code&gt; package. Essentially this is a machine learning algorithm that has been trained to identify the sentiment underlying a post, with positive values representing a generally positive sentiment (and vice versa). So let’s break down the Garmin and Suunto posts to see how they stack up:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;qdap&amp;quot;)

## Convert to ASCII and get rid of upper case
suunto_msg &amp;lt;- URC$message[suunto]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                         
garmin_msg &amp;lt;- URC$message[garmin]                %&amp;gt;%
              iconv(&amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub = &amp;quot;&amp;quot;) %&amp;gt;%
              tolower                                            

## Calculate the sentiment polarity
suunto_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, suunto_msg))
garmin_sentiment &amp;lt;- polarity(gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot;&amp;quot;, garmin_msg))

## Plot in a stacked barplot
sent_dat &amp;lt;- data.frame(Watch     = c(rep(&amp;quot;Suunto&amp;quot;, length(suunto)), 
                                     rep(&amp;quot;Garmin&amp;quot;, length(garmin))),
                       Sentiment = c(suunto_sentiment$all$polarity,
                                     garmin_sentiment$all$polarity))
ggplot(sent_dat, aes(x = Sentiment, col = Watch, fill = Watch)) + 
  geom_density(alpha = 0.1) +
  labs(x = &amp;quot;Sentiment Polarity&amp;quot;, y = &amp;quot;Density&amp;quot;) +
  theme(axis.text    = element_text(size = 16),
        axis.title   = element_text(size = 20),
        legend.text  = element_text(size = 18),
        legend.title = element_text(size = 24))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-04-18-suunto-or-garmin/index_files/figure-html/2017-04-15-suunto_vs_garmin_sentiment-1.png&#34; width=&#34;3000&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So a value of zero on this distribution plot suggests a neutral sentiment to the post, a positive number suggests a positive sentiment, and a negative number suggests a negative sentiment. While the majority of the posts seem to be fairly neutral in both cases, it certainly seems that the majority of the Garmin posts are positive whilst the Suunto posts are more neutral with many positive &lt;em&gt;and&lt;/em&gt; negative posts.&lt;/p&gt;
&lt;p&gt;We can actually put a number on this, for whether or not there is truly a statistically significant difference between the distribution of sentiment scores for the two watches. To do this, we will us a statistical test that checks how likely it is that we would see a difference of the magnitude seen here given that there is no difference between what people actually think of the watch. This is the so-called “null-hypothesis”, and essentially says that there is no difference, and any differences that we do see are purely random errors. We can test this hypothesis using one of a number of different tests, with the aim to see whether there is evidence that we can reject this null hypothesis, thus suggesting that there is indeed a true difference between the distributions. So we never really “prove” that there is a difference, but instead show that there is suitable evidence to disprove the null hypothesis.&lt;/p&gt;
&lt;p&gt;To do this some test statistic is calculated and is tested to see if it is significantly different than what you would expect to see by chance. Typically this is assessed using a “p-value”, which is one of the most misunderstood measurements in statistics. This value represents the probability that you would see a test statistic &lt;em&gt;at least as high&lt;/em&gt; as that measured purely by chance, even if both sets of data were drawn from the same distribution. So both the Garmin and Suunto scores are a tiny subset of all possible opinions of people in the world, the population distribution. Our two sample populations are either drawn from an overall population where there is no difference, or from two distinct populations for people who have a view one way or the other.&lt;/p&gt;
&lt;p&gt;It is pretty clear from the above figure that these values are not normally distributed (a so-called “bell-curve” distribution), so we cannot use a simple t-test which basically tests the difference in the means between two distributions (after taking the variance into account). Instead we would be better off using a non-parametric test which does not require the data to be parameterised into some fixed probability density function. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test&#34;&gt;Kolmogorov Smirnov test&lt;/a&gt; is one method that can be used, and works by looking at how the cummulative distribution functions of two distinct samples differ:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ks.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]],
## subset(sent_dat, : p-value will be approximate in the presence of ties&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two-sample Kolmogorov-Smirnov test
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## D = 0.093653, p-value = 0.1091
## alternative hypothesis: two-sided&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we run a two-sided test, which simply means that we have no &lt;em&gt;a priori&lt;/em&gt; reason to suspect one distribution to be greater than the other. We could do a one-sided test where the alternative hypothesis that we are testing is “A is greater than B”, rather than the two-sided test where we are testing the alternative hypothesis that “A is not equal to B”. &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the maximum distance between the empirical distribution functions, and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the p-value. Typically, a threshold used to reject the null hypothesis is for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be less than 0.05 (5 %), although it is fairly arbitrary. But in this case, we would conclude that there is not sufficient evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;As an alternative, we can instead use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mann–Whitney_U_test&#34;&gt;Wilcoxon rank sum test&lt;/a&gt; (also called the Mann-Whitney &lt;em&gt;U&lt;/em&gt; test). The idea is to rank all of the data, sum up the ranks from one of the samples, and use this to calculate the test statistic &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; (which takes into account the sample sizes). So if the distributions are pretty similar, the sum of the ranks will be similar for sample 1 and sample 2. If there is a big difference, one sample will have more higher ranked values than the other, resulting in a higher value for &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;. Let’s take a look at this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wilcox.test(subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]], subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  subset(sent_dat, Watch == &amp;quot;Suunto&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]] and subset(sent_dat, Watch == &amp;quot;Garmin&amp;quot;)[[&amp;quot;Sentiment&amp;quot;]]
## W = 59157, p-value = 0.03066
## alternative hypothesis: true location shift is not equal to 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the test statistic here is actually &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, but in this case &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are equivalent. So this test would result in us rejecting the null hypothesis with the same threshold as above. So which one is correct? Well, this is a great example of why you should never trust statistics. Both of these are perfectly reasonable tests to perform in this case but give different results. Many people would simply choose the one that gives the lowest p-value, but this is pretty naughty and is often called “p-hacking”. At the end of the day, a p-value higher than 0.05 does not mean that there is &lt;em&gt;not&lt;/em&gt; a true difference between the distributions, just that the current data does not provide enough evidence to reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;So my conclusion from this is that I made the wrong decision, and will from now on look upon my useless Suunto watch with hatred and resentment. I can only hope that this post will save anyone else from making such an awful mistake.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summing-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;11&lt;/span&gt; Summing Up&lt;/h1&gt;
&lt;p&gt;It has been quite &lt;em&gt;fun&lt;/em&gt; playing around with these data tonight, and I have had an opportunity to try out a few new techniques that I have wanted to play with for a while. As ever, there is loads more that can be gleaned from these data, but I should probably do something a little more productive right now. Like sleeping. I have actually done a while load more playing with machine learning algorithms of my own, but this post has already become a little too unruly so I will post this later as a separate post.&lt;/p&gt;
&lt;p&gt;But in summary, people on the Ultra Running Community page spend far too much time posting during working hours, seem to be talking less and less about ultra running, and definitely prefer Garmins to Suuntos. So this has all been completely worth it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;12&lt;/span&gt; Session Info&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 3.6.1 (2019-07-05)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Sierra 10.12.6
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] qdap_2.3.2             RColorBrewer_1.1-2     qdapTools_1.3.3       
##  [4] qdapRegex_0.7.2        qdapDictionaries_1.0.7 xts_0.11-2            
##  [7] zoo_1.8-6              ggplot2_3.2.0          stringr_1.4.0         
## [10] dplyr_0.8.3            tidyr_1.0.0            randomForest_4.6-14   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.2          lattice_0.20-38     xlsxjars_0.6.1     
##  [4] gtools_3.8.1        assertthat_0.2.1    zeallot_0.1.0      
##  [7] digest_0.6.20       slam_0.1-45         R6_2.4.0           
## [10] plyr_1.8.4          chron_2.3-54        backports_1.1.4    
## [13] evaluate_0.14       blogdown_0.16       pillar_1.4.2       
## [16] rlang_0.4.0         lazyeval_0.2.2      data.table_1.12.2  
## [19] gdata_2.18.0        rmarkdown_1.14      gender_0.5.2       
## [22] labeling_0.3        igraph_1.2.4.1      RCurl_1.95-4.12    
## [25] munsell_0.5.0       compiler_3.6.1      xfun_0.8           
## [28] pkgconfig_2.0.2     htmltools_0.3.6     reports_0.1.4      
## [31] tidyselect_0.2.5    tibble_2.1.3        gridExtra_2.3      
## [34] bookdown_0.12       codetools_0.2-16    XML_3.98-1.20      
## [37] crayon_1.3.4        withr_2.1.2         bitops_1.0-6       
## [40] openNLP_0.2-6       grid_3.6.1          gtable_0.3.0       
## [43] lifecycle_0.1.0     magrittr_1.5        scales_1.0.0       
## [46] xlsx_0.6.1          stringi_1.4.3       reshape2_1.4.3     
## [49] NLP_0.2-0           openNLPdata_1.5.3-4 xml2_1.2.0         
## [52] venneuler_1.1-0     ellipsis_0.2.0.1    vctrs_0.2.0        
## [55] wordcloud_2.6       tools_3.6.1         glue_1.3.1         
## [58] purrr_0.3.3         plotrix_3.7-6       parallel_3.6.1     
## [61] yaml_2.2.0          tm_0.7-6            colorspace_1.4-1   
## [64] rJava_0.9-11        knitr_1.23&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
