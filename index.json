[{"authors":["samrobson"],"categories":null,"content":" Biography Dr. Sam Robson is a Senior Research Fellow at the University of Portsmouth and is the Bioinformatics Lead at the Centre for Enzyme Innovation. He is a data scientist and computational biologist with an extensive publication history (including 6 Nature papers) and particular expertise in maintenance, processing and analysis of large whole genome sequencing data sets. Previous to his appointment, he worked as a Bioinformatician in the group of Prof. Tony Kouzarides at the Wellcome Trust/CRUK Gurdon Institute. The main research focus of the lab was to analyse the role of histone and RNA modifications, and in particular their role in diseases such as cancer. Prior to this, he held a Post-Doctoral Fellowship at the Wellcome Trust Sanger Institute in the lab of Dr. Matt Hurles. This work focused on the analysis of large scale copy-number variations in the human genome and their role in common diseases such as breast cancer and Crohn\u0026rsquo;s Disease. His background is in pure Mathematics having achieved his Bachelor\u0026rsquo;s degree at the University of Warwick, and he holds a Masters and PhD in Mathematical Biology and Biophysical Chemistry from the MOAC Doctoral Training Centre. He is a Professional Member of the International Society for Computational Biology and a Fellow of the Royal Statistical Society and holds CStat and CSci Professional qualifications.\nBioinformatics Group The Bioinformatics Group at the University of Portsmouth was formed in 2017 by Dr. Sam Robson. We collaborate across the faculty on research projects utilising powerful techniques such as high-throughput sequencing, which require extensive processing and rigorous statistical analyses. We also work to build bioinformatics tools for use by the wider research community. We work on a variety of different projects and data sets, in diverse fields such as environmental biology, marine biology, microbiology, clinical research projects and paleogenomics.\nBioinformatics Computing The University of Portsmouth hosts a Bioinformatics-specific compute cluster, with well-maintained pipelines for RNA-seq, ChIP-seq, CLIP-seq, BS-seq, Exome-seq, amplicon sequencing, and other sequencing data types used by researchers throughout the University. The cluster consists of 4 compute nodes and 1 head node. The compute nodes consist of Dell PowerEdge R630 Servers with Intel Xeon E5-2650 v4 Processors (12 cores, 2.2GHz), 128GB RAM and 2x 240GB flash (SSD) storage. This provides a total of 48 cores, or 96 threads (via Hyperthreading). The head node is a Dell PowerEdge R630 Server with 2x Intel Xeon E5-2650 v4 Processors (12 cores, 2.2GHz), 384GB RAM and 2x 240GB flash (SSD) storage. Local storage is provided by a Synology RS3617RPxs NAS Server with 120TB HDD storage, connected to the compute nodes via a 10GbE Network. We also use both Amazon Web Services (AWS) and Google Cloud Platform for cloud computing resources.\n","date":1560816000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1560816000,"objectID":"16a4c792321a36ee423a84226734c9da","permalink":"/authors/samrobson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/samrobson/","section":"authors","summary":"Biography Dr. Sam Robson is a Senior Research Fellow at the University of Portsmouth and is the Bioinformatics Lead at the Centre for Enzyme Innovation. He is a data scientist and computational biologist with an extensive publication history (including 6 Nature papers) and particular expertise in maintenance, processing and analysis of large whole genome sequencing data sets. Previous to his appointment, he worked as a Bioinformatician in the group of Prof.","tags":null,"title":"Sam Robson","type":"authors"},{"authors":null,"categories":["Blog","Data Science","Machine Learning","R"],"content":"  1 Introduction 2 TensorFlow 3 Keras 4 MNIST Database 5 Data Transformation 6 Sequential Models 7 Dense Layer 8 Activation Layer 9 Dropout Layer 10 Define Initial Model 11 Compile Model 12 Training the Model 13 Results 14 Conclusions 15 Session Info   1 Introduction Machine learning and artificial intelligence is a hot topic in the tech world, but the expression “machine learning” can describe anything from fitting a straight line through some data, to a machine able to think, learn and react to the world in highly sophisticated ways (e.g. self-driving cars if you want to be positive about AI, or SkyNet from Terminator if you want to be a naysayer). Whilst common machine learning techniques like support vector machines and k-Nearest Neighbour algorithms can be used to solve a huge number of problems, deep learning algorithms like neural networks are required to create these highly sophisticated models.\nIn this blog, I will explore the use of some commonly used tools for generating neural networks within the R programming language.\n 2 TensorFlow TensorFlow is one of the most powerful tools for deep learning, and in particular is widely used for training neural networks to classify various aspects of images. It is a freely distributed open-source library in python (but mainly written in C++) originally created by Google, but has become the cornerstone of many deep learning models currently out there\nA Tensor is a multi-dimensional array, and the TensorFlow libraries represent a highly efficient pipeline for the myriad linear algebra calculations required to generate new tensors through the layers of the network.\n 3 Keras The Keras API is a high-level user-friendly neural network API (application programming interface) designed for accessing deep neural networks. One of the benefits is that it is able to run on GPUs as well as CPUs, which have been shown to work better for training neural networks since they are able more efficient at running the huge number of simple calculations required for model training (for example convolutions of image data).\nKeras can be used an interface to TensorFlow for training deep multi-level networks for use in deep learning applications. Both are developed in python, but here I am going to use the RStudio interface to run a few simple deep learning models to trial the process ahead of a more in-depth application. R and python are somewhat at war in the data science community, with (in my opinion) R being better for more general data analysis and visualisation (for instance, whilst the python seaborn package produces beautiful images, the ggplot2 package is far more elaborate). However, with the Keras and TensorFlow packages (and the generally higher memory impact of using R), python is typically far more suited for deep learning applications.\nHowever, the ability to access the Keras API through RStudio, and the amazing power of using RStudio to develop workflows, will make this a perfect “one stop shop” for data science needs. Much of this work is developed from the RStudio Keras and TensorFlow tutorials.\nWe first load the reticulate package to pipe python commands through R:\nlibrary(\u0026quot;reticulate\u0026quot;) Then install and load the keras package. When we load it using the install_keras() function, we can define different backend engines and choose to use GPUs rather than CPUs, but for this example I will simply use the default TensorFlow backend on my laptop CPU:\ndevtools::install_github(\u0026quot;rstudio/keras\u0026quot;) library(\u0026quot;keras\u0026quot;)  4 MNIST Database So let’s have a little play by looking at a standard machine learning approach, looking at the MNIST dataset. This is the Modified National Institute of Standards and Technology database, and contains a large amount of images of handwritten digits that is used to train models for handwriting recognition. Ultimately, the same models can be used for a huge number of classification tasks.\nmnist_dat \u0026lt;- dataset_mnist() The dataset contains a training set of 60,000 images, and a test set of 10,000 images. Each image is pre-normalised such that each digit is a grayscale image that fits into a 28x28 pixel bounding box.Each image is also supplied with a label that tells us what the digit should really be. This dataset is commonly used as a kind of benchmark for new models, with people vying to build the model with the lowest error rate possible:\nSo let’s define our data sets. We will require two main data sets; a training set where we show the model the images and tell it what it should recognise, and a test dataset where we can predict the result and check against the ground level label. For each data set, we will create a dataset x containing all of the images, and a dataset y containing the labels:\ntraining_dat_x_raw \u0026lt;- mnist_dat[[\u0026quot;train\u0026quot;]][[\u0026quot;x\u0026quot;]] training_dat_y_raw \u0026lt;- mnist_dat[[\u0026quot;train\u0026quot;]][[\u0026quot;y\u0026quot;]] test_dat_x_raw \u0026lt;- mnist_dat[[\u0026quot;test\u0026quot;]][[\u0026quot;x\u0026quot;]] test_dat_y_raw \u0026lt;- mnist_dat[[\u0026quot;test\u0026quot;]][[\u0026quot;y\u0026quot;]] Each of the images is essentially a 2D array, with 28 rows and 28 columns, with each cell representing the greyscale value of the pixel. So the _dat_x data sets are 3D arrays. So accessing specific elements from these arrays in R is similar to accessing rows and columns using the [x,y] style axis, but we need to specify a third element z for the specific array that we want to access – [z,x,y]. So lets take a look at an exampl of the input data:\npar(mfcol = c(3,6)) par(mar = c(0, 0, 3, 0), xaxs = \u0026#39;i\u0026#39;, yaxs = \u0026#39;i\u0026#39;) for (i in 1:18) { plot_dat \u0026lt;- training_dat_x_raw[i, , ] plot_dat \u0026lt;- t(apply(plot_dat, MAR = 2, rev)) image(1:28, 1:28, plot_dat, col = gray((0:255)/255), xaxt =\u0026#39;n\u0026#39;, main = training_dat_y_raw[i], cex = 4, axes = FALSE) }  5 Data Transformation We can easily reduce this 3D data by essentially taking each 28x28 matrix and collapsing the 784 values down into a 1D vector. Then we can make one big 2D matrix containing all of the data. Ordinarily, this could be done by reassigning the dimensions of the array, but by using the array_reshape() function, the data is adjusted to meet the requirements for Keras:\ntraining_dat_x \u0026lt;- array_reshape(training_dat_x_raw, c(nrow(training_dat_x_raw), 784)) test_dat_x \u0026lt;- array_reshape(test_dat_x_raw, c(nrow(test_dat_x_raw), 784)) dim(training_dat_x) ## [1] 60000 784 The values in these arrays are greyscale values, representing 256 integer values between 0 (black) and 255 (white). It will be useful for downstream analyses to rescale these values to real values in the range \\([0,1]\\):\ntraining_dat_x \u0026lt;- training_dat_x/255 test_dat_x \u0026lt;- test_dat_x/255 The R-specific way to deal with categorical data would be to encode the values in the y datasets to a factor with 10 levels (“0”, “1”, “2”, etc). However, Keras requires the data to be in a slightly different format, so we use the to_categorical() function instead. This will encode the value in a new matrix with 10 columns and n rows, such that every row contains exactly one 1 (representing the label) and nine 0s. This is known as an identity matrix. Keras uses a lot of linear algebra, and this encoding makes these calculations much simpler:\ntraining_dat_y \u0026lt;- to_categorical(training_dat_y_raw, 10) test_dat_y \u0026lt;- to_categorical(test_dat_y_raw, 10) dim(training_dat_y) ## [1] 60000 10  6 Sequential Models A standard deep learning neural network model can be thought of as a number of sequential layers, with each layer representing a different abstraction of the data. For instance, consider a model looking at facial recognition from image data. The first layer might represent edges of different aspects of the image. The next layer might be designed to pick out nose shape. The next might pick out hair. The next might determine the orientation of the face. etc. Then by adding more and more layers, we can develop models able to classify samples based on a wide range of different features.\nOf course, there is a danger in statistics of over-fitting data, which is when we create a model so specific for the training data that it becomes practically worthless. By definition, adding more variables into a model will always improve the fit, but at the cost of its applicability to other data sets. In models such as linear models, we look for parsimony – a model should be as complicated as it needs to be and no more complicated. The old phrase is:\n When you hear hooves, think horse not zebra\n However, deep learning sequential models such as these are robust to these problems, since model training can back-propagate, allowing us to incorporate far more levels than would be possible with other machine learning techniques.\nThe general steps involved in using Keras for deep learning are to first build your model, compile it to configure the parameters that will be used to develop the “best” model, train it using your training data, then test it on an additional data set to see how it copes.\nSo let’s build a simple sequential neural network model object using keras_model_sequential(), then add a series of additional layers that we hope will accurately identify our different categories. Adding sequential layers uses similar syntax to the tidyverse libraries such as dplyr, by using the pipe operator %\u0026gt;%:\nmodel \u0026lt;- keras_model_sequential() model %\u0026gt;% layer_dense(units = 28, input_shape = c(784)) %\u0026gt;% layer_activation(activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(rate = 0.4)  7 Dense Layer The first layer is a densely connected neural network layer, which takes a set of nD input tensors (in this case 1D input tensors), and generate a weights matrix by breaking the tensor into subsets and using this to learn the weights by doing some linear algebra (vector and matrix multiplication). The output from the dense layer is then generated as follows:\n\\[output = activation(dot(input, kernel) + bias)\\]\nSo the weights kernel is generated and multiplied (dot product) with the input. If requested, a bias is also calculated and added to account for any systematic bias identified in the data. An activation function is then used to generate the final tensor to go on to the following layer (in this case we have specified this is a separate layer).\n 8 Activation Layer An activation function can often be necessary to ensure the back-propogation and gradient descent algorithms work. By default, no activation is used. However, this is a linear identity function, which is very limited. A common activation function is the Rectified Linear Unit (ReLU), which is linear for positive values, but zero for negative values. This is usually a good starting point as it is very simple and fast. Another option is the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function, which transforms each input logit (the pre-activated values) by taking the exponential and normalizing by the sum of exponentials over all inputs so that the sum is exactly 1:\n\\[\\sigma(y_i) = \\frac{e^{y_i}}{\\sum^{K}_{j=1}e^{y_j}}\\]\nIt is commonly used for multinomial logistic regression, where a different softmax function is applied for each class with a different probability incorporated, since it is able to transform input numbers into probabilities. The use of exponentials ensures that there are no negative values, no matter how negative the input logit. So the softmax function outputs a probability distribution for potential outcomes in the range \\([0,1]\\).\n 9 Dropout Layer Finally, we specify a dropout layer, which essentially drops a proportion of the nodes in the neural network to prevent over-fitting. In this case we have connections in the network between all of the tensor subsets generated. However, many of them are more useful in the model than others, so here we deactivate the 40% least useful nodes. Of course, this will reduce the training performance, but will prevent the issue of over-fitting making the model more generalised and applicable to other data sets. Model fitting is all about tweaking parameters and layers to get the most effective model, and this is one way in which we can improve the effectiveness of the model at predicting unseen data.\n 10 Define Initial Model So let’s generate a more thorough model of 4 dense layers, gradually filtering down to a final output of 10 probabilities using the softmax activation function – the probabilities for the 10 digits:\nMNIST_model \u0026lt;- keras_model_sequential() MNIST_model %\u0026gt;% layer_dense(units = 256, input_shape = c(784)) %\u0026gt;% layer_activation(activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(rate = 0.4) %\u0026gt;% layer_dense(units = 128) %\u0026gt;% layer_activation(activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(rate = 0.3) %\u0026gt;% layer_dense(units = 56) %\u0026gt;% layer_activation(activation = \u0026quot;relu\u0026quot;) %\u0026gt;% layer_dropout(rate = 0.2) %\u0026gt;% layer_dense(units = 10) %\u0026gt;% layer_activation(activation = \u0026quot;softmax\u0026quot;) summary(MNIST_model) ## Model: \u0026quot;sequential_1\u0026quot; ## ___________________________________________________________________________ ## Layer (type) Output Shape Param # ## =========================================================================== ## dense_1 (Dense) (None, 256) 200960 ## ___________________________________________________________________________ ## activation_1 (Activation) (None, 256) 0 ## ___________________________________________________________________________ ## dropout_1 (Dropout) (None, 256) 0 ## ___________________________________________________________________________ ## dense_2 (Dense) (None, 128) 32896 ## ___________________________________________________________________________ ## activation_2 (Activation) (None, 128) 0 ## ___________________________________________________________________________ ## dropout_2 (Dropout) (None, 128) 0 ## ___________________________________________________________________________ ## dense_3 (Dense) (None, 56) 7224 ## ___________________________________________________________________________ ## activation_3 (Activation) (None, 56) 0 ## ___________________________________________________________________________ ## dropout_3 (Dropout) (None, 56) 0 ## ___________________________________________________________________________ ## dense_4 (Dense) (None, 10) 570 ## ___________________________________________________________________________ ## activation_4 (Activation) (None, 10) 0 ## =========================================================================== ## Total params: 241,650 ## Trainable params: 241,650 ## Non-trainable params: 0 ## ___________________________________________________________________________ Here we can see the change in shape of the tensors throughout the model, and the number of trainable parameters at each layer level. These are fully connected layers, so every neuron (values in the tensors) is connected to every other neuron. So the number of parameters (or connections) is given by multiplying the number of values in the input layer by the number in the previous layer plus one. So in total we have nearly a quarter of a million parameters to estimate here.\n 11 Compile Model So next we can compile this model to tell it which methods we want to use to estimate these parameters:\nMNIST_model %\u0026gt;% compile( loss = \u0026quot;categorical_crossentropy\u0026quot;, optimizer = optimizer_rmsprop(), metrics = c(\u0026quot;accuracy\u0026quot;) ) The loss function here is the method that will be used to optimise the parameters by comparing the predicted value with that of the actual value. Categorical crossentropy is commonly used in classification models when the output is a probability. It increases logarithmically as the predicted value diverges from the true value.\nThe optimizer is used to ensure that the algorithm converges in training. We are trying to minimise the loss function, so Gradient Descent can be used to optimse by iteratively recalculating the weights and bias until the minima is found. There is a danger of getting stuck at a local minima value, so sometimes it may be necessary to tune the parameters to avoid this. In this case, we are using RMSProp optimizer, which is similar to Gradient Descent but attempts to avoid this by adding oscillations to the descent.\nFinally, we can specify which metrics we wish to output inorder to evaluate the model during training. Here we look at the accuracy to determine how often our model gives the correct prediction in the trained data.\n 12 Training the Model So now let’s train our model with our training data to estimate the parameters:\ntraining_output \u0026lt;- MNIST_model %\u0026gt;% fit( training_dat_x, training_dat_y, batch_size = 128, epochs = 30, verbose = 1, validation_split = 0.2 ) Here we are going to run the model using our reformated training data above. The epoch argument determines the number of iterations used to optimize the model parameters. In each epoch, we will use batch_size samples per epoch for the gradient update.\nThe validation_split argument is used for running cross-validation in order to evaluate the quality of the model. A portion of the training data is kept aside, and is used to validate the current model parameters and calculate the accuracy.\n 13 Results Let’s take a look at how the accuracy and the loss (caluclated as categorical cross-entropy) change as the model training progresses:\nplot(training_output) We can see that the loss is high and the accuracy low at the start of the training, but they quickly improve within the first 10 epochs. After this, they begin to plateau, resulting in a loss of 0.07 and accuracy of 98.39%.\nThis is pretty good, so let’s see how it works with the test set:\ntest_output \u0026lt;- MNIST_model %\u0026gt;% evaluate(test_dat_x, test_dat_y) test_output ## $loss ## [1] 0.1082727 ## ## $acc ## [1] 0.9817 So 98.17% of the 10,000 test cases were predicted accurately. So this means that 183 were wrong. Let’s take a look at some of these:\npredicted \u0026lt;- MNIST_model %\u0026gt;% predict_classes(test_dat_x) which_wrong \u0026lt;- which(predicted != test_dat_y_raw) par(mfcol = c(3,6)) par(mar = c(0, 0, 3, 0), xaxs = \u0026#39;i\u0026#39;, yaxs = \u0026#39;i\u0026#39;) for (i in which_wrong[1:18]) { plot_dat \u0026lt;- test_dat_x_raw[i, , ] plot_dat \u0026lt;- t(apply(plot_dat, MAR = 2, rev)) image(1:28, 1:28, plot_dat, col = gray((0:255)/255), xaxt =\u0026#39;n\u0026#39;, main = paste(\u0026quot;Predict =\u0026quot;, predicted[i], \u0026quot;\\nReal =\u0026quot;, test_dat_y_raw[i]), cex = 2, axes = FALSE) } So we can see why the algorithm struggled with some of these, such as predicting 6s as 0s, and numbers that are slanted or squished. However, this obviously shows a lack of generalisation in the model, which is not brilliant for dealing with hand written numbers.\nObviously this is a fairly basic example of a neural network model, and the sorts of models being used in technology like self-driving cars contain far more layers than this. Model tuning is essential to compare and contrast models to identify the optimum model.\n% -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % -- % --  14 Conclusions According to Wikipedia, one of the best results for the MNIST database used a hierarchical system of convolutional neural networks and managed to get an error rate of 0.23%. Here I have an error rate of 1.83%, so I clearly have a way to go! Often in classification algorithms, using standard machine learning algorithms will get you pretty far with pretty good error rates. However, to tune the models further to get error rates down to these sorts of levels, more complex models are required. Neural networks can be used to push the error rates down further. Getting the right answer 96% of the time is pretty good, but if you’re relying on that classification to tell whether there is a pedestrian stood in front of a self-driving car, it is incredibly important to ensure that this error rate is as close to 0 as possible.\nHowever, this has been a very useful attempt at incorparting the powerful interface of Keras and the workflow of TensorFlow in R. Being able to incorporate powerful deep learning networks in R is incredobly useful, and will allow for incorporation with pre-existing pipelines already developed for bioinformatics analyses utilising the powerful pacakges available from Bioconductor.\nDeep learning algorithms currently have a huge number of applications, from self-driving cars to facial recognition, and are being incorporated into technology in many industries. Development of deep learning algorithms and Big Data processing approaches will provide significant technological advancements. I am currently working on some potentially interesting applications, and hope to further my expertise in this area by working more with the R Keras API interface.\n 15 Session Info sessionInfo() ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Sierra 10.12.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] keras_2.2.5.0 reticulate_1.13 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.8 remotes_2.1.0 ## [4] reshape2_1.4.3 purrr_0.3.3 lattice_0.20-38 ## [7] colorspace_1.4-1 generics_0.0.2 testthat_2.1.1 ## [10] htmltools_0.3.6 usethis_1.5.1 yaml_2.2.0 ## [13] base64enc_0.1-3 rlang_0.4.0 pkgbuild_1.0.3 ## [16] pillar_1.4.2 glue_1.3.1 withr_2.1.2 ## [19] sessioninfo_1.1.1 plyr_1.8.4 tensorflow_2.0.0 ## [22] stringr_1.4.0 munsell_0.5.0 blogdown_0.16 ## [25] gtable_0.3.0 devtools_2.1.0 codetools_0.2-16 ## [28] memoise_1.1.0 evaluate_0.14 labeling_0.3 ## [31] knitr_1.23 callr_3.3.0 ps_1.3.0 ## [34] tfruns_1.4 curl_3.3 Rcpp_1.0.2 ## [37] backports_1.1.4 scales_1.0.0 desc_1.2.0 ## [40] pkgload_1.0.2 jsonlite_1.6 fs_1.3.1 ## [43] ggplot2_3.2.0 digest_0.6.20 stringi_1.4.3 ## [46] dplyr_0.8.3 bookdown_0.12 processx_3.4.1 ## [49] grid_3.6.1 rprojroot_1.3-2 cli_1.1.0 ## [52] tools_3.6.1 magrittr_1.5 tibble_2.1.3 ## [55] lazyeval_0.2.2 pkgconfig_2.0.2 crayon_1.3.4 ## [58] whisker_0.4 zeallot_0.1.0 Matrix_1.2-17 ## [61] prettyunits_1.0.2 assertthat_0.2.1 rmarkdown_1.14 ## [64] R6_2.4.0 compiler_3.6.1  ","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"3f61734dd90e3d6faa41fee2241efd9b","permalink":"/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/post/2019-10-05-deep-learning-using-tensorflow-through-the-keras-api-in-rstudio/","section":"post","summary":"Use of TensorFlow through the Keras API in RStudio to explore deep learning model training","tags":["R","Data Science","Machine Learning","TensorFlow","Keras","MNIST","prediction","classification"],"title":"Deep Learning using TensorFlow Through the Keras API in RStudio","type":"post"},{"authors":null,"categories":["Blog","Data Science","Machine Learning","R"],"content":"  1 Introduction 2 Download data 3 Data Cleaning 3.1 Missing Data 3.2 Numeric Range 3.3 Change Variable Class  4 Exploratory Analyses 4.1 Attack 4.2 Defence 4.3 Other 4.4 Ensuring we use accurate data classes throughout  5 Normalization 5.1 Min-Max Normalization 5.2 Z-Score Normalization 5.3 Choosing a Normalization Method  6 Looking for Patterns 6.1 Correlation of Variables 6.2 Height vs Weight 6.3 Correlation between Pokémon 6.4 Principal Component Analysis 6.5 Correlation-Based Recommendation  7 Predicting Legendary Pokémon 7.1 Support-Vector Machine 7.2 k-Nearest Neighbour 7.3 Logistic Regression 7.4 Random Forest  8 Conclusion 9 Session Info   1 Introduction When I saw the Complete Pokémon Dataset on Kaggle, I just had to download it and have a look! When I was younger, I was a big fan of Pokémon and used to play it regularly and watch the TV show (to this day I can recite much of the original Pokémon Rap). More recently, my daughter has become a fan, and watches the show incessently (although it beats watching Peppa Pig…). So I am going to have a look over these data and see what they can show us about these pocket monsters.\nThis is a fairly comprehensive analysis of the data, and will include introductions to a number of different data science techniques. I may further develop these into posts of their own in the future, so will only skim over most of them here. I hope that this post shows a fairly complete example of the types of analyses that it is possible to do with data such as these for prediction and recommendation.\n 2 Download data The data about each of the Pokémon can be downloaded directly from Kaggle here, and I have also downloaded some images for each of the Pokémon (at least for Generations 1 to 6) from Kaggle here.\nThe main data are in the form of a compressed comma separated values (CSV) file. After unzipping the file, we are left with a plain text file where every row is a separate entry, and the various columns of the data set are separated by commas. So let’s load the data in using the read.csv function, which will generate a data.frame object, and take a look at it:\npokedat \u0026lt;- read.csv(\u0026quot;pokemon.csv\u0026quot;) rownames(pokedat) \u0026lt;- pokedat[[\u0026quot;name\u0026quot;]] dim(pokedat) ## [1] 801 41 So we have information for 41 variables for 801 unique Pokémon. Each Pokémon has a unique number assigned to it in the so called “Pokédex”, which is common between this data set and the list of Pokémon images. This makes it easy to link the two. Back in my day, there were only 151 Pokémon to keep track of, but many years later they have added more and more with each “Generation”. Pokémon Generation 8 is the most recent and has only recently been released, so we can see which generations are present in this data set by using the table function, which will show us the number of entries with each value:\ntable(pokedat[[\u0026quot;generation\u0026quot;]]) ## ## 1 2 3 4 5 6 7 ## 151 100 135 107 156 72 80 So we can see the 151 Generation 1 Pokémon, but also additional Pokémon from up to Generation 7. We can get a fairly broad overview of the data set by using the str function to gain an overview of what is contained within each of the columns of this data.frame:\nstr(pokedat) ## \u0026#39;data.frame\u0026#39;: 801 obs. of 41 variables: ## $ abilities : Factor w/ 482 levels \u0026quot;[\u0026#39;Adaptability\u0026#39;, \u0026#39;Download\u0026#39;, \u0026#39;Analytic\u0026#39;]\u0026quot;,..: 244 244 244 22 22 22 453 453 453 348 ... ## $ against_bug : num 1 1 1 0.5 0.5 0.25 1 1 1 1 ... ## $ against_dark : num 1 1 1 1 1 1 1 1 1 1 ... ## $ against_dragon : num 1 1 1 1 1 1 1 1 1 1 ... ## $ against_electric : num 0.5 0.5 0.5 1 1 2 2 2 2 1 ... ## $ against_fairy : num 0.5 0.5 0.5 0.5 0.5 0.5 1 1 1 1 ... ## $ against_fight : num 0.5 0.5 0.5 1 1 0.5 1 1 1 0.5 ... ## $ against_fire : num 2 2 2 0.5 0.5 0.5 0.5 0.5 0.5 2 ... ## $ against_flying : num 2 2 2 1 1 1 1 1 1 2 ... ## $ against_ghost : num 1 1 1 1 1 1 1 1 1 1 ... ## $ against_grass : num 0.25 0.25 0.25 0.5 0.5 0.25 2 2 2 0.5 ... ## $ against_ground : num 1 1 1 2 2 0 1 1 1 0.5 ... ## $ against_ice : num 2 2 2 0.5 0.5 1 0.5 0.5 0.5 1 ... ## $ against_normal : num 1 1 1 1 1 1 1 1 1 1 ... ## $ against_poison : num 1 1 1 1 1 1 1 1 1 1 ... ## $ against_psychic : num 2 2 2 1 1 1 1 1 1 1 ... ## $ against_rock : num 1 1 1 2 2 4 1 1 1 2 ... ## $ against_steel : num 1 1 1 0.5 0.5 0.5 0.5 0.5 0.5 1 ... ## $ against_water : num 0.5 0.5 0.5 2 2 2 0.5 0.5 0.5 1 ... ## $ attack : int 49 62 100 52 64 104 48 63 103 30 ... ## $ base_egg_steps : int 5120 5120 5120 5120 5120 5120 5120 5120 5120 3840 ... ## $ base_happiness : int 70 70 70 70 70 70 70 70 70 70 ... ## $ base_total : int 318 405 625 309 405 634 314 405 630 195 ... ## $ capture_rate : Factor w/ 34 levels \u0026quot;100\u0026quot;,\u0026quot;120\u0026quot;,\u0026quot;125\u0026quot;,..: 26 26 26 26 26 26 26 26 26 21 ... ## $ classfication : Factor w/ 588 levels \u0026quot;Abundance Pokémon\u0026quot;,..: 449 449 449 299 187 187 531 546 457 585 ... ## $ defense : int 49 63 123 43 58 78 65 80 120 35 ... ## $ experience_growth: int 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1059860 1000000 ... ## $ height_m : num 0.7 1 2 0.6 1.1 1.7 0.5 1 1.6 0.3 ... ## $ hp : int 45 60 80 39 58 78 44 59 79 45 ... ## $ japanese_name : Factor w/ 801 levels \u0026quot;Abagouraアバゴーラ\u0026quot;,..: 200 201 199 288 417 416 794 334 336 80 ... ## $ name : Factor w/ 801 levels \u0026quot;Abomasnow\u0026quot;,\u0026quot;Abra\u0026quot;,..: 73 321 745 95 96 93 656 764 56 88 ... ## $ percentage_male : num 88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 88.1 50 ... ## $ pokedex_number : int 1 2 3 4 5 6 7 8 9 10 ... ## $ sp_attack : int 65 80 122 60 80 159 50 65 135 20 ... ## $ sp_defense : int 65 80 120 50 65 115 64 80 115 20 ... ## $ speed : int 45 60 80 65 80 100 43 58 78 45 ... ## $ type1 : Factor w/ 18 levels \u0026quot;bug\u0026quot;,\u0026quot;dark\u0026quot;,\u0026quot;dragon\u0026quot;,..: 10 10 10 7 7 7 18 18 18 1 ... ## $ type2 : Factor w/ 19 levels \u0026quot;\u0026quot;,\u0026quot;bug\u0026quot;,\u0026quot;dark\u0026quot;,..: 15 15 15 1 1 9 1 1 1 1 ... ## $ weight_kg : num 6.9 13 100 8.5 19 90.5 9 22.5 85.5 2.9 ... ## $ generation : int 1 1 1 1 1 1 1 1 1 1 ... ## $ is_legendary : int 0 0 0 0 0 0 0 0 0 0 ... So we gave a lot of different information contained in this data set, with the vast majority being represented by numerical values. As well as the name by which we likely know them, we have the original Japanese name, as well as their fighting statistics such as hp (Health Points), speed, attack and defense. We also get their specific abilities, which are given in a listed format within square brackets (e.g. three abilities – ['Adaptability', 'Download', 'Analytic'] – for Abomasnow). There are also various other things that we will explore now in the following sections. So let’s explore these data to see how they look, and to check for any inconsistencies that need to be corrected.\n 3 Data Cleaning The first step in any data analysis is to check the consistency of the data to ensure that there are no missing values (and if there are, decide the best thing to do with them), to make sure that the data are consistent and fit within expected bounds, and to generally make sure that these data make sense. These are data that somebody else has generated, so it is best not to assume that they are perfect. If there are any issues, this will propogate to our downstream analyses.\n3.1 Missing Data First of all, let’s take a look to see which of the entries for each of the columns is missing (i.e. is coded as NA):\nhas_na \u0026lt;- apply(pokedat, MAR = 2, FUN = function(x) which(is.na(x))) has_na[sapply(has_na, length) \u0026gt; 0] ## $height_m ## Rattata Raticate Raichu Sandshrew Sandslash Vulpix Ninetales ## 19 20 26 27 28 37 38 ## Diglett Dugtrio Meowth Persian Geodude Graveler Golem ## 50 51 52 53 74 75 76 ## Grimer Muk Exeggutor Marowak Hoopa Lycanroc ## 88 89 103 105 720 745 ## ## $percentage_male ## Magnemite Magneton Voltorb Electrode Staryu Starmie ## 81 82 100 101 120 121 ## Ditto Porygon Articuno Zapdos Moltres Mewtwo ## 132 137 144 145 146 150 ## Mew Unown Porygon2 Raikou Entei Suicune ## 151 201 233 243 244 245 ## Lugia Ho-Oh Celebi Shedinja Lunatone Solrock ## 249 250 251 292 337 338 ## Baltoy Claydol Beldum Metang Metagross Regirock ## 343 344 374 375 376 377 ## Regice Registeel Kyogre Groudon Rayquaza Jirachi ## 378 379 382 383 384 385 ## Deoxys Bronzor Bronzong Magnezone Porygon-Z Rotom ## 386 436 437 462 474 479 ## Uxie Mesprit Azelf Dialga Palkia Regigigas ## 480 481 482 483 484 486 ## Giratina Phione Manaphy Darkrai Shaymin Arceus ## 487 489 490 491 492 493 ## Victini Klink Klang Klinklang Cryogonal Golett ## 494 599 600 601 615 622 ## Golurk Cobalion Terrakion Virizion Reshiram Zekrom ## 623 638 639 640 643 644 ## Kyurem Keldeo Meloetta Genesect Carbink Xerneas ## 646 647 648 649 703 716 ## Yveltal Zygarde Diancie Hoopa Volcanion Type: Null ## 717 718 719 720 721 772 ## Silvally Minior Dhelmise Tapu Koko Tapu Lele Tapu Bulu ## 773 774 781 785 786 787 ## Tapu Fini Cosmog Cosmoem Solgaleo Lunala Nihilego ## 788 789 790 791 792 793 ## Buzzwole Pheromosa Xurkitree Celesteela Kartana Guzzlord ## 794 795 796 797 798 799 ## Necrozma Magearna ## 800 801 ## ## $weight_kg ## Rattata Raticate Raichu Sandshrew Sandslash Vulpix Ninetales ## 19 20 26 27 28 37 38 ## Diglett Dugtrio Meowth Persian Geodude Graveler Golem ## 50 51 52 53 74 75 76 ## Grimer Muk Exeggutor Marowak Hoopa Lycanroc ## 88 89 103 105 720 745 In general, this seems to be a fairly complete data set, with only three of the variables showing any NA data. We can see that there are 20 Pokémon with no height nor weight data:\nsubset(pokedat, is.na(height_m) | is.na(weight_kg))[, c(\u0026quot;name\u0026quot;, \u0026quot;height_m\u0026quot;, \u0026quot;weight_kg\u0026quot;)] ## name height_m weight_kg ## Rattata Rattata NA NA ## Raticate Raticate NA NA ## Raichu Raichu NA NA ## Sandshrew Sandshrew NA NA ## Sandslash Sandslash NA NA ## Vulpix Vulpix NA NA ## Ninetales Ninetales NA NA ## Diglett Diglett NA NA ## Dugtrio Dugtrio NA NA ## Meowth Meowth NA NA ## Persian Persian NA NA ## Geodude Geodude NA NA ## Graveler Graveler NA NA ## Golem Golem NA NA ## Grimer Grimer NA NA ## Muk Muk NA NA ## Exeggutor Exeggutor NA NA ## Marowak Marowak NA NA ## Hoopa Hoopa NA NA ## Lycanroc Lycanroc NA NA Many of these are Pokémon that I know from Generation 1, and in fact seem to be sets of evolutions. For instance, Rattata evolves into Raticate:\nSandshrew evolves into Sandslash:\nAnd Vulpix evolves into Ninetales:\nThere are also a couple of other none-Generation 1 Pokémon, including Lycanroc which is one of my daughter’s favourited from Pokémon Sun and Moon:\nLycanroc\n However, there is no obvious reason why values are missing. There are methods that can be used to account for missing data. One possible approach is to impute the data – that is, we use the rest of the data to give us a rough idea of what we should see for these missing values. An example of this is to simply use the mean of the non-missing values for the missing variable. However, in this case, we can actually find these missing values by visiting an online Pokedex, so let’s correct these, ensuring that we match the units for weight (kg) and height (m):\nmissing_height \u0026lt;- list(Rattata = c(height_m = 0.3, weight_kg = 3.5), Raticate = c(height_m = 0.7, weight_kg = 18.5), Raichu = c(height_m = 0.8, weight_kg = 30.0), Sandshrew = c(height_m = 0.6, weight_kg = 12.0), Sandslash = c(height_m = 1.0, weight_kg = 29.5), Vulpix = c(height_m = 0.6, weight_kg = 9.9), Ninetales = c(height_m = 1.1, weight_kg = 19.9), Diglett = c(height_m = 0.2, weight_kg = 0.8), Dugtrio = c(height_m = 0.7, weight_kg = 33.3), Meowth = c(height_m = 0.4, weight_kg = 4.2), Persian = c(height_m = 1.0, weight_kg = 32.0), Geodude = c(height_m = 0.4, weight_kg = 20.0), Graveler = c(height_m = 0.3, weight_kg = 105.0), Golem = c(height_m = 1.4, weight_kg = 300.0), Grimer = c(height_m = 0.9, weight_kg = 30.0), Muk = c(height_m = 1.2, weight_kg = 30.0), Exeggutor = c(height_m = 2.0, weight_kg = 120.0), Marowak = c(height_m = 1.0, weight_kg = 45.0), Hoopa = c(height_m = 0.5, weight_kg = 9.0), Lycanroc = c(height_m = 0.8, weight_kg = 25.0)) missing_height \u0026lt;- t(rbind.data.frame(missing_height)) pokedat[match(rownames(missing_height), pokedat[[\u0026quot;name\u0026quot;]]), c(\u0026quot;height_m\u0026quot;, \u0026quot;weight_kg\u0026quot;)] \u0026lt;- missing_height There are also 98 Pokémon with a missing percentage_male value. This value gives the proportion of the Pokémon out in the world that you might come across in the game that are male as a percentage. These seem to be spread throughout the entire list of Pokémon across all Generations, with no clear reason as to why they have missing values:\nhead(subset(pokedat, is.na(percentage_male))) ## abilities against_bug ## Magnemite [\u0026#39;Magnet Pull\u0026#39;, \u0026#39;Sturdy\u0026#39;, \u0026#39;Analytic\u0026#39;] 0.5 ## Magneton [\u0026#39;Magnet Pull\u0026#39;, \u0026#39;Sturdy\u0026#39;, \u0026#39;Analytic\u0026#39;] 0.5 ## Voltorb [\u0026#39;Soundproof\u0026#39;, \u0026#39;Static\u0026#39;, \u0026#39;Aftermath\u0026#39;] 1.0 ## Electrode [\u0026#39;Soundproof\u0026#39;, \u0026#39;Static\u0026#39;, \u0026#39;Aftermath\u0026#39;] 1.0 ## Staryu [\u0026#39;Illuminate\u0026#39;, \u0026#39;Natural Cure\u0026#39;, \u0026#39;Analytic\u0026#39;] 1.0 ## Starmie [\u0026#39;Illuminate\u0026#39;, \u0026#39;Natural Cure\u0026#39;, \u0026#39;Analytic\u0026#39;] 2.0 ## against_dark against_dragon against_electric against_fairy ## Magnemite 1 0.5 0.5 0.5 ## Magneton 1 0.5 0.5 0.5 ## Voltorb 1 1.0 0.5 1.0 ## Electrode 1 1.0 0.5 1.0 ## Staryu 1 1.0 2.0 1.0 ## Starmie 2 1.0 2.0 1.0 ## against_fight against_fire against_flying against_ghost ## Magnemite 2.0 2.0 0.25 1 ## Magneton 2.0 2.0 0.25 1 ## Voltorb 1.0 1.0 0.50 1 ## Electrode 1.0 1.0 0.50 1 ## Staryu 1.0 0.5 1.00 1 ## Starmie 0.5 0.5 1.00 2 ## against_grass against_ground against_ice against_normal ## Magnemite 0.5 4 0.5 0.5 ## Magneton 0.5 4 0.5 0.5 ## Voltorb 1.0 2 1.0 1.0 ## Electrode 1.0 2 1.0 1.0 ## Staryu 2.0 1 0.5 1.0 ## Starmie 2.0 1 0.5 1.0 ## against_poison against_psychic against_rock against_steel ## Magnemite 0 0.5 0.5 0.25 ## Magneton 0 0.5 0.5 0.25 ## Voltorb 1 1.0 1.0 0.50 ## Electrode 1 1.0 1.0 0.50 ## Staryu 1 1.0 1.0 0.50 ## Starmie 1 0.5 1.0 0.50 ## against_water attack base_egg_steps base_happiness base_total ## Magnemite 1.0 35 5120 70 325 ## Magneton 1.0 60 5120 70 465 ## Voltorb 1.0 30 5120 70 330 ## Electrode 1.0 50 5120 70 490 ## Staryu 0.5 45 5120 70 340 ## Starmie 0.5 75 5120 70 520 ## capture_rate classfication defense experience_growth ## Magnemite 190 Magnet Pokémon 70 1000000 ## Magneton 60 Magnet Pokémon 95 1000000 ## Voltorb 190 Ball Pokémon 50 1000000 ## Electrode 60 Ball Pokémon 70 1000000 ## Staryu 225 Starshape Pokémon 55 1250000 ## Starmie 60 Mysterious Pokémon 85 1250000 ## height_m hp japanese_name name percentage_male ## Magnemite 0.3 25 Coilコイル Magnemite NA ## Magneton 1.0 50 Rarecoilレアコイル Magneton NA ## Voltorb 0.5 40 Biriridamaビリリダマ Voltorb NA ## Electrode 1.2 60 Marumineマルマイン Electrode NA ## Staryu 0.8 30 Hitodemanヒトデマン Staryu NA ## Starmie 1.1 60 Starmieスターミー Starmie NA ## pokedex_number sp_attack sp_defense speed type1 type2 ## Magnemite 81 95 55 45 electric steel ## Magneton 82 120 70 70 electric steel ## Voltorb 100 55 55 100 electric ## Electrode 101 80 80 150 electric ## Staryu 120 70 55 85 water ## Starmie 121 100 85 115 water psychic ## weight_kg generation is_legendary ## Magnemite 6.0 1 0 ## Magneton 60.0 1 0 ## Voltorb 10.4 1 0 ## Electrode 66.6 1 0 ## Staryu 34.5 1 0 ## Starmie 80.0 1 0 However, by looking at a few of these in the Pokedex, it would appear that these are generally genderless Pokémon, which would explain the missing values. A sensible value to use in these cases would therefore be 0.5, representing an equal spit of male and female:\npokedat[is.na(pokedat[[\u0026quot;percentage_male\u0026quot;]]), \u0026quot;percentage_male\u0026quot;] \u0026lt;- 0.5 It is also worth noting that there are also missing values for the type2 Pokémon. These were not picked up as NA values, because they are encoded as a blank entry \u0026quot;“. We can convert this to a more descriptive factor such as”none\u0026quot;:\nlevels(pokedat[[\u0026quot;type2\u0026quot;]])[levels(pokedat[[\u0026quot;type2\u0026quot;]]) == \u0026quot;none\u0026quot;] \u0026lt;- \u0026quot;none\u0026quot;  3.2 Numeric Range The vast majority of these variables are numeric in nature, so it is worth checking the range of these values to ensure that they are within typical ranges that we might expect. For instance, we would not expect negative values, zero values, or values greater than some sensible limit for things like height, weight, etc. So let’s take an overall look at these data ranges by using the summary() function:\nsummary(pokedat) ## abilities against_bug ## [\u0026#39;Levitate\u0026#39;] : 29 Min. :0.2500 ## [\u0026#39;Beast Boost\u0026#39;] : 7 1st Qu.:0.5000 ## [\u0026#39;Shed Skin\u0026#39;] : 5 Median :1.0000 ## [\u0026#39;Clear Body\u0026#39;, \u0026#39;Light Metal\u0026#39;] : 4 Mean :0.9963 ## [\u0026#39;Justified\u0026#39;] : 4 3rd Qu.:1.0000 ## [\u0026#39;Keen Eye\u0026#39;, \u0026#39;Tangled Feet\u0026#39;, \u0026#39;Big Pecks\u0026#39;]: 4 Max. :4.0000 ## (Other) :748 ## against_dark against_dragon against_electric against_fairy ## Min. :0.250 Min. :0.0000 Min. :0.000 Min. :0.250 ## 1st Qu.:1.000 1st Qu.:1.0000 1st Qu.:0.500 1st Qu.:1.000 ## Median :1.000 Median :1.0000 Median :1.000 Median :1.000 ## Mean :1.057 Mean :0.9688 Mean :1.074 Mean :1.069 ## 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :4.000 Max. :2.0000 Max. :4.000 Max. :4.000 ## ## against_fight against_fire against_flying against_ghost ## Min. :0.000 Min. :0.250 Min. :0.250 Min. :0.000 ## 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:1.000 1st Qu.:1.000 ## Median :1.000 Median :1.000 Median :1.000 Median :1.000 ## Mean :1.066 Mean :1.135 Mean :1.193 Mean :0.985 ## 3rd Qu.:1.000 3rd Qu.:2.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :4.000 ## ## against_grass against_ground against_ice against_normal ## Min. :0.250 Min. :0.000 Min. :0.250 Min. :0.000 ## 1st Qu.:0.500 1st Qu.:1.000 1st Qu.:0.500 1st Qu.:1.000 ## Median :1.000 Median :1.000 Median :1.000 Median :1.000 ## Mean :1.034 Mean :1.098 Mean :1.208 Mean :0.887 ## 3rd Qu.:1.000 3rd Qu.:1.000 3rd Qu.:2.000 3rd Qu.:1.000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :1.000 ## ## against_poison against_psychic against_rock against_steel ## Min. :0.0000 Min. :0.000 Min. :0.25 Min. :0.2500 ## 1st Qu.:0.5000 1st Qu.:1.000 1st Qu.:1.00 1st Qu.:0.5000 ## Median :1.0000 Median :1.000 Median :1.00 Median :1.0000 ## Mean :0.9753 Mean :1.005 Mean :1.25 Mean :0.9835 ## 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.:2.00 3rd Qu.:1.0000 ## Max. :4.0000 Max. :4.000 Max. :4.00 Max. :4.0000 ## ## against_water attack base_egg_steps base_happiness ## Min. :0.250 Min. : 5.00 Min. : 1280 Min. : 0.00 ## 1st Qu.:0.500 1st Qu.: 55.00 1st Qu.: 5120 1st Qu.: 70.00 ## Median :1.000 Median : 75.00 Median : 5120 Median : 70.00 ## Mean :1.058 Mean : 77.86 Mean : 7191 Mean : 65.36 ## 3rd Qu.:1.000 3rd Qu.:100.00 3rd Qu.: 6400 3rd Qu.: 70.00 ## Max. :4.000 Max. :185.00 Max. :30720 Max. :140.00 ## ## base_total capture_rate classfication defense ## Min. :180.0 45 :250 Dragon Pokémon : 8 Min. : 5.00 ## 1st Qu.:320.0 190 : 75 Mouse Pokémon : 6 1st Qu.: 50.00 ## Median :435.0 255 : 69 Mushroom Pokémon: 6 Median : 70.00 ## Mean :428.4 75 : 61 Balloon Pokémon : 5 Mean : 73.01 ## 3rd Qu.:505.0 3 : 58 Fairy Pokémon : 5 3rd Qu.: 90.00 ## Max. :780.0 120 : 55 Flame Pokémon : 5 Max. :230.00 ## (Other):233 (Other) :766 ## experience_growth height_m hp ## Min. : 600000 Min. : 0.100 Min. : 1.00 ## 1st Qu.:1000000 1st Qu.: 0.600 1st Qu.: 50.00 ## Median :1000000 Median : 1.000 Median : 65.00 ## Mean :1054996 Mean : 1.155 Mean : 68.96 ## 3rd Qu.:1059860 3rd Qu.: 1.500 3rd Qu.: 80.00 ## Max. :1640000 Max. :14.500 Max. :255.00 ## ## japanese_name name percentage_male ## Abagouraアバゴーラ : 1 Abomasnow : 1 Min. : 0.00 ## Absolアブソル : 1 Abra : 1 1st Qu.: 50.00 ## Abulyアブリー : 1 Absol : 1 Median : 50.00 ## Aburibbonアブリボン: 1 Accelgor : 1 Mean : 48.47 ## Achamoアチャモ : 1 Aegislash : 1 3rd Qu.: 50.00 ## Agehuntアゲハント : 1 Aerodactyl: 1 Max. :100.00 ## (Other) :795 (Other) :795 ## pokedex_number sp_attack sp_defense speed ## Min. : 1 Min. : 10.00 Min. : 20.00 Min. : 5.00 ## 1st Qu.:201 1st Qu.: 45.00 1st Qu.: 50.00 1st Qu.: 45.00 ## Median :401 Median : 65.00 Median : 66.00 Median : 65.00 ## Mean :401 Mean : 71.31 Mean : 70.91 Mean : 66.33 ## 3rd Qu.:601 3rd Qu.: 91.00 3rd Qu.: 90.00 3rd Qu.: 85.00 ## Max. :801 Max. :194.00 Max. :230.00 Max. :180.00 ## ## type1 type2 weight_kg generation ## water :114 :384 Min. : 0.10 Min. :1.00 ## normal :105 flying : 95 1st Qu.: 9.00 1st Qu.:2.00 ## grass : 78 ground : 34 Median : 27.30 Median :4.00 ## bug : 72 poison : 34 Mean : 60.94 Mean :3.69 ## psychic: 53 fairy : 29 3rd Qu.: 63.00 3rd Qu.:5.00 ## fire : 52 psychic: 29 Max. :999.90 Max. :7.00 ## (Other):327 (Other):196 ## is_legendary ## Min. :0.00000 ## 1st Qu.:0.00000 ## Median :0.00000 ## Mean :0.08739 ## 3rd Qu.:0.00000 ## Max. :1.00000 ##  We can learn a few things from this. For instance, if we look at the character type variables, “Levitate” is the most common ability (although note that this is only counting the cases where there is only a single ability – more on this later), “Dragon Pokémon” are the most common classfication [sic] (although the majority of the Pokémon are given a unique classification, which seems to go against the idea of classification), and “water” and “normal” type Pokémon are very common, and many Pokémon have flying as their secondary type. If we look at the numeric type variables, we can see that the against_ values appear to be numeric values between 0 and 4 in multiples of 0.25 (they represent the Pokémon’s strength against particular Pokémon types during battle), attack, defense and speed vary between 5 and 185, 230 and 180 respectively (so there is quite a big range depending on which Pokémon you have in a fight), there is at least one Pokémon that starts with a base happiness score of 0 (in fact there are 36, which is quite sad!), the percentage of males varies between 0 and 100 with 98 missing values (as expected). But overall there do not appear to be any strange outliers in these data. There are some pretty big Pokémon, but we will explore this in a little more detail later.\n 3.3 Change Variable Class The m ajority of these data have been classified correctly by the read.data() function as either numeric or characters (which are factorised automaticallyfor use in model and potting functions later). However, there are some changes that we may wish to make. Firstly, the generation and is_legendary variables should not be considered numeric, even though they are represented by numbers. For instance, a Pokémon could not come from Generation 2.345. So let’s make these changes from numeric to factorised character variables. Note that there is a natural order to the generation values, so I will ensure that these are factorised as ordinal, just in case this comes into play later:\npokedat[[\u0026quot;generation\u0026quot;]] \u0026lt;- factor(as.character(pokedat[[\u0026quot;generation\u0026quot;]]), order = TRUE) pokedat[[\u0026quot;is_legendary\u0026quot;]] \u0026lt;- factor(as.character(pokedat[[\u0026quot;is_legendary\u0026quot;]])) These variabbles have been classified as factors even though they are represented by numeric values. The values representing the factor levels can actually be anything we choose, so it may be useful to change these to something more descriptive. For instance, it may be worth adding the word “Generation” to the generation variable, whilst the is_legendary variable may be more useful as a binary TRUE/FALSE value. It doesn’t matter what we put, since for later analyses dummy numeric variables will be used for calculations:\nlevels(pokedat[[\u0026quot;generation\u0026quot;]]) \u0026lt;- paste(\u0026quot;Generation\u0026quot;, levels(pokedat[[\u0026quot;generation\u0026quot;]])) levels(pokedat[[\u0026quot;is_legendary\u0026quot;]]) \u0026lt;- c(FALSE, TRUE) The defaults for the other factors seem to be suitable, so I will leave thse as they are.\nIn comparison, the capture_rate value is being classed as a factor, even though it should be a number. Why is this?\nlevels(pokedat[[\u0026quot;capture_rate\u0026quot;]]) ## [1] \u0026quot;100\u0026quot; \u0026quot;120\u0026quot; ## [3] \u0026quot;125\u0026quot; \u0026quot;127\u0026quot; ## [5] \u0026quot;130\u0026quot; \u0026quot;140\u0026quot; ## [7] \u0026quot;145\u0026quot; \u0026quot;15\u0026quot; ## [9] \u0026quot;150\u0026quot; \u0026quot;155\u0026quot; ## [11] \u0026quot;160\u0026quot; \u0026quot;170\u0026quot; ## [13] \u0026quot;180\u0026quot; \u0026quot;190\u0026quot; ## [15] \u0026quot;200\u0026quot; \u0026quot;205\u0026quot; ## [17] \u0026quot;220\u0026quot; \u0026quot;225\u0026quot; ## [19] \u0026quot;235\u0026quot; \u0026quot;25\u0026quot; ## [21] \u0026quot;255\u0026quot; \u0026quot;3\u0026quot; ## [23] \u0026quot;30\u0026quot; \u0026quot;30 (Meteorite)255 (Core)\u0026quot; ## [25] \u0026quot;35\u0026quot; \u0026quot;45\u0026quot; ## [27] \u0026quot;50\u0026quot; \u0026quot;55\u0026quot; ## [29] \u0026quot;60\u0026quot; \u0026quot;65\u0026quot; ## [31] \u0026quot;70\u0026quot; \u0026quot;75\u0026quot; ## [33] \u0026quot;80\u0026quot; \u0026quot;90\u0026quot; Aha. So there is a non-numeric value in there – 30 (Meteorite)255 (Core). Let’s see which Pokémon this applies to:\nas.character(subset(pokedat, capture_rate == \u0026quot;30 (Meteorite)255 (Core)\u0026quot;)[[\u0026quot;name\u0026quot;]]) ## [1] \u0026quot;Minior\u0026quot; This is a “Meteor” type Pokémon that appears to have a different capture rate under different conditions. Baased on the online Pokedex, the canonical value to use here is 30 for its Meteorite form, so let’s use this and convert it into a numeric value. One thing to bear in mind here is that factors can be a little funny. When I convert to a number, it will convert the level values to a number, not necessarily the values themselves. So in this case, it will not give me the values but instead will give me the order of the values. For instance. look at the output of the following example:\nt \u0026lt;- factor(c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;20\u0026quot;)) data.frame(factor = t, level = levels(t), number = as.numeric(t)) ## factor level number ## 1 1 1 1 ## 2 2 2 2 ## 3 3 20 4 ## 4 20 3 3 So the levels are based on a character sort rather than a numeric sort, which puts “20” ahead of “4”. Then, to add to this further, the factor “20” is actually the 3rd factor so gets the value 3, whilst the factor “3” is the 4th level, and so gets a value 4. So the output is definitely not what we would expect when converting this to a number. To avoid this, we need to convert to a character value before we do anything else:\npokedat[[\u0026quot;capture_rate\u0026quot;]] \u0026lt;- as.character(pokedat[[\u0026quot;capture_rate\u0026quot;]]) pokedat[pokedat[[\u0026quot;name\u0026quot;]] == \u0026quot;Minior\u0026quot;, \u0026quot;capture_rate\u0026quot;] = \u0026quot;30\u0026quot; pokedat[[\u0026quot;capture_rate\u0026quot;]] \u0026lt;- as.numeric(pokedat[[\u0026quot;capture_rate\u0026quot;]]) This should now be a pretty clean data set ready for analysis.\n  4 Exploratory Analyses There are a lot of data here, and I could spend ages exploring every facet, but I just really wanrt to get a tester for these data here. First of all, let’s take a look at the distribution of some of the main statistics to see how they look across the dataset. For the majority of these plots, I will be using the ggplot2 library which offers a very nice way to produce publication-quality figures using a standardised lexicon, along with the dplyr package which provides a simple way to rearrange data into suitable formats for producing plots. These are part of the Tidyverse suite of packages from Hadley Wickham, and form a very useful suite of packages with a common grammar that can be used together to make Data Science more efficient and to produce beautiful plots easily. Handy cheat sheets can be found for dplyr and ggplot2:\nlibrary(\u0026quot;ggplot2\u0026quot;) library(\u0026quot;dplyr\u0026quot;) library(\u0026quot;ggrepel\u0026quot;) library(\u0026quot;RColorBrewer\u0026quot;) 4.1 Attack First of all, let’s take a look at the attack strength of all Pokémon split by their main type:\npokedat %\u0026gt;% mutate(\u0026#39;MainType\u0026#39; = type1) %\u0026gt;% ggplot(aes(x = attack, fill = MainType)) + geom_density(alpha = 0.2) + xlab(\u0026quot;Attack Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20)) In general, there is a wide range of attack strengths and they are largely distributed with a roughly normal distribution around the mean attack strength of 77.86 that we saw earlier. However, we do see that there are some differences in the different Pokémon types available. However, this probably isn’t the easiest way to see differences between the groups. Instead, let’s use a boxplot. Here, we can see the overall distribution, with the 25th percentile, the median (50th percentile), and the 75th percentile making up the range of the box, and outliers (I believe those with values in the bottom 2.5th percentile or upper 97.5th percentile) highlighted as points on the plot. I have added notches to the boxplots, which show the confidence interval represeneted by:\n\\[median \\pm \\frac{1.58*IQR}{\\sqrt{n}}\\] Where IQR is the interquartile range. Essentially, if the notch values do not overlap between two boxes, it suggests that there may be a statistically significant difference between them:\npokedat %\u0026gt;% mutate(\u0026#39;MainType\u0026#39; = type1) %\u0026gt;% ggplot(aes(y = attack, x = MainType, fill = MainType)) + geom_boxplot(notch = TRUE) + xlab(\u0026quot;Attack Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(size = 18, angle = 90, hjust = 1), axis.text.y = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20)) From this plot, we can see that ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ type Pokémon have higher attack strength, whilst ‘fairy’ and ‘psychic’ type seem to have slightly lower attack strength. This would make sense based on the names, and we can test this a little more formally by using an ANOVA (analysis of variance) analysis to look for significant effects of the Pokémon type on the attack strength. This can be done quite simply by using linear models:\nattack_vs_type1 \u0026lt;- lm(attack ~ type1, data = pokedat) summary(attack_vs_type1) ## ## Call: ## lm(formula = attack ~ type1, data = pokedat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.16 -22.31 -3.50 19.26 114.88 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 70.1250 3.6263 19.338 \u0026lt; 0.0000000000000002 *** ## type1dark 17.6681 6.7675 2.611 0.009208 ** ## type1dragon 36.2824 6.9439 5.225 0.000000223 *** ## type1electric 0.6955 6.1178 0.114 0.909515 ## type1fairy -8.0139 8.1087 -0.988 0.323308 ## type1fighting 29.0536 6.8531 4.239 0.000025078 *** ## type1fire 11.3750 5.5998 2.031 0.042561 * ## type1flying -3.4583 18.1316 -0.191 0.848783 ## type1ghost 2.6157 6.9439 0.377 0.706501 ## type1grass 3.6442 5.0288 0.725 0.468871 ## type1ground 24.6875 6.5375 3.776 0.000171 *** ## type1ice 3.1793 7.3700 0.431 0.666301 ## type1normal 5.0369 4.7082 1.070 0.285036 ## type1poison 2.5313 6.5375 0.387 0.698719 ## type1psychic -4.5590 5.5691 -0.819 0.413253 ## type1rock 20.5417 5.8473 3.513 0.000468 *** ## type1steel 22.9583 7.2527 3.166 0.001608 ** ## type1water 3.1820 4.6320 0.687 0.492311 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 30.77 on 783 degrees of freedom ## Multiple R-squared: 0.1039, Adjusted R-squared: 0.08448 ## F-statistic: 5.343 on 17 and 783 DF, p-value: 0.00000000002374 It is not a great model by any means, as the R-squared values suggest that Pokémon type alone explains only a small proportion of the variance in the data. From the summary of the model, we can see that the fit model has an intercept of 70.1 (which is close to the overall mean of 77.86), with an offset specific to each of the different types. As we saw from the boxplot, ‘dragon’, ‘fighting’, ‘ground’, ‘rock’ and ‘steel’ all have significant increases in attack (as can be seen by the resulting p-value), but also ‘dark’ and ‘fire’ type which I missed from looking at the boxplot. We see also that ‘fairy’ and ‘psychic’ type Pokémon show a decrease in attack strength, however this difference is not significant. So if you want a strong Pokémon, then these types are your best shot:\n dark dragon fighting fire ground rock steel  It is also worth mentioning that there are a few outliers here, which represent ridiculously powerful Pokémon. In general, these are those with a strength greater than 150, but there are also two fairy Pokémon with higher strength than is typically seen within the ‘fairy’ type Pokémon. Let’s use the Cook’s Distance to identify the outliers from the above model. The idea here is to test the influence of each Pokémon by comparing the least-squares regression with and without the sample included. A higher value indicates a possible outlier. So let’s have a look at the outliers, which we are going to class as those with a Cook’s Distance greater than 4 times the mean of the Cook’s Distance over the entire data set (note that this is a commonly used threshold, but is entirely arbitrary):\nlibrary(\u0026quot;ggrepel\u0026quot;) library(\u0026quot;RColorBrewer\u0026quot;) cookdist \u0026lt;- cooks.distance(attack_vs_type1) cookdist_lim \u0026lt;- 4*mean(cookdist, na.rm=T)#0.012 data.frame(Pokenum = pokedat[[\u0026quot;pokedex_number\u0026quot;]], Pokename = pokedat[[\u0026quot;name\u0026quot;]], CooksDist = cookdist, Pokelab = ifelse(cookdist \u0026gt; cookdist_lim, as.character(pokedat[[\u0026quot;name\u0026quot;]]), \u0026quot;\u0026quot;), Outlier = ifelse(cookdist \u0026gt; cookdist_lim, TRUE, FALSE), PokeCol = ifelse(cookdist \u0026gt; cookdist_lim, as.character(pokedat[[\u0026quot;type1\u0026quot;]]), \u0026quot;\u0026quot;)) %\u0026gt;% ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) + geom_point() + scale_shape_manual(values = c(16, 8)) + scale_size_manual(values = c(2, 5)) + scale_color_manual(values = c(\u0026quot;black\u0026quot;, colorRampPalette(brewer.pal(9, \u0026quot;Set1\u0026quot;))(length(table(pokedat[[\u0026quot;type1\u0026quot;]]))))) + geom_text_repel(nudge_x = 0.2, size = 8) + geom_hline(yintercept = cookdist_lim, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) + xlab(\u0026quot;Pokedex Number\u0026quot;) + ylab(\u0026quot;Cook\u0026#39;s Distance\u0026quot;) + theme_bw() + theme(#legend.position = \u0026quot;name\u0026quot;, axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20), ) The flying Pokémon Tornadus and Noibat are the biggest outliers. However, interestingly whilst Tornadus has a particularly high attack strength of 100 for a flying type Pokémon, Noibat has a low attack strength of 30. So both are outliers compared to the flyiong type Pokémon as a whole, with a mean of 66.6666667 ± 35.1188458. It is worth mentioning that Tornadus is a Legendary Pokémon which probably explains why it has a higher attack strength.\nTornadus\n Whilst these two do not show up on the barplots, it is likely that these are responsible for the strange distribution of the notched boxplot. I suspect this odd shape is a result of the calculation of the confidence interval for the notches exceeding the 25th and 75th percentiles due to a wide range of values, but I do enjoy the fact that it looks like it is itself flying!\nThe two outlying fairy type Pokémon are Xerneas and Togepi, another favourite of my daughter’s:\nTogepi\n  4.2 Defence Let’s repeat this to look at the defence values across the different Pokémon types:\npokedat %\u0026gt;% mutate(\u0026#39;MainType\u0026#39; = type1) %\u0026gt;% ggplot(aes(x = defense, fill = MainType)) + geom_density(alpha = 0.2) + xlab(\u0026quot;Defence Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20)) There seems to be a bit more of a distinction between the different types as compared to the attack distribution, so let’s also look at the boxplots:\npokedat %\u0026gt;% mutate(\u0026#39;MainType\u0026#39; = type1) %\u0026gt;% ggplot(aes(y = defense, x = MainType, fill = MainType)) + geom_boxplot(notch = TRUE) + xlab(\u0026quot;Defence Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(size = 18, angle = 90, hjust = 1), axis.text.y = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20)) We see more outliers here as compared to the attack strength and a very clear increase in defence for steel and rock type Pokémon (unsurprisingly), as well as dragon type. There are no clear types with lower defence, similar again to what we saw in the attack strength distribution plots. Let’s once again look at this using linear models:\ndefence_vs_type1 \u0026lt;- lm(defense ~ type1, data = pokedat) summary(defence_vs_type1) ## ## Call: ## lm(formula = defense ~ type1, data = pokedat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -70.208 -20.847 -3.031 16.518 159.153 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 70.84722 3.37247 21.008 \u0026lt; 0.0000000000000002 *** ## type1dark -0.32998 6.29376 -0.052 0.9582 ## type1dragon 15.41204 6.45779 2.387 0.0172 * ## type1electric -9.02671 5.68954 -1.587 0.1130 ## type1fairy -2.68056 7.54108 -0.355 0.7223 ## type1fighting -4.45437 6.37337 -0.699 0.4848 ## type1fire -3.05876 5.20784 -0.587 0.5571 ## type1flying -5.84722 16.86236 -0.347 0.7289 ## type1ghost 8.67130 6.45779 1.343 0.1797 ## type1grass 0.02457 4.67678 0.005 0.9958 ## type1ground 13.05903 6.07981 2.148 0.0320 * ## type1ice 1.06582 6.85403 0.156 0.8765 ## type1normal -11.15198 4.37865 -2.547 0.0111 * ## type1poison -0.81597 6.07981 -0.134 0.8933 ## type1psychic -1.58307 5.17923 -0.306 0.7599 ## type1rock 25.41944 5.43795 4.674 0.000003469793310 *** ## type1steel 49.36111 6.74494 7.318 0.000000000000623 *** ## type1water 2.63523 4.30777 0.612 0.5409 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 28.62 on 783 degrees of freedom ## Multiple R-squared: 0.1534, Adjusted R-squared: 0.135 ## F-statistic: 8.347 on 17 and 783 DF, p-value: \u0026lt; 0.00000000000000022 As expected, we see very clear significant increase in defence strength for rock and steel type Pokémon, with a slight increase seen also for dragon and ground type Pokémon. There is also a slight reduction for normal type Pokémon. Flying Pokémon again show an odd distribution, so I expect again to see outliers for this class. Let’s look at the outliers now, again using the COok’s Distance:\ncookdist \u0026lt;- cooks.distance(defence_vs_type1) cookdist_lim \u0026lt;- 4*mean(cookdist, na.rm=T)#0.012 data.frame(Pokenum = pokedat[[\u0026quot;pokedex_number\u0026quot;]], Pokename = pokedat[[\u0026quot;name\u0026quot;]], CooksDist = cookdist, Pokelab = ifelse(cookdist \u0026gt; cookdist_lim, as.character(pokedat[[\u0026quot;name\u0026quot;]]), \u0026quot;\u0026quot;), Outlier = ifelse(cookdist \u0026gt; cookdist_lim, TRUE, FALSE), PokeCol = ifelse(cookdist \u0026gt; cookdist_lim, as.character(pokedat[[\u0026quot;type1\u0026quot;]]), \u0026quot;\u0026quot;)) %\u0026gt;% ggplot(aes(x = Pokenum, y = CooksDist, color = PokeCol, shape = Outlier, size = Outlier, label = Pokelab)) + geom_point() + scale_shape_manual(values = c(16, 8)) + scale_size_manual(values = c(2, 5)) + scale_color_manual(values = c(\u0026quot;black\u0026quot;, colorRampPalette(brewer.pal(9, \u0026quot;Set1\u0026quot;))(length(table(pokedat[[\u0026quot;type1\u0026quot;]]))))) + geom_text_repel(nudge_x = 0.2, size = 8) + geom_hline(yintercept = cookdist_lim, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;) + xlab(\u0026quot;Pokedex Number\u0026quot;) + ylab(\u0026quot;Cook\u0026#39;s Distance\u0026quot;) + theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20), ) As expected, Noibat and Tornadus are outliers here, and are joined by Noivern (which is an evolution of Noibat). As it turns out, these are the only 3 Pokémon in the flying type, which explains why they show up as outliers and why the boxplot has such a strange distribution. Other outliers include ice-type Pokémon Avalugg (defence 184), steel-type Pokémon Steelix and Aggron (both with defence 230), and bug-type Pokémon Shuckle (defence 230) which doubles up as a rock type Pokémon:\nShuckle\n As seen with Shuckle, we should bear in mind that we have only focussed on the primary Pokémon type here, and have not considered the type2 values. So it is possible that we are missing the full picture when looking only at the first class for some of these Pokémon.\n 4.3 Other We could do this for all of the different values, and there are many different ways that we may want to examine these data. We can look at eaxch variable in the data set and examine them for odd distributions, we can look for outliers (as we have done above), we can start to examine relationships between variables, and we can look for correlation between different values (which we will do further below). Indeed a considerable amount of time should be spent exploring data in this way to ensure it is of good quality – the old saying “garbage in, garbage out” is very true. But to be honest there are more interesting things that I want to do with these data, so let’s crack on…\n 4.4 Ensuring we use accurate data classes throughout It is important to ensure that we are using the data in the correct way for our analyses. First of all let’s take a look at the abilities that each Pokémon has. As it stands, the abilities variable is not terribly useful, as it fails to link Pokémon who may share abilities but not exactly. We could spend some time decoding the abilities, and create a “dictionary” of different abilities for each Pokémon, but perhaps a better measure may be to simply look at the number of abilities that each Pokémon has:\nabilities \u0026lt;- strsplit(as.character(pokedat[[\u0026quot;abilities\u0026quot;]]), \u0026quot;, \u0026quot;) abilities \u0026lt;- sapply(abilities, FUN = function(x) gsub(\u0026quot;\\\\[|\\\\\u0026#39;|\\\\]\u0026quot;, \u0026quot;\u0026quot;, x)) names(abilities) \u0026lt;- rownames(pokedat) pokedat[[\u0026quot;number_abilities\u0026quot;]] \u0026lt;- sapply(abilities, FUN = length) table(pokedat[[\u0026quot;number_abilities\u0026quot;]]) ## ## 1 2 3 4 6 ## 109 245 427 7 13 So the majority of Pokémon have only 1, 2 or 3 abilities, with only a small number of Pokémon have more than 3 abilities, and around half having exactly 3.\nI want to now look only at the variables that might theoretically be descriptive of the Pokémon in some kind of model. So we can remove the individual abilties (although the number of abiltities will likely be of interest), the names and Pokedex number, and the classification which seems to be fairly ambiguous. We also need to ensure that we use the dummy variables for the variables encoded as factors.\n  5 Normalization Prior to doing this, I want to look at a few different ways to normalize all of the variables to ensure that they all have values that are comparable. Some algorithms are more sensitive to normalization than others, and it becomes quite obvious why this may be something to consider here when you consider that whilst the stats value attack lies between 5 and 185, the experience_growth value ranges between 600,000 and 1,640,000. That’s a \u0026gt;10,000-fold increase. This variable will completely dominate the calculations for certain machine-learning algorithms like K-Nearest Neighbours (KNN) and Support Vector Machines (SVM) where the distance between data points is important. SO let’s look at how we might want to normalize these data prior to analysis.\n5.1 Min-Max Normalization There are a few different ways to do this. One way is to standardize the data, so that we bring them all into a common scale of \\([0,1]\\), but we maintain the distribution for each specific variable. So if one variable has a very skewed distriburtion, this will be maintained. A simple way to do this is to use the min-max scaling approach, where you subtract the lowest value (so that the minimum value is always zero) and then divide by the range of the data so that the data are spread in the range \\([0,1]\\):\n\\[x\u0026#39; = \\frac{x - x_{min}}{x_{max} - x_{min}}\\]\nLet’s compare the standardized and non-standardized attack values:\nattack_std \u0026lt;- (pokedat[[\u0026quot;attack\u0026quot;]] - min(pokedat[[\u0026quot;attack\u0026quot;]]))/(max(pokedat[[\u0026quot;attack\u0026quot;]]) - min(pokedat[[\u0026quot;attack\u0026quot;]])) data.frame(Raw = pokedat[[\u0026quot;attack\u0026quot;]], Standardized = attack_std) %\u0026gt;% tidyr::gather(\u0026quot;class\u0026quot;, \u0026quot;attack\u0026quot;, Raw, Standardized) %\u0026gt;% mutate(class = factor(class, levels = c(\u0026quot;Raw\u0026quot;, \u0026quot;Standardized\u0026quot;))) %\u0026gt;% ggplot(aes(x = attack, fill = class)) + geom_density(alpha = 0.2) + facet_wrap(. ~ class, scales = \u0026quot;free\u0026quot;) + xlab(\u0026quot;Attack Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;, axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20), strip.text = element_text(size = 24, face = \u0026quot;bold\u0026quot;) ) So as you can see here, the distribution of the attack scores remains the same, but the scales over which the distribution is spread are different.\n 5.2 Z-Score Normalization However, sometimes it is preferable to instead that every feature has a standard distribution that can be easily described to make them more comparable. In this case, you can normalize the data so that the distribution of all features is shifted towards that of a normal (Gaussian) distribution. This is a typical “bell-shaped” curve, which can be exclusively described by the mean and the standard deviation. An example where I use this approach regularly is in gene-expression analysis, where we normalize the data such that the levels of expression of each gene is represented by a normal distribution, so that we can test for samples where the expression is significantly outside of the confines of this distribution. We can then look at all of the genes that seem to show significantly different expression than we would expect using hypothesis testing approaches, and look for common functions of these genes through the use of network analysis, gene ontology analysis and other downstream analyses.\nA simple normalization approach is the z-score normalization, which will transform the data so that the distribution has a mean of 0 and a standard deviation of 1. The transformation is as follows:\n\\[x\u0026#39; = \\frac{x - \\mu}{\\sigma}\\]\nSo we simply subtract the mean \\(\\mu\\) (to center the data), and divide by the standard deviation \\(\\sigma\\). Let’s again visualise this by comparing the raw and normalized data:\nattack_norm \u0026lt;- (pokedat[[\u0026quot;attack\u0026quot;]] - mean(pokedat[[\u0026quot;attack\u0026quot;]]))/sd(pokedat[[\u0026quot;attack\u0026quot;]]) data.frame(Raw = pokedat[[\u0026quot;attack\u0026quot;]], Normalized = attack_norm) %\u0026gt;% tidyr::gather(\u0026quot;class\u0026quot;, \u0026quot;attack\u0026quot;, Raw, Normalized) %\u0026gt;% mutate(class = factor(class, levels = c(\u0026quot;Raw\u0026quot;, \u0026quot;Normalized\u0026quot;))) %\u0026gt;% ggplot(aes(x = attack, fill = class)) + geom_density(alpha = 0.2) + facet_wrap(. ~ class, scales = \u0026quot;free\u0026quot;) + xlab(\u0026quot;Attack Strength\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;, axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20), strip.text = element_text(size = 24, face = \u0026quot;bold\u0026quot;) ) So the normalized data now has a mean of 0, with the majority of values lying between -2 and 2. Note that since this is a fairly simple normalization technique, the distribution itself is not really changed, but there are other methods such as quantile normalization which can reshape the data to a Gaussian curve, or indeed any other dstribution as required.\n 5.3 Choosing a Normalization Method The choice of which method to use will largely depend on what you are trying to do. The min-max scaling method is a common method used in machine learning, but does not handle outliers very well – a sample with an extreme value in the raw data will still have an extreme value in the scaled data, resulting in bunching up of the remaining data since it is all bounded within the range \\([0,1]\\). The Z-score normalization is better at dealing with outliers as they will appear far away from the mean value of 0, but the range of data is no longer bounded meaning that different variables will now have different ranges. Other normalization processes may make your data more homogenous, but at the cost of potentially losing aspects of the data that may be useful.\nMany machine learning algorithms use some kind of distance measure, in order to look for similarity between different items. This can be a simple Euclidean distance, which is a k-dimensional measure of “as the crow flies”. If the scales are very different between your variables, then this will cause a significant issue.\nFor the following analyses, I will use Z-score normalization, since I know that there are a few extreme outliers in these data.\n  6 Looking for Patterns 6.1 Correlation of Variables One thing that it is worth doing before heading down a route of data modelling is to look at the correlation structure between the variables in the data set. If there are highly correlated variables (for instance one might imagine height and weight to be highly correlated), then both will offer the same information to the model – adding both will give no additional predictive power than adding only one. There is then a danger of over-fitting the data, which can happen if you create a model so complicated that, whilst it may fit the training data very well, it is so complex as to no longer be applicable to external data sets making it essentially useful. An extreme example is if we create a model where we have one variable for each sample, that is equal to 1 for that specific sample, and 0 for every other sample. This would fit the data perfectly, but would be of absolutely no use for any other data. In addition, we can risk biasing the data by including multiple variables that essentially encode the same information.\nSo we can calculate a pairwise correlation matrix, which will give us a measure of similarity between each pair of variables by comparing the two vectors of values for the 801 Pokémon. There are multiple measures of correlation that can be used. The standard is the Pearson Correlation, which is a measure of the linearity of the relationship between two values. A value of 1 represents an entirely linear monotonic relationship (as one value increases, so does the other, but with every unit increase in one variable matching a unit increase in the other in a linear way), a value of 0 represents no linearity between the values (no clear relationship between the two), and a value of -1 represents an inverse monotonic linear relationship (anti-correlated).\nFor this analysis, I will be using the Spearman Correlation coefficient. Instead of using the values themselves, this method first ranks the data, and then looks at the correlation. The idea here is that the unit difference between each successive value is kept constant, meaning that outliers do not have a big impact. This is therefore independent of the distribution of the data, and is therefore a non-parametric method. Since we identified some outliers with some of these variables, this method will remove the effect that these might have.\nWe can represent these values by using a heatmap, which is a way of representing 3-dimensional data. The value of the correlation, rather than being represented on an axis as a value, will be represented by a colour. Values of the SPearman correlation closer to 1 will appear more red, whilst those closer to -1 will appear more blue. Those closer to 0 will appear white.\nIn addition, I will apply a hierarchical clustering method to ensure that the variables most similar to one another are located close to one another on the figure. Pairwise distances are calculated by looking at the Euclidean distance, and similar variables are clustered together, allowing us to pick out by eye those most similar to one another.\nAs mentioned earlier, we first want to ensure that we are looking at numerical values, or at least numerical representations of factors.\npokedat_vals \u0026lt;- pokedat[, !names(pokedat) %in% c(\u0026quot;abilities\u0026quot;, \u0026quot;classfication\u0026quot;, \u0026quot;japanese_name\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;pokedex_number\u0026quot;)] pokedat_vals[[\u0026quot;type1\u0026quot;]] \u0026lt;- as.numeric(pokedat_vals[[\u0026quot;type1\u0026quot;]]) pokedat_vals[[\u0026quot;type2\u0026quot;]] \u0026lt;- as.numeric(pokedat_vals[[\u0026quot;type2\u0026quot;]]) pokedat_vals[[\u0026quot;generation\u0026quot;]] \u0026lt;- as.numeric(pokedat_vals[[\u0026quot;generation\u0026quot;]]) pokedat_vals[[\u0026quot;is_legendary\u0026quot;]] \u0026lt;- as.numeric(pokedat_vals[[\u0026quot;is_legendary\u0026quot;]]) This will give us a purely numeric data set for use in numeric calculations. Finally we will normalize the data using Z-score normalization:\npokedat_norm \u0026lt;- scale(pokedat_vals, center = TRUE, scale = TRUE) Finally let’s take a look at the correlation plot:\nlibrary(\u0026quot;pheatmap\u0026quot;) pokecor_var \u0026lt;- cor(pokedat_norm, method = \u0026quot;spearman\u0026quot;) colors \u0026lt;- colorRampPalette(c(\u0026#39;dark blue\u0026#39;,\u0026#39;white\u0026#39;,\u0026#39;dark red\u0026#39;))(255)#colorRampPalette( rev(brewer.pal(9, \u0026quot;Blues\u0026quot;)) )(255) pheatmap(pokecor_var, clustering_method = \u0026quot;complete\u0026quot;, show_colnames = FALSE, show_rownames = TRUE, col=colors, fontsize_row = 24)  We can see a few clear cases of correlated variables here, especially between a few of the values giving attack strength against certain Pokémon types. For instance, attack against ghost- and dark-type Pokémon are very similar (which makes sense), as are attacks against electric- and rock-type Pokémon (makes less sense). The clearest aspect of the figure is a block of high positive correlations between a number of variables associated with the Pokémon’s vital statistics. So the most similar variables are those like speed, defence strength, attack strength, height, weight, health points, experience growth, etc. This makes a lot of sense, with bigger Pokémon having better attack and defence, more health points, etc. We also see that these variables are very highly anti-correlated with the capture-rate, which again makes sense – the better Pokémon are harder to catch. However, none of these correlations seem significantly high to require the remobval of any variables prior to analysis.\n 6.2 Height vs Weight Let’s take a look at the height vs the weight. We can also include a few additional variables to see how the Pokémon strength and defence affect the relationship. I am going to scale both the x-and y-axes by using a \\(log_{10}\\) transformation to avoid much larger Pokémon drowing out the smaller ones:\nggplot(aes(x = height_m, y = weight_kg, color = attack, size = defense), data = pokedat) + geom_point(alpha = 0.2) + scale_color_gradient2(name = \u0026quot;Attack\u0026quot;, midpoint = mean(pokedat[[\u0026quot;attack\u0026quot;]]), low = \u0026quot;blue\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;red\u0026quot;) + scale_size_continuous(name = \u0026quot;Defence\u0026quot;, range = c(1, 10)) + xlab(\u0026quot;Height (m)\u0026quot;) + ylab(\u0026quot;Weight (Kg)\u0026quot;) + scale_x_log10() + scale_y_log10() + #theme_bw() + theme(axis.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 18), legend.title = element_text(size = 24, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 20), strip.text = element_text(size = 24, face = \u0026quot;bold\u0026quot;) ) So there is a clear relationship, and in general taller Pokémon (such as Walor, the largest of the Pokémon) also weigh more as we might expect:\nWalord, the largest Pokemon\n We also see that in general the attack strength of the larger Pokémon is higher than the smaller ones, although there are definitely some outliers, such as Cosmeom, a legendary Pokémon only 10 cm tall that weighs nearly 1,000 kg!\nCosmoem, the smallest Pokemon\n  6.3 Correlation between Pokémon pokecor_sample \u0026lt;- cor(t(pokedat_norm), method = \u0026quot;spearman\u0026quot;) colors \u0026lt;- colorRampPalette(c(\u0026#39;dark blue\u0026#39;,\u0026#39;white\u0026#39;,\u0026#39;dark red\u0026#39;))(255) pheatmap(pokecor_sample, clustering_method = \u0026quot;complete\u0026quot;, show_colnames = FALSE, show_rownames = FALSE, annotation = pokedat[, c(\u0026quot;generation\u0026quot;, \u0026quot;type1\u0026quot;, \u0026quot;type2\u0026quot;, \u0026quot;is_legendary\u0026quot;)], col=colors)  So here we see the correlation structure between the 801 Pokémon, and it very clearly shows similarities between groups of Pokémon. The red boxes that we see down the diagonal represent highly similar groups, and at the top I have annotated the factor variables generation, type1, type2 and is_legendary. Almost all of the legendary Pokémon cluster together, suggesting that these are all similar to one another across these variables. Similarly there are a number of Pokémon types that clearly cluster together, such as rock type and dark type Pokémon. Interestingly the Generation number seems to be unrelated to Pokémon similarity.\n 6.4 Principal Component Analysis A good way to explore data such as these to look for underlying trends in the data is to use a dimensional-reduction algorithm to reduce these high-dimensional data down into asmaller number of easy to digest chunks. Principal component analysis (PCA) is one such approach, and can be used to look for the largest sources of variation within a high dimensional data set. For a data set with n variables, we can think of these data existing in an n-dimensional space. PCA is a mathematical trick that rotates these axes in n-dimensional space so that the x-axis of the rotation explains the largest possible amount of variation in the data, the y-axis then explains the next largest possible amount of variation, the z-axis the next largest amount, etc. In many cases, a very large amount of the total variation in the original n-dimensional data set can be captured by only a small number of so-called principal components. This can be used to look for underlying trends in the data. So let’s have a look at how this looks:\npc \u0026lt;- prcomp(pokedat_vals, scale. = TRUE) pc_plot \u0026lt;- as.data.frame(pc[[\u0026quot;x\u0026quot;]]) pc_plot \u0026lt;- cbind(pc_plot, pokedat[rownames(pc_plot), c(\u0026quot;generation\u0026quot;, \u0026quot;type1\u0026quot;, \u0026quot;type2\u0026quot;, \u0026quot;is_legendary\u0026quot;)]) explained_variance \u0026lt;- 100*((pc[[\u0026quot;sdev\u0026quot;]])^2 / sum(pc[[\u0026quot;sdev\u0026quot;]]^2)) screeplot(pc, type = \u0026quot;line\u0026quot;, main = \u0026quot;Principal Component Loadings\u0026quot;) In this plot, we see that PC1 and PC2 explain a lot of variance compared to the other PCs, but the drop off is not complete at this point. In a lot of data sets, the first few PCs explain the vast majority of the variance, and so this plot drops off considerably to a flat line by PC3 or PC4. Let’s take a look at how discriminatory these PCs are to these data:\nggplot(aes(x = PC1, y = PC2, shape = is_legendary, color = type1), data = pc_plot) + geom_point(size = 5, alpha = 0.7) + xlab(sprintf(\u0026quot;PC%d (%.2f%%)\u0026quot;, 1, explained_variance[1])) + ylab(sprintf(\u0026quot;PC%d (%.2f%%)\u0026quot;, 2, explained_variance[2])) + theme(axis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 12), legend.title = element_text(size = 18, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 14)) We can see that this PCA approach clearly discriminates the legendary Pokémon (triangles) from the other Pokémon (circles) in PC1, which is the new axis that explains the most variance in the data (although note that this is still only 17.2%, so not a huge amount really). There is some separation seen between the different Pokémon groups, which seems to represent the second most significant source of variation, accounting for another 10.4%. Let’s look at PC3 as well to see if this is able to discriminate the samples further:\nggplot(aes(x = PC2, y = PC3, shape = is_legendary, color = type1), data = pc_plot) + geom_point(size = 5, alpha = 0.7) + xlab(sprintf(\u0026quot;PC%d (%.2f%%)\u0026quot;, 2, explained_variance[2])) + ylab(sprintf(\u0026quot;PC%d (%.2f%%)\u0026quot;, 3, explained_variance[3])) + theme(axis.title = element_text(size = 14, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size = 12), legend.title = element_text(size = 18, face = \u0026quot;bold\u0026quot;), legend.text = element_text(size = 14)) By looking at the 2nd and 3rd PCs, we do see a slight separation between the Pokémon types, but it is not strong.\n 6.5 Correlation-Based Recommendation As a little aside, I want to see whether it is possible to recommend similar Pokémon to any Pokémon that you might suggest. For instance, my daughter’s absolute favourite is Eevee (a cuddly fox-like Pokémon):\nEeevee\n So let’s see whether there are any other Pokémon that are similar to Eevee, based on a simple correlation match. To do this, I will calculate the Spearman correlation coefficient between Eevee and every other Pokémon, and see which the most similar are:\neevee \u0026lt;- as.numeric(pokedat_vals[\u0026quot;Eevee\u0026quot;, ]) eevee_cor \u0026lt;- apply(pokedat_vals, MAR = 1, FUN = function (x) cor(x, eevee, method = \u0026quot;spearman\u0026quot;)) head(sort(eevee_cor, decreasing = TRUE), 10) ## Eevee Aipom Sentret Teddiursa Patrat Zigzagoon Rattata ## 1.0000000 0.9781018 0.9728923 0.9695460 0.9693680 0.9690263 0.9689643 ## Meowth Raticate Bidoof ## 0.9687552 0.9676089 0.9669312 So here are a few of the most similar Pokémon:\nAipom\n Sentret\n Teddiursa\n Patrat\n Well they all look kind of cuddly like Eevee, except for the really creepy evil beaver Patrat at the end!\nHowever, what would be even more efficient (maybe not in this case, but in the case of a much higher-dimensional data set like a gene-expression data set of 30,000 genes) is to use the reduced dataset after using PCA. The first 10 PCs explained around two thirds of the variance on the data, so by using these 10 values rather than the 37 values originally, we reduce the computations with a relatively small loss of data. Let’s see what the outputs are in this case:\npc_df_sub \u0026lt;- as.data.frame(pc[[\u0026quot;x\u0026quot;]])[, 1:10] eevee \u0026lt;- as.numeric(pc_df_sub[\u0026quot;Eevee\u0026quot;, ]) eevee_cor_pca \u0026lt;- apply(pc_df_sub, MAR = 1, FUN = function (x) cor(x, eevee, method = \u0026quot;spearman\u0026quot;)) head(sort(eevee_cor_pca, decreasing = TRUE), 10) ## Eevee Spinda Delcatty Watchog Aipom Furret Smeargle ## 1.0000000 0.9636364 0.9515152 0.9515152 0.9393939 0.9272727 0.9030303 ## Herdier Vanillish Meowth ## 0.9030303 0.9030303 0.8909091 Spinda\n Delcatty\n Watchog\n Furret\n Similarly cute, but with another creepy one! After checking with the experiment subject (my daughter), it seems that the best hits are definitely Watchog and Furret, so this seems to cope well at picking out similarly cuddly looking Pokémon.\nObviously here we are simply looking for Pokémon with similar characteristics. This is not as in-depth as a collaboritive filtering method where we have some subjective ranking of items to help us to determine the best Pokémon to match somebody’s needs. However, by using the reduced PCA data set we are able to find very close matches using only a reduced subset of the data.\n  7 Predicting Legendary Pokémon Based on our previous analyses, we see that there is a clear discrimination between normal and Legendary Pokémon. These are incredibly rare, and very powerful Pokémon in the game. So is it possible to identify a Legendary Pokémon based on the variables available from this database? And if so, which variables are the most important?\nTo do predictive modelling, we need to first split our data up into a training data set to use to train the model, and then a validation data set to use to confirm the accuracy of the predictions that this model makes. There are more robust ways to do this, such as cross-validation and bootstrapping, which allow you to assess the accuracy of the model. Cross validation can be applied simply by using the caret package in R.\nWe will also split the data randomly into two data sets – one containing 80% of the Pokémon for training the model, and one containing 20% of the Pokémon for validation purposes. For the following model fitting approaches, we do not need to have the categorical data converted into numbers as we did for the correlation analysis, as the factors will be treated correctly automatically We do however still need to ignore the non-informative variables. In addition, I am going to remove the “against_” columns to avoid overfitting of the data. So let’s generate our two data sets:\nlibrary(\u0026quot;caret\u0026quot;) set.seed(0) split_index \u0026lt;- createDataPartition(y = pokedat[[\u0026quot;is_legendary\u0026quot;]], p = 0.8, list = FALSE) rm_index \u0026lt;- which(names(pokedat) %in% c(\u0026quot;abilities\u0026quot;, \u0026quot;classfication\u0026quot;, \u0026quot;japanese_name\u0026quot;, \u0026quot;name\u0026quot;, \u0026quot;pokedex_number\u0026quot;)) rm_index \u0026lt;- c(grep(\u0026quot;against\u0026quot;, names(pokedat)), rm_index) training_dat \u0026lt;- pokedat[split_index, -rm_index] validation_dat \u0026lt;- pokedat[-split_index, -rm_index] So let’s try a few different common methods used for classification purposes.\n7.1 Support-Vector Machine We can then train our SVM model, incorporating a preprocessing step to center and scale the data, and performing 10-fold repeated cross validation which we repeat 3 times:\nset.seed(0) trctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) model_svm \u0026lt;- train(is_legendary ~., data = training_dat, method = \u0026quot;svmLinear\u0026quot;, trControl = trctrl, preProcess = c(\u0026quot;center\u0026quot;, \u0026quot;scale\u0026quot;), tuneLength = 10) model_svm ## Support Vector Machines with Linear Kernel ## ## 641 samples ## 18 predictor ## 2 classes: \u0026#39;FALSE\u0026#39;, \u0026#39;TRUE\u0026#39; ## ## Pre-processing: centered (56), scaled (56) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... ## Resampling results: ## ## Accuracy Kappa ## 0.9802138 0.8782632 ## ## Tuning parameter \u0026#39;C\u0026#39; was held constant at a value of 1 So this model is able to predict whether the Pokémon is Legendary based on these 18 predictor variables with 98.02% accuracy (correct predictions). The Kappa value is normalised to account for the fact that we could get pretty good accuracy if we just called everything not Legendary due to the imbalance in the classes.\nThis accuracy is based on resampling of the training data. How does it cope with the validation data? Let’s take a look at the confusion matrix for the predicted outcomes compared to the true values:\npredict_legendary \u0026lt;- predict(model_svm, newdata = validation_dat) svm_confmat \u0026lt;- confusionMatrix(predict_legendary, validation_dat[[\u0026quot;is_legendary\u0026quot;]]) svm_confmat ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 144 1 ## TRUE 2 13 ## ## Accuracy : 0.9812 ## 95% CI : (0.9462, 0.9961) ## No Information Rate : 0.9125 ## P-Value [Acc \u0026gt; NIR] : 0.000314 ## ## Kappa : 0.8863 ## ## Mcnemar\u0026#39;s Test P-Value : 1.000000 ## ## Sensitivity : 0.9863 ## Specificity : 0.9286 ## Pos Pred Value : 0.9931 ## Neg Pred Value : 0.8667 ## Prevalence : 0.9125 ## Detection Rate : 0.9000 ## Detection Prevalence : 0.9062 ## Balanced Accuracy : 0.9574 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##  SO according to these multiple statistics, of the 14 Legendary Pokémon in the validation data, we correctly identified 13 of them, but missed 1. 2 were identified incorrectly. Our ultimate accuracy is 98.12%, which seems to be pretty good. However, it is possible to further tune this model, by adjusting the tuning parameter C, by tweaking the parameters included in the model, and moving from a linear SVM model. However, all told, this is a pretty good result.\n 7.2 k-Nearest Neighbour Another classification method is the k-Nearest Neighbour (kNN) algorithm. The idea here is that a record is kept of all of the data in the training data, and a new sample is compared to find the k samples “closest” to it. The classification is then calculated based on some average of the classifications of these nearest neighbours. The method of determining the “nearest” neighbour can be one of a number of different methods, including Euclidean distance as described earlier.\nAlso, the value of k is very important. Using the caret package in R, we are able to test using multiple different values of k to find the value that optimises the model accuracy. So let’s train our model, again performing 10-fold repeated cross validation which we repeat 3 times:\nset.seed(0) trctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) model_knn \u0026lt;- train(is_legendary ~., data = training_dat, method = \u0026quot;knn\u0026quot;, trControl = trctrl, preProcess = c(\u0026quot;center\u0026quot;, \u0026quot;scale\u0026quot;), tuneLength = 10) model_knn ## k-Nearest Neighbors ## ## 641 samples ## 18 predictor ## 2 classes: \u0026#39;FALSE\u0026#39;, \u0026#39;TRUE\u0026#39; ## ## Pre-processing: centered (56), scaled (56) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.9470304 0.5515501 ## 7 0.9444345 0.5158836 ## 9 0.9428470 0.4854601 ## 11 0.9376461 0.4246775 ## 13 0.9350660 0.3903400 ## 15 0.9355788 0.3898290 ## 17 0.9376542 0.4161790 ## 19 0.9376784 0.4063420 ## 21 0.9355786 0.3765296 ## 23 0.9381750 0.4070014 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 5. We can see that the accuracy is optimised by using k = 5 nearest neighbours, which gives an accuracy of 94.7% – below that of the SVM accuracy. Now let’s test it on the validation data set:\npredict_legendary \u0026lt;- predict(model_knn, newdata = validation_dat) knn_confmat \u0026lt;- confusionMatrix(predict_legendary, validation_dat[[\u0026quot;is_legendary\u0026quot;]]) knn_confmat ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 145 4 ## TRUE 1 10 ## ## Accuracy : 0.9688 ## 95% CI : (0.9286, 0.9898) ## No Information Rate : 0.9125 ## P-Value [Acc \u0026gt; NIR] : 0.004163 ## ## Kappa : 0.7833 ## ## Mcnemar\u0026#39;s Test P-Value : 0.371093 ## ## Sensitivity : 0.9932 ## Specificity : 0.7143 ## Pos Pred Value : 0.9732 ## Neg Pred Value : 0.9091 ## Prevalence : 0.9125 ## Detection Rate : 0.9062 ## Detection Prevalence : 0.9313 ## Balanced Accuracy : 0.8537 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##  So we can already see that the results of this model are less positive than the SVM model.\n 7.3 Logistic Regression For classification problems with only two groups, linear regression is often a good first option. This is a generalised linear model, where we require a transformation of the response variable to ensure that it fits a continuous scale. In this case, the response variable is the probability of being in the Legendary class, so we need to map this to the simple yes/no outcome of the input training data. The logit function is often used, which is a transformation between a probability p and a real number:\n\\[logit(p) = log(\\frac{p}{1-p})\\]\nSo let’s fit a logistic regression model containing all of our data, again using 3 repeats of 10-fold cross validation, and see what we get back:\nset.seed(0) trctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) model_logreg \u0026lt;- train(is_legendary ~., data = training_dat, method = \u0026quot;glm\u0026quot;, family = \u0026quot;binomial\u0026quot;, trControl = trctrl, preProcess = c(\u0026quot;center\u0026quot;, \u0026quot;scale\u0026quot;), tuneLength = 10) model_logreg ## Generalized Linear Model ## ## 641 samples ## 18 predictor ## 2 classes: \u0026#39;FALSE\u0026#39;, \u0026#39;TRUE\u0026#39; ## ## Pre-processing: centered (56), scaled (56) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... ## Resampling results: ## ## Accuracy Kappa ## 0.9667197 0.802881 So we actually get a better accuracy from the cross-validation than for SVM or kNN at 96.67% accuracy. So let’s test it on the validation data set:\npredict_legendary \u0026lt;- predict(model_logreg, newdata = validation_dat) logreg_confmat \u0026lt;- confusionMatrix(predict_legendary, validation_dat[[\u0026quot;is_legendary\u0026quot;]]) logreg_confmat ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 141 1 ## TRUE 5 13 ## ## Accuracy : 0.9625 ## 95% CI : (0.9202, 0.9861) ## No Information Rate : 0.9125 ## P-Value [Acc \u0026gt; NIR] : 0.01131 ## ## Kappa : 0.792 ## ## Mcnemar\u0026#39;s Test P-Value : 0.22067 ## ## Sensitivity : 0.9658 ## Specificity : 0.9286 ## Pos Pred Value : 0.9930 ## Neg Pred Value : 0.7222 ## Prevalence : 0.9125 ## Detection Rate : 0.8812 ## Detection Prevalence : 0.8875 ## Balanced Accuracy : 0.9472 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##  The accuracy of this model on the validation data set is 96.25%.\n 7.4 Random Forest As a final model, we will look at using a random forest classifier. A random forest is essentially created by creating a number of decision trees and averaging over them all. I will use the same cross-validation scheme as previously, and will allow caret to identify the optimum parameters:\nset.seed(0) trctrl \u0026lt;- trainControl(method = \u0026quot;repeatedcv\u0026quot;, number = 10, repeats = 3) model_rf \u0026lt;- train(is_legendary ~., data = training_dat, method = \u0026quot;rf\u0026quot;, trControl = trctrl, preProcess = c(\u0026quot;center\u0026quot;, \u0026quot;scale\u0026quot;), tuneLength = 10) model_rf ## Random Forest ## ## 641 samples ## 18 predictor ## 2 classes: \u0026#39;FALSE\u0026#39;, \u0026#39;TRUE\u0026#39; ## ## Pre-processing: centered (56), scaled (56) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 577, 576, 576, 577, 577, 578, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.9911611 0.9396362 ## 8 0.9984292 0.9905178 ## 14 0.9984292 0.9905178 ## 20 0.9984292 0.9905178 ## 26 0.9984292 0.9905178 ## 32 0.9984292 0.9905178 ## 38 0.9984292 0.9905178 ## 44 0.9984292 0.9905178 ## 50 0.9984292 0.9905178 ## 56 0.9984292 0.9905178 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 8. This approach identified an accuracy of 99.84% (with a normalised kappa value of 99.05%) with an mtry value of 8. This is incredibly good, and is the best outcome so far. However, it is worth noting that this approach is significantly slower than the others. Let’s check against our validation data set:\npredict_legendary \u0026lt;- predict(model_rf, newdata = validation_dat) confusionMatrix(predict_legendary, validation_dat[[\u0026quot;is_legendary\u0026quot;]]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 145 1 ## TRUE 1 13 ## ## Accuracy : 0.9875 ## 95% CI : (0.9556, 0.9985) ## No Information Rate : 0.9125 ## P-Value [Acc \u0026gt; NIR] : 0.00005782 ## ## Kappa : 0.9217 ## ## Mcnemar\u0026#39;s Test P-Value : 1 ## ## Sensitivity : 0.9932 ## Specificity : 0.9286 ## Pos Pred Value : 0.9932 ## Neg Pred Value : 0.9286 ## Prevalence : 0.9125 ## Detection Rate : 0.9062 ## Detection Prevalence : 0.9125 ## Balanced Accuracy : 0.9609 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##  So whilst the training accuracy is nearly 100%, here we see slightly lower accuracy on the test data set. It is still pretty good going, but could still be improved.\n  8 Conclusion We have managed to explore these data in a number of different ways and have identified some interesting themes and patterns. However, there is a lot of additional work that can be done. Correlation-based recommendation seems to work to identify similar Pokémon, at least in the small number of cases that I looked at. My wife showed me this toy that can guess any Pokémon that you think of, which probably uses a similar approach. I’m in the wrong business…\nIt is worth noting that here I have concentrated predominantly on the accuracy of the model, but in general this is not the only metric that we should be interested in. As mentioned previously, given how many more non-Legendary Pokémon there are, we would get a pretty good accuracy if we just assumed that none of the test samples were Legendary. Sensitivity and Specificity for instance are useful to look at, which define the proportion of samples called correctly as Legendary (True Positives) and the proportion of those called correctly as not Legendary (Truse Negatives). Often, if we think of the opposite of these, False Negatives are more of an issue than False Positives – e.g. for disease prediction we would probably prefer not to miss any potential cases.\nAlso, whilst it was interesting to play with some of the most commonly used machine learning methods, I have spent no real time tuning the model parameters. The variables themselves used in the data set could be tweaked to identify those variables most associated with the response variable. Given that these are incredibly rare and powerful Pokémon, we will inevitably find that the fighting based variables like attack and defence are highly discriminative, but also those such as the capture rate and experience growth associated. In addition, we have looked only at additive models here and have not considered interaction terms. We can also spend time fine-tuning the parameters of the models themselves to improve the accuracy further.\nHowever, despite this, we got some very good results from the default model parameters for several commonly used methods. kNN, SVM, logistic regression and random forests gave very similar results, but with SVM and random forest giving the best predictive outcomes.\nUltimately, the key to developing any machine learning algorithm is to trial multiple different approaches and tune the parameters for the specific data in question. Also, a machine learning algorithm is only as good as the data that it is trained on, so feeding in more good quality cleaned data will help the model to learn. Ultimately, machine learning is aimed at developing a model that is able to predict something from new data, so it needs to be as generalised as possible.\nSo hopefully when I come across a new Pokémon not currently found in my Pokedex, I can easily check whether it is Legendary by asking it a few questions about its vital statistics. Very useful.\nGotta catch ’em all!\n 9 Session Info sessionInfo() ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Sierra 10.12.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] caret_6.0-84 lattice_0.20-38 pheatmap_1.0.12 ## [4] RColorBrewer_1.1-2 ggrepel_0.8.1 dplyr_0.8.3 ## [7] ggplot2_3.2.0 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.2 lubridate_1.7.4 tidyr_1.0.0 ## [4] class_7.3-15 assertthat_0.2.1 zeallot_0.1.0 ## [7] digest_0.6.20 ipred_0.9-9 foreach_1.4.7 ## [10] R6_2.4.0 plyr_1.8.4 backports_1.1.4 ## [13] stats4_3.6.1 evaluate_0.14 e1071_1.7-2 ## [16] blogdown_0.16 pillar_1.4.2 rlang_0.4.0 ## [19] lazyeval_0.2.2 data.table_1.12.2 kernlab_0.9-27 ## [22] rpart_4.1-15 Matrix_1.2-17 rmarkdown_1.14 ## [25] labeling_0.3 splines_3.6.1 gower_0.2.1 ## [28] stringr_1.4.0 munsell_0.5.0 compiler_3.6.1 ## [31] xfun_0.8 pkgconfig_2.0.2 htmltools_0.3.6 ## [34] nnet_7.3-12 tidyselect_0.2.5 tibble_2.1.3 ## [37] prodlim_2018.04.18 bookdown_0.12 codetools_0.2-16 ## [40] randomForest_4.6-14 crayon_1.3.4 withr_2.1.2 ## [43] MASS_7.3-51.4 recipes_0.1.7 ModelMetrics_1.2.2 ## [46] grid_3.6.1 nlme_3.1-140 gtable_0.3.0 ## [49] lifecycle_0.1.0 magrittr_1.5 scales_1.0.0 ## [52] stringi_1.4.3 reshape2_1.4.3 timeDate_3043.102 ## [55] ellipsis_0.2.0.1 generics_0.0.2 vctrs_0.2.0 ## [58] lava_1.6.6 iterators_1.0.12 tools_3.6.1 ## [61] glue_1.3.1 purrr_0.3.3 survival_2.44-1.1 ## [64] yaml_2.2.0 colorspace_1.4-1 knitr_1.23  ","date":1563148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563148800,"objectID":"7e0575f431460f3fc0f92e8877739b08","permalink":"/post/2019-07-15-building-a-pokemon-recommendation-machine/","publishdate":"2019-07-15T00:00:00Z","relpermalink":"/post/2019-07-15-building-a-pokemon-recommendation-machine/","section":"post","summary":"Use of multiple machine learning techniques to explore a database of Pokémon, including creation of a recommendation machine and development of prediction alogorithms for determining Legendary Pokémon","tags":["R","Data Science","Machine Learning","KNN","SVM","Random Forest","Logistic Regression","PCA","Recommendation","Normalization"],"title":"Building a Pokémon Recomendation Machine","type":"post"},{"authors":["Sam Robson"],"categories":[],"content":"The Centre for Enzyme Innovation is a new initiative set up at the University of Portsmouth, with a goal to find solutions to some of the most pressing global environmental problems. In particular, we aim to examine environmental samples to find how the natural world has solved the problem of plastic pollution, by identifying microorganisms who have evolved to be able to break down plastics in the environment. By identifying these microorgansisms, and further understanding the enzymatic methods that they use towards this goal, we aim to harness the amazing work that nature has done, and find ways to enhance this and scale it up on an industrial scale to allow their use in circular recycling of plastics.\nThe Centre was recently awarded £5.8 million in funding from the Research England Expanding Excellence Fund. Together with significant investment from the University of Portsmouth, we aim to use this funding to speed up our progress towards finding a solution to one of the world’s greatest environmental challenges – plastic waste.\nWe will be working in the Discovery phase of the pipeline, aiming to use next generation Nanopore sequencing to explore microorganisms present on samples taken from environmental sites of interest (e.g. plastic receycling centres) to try and profile what is present and how they have developed to survive on plastic substrates. We aim to use this bioprospecting approach to explore metagenomics and metatranscriptomics of samples to identify mechanisms of action for plastic degaradation, and in particular to compare with previously explored species to identify what makes these species unique. This information, and information on the make-up of enzymes involved in the processes, will feed through the pipeline to be further engineered to create enzymes that can be scaled up on an industrial scale. These data will be combined with data from public repositories to feed into novel machine learning algorithms to help to train algorithms for the identification of further potential candidates for feeding through the pipeline.\n","date":1560816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560816000,"objectID":"a32bc40931bf7ed3758f0c1f0f67f277","permalink":"/project/2019-06-18-center-for-enzyme-innovation/","publishdate":"2019-06-18T00:00:00Z","relpermalink":"/project/2019-06-18-center-for-enzyme-innovation/","section":"project","summary":"An innovative new Research Centre to identify novel enzymatic solutions to environmental waste problems such as plastic","tags":["Research Centres","Bioinformatics","Genomics","Transcriptomics","Machine Learning","Whole Genome"],"title":"Center for Enzyme Innovation","type":"project"},{"authors":["Sam Robson"],"categories":[],"content":"The Portsmouth Heritage Hub was set up by Dr Sam Robson, Dr Robert Inkpen and Dr Garry Scarlett, and aims to provide a centralised hub for researchers within the University of Portsmouth currently working on projects involving preservation, conservation, interpretation and education about our local cultural heritage.\nThe Solent, and the surrounding Wessex area has a rich array of history. Portsmouth’s Historic Dockyard, home to world famous ships such as the Mary Rose and HMS Victory, attracts thousands of visitors from around the world, and provides a window into the life of sailors throughout the ages and how this has shaped the City today. The Isle of Wight has proved to be one of the richest sources of dinosaur fossils in all of Europe, and has earned it the title of the UK’s Dinosaur Capital. Fishbourne Roman. Palace in Chichester is the largest Roman residential building to have been discovered in Britain, and offers a unique glimpse into the life of Britons following the Roman conquest. And of course, Portsmouth will today play host to Royalty and international leaders to celebrate its part in the D-Day landings in Normandy, one of the most important operations in World War II.\nThe skills and expertise of researchers throughout the University are multifaceted and span a wide range of themes and fields. These include (but are not limited to) expertise with ancient DNA analysis to understand our past, identifying novel ways to preserve monuments and buildings, interpretation of ancient texts and oral histories, and using modern technology such as virtual reality to provide interactive ways to engage the public with their past.\nThere are many more projects besides, and our aim is to provide a framework for these researchers to work together with local stakeholders in historical monuments, buildings and other sites to identify ways to improve the heritage of our local area, to interpret our history to better inform our understanding of our past, and to provide new and interesting ways to engage the local area with our history.\n","date":1560816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560816000,"objectID":"a380e1c37e30c875bf0d76e378981045","permalink":"/project/2019-06-18-portsmouth-heritage-hub/","publishdate":"2019-06-18T00:00:00Z","relpermalink":"/project/2019-06-18-portsmouth-heritage-hub/","section":"project","summary":"A network for researchers and stakeholders in local historical monuments and buildings to develop collaborative research projects with an aim to conserve, interpret, preserve and educate about our local heritage","tags":["Research Centres","Heritage"],"title":"Portsmouth Heritage Hub","type":"project"},{"authors":null,"categories":["Blog","Heritage"],"content":"On Friday 7th June, the Mary Rose Museum in the Portsmouth Historic Dockyard hosted the inaugural meeting of the Portsmouth Heritage Hub (PHH). The workshop aimed to bring together around 30 researchers from the University of Portsmouth with interests and expertise in the heritage field, with members of 15 organisations associated with local historical artefacts, architecture and monuments in Portsmouth and the surrounding area. It was a fantastic opportunity to foster collaborative research networks between people with the skills and expertise to solve problems, with stakeholders in our history with problems to solve. The ultimate aim of the PHH is to enrich the community through preservation, conservation, interpretation and education about our rich and unique regional history \u0026ndash; from the dinosaurs of the Cretaceous Period, through the maritime history of our island port city, to our place in history as the launch point for much of the D-Day Landing fleet in 1944, recently celebrated by World Leaders on the 75th Anniversary of this pivotal turning point to World War II. In this way, we can help to protect our heritage, understand how it has shaped our life today, and how it might inform our future.\nThe day began with an introduction from Prof. Leila Choukroune, Director of the University of Portsmouth Democratic Citizenship Theme, who kindly sponsored the event. She outlined the goals of the hub, and the need to develop close working relationships between the University and our surrounding community.\nA series of short talks followed, which highlighted the vast array of multidisciplinary work currently ongoing within the field. The talks were opened by Dr. Sam Robson, a researcher from the School of Pharmacy and Biomedical Science, who talked about how he and other researchers at the University have applied cutting edge genetic research to learn more about members of the Mary Rose crew, in a program of work recently highlighted in the Channel 4 documentary Skeletons of the Mary Rose: The New Evidence.\nNext, Dr. Joy Watts from the School of Biological Sciences talked about work that she has been conducting with Fishbourne Roman Palace, looking at ways of understanding the effects of microorganisms growing on the ruins, and how these can be targeted to better preserve the architecture.\nDr Richard Madgwick, an osteological researcher from Cardiff University who has previously worked on projects with the Mary Rose Museum, talked about some recent work looking at methods of tracking the origin of early settlers through analysis of livestock remains. His work showed that feasts and rituals in Late Neolithic sites such as Stonehenge may have attracted attendees from as far away as Scotland, North East England and West Wales.\nMartin Munt, Curator and General Manager of the Dinosaur Isle Museum on the Isle of Wight, moved us away from the scientific to give us a fascinating overview of the Isle of Wight’s rich treasure trove of fossilized dinosaur remains. With over 30,000 specimens, the region is the richest dinosaur locality in Europe, and the museum works hard not only to engage in significant research projects with the University, but also to engage the public through guided tours along the shore line where many specimens were discovered.\nDr Karl Bell, a researcher in the School of Social, Historical and literary Studies, next led a fascinating talk about his work on understanding the social and cultural context of Portsmouth and port towns across the globe, as unique sites of socio-cultural exchange. In particular, he gave an insight into his work on assembling and preserving local folklore and urban myth as a form of intangible heritage.\nDr Tarek Teba from Portsmouth School of Architecture gave an introduction to some of the many modern methods that he is currently using to create digital reconstructions of ruins as a way to bring context to visitors that may otherwise be missing when visiting the sites. The ability to explore long-dead civilisations in “game-ified” recreations offers a tangible way to interact with our past history.\nFinally, Dr Alexzandra Hildred, Head of research and Curator of Ordnance and Human Remains at the Mary Rose Trust, gave an incredibly revealing talk about the wealth of research projects undertaken by the trust. From developing ways to preserve the timbers of the ship that have led to the incredible museum exhibit that we see today, through using cutting edge technology to glean understanding from the many artefacts discovered, to using genetics, osteology and morphological analysis to understand who the men of the Mary Rose really were.\nFollowing an incredibly incisive tour around the Mary Rose Museum, where the immense work conducted to preserve this 500 year old ship and its inhabitants can be seen, participants returned to take part in a series of workshops to help guide the direction of the PHH. The first workshop focussed on identifying key focus themes for members of the hub, to understand where the critical mass of our members might lie. The focus areas that were identified and discussed throughout the workshop were:\n Heritage buildings and structures New ways of interpreting the past Conserving and preserving A changing Portsmouth How we got here: How the past has influenced the present Recreating the past Heritage and the community  Discussions were insightful and provide a huge amount of information to help us to develop the direction in which the hub should be headed. Much of the discussion is currently being digested to determine the next steps that we hope to take.\nThe second workshop had a more focussed aim; to identify short term and long-term projects that might be easily developed into funding-generating projects able to capitalize on the networks that we had built up through the day. The session did not disappointment, and a number of incredibly exciting projects arose that instantly show the worth of collaborative events such as this.\nLooking forward to the future, we aim to maintain the momentum that the workshop has generated, and attract even more members to promote further cross-disciplinary working to help galvanise work within the heritage sphere. The first projects identified for short-term grant applications will be explored in more specific workshops, whilst in the meantime we will explore the incredibly exciting longer term goals that have come out from our work together. The outcome of the PHH working in partnership with local heritage stakeholders will be an improvement to maintenance of our local community history, helping to show the benefits and positive role that academic institutions can play in developing the local community. We look fo\n","date":1560643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560643200,"objectID":"ae88151527f7efe5f4f91e5bc7a58522","permalink":"/post/2019-06-16-portsmouth-heritage-hub-inaugural-workshop/","publishdate":"2019-06-16T00:00:00Z","relpermalink":"/post/2019-06-16-portsmouth-heritage-hub-inaugural-workshop/","section":"post","summary":"Introduction to the Portsmouth Heritage Hub and results from the inaugural Heritage Hub meeting at the Mary Rose Museum.","tags":["University of Portsmouth","Portsmouth Heritage Hub"],"title":"Portsmouth Heritage Hub Inaugural Workshop","type":"post"},{"authors":["Sam Robson"],"categories":[],"content":"As well as collaborating with researchers from throughout the School of Pharmacy and Biomedical Sciences, we are also working to develop several tools for use alongside commonly used and freely available analysis tools. These tools will be made available via the University of Portsmouth GitHub account.\nThis includes development of a next generation sequencing data analysis platform created by Dr. Sam Robson, which allows for processing, mapping and analysis of a number of data types, including data generated from ChIP-Seq, RNA-Seq, 16S.rDNA amplicon sequencing, whole genome sequencing and ancient DNA sequencing approaches.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"1b2a1a0b3848ec3758d7aab812e505d0","permalink":"/project/2017-08-17-development-of-bioinformatics-tools/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/project/2017-08-17-development-of-bioinformatics-tools/","section":"project","summary":"Development of in-house bioinformatics tools and analysis models for use in combination with publicly available data analysis software","tags":["Bioinformatics","NGS"],"title":"Development of Bioinformatics Tools","type":"project"},{"authors":["Samudyata","Amaral, PP","Engström, PG","Sam Robson","Nielsen, ML","Kouzarides, T","Castelo-Branco, G"],"categories":null,"content":"","date":1557360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557360000,"objectID":"c1376507e20418960a9ef5a607be4844","permalink":"/publication/2019_interaction_of_sox2_with_rna_binding_proteins_in_mouse_embryonic_stem_cells/","publishdate":"2019-05-09T00:00:00Z","relpermalink":"/publication/2019_interaction_of_sox2_with_rna_binding_proteins_in_mouse_embryonic_stem_cells/","section":"publication","summary":"Sox2 is a master transcriptional regulator of embryonic development. In this study, we determined the protein interactome of Sox2 in the chromatin and nucleoplasm of mouse embryonic stem (mES) cells. Apart from canonical interactions with pluripotency-regulating transcription factors, we identified interactions with several chromatin modulators, including members of the heterochromatin protein 1 (HP1) family, suggesting a role of Sox2 in chromatin-mediated transcriptional repression. Sox2 was also found to interact with RNA binding proteins (RBPs), including proteins involved in RNA processing. RNA immunoprecipitation followed by sequencing revealed that Sox2 associates with different messenger RNAs, as well as small nucleolar RNA Snord34 and the non-coding RNA 7SK. 7SK has been shown to regulate transcription at regulatory regions, which could suggest a functional interaction with Sox2 for chromatin recruitment. Nevertheless, we found no evidence of Sox2 modulating recruitment of 7SK to chromatin when examining 7SK chromatin occupancy by Chromatin Isolation by RNA Purification (ChIRP) in Sox2 depleted mES cells. In addition, knockdown of 7SK in mES cells did not lead to any change in Sox2 occupancy at 7SK-regulated genes. Thus, our results show that Sox2 extensively interact with RBPs, and suggest that Sox2 and 7SK co-exist in a ribonucleoprotein complex whose function is not to regulate chromatin recruitment, but might rather regulate other processes in the nucleoplasm.","tags":[""],"title":"Interaction of Sox2 with RNA binding proteins in mouse embryonic stem cells","type":"publication"},{"authors":["Pandolfini, L","Barbieri, I","Bannister, AJ","Hendrick, A","Andrews, B","Webster, N","Murat, P","Mach, P","Brandi, R","Sam Robson","Migliori, V","Alendar, A","D’Onofrio, M","Balasubramanian, S","Kouzarides, T"],"categories":null,"content":"","date":1556150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556150400,"objectID":"bd3dc8290d337cb33d91f5a714d73c58","permalink":"/publication/2019_mettl1_promotes_let-7_microrna_processing_via_m7g_methylation/","publishdate":"2019-04-25T00:00:00Z","relpermalink":"/publication/2019_mettl1_promotes_let-7_microrna_processing_via_m7g_methylation/","section":"publication","summary":"7-methylguanosine (m7G) is present at mRNA caps and at defined internal positions within tRNAs and rRNAs. However, its detection within low-abundance mRNAs and microRNAs (miRNAs) has been hampered by a lack of sensitive detection strategies. Here, we adapt a chemical reactivity assay to detect internal m7G in miRNAs. Using this technique (Borohydride Reduction sequencing [BoRed-seq]) alongside RNA immunoprecipitation, we identify m7G within a subset of miRNAs that inhibit cell migration. We show that the METTL1 methyltransferase mediates m7G methylation within miRNAs and that this enzyme regulates cell migration via its catalytic activity. Using refined mass spectrometry methods, we map m7G to a single guanosine within the let-7e-5p miRNA. We show that METTL1-mediated methylation augments let-7 miRNA processing by disrupting an inhibitory secondary structure within the primary miRNA transcript (pri-miRNA). These results identify METTL1-dependent N7-methylation of guanosine as a new RNA modification pathway that regulates miRNA structure, biogenesis, and cell migration.","tags":[""],"title":"METTL1 Promotes let-7 MicroRNA Processing via m7G Methylation","type":"publication"},{"authors":["Róg, J","Oksiejuk, A","Gosselin, MRF","Brutkowski, W","Dymkowska, D","Nowak, N","Sam Robson","Górecki, DC","Zabłocki, K"],"categories":null,"content":"","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"b9726ccc39dcefc9773305f93534e0aa","permalink":"/publication/2019_dystrophic_mdx_mouse_myoblasts_exhibit_elevated_atp_utp-evoked_metabotropic_purinergic_responses_and_alterations_in_calcium_signalling/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/2019_dystrophic_mdx_mouse_myoblasts_exhibit_elevated_atp_utp-evoked_metabotropic_purinergic_responses_and_alterations_in_calcium_signalling/","section":"publication","summary":"Pathophysiology of Duchenne Muscular Dystrophy (DMD) is still elusive. Although progressive wasting of muscle fibres is a cause of muscle deterioration, there is a growing body of evidence that the triggering effects of DMD mutation are present at the earlier stage of muscle development and affect myogenic cells. Among these abnormalities, elevated activity of P2X7 receptors and increased store-operated calcium entry myoblasts have been identified in mdx mouse. Here, the metabotropic extracellular ATP/UTP-evoked response has been investigated. Sensitivity to antagonist, effect of gene silencing and cellular localization studies linked these elevated purinergic responses to the increased expression of P2Y2 but not P2Y4 receptors. These alterations have physiological implications as shown by reduced motility of mdx myoblasts upon treatment with P2Y2 agonist. However, the ultimate increase in intracellular calcium in dystrophic cells reflected complex alterations of calcium homeostasis identified in the RNA seq data and with significant modulation confirmed at the protein level, including a decrease of Gq11 subunit α, plasma membrane calcium ATP-ase, inositol-2,4,5-trisphosphate-receptor proteins and elevation of phospholipase Cβ, sarco-endoplamatic reticulum calcium ATP-ase and sodium‑calcium exchanger. In conclusion, whereas specificity of dystrophic myoblast excitation by extracellular nucleotides is determined by particular receptor overexpression, the intensity of such altered response depends on relative activities of downstream calcium regulators that are also affected by Dmd mutations. Furthermore, these phenotypic effects of DMD emerge as early as in undifferentiated muscle. Therefore, the pathogenesis of DMD and the relevance of current therapeutic approaches may need re-evaluation.","tags":[""],"title":"Dystrophic mdx mouse myoblasts exhibit elevated ATP/UTP-evoked metabotropic purinergic responses and alterations in calcium signalling","type":"publication"},{"authors":["Tzelepis, K","De Braekeleer, E","Aspris, D","Barbieri, I","Vijayabaskar, MS","Liu, W","Gozdecka, M","Metzakopian, E","Toop, HD","Dudek, M","Sam Robson","Hermida-Prado, F","Yang, YH","Babaei-Jadidi, R","Garyfallos, DA","Ponstingl, H","Dias, JML","Gallipoli, P","Seiler, M","Buonamici, S","Vick, B","Bannister, AJ","Rad, R","Prinjha, RK","Marioni, JC","Huntly, B","Batson, J","Morris, JC","Pina, C","Bradley, A","Jeremias, I","Bates, DO","Yusa, K","Kouzarides, T","Vassiliou, GS"],"categories":null,"content":"","date":1545177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545177600,"objectID":"7fd26588f1aed859593a481962335824","permalink":"/publication/2018_srpk1_maintains_acute_myeloid_leukemia_through_effects_on_isoform_usage_of_epigenetic_regulators_including_brd4/","publishdate":"2018-12-19T00:00:00Z","relpermalink":"/publication/2018_srpk1_maintains_acute_myeloid_leukemia_through_effects_on_isoform_usage_of_epigenetic_regulators_including_brd4/","section":"publication","summary":"We recently identified the splicing kinase gene SRPK1 as a genetic vulnerability of acute myeloid leukemia (AML). Here, we show that genetic or pharmacological inhibition of SRPK1 leads to cell cycle arrest, leukemic cell differentiation and prolonged survival of mice transplanted with MLL-rearranged AML. RNA-seq analysis demonstrates that SRPK1 inhibition leads to altered isoform levels of many genes including several with established roles in leukemogenesis such as MYB, BRD4 and MED24. We focus on BRD4 as its main isoforms have distinct molecular properties and find that SRPK1 inhibition produces a significant switch from the short to the long isoform at the mRNA and protein levels. This was associated with BRD4 eviction from genomic loci involved in leukemogenesis including BCL2 and MYC. We go on to show that this switch mediates at least part of the anti-leukemic effects of SRPK1 inhibition. Our findings reveal that SRPK1 represents a plausible new therapeutic target against AML.","tags":[""],"title":"SRPK1 maintains acute myeloid leukemia through effects on isoform usage of epigenetic regulators including BRD4","type":"publication"},{"authors":["Millan-Zambrano G","Santos-Rosa H","Puddu F","Sam Robson","Jackson S","Kouzarides T"],"categories":null,"content":"","date":1542240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542240000,"objectID":"2fcab5c2c2299107c64116b6effc416d","permalink":"/publication/2018_phosphorylation_of_histone_h4t80_triggers_dna_damage_checkpoint_recovery/","publishdate":"2018-11-15T00:00:00Z","relpermalink":"/publication/2018_phosphorylation_of_histone_h4t80_triggers_dna_damage_checkpoint_recovery/","section":"publication","summary":"In response to genotoxic stress, cells activate a signaling cascade known as the DNA damage checkpoint (DDC) that leads to a temporary cell cycle arrest and activation of DNA repair mechanisms. Because persistent DDC activation compromises cell viability, this process must be tightly regulated. However, despite its importance, the mechanisms regulating DDC recovery are not completely understood. Here, we identify a DNA-damage-regulated histone modification in Saccharomyces cerevisiae, phosphorylation of H4 threonine 80 (H4T80ph), and show that it triggers checkpoint inactivation. H4T80ph is critical for cell survival to DNA damage, and its absence causes impaired DDC recovery and persistent cell cycle arrest. We show that, in response to genotoxic stress, p21-activated kinase Cla4 phosphorylates H4T80 to recruit Rtt107 to sites of DNA damage. Rtt107 displaces the checkpoint adaptor Rad9, thereby interrupting the checkpoint-signaling cascade. Collectively, our results indicate that H4T80ph regulates DDC recovery.","tags":[""],"title":"Phosphorylation of histone H4T80 triggers DNA damage checkpoint recovery","type":"publication"},{"authors":null,"categories":["Blog","Data Science","Running","R"],"content":"  1 Note 2 Introduction 3 Data Are Key 4 Get to the point 5 Full disclosure 6 Analysis 6.1 Delayed uploads 6.2 “Mimi is pausing her watch” 6.3 Dodgy fluctuations in cadence data 6.4 Mimi running in the 185-195 steps per minute range: 6.5 Spoofed data  7 Conclusion 8 Summary 9 Session Info   1 Note This blog post was originally written for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.\nI ran most of this analysis one evening at the beginning of the week before the sad news that Mimi was going to give up her World Record attempt due to injury. Whether Mimi’s data is valid to claim the World Record is now moot, but the effect of the damage to her reputation is not. She is understandably devastated at the turn of events, and I can only hope to alleviate some of her grief with what I have written here.\n 2 Introduction On 7th September 2017, the Marvellous Mimi Anderson began a world record attempt to run across the United States of America. She is planning on running the 2,850 miles in 53 days to beat the current Guinness World Record of 69 days 2 hours 40 minutes by Mavis Hutchinson from South Africa back in 1979.\nMimi is somewhat of an institution in the UK, and can often be seen both running and crewing at many races in her trademark pink ensemble. I have run with her on many occasions, and have seen first hand her amazing ability at running stupid races, even going so far as to make them more stupid by running there and back again on races such as Badwater, the Grand Union Canal Race, and Spartathlon. She holds records for running across Ireland and across the UK, so going for the USA record is a natural progression for somebody who loves hunting for ever bigger challenges.\nUnfortunately, Over the past year, we have seen some controversial stunt runs from the likes of Mark Vaz (who drove 860 miles very slowly from Lands End to John O’ Groats to “smash” the previous running record), Dave Reading (who tried to do the same, but was “cyber-bullied” into giving up apparently), and Marathon Man UK himself Robert Young (who sat in the back of an RV and was slowly driven across America until some geezers made him run a bit and he hurt himself). I confess that when the Mark Vaz thing happened I could genuinely not believe that somebody would actually do that. I mean, what a waste of time - and for what?! I did not believe that somebody would go to that much effort for such a niche accolade, but that was exactly the point. To most people, running across the country is already pretty ridiculous, but they don’t have any baseline for what a good time would be. Is 7 days, 18 hours and 45 minutes good? Of course the running community knew that the time was bloody incredible for the top ultrarunners in the country, never mind an overweight window cleaner in a puffa jacket.\nWhen Robert Young attempted his transcontinental run, it was a different kettle of fish. Robert had developed somewhat of a following as an inspirational runner, having started running on a whim to complete 370 marathons in a year, beating Dean Karnazes “record” for running without sleeping, and winning the Race Across the USA. So he had a history of running long, but that didn’t stop questions from being asked. In particular, posters at the LetsRun forum smelled a rat very quickly, after one of their own went out to run with him in the middle of the night only to find a slow moving RV with no runner anywhere to be seen. After a lot of vehement denial from the Robert Young fans, the sleuths over at LetsRun were able to provide enough evidence for Robert’s main sponsor, Skins, to hire independent experts to prepare a report on whether or not any subterfuge had occurred. The results were not good for the Marathon Man UK brand (although to this day he denies any wrongdoings).\nThe main take-home message from the Robert Young (RY) affair was that data transparency is key. RY refused to provide his Strava data for people to check, but to be fair he had a very good reason - he hadn’t got around to doctoring it yet. So anybody looking to take on this record would have to work very hard to ensure that their data was squeaky clean and stood up to scrutiny.\n 3 Data Are Key In leading up to the challenge, Mimi gave several interviews where the RY affair and, in particular, the importance of data transparency were discussed. And it was pretty clear that this was most definitely clear to Mimi. She was aware of the LetsRun community, and the importance of making her attempt as free from controversy as possible. She had arranged for a RaceDrone tracker to be used to follow her progress in real time, and would be using four different GPS watches (two at any one time for redundancy) uploading the data immediately following every run. In addition, as required by Guinness World Records, they would obtain witness testimonies along the way. Add in social media to provide a running commentary and it would seem to be foolproof.\nUnfortunately it did not work out that way. The LetsRun board was already lighting up with posts questioning her attempt (EDIT while the posts were skeptical from the start, accusations of foul play did not begin until after the first few weeks with the tracker issues). In addition, a second runner - Sandra Villines (aka Sandra Vi) - was joining in the fun, and was planning to also attempt the record (albeit taking a different route) 4 days days later. Suddenly we had a race on.\nUnfortunately, within the first few weeks, questions surrounding Mimi’s attempts had surfaced, largely as a result of failures in the tracker. There are contradicting reports about what happened in this time, and I don’t pretend to understand all of them. Mimi’s crew claim that the issues were due to lack of coverage, whilst RaceDrone’s founder Richard Weremuik claims that the trackers were intentionally turned off. If Richard’s claims are correct, it raises a lot of serious concerns.\nIn addition, there have been several other events that have received a lot of criticism, including against the reaction of Mimi’s fans to questions of her integrity (“trolls”, “haterz gonna hate”, etc.), and an incident whereby a stalker presumed (by Mimi’s crew) to be a poster on the LetsRun forum was arrested, despite no record of an arrest being found and no poster admitting to it (which they likely would). Since then, the whole run has been torn apart, and in particular she is now accused of faking the only source of data that is consistently available for review - her Strava data.\n 4 Get to the point There are definitely things to do with this whole affair that I cannot comment on, such as the Race Drone incident and the arrest report. I also agree with many of the detractors that Sandy’s set up seems to be a far more open approach, and seems to be a good gold standard to use in the future. You will get no arguments from me that mistakes were made and perhaps things should have been done differently. But I genuinely believe that Mimi went out to the USA in the belief that what she had planned was foolproof and would cover all bases and supply the necessary evidence to convince anybody that might choose to look into it. Any mistakes were a result of the fact that Mimi has limited knowledge of technology - by her own omission she is a Luddite. Having said that, it is clear that the focus was on satisfying primarily the requirements of Guinness, which most runners in my experience consider to be very weak.\nHowever, what does concern me is the insinuation of fabricating her data, so I want to tackle some of these allegations to see if I can help defend her reputation. If I do find evidence of fabrication, so be it. But the idea that “data can be faked therefore we cannot believe any of it” is absurd. All data can be faked. Of course they can. But if we followed this impetus to discount all data out of hand, then scientific research would very quickly stall. What we have here is a peer review process - of course the burden of proof is on Mimi to provide evidence of her claims, but she has done that with daily Strava uploads (already a big improvement over Rob Young). If you subsequently suspect the data are forged, the onus is on you to show evidence of that.\nThere are certainly some inconsistencies that need addressing and I am hoping that I can address some of these here. I don’t for one minute believe that I am covering all of the issues here. I am sure there are many more that will be pointed out to me (at over 200 pages, I really don’t feel like wading through the entire thread to pull everything out), but I figured I would make a start. This is all fairly rough and these are basic analyses conducted over a very short period of time, but I hope to look into things using some more statistical methods in a follow-up post.\n 5 Full disclosure In full disclosure, I consider Mimi to be a friend. I have run with her many times, including running over 50 miles together on a recce for the Viking Way, and have seen first hand that she is an incredibly accomplished runner who has achieved amazing things. I don’t expect this to mean anything, and completely understand that plenty of people came out saying similar things for Robert Young, I just wanted to lay my cards in the table. I am, however, typically objective when it comes to data, so I am trying to look at this without letting too many of my biases interfere. For the record, I think that her pedigree and the changes seen in her body over the last few weeks should themselves clearly distinguish her from Rob Young. She is clearly doing something out there and not just riding in an RV.\nHaving said that, I am not of the opinion that anybody that questions these runs (or indeed anything) are haterz and trolls. Extraordinary claims require extraordinary evidence, and if you are not prepared to provide that and have it interrogated then you shouldn’t be doing it. With fame comes a loss of anonymity. I believe that mistakes were made at the start of this run, and I believe that the impulse to fight back against criticism with a similar tact used by Mark Vaz, Robert Young and Dave Reading was the wrong choice. But then I am a naive fool who thinks that everybody is reasonable and open to logical discourse - I suspect I am about to be schooled on this thought.\nIn all honesty I have stayed away from this whole debacle for exactly that reason. I do not like the “them vs us” mentality that seems to crop up in all of these discussions. Be that Brits vs Americans, URC vs LR, whatever. I think that the LetsRun community have done an amazing job at routing out cheats over the years, and without them many people would be reaping the benefits of false claims. I have no problem with witch hunts. Witch hunts are only a problem if you don’t live in a world where witches demonstrably exist. I don’t want to feed into that, I don’t want to make enemies here - I just want to help a friend by providing an alternative perspective.\nMy aim here then is to look (hopefully) objectively at some of the claims against Mimi in her transcon attempt. I work in a data-heavy scientific discipline, and I believe that I can remain objective in this, although it should be fairly clear already that I know what I am hoping to see. But believe me when I say that if I find something that I do not like I will not hide it. I do not claim anything I say is any more valid than what anybody else says, and I do not want confirmation bias to creep in from Mimi’s fellow supporters. I just want all of the facts to be available to allow people to make an informed assessment. If anyone disagrees with any of my conclusions, or if you identify errors, by all means get in touch and I will try and follow up.\nI decided to write this up as a blog post as the message that I put together for the LetsRun forum got a bit ridiculous, and I thought that this way I could attach and annotate my figures, flesh out my thoughts, and importantly include my code for full transparency.\nA lot of the data that I am showing here is based on runs between 1st October and 7th October. I chose these as these overlap with the faked data generated by Scam_Watcheroo a user on LetsRun (who also runs the Marathon Investigation website) (EDIT) has looked at a lot of these data and made many of the claims of faked data. In addition, he was able to generate spoofed data files for a fake world record run to show how easy it is to do, which I will incorporate into these analyses.\nI have also included a short run that Mimi did before beginning her transcon run, which is the only other run available on Strava (presumably this was done to test the data upload). I figured that this could be useful as a baseline of her “normal” running, but of course you could always argue that this was also fabricated.\nI obtained the .gpx files directly from Strava by using a tool that is able to slurp the data directly from the Strava session webpage. I can obviously repeat any of this for other data sets, but I had to start somewhere and my free time to look at these things is limited.\nFor each day in this period, I downloaded runs for Mimi, Sandra, and the spoofed files. I also downloaded a few of my own runs (much shorter) as a baseline as I am quite confident that these files are genuine. My thesis is that the spoofed data should be identifiable as such, while the other data sets should (hopefully) stand up to scrutiny. Note that Sandra’s data are single runs per day, whereas Mimi uploads two per day.\nI am using R, which is a freely available and well maintained statistical programming language, for these analyses. I haven’t had time to do too much yet, so here I am concentrating on the cadence data as that seems to be the most contentious point at the moment. In a follow up post I will look at the whole run thus far and look at more factors such as the distance traveled and correlations with terrain (which I haven’t even touched so far).\n 6 Analysis First of all let’s load in the packages I will be using:\nlibrary(XML) library(plyr) library(geosphere) library(pheatmap) library(ggplot2) library(benford.analysis) library(dplyr) Next I will load in the .gpx files. They are basically .xml (Extensible Markup Language) files, so a bit of parsing is required to get them into a usable data.frame format:\n## I/O directory root_dir \u0026lt;- \u0026quot;../../static/post/2017-10-18-assessing-Mimi-Andersons-World_Record-run_files/\u0026quot; ## Load in the gpx data for each run all_gpx \u0026lt;- list() for (n in c(\u0026quot;Mine\u0026quot;, \u0026quot;SandraVi\u0026quot;, \u0026quot;MimiAnderson\u0026quot;, \u0026quot;Fake\u0026quot;)) { dirname \u0026lt;- paste0(root_dir, n) all_fnames \u0026lt;- list.files(dirname) for (fname in all_fnames) { ## Load .gpx and parse to data.frame gpx_raw \u0026lt;- xmlTreeParse(paste0(dirname, \u0026quot;/\u0026quot;, fname), useInternalNodes = TRUE) rootNode \u0026lt;- xmlRoot(gpx_raw) gpx_rawlist \u0026lt;- xmlToList(rootNode)[[\u0026quot;trk\u0026quot;]] gpx_list \u0026lt;- unlist(gpx_rawlist[names(gpx_rawlist) == \u0026quot;trkseg\u0026quot;], recursive = FALSE) gpx \u0026lt;- do.call(rbind.fill, lapply(gpx_list, function(x) as.data.frame(t(unlist(x)), stringsAsFactors=F))) ## Convert cadence and GPS coordinates to numeric for (i in c(\u0026quot;extensions.cadence\u0026quot;, \u0026quot;.attrs.lat\u0026quot;, \u0026quot;.attrs.lon\u0026quot;)) { gpx[[i]] \u0026lt;- as.numeric(gpx[[i]]) } ## Convert cadence to steps per minute gpx[[\u0026quot;extensions.cadence\u0026quot;]] \u0026lt;- gpx[[\u0026quot;extensions.cadence\u0026quot;]] * 2 ## Convert time to POSIXct date-time format gpx[[\u0026quot;time\u0026quot;]] \u0026lt;- as.POSIXct(gpx[[\u0026quot;time\u0026quot;]], format=\u0026quot;%Y-%m-%dT%H:%M:%S\u0026quot;) ## Calculate the time difference between data points gpx[[\u0026quot;time.diff\u0026quot;]] \u0026lt;- c(0, (gpx[-1, \u0026quot;time\u0026quot;] - gpx[-nrow(gpx), \u0026quot;time\u0026quot;])) ## Calculate the shortest distance between successive points (in miles) gpx[[\u0026quot;dist.travelled\u0026quot;]] \u0026lt;- c(0, distHaversine(gpx[-nrow(gpx), c(\u0026quot;.attrs.lon\u0026quot;, \u0026quot;.attrs.lat\u0026quot;)], gpx[-1, c(\u0026quot;.attrs.lon\u0026quot;, \u0026quot;.attrs.lat\u0026quot;)], r = 3959)) # Radius of earth in miles ## Save to main list all_gpx[[n]][[gsub(\u0026quot;\\\\.gpx\u0026quot;, \u0026quot;\u0026quot;, fname)]] \u0026lt;- gpx } } Now let’s consider some of the specific criticisms being made against Mimi.\n6.1 Delayed uploads One of the criticisms that comes up regarding Mimi’s practice is that it sometimes takes a long time for the data to be uploaded to Strava. I believe that it is typically up within a couple of hours (EDIT - there were also times when the uploads were not made for several days which is obviously more than a delay in tranferring the data), but many people suggest that anything longer than 15 minutes is unacceptable as it provides time to doctor the data. I mean, I guess that this is true, but it is my understanding that, given Mimi’s lack of technological prowess, her crew is using the method that requires the least amount of knowledge; that being syncing to a phone via Bluetooth, which will then upload to Movescount when there is a wifi or mobile data signal. I do this sometimes with my own runs and it takes bloody ages to sync, and that doesn’t take into account the time to then takes to upload it from the phone to Movescount, dealing with blackspots, etc. So it does not surprise me in the least that she rarely uploads things within 15 minutes of stopping. This is not evidence based by any means, just an observation from my own experience (and something others have pointed out). Is this good practice for somebody out to claim a world record? Perhaps not. Is it evidence of subterfuge? I don’t know, but personally I doubt it.\n 6.2 “Mimi is pausing her watch” In page 201 of the LetsRun thread, user So Far Today says the following:\n Elapsed time is total time from start of the day until end of the day. Sandy’s Strava is based on total elapsed time. Mimi’s excludes lunch breaks, and it appears it it also excludes other mini-breaks. If you want to figure out actual running time for Sandy remove the lunch breaks from the total elapsed time.\n Now, these guys have been looking into this in a heck of a lot more detail than I have, so I apologise if I have got this wrong here or misunderstood. But the idea that Mimi’s data excludes mini-breaks disagrees with something that I noticed right at the start of this analysis. Let’s look at the time difference between successive data points for Mimi’s data using the table() function which will simply count the number of occurrences of each time delay between data points:\nlapply(all_gpx[[\u0026quot;MimiAnderson\u0026quot;]], FUN = function (x) table(x[[\u0026quot;time.diff\u0026quot;]])) $MimiAnderson_170828_preUSA\n0 1 2 5 1 4160 4 1\n$MimiAnderson_171001_afternoon\n0 1 2 1 25883 11  $MimiAnderson_171002_afternoon\n0 1 2 1 26174 31  $MimiAnderson_171002_morning\n0 1 2 3 1 29046 44 1  $MimiAnderson_171003_afternoon\n0 1 2 1 10926 12  $MimiAnderson_171003_morning\n0 1 2 3 1 30896 7 1  $MimiAnderson_171004_afternoon\n0 1 2 3 4 1 24567 19 1 1  $MimiAnderson_171004_morning\n0 1 2 1 28343 6  $MimiAnderson_171005_afternoon\n0 1 2 1 18492 42  $MimiAnderson_171005_morning\n0 1 2 3 1 32400 34 2  $MimiAnderson_171006_afternoon\n0 1 2 3 1 26461 7 1  $MimiAnderson_171006_morning\n0 1 2 7 1 27747 42 1  $MimiAnderson_171007_afternoon\n0 1 2 3 1 21215 36 1  $MimiAnderson_171007_morning\n0 1 2 1 32114 27  $MimiAnderson_171012_morning\n0 1 2 1 28411 17  So every run has one 0 (which is a result of the way that I have calculated the time difference, so the first value is always 0), a few sporadic 2-7 sec intervals (presumably due to brief signal drop out and the like), but the vast majority are 1 sec. This is because Mimi has her watch set to 1 sec recording, and leaves it on for the duration of the run. From this I suggest that the assertion made that Mimi stops her watch for lunch breaks etc. is false.\nMy watch is set to the same sampling rate, and I see exactly the same thing with my data (albeit with fewer errant digits):\nlapply(all_gpx[[\u0026quot;Mine\u0026quot;]], FUN = function (x) table(x[[\u0026quot;time.diff\u0026quot;]])) $mine_170514\n0 1 2 1 9790 1\n$mine_170521\n0 1 2 1 6860 1\n$mine_170525\n0 1 1 3056\n$mine_170603\n0 1 1 5531\n$mine_170618\n0 1 1 11297  Interestingly, however, when we look at Sandra’s data we see a completely different result:\nlapply(all_gpx[[\u0026quot;SandraVi\u0026quot;]], FUN = function (x) table(x[[\u0026quot;time.diff\u0026quot;]])) $SandraVi_171002\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1 82 79 77 61 57 48 51 65 126 332 348 335 160 51 14 4 1 18 21 22 29 30 32 35 38 39 43 47 48 51 54 55 61 63 66 1 1 2 1 1 1 1 1 3 1 1 1 2 1 1 1 1 1 86 89 133 1 1 1\n$SandraVi_171003\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1 243 238 233 187 192 172 210 305 1459 1424 401 142 54 40 15 16 17 18 19 20 21 22 23 24 25 27 28 30 31 17 7 1 2 4 1 4 2 1 2 2 1 1 1 1 33 34 35 36 37 39 40 41 44 45 46 47 49 50 51 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 52 53 55 57 58 59 60 63 64 65 66 73 79 82 83 2 3 3 1 1 1 1 1 1 1 1 1 1 1 1 88 89 97 100 102 113 118 119 133 136 162 166 168 183 223 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 249 266 1 1\n$SandraVi_171004\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1 299 425 356 333 267 311 311 697 1708 1022 327 157 76 33 15 16 17 18 19 20 21 22 23 24 26 27 28 29 32 26 15 6 6 1 1 1 3 1 3 1 1 3 1 1 33 35 36 37 39 41 43 44 49 50 51 52 55 56 57 2 2 1 1 4 1 1 1 1 1 2 1 1 2 1 58 59 61 62 64 67 70 74 75 84 88 94 98 130 149 1 1 2 1 1 2 1 1 1 1 1 3 1 1 1 169 1\n$SandraVi_171005\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1 173 167 157 133 156 129 158 400 1891 1393 259 109 53 55 15 16 17 18 19 20 21 22 23 24 26 27 29 30 32 16 6 5 2 1 5 1 1 2 1 1 1 1 1 1 33 34 35 36 37 39 41 44 46 49 50 52 61 68 74 2 1 1 1 2 1 1 1 1 1 3 1 1 2 1 75 76 93 128 138 140 145 153 163 233 1 1 1 1 2 1 1 2 2 1\n$SandraVi_171006\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1 112 128 123 132 115 122 101 291 1658 1696 358 85 42 33 15 16 17 18 19 20 24 27 28 32 35 37 42 46 49 8 5 4 2 1 2 1 2 1 2 1 1 1 1 2 51 57 58 60 65 67 76 77 78 86 88 90 96 98 99 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 106 108 109 126 128 129 147 164 173 196 205 1 1 1 1 1 1 1 1 1 1 1\n$SandraVi_171007\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1 57 85 83 73 81 74 64 197 2038 1707 275 95 64 39 15 16 17 18 22 23 27 28 31 32 38 39 40 42 45 17 11 2 1 2 1 1 5 2 1 1 1 3 3 1 46 49 53 54 55 60 65 67 77 84 109 115 126 128 149 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 151 154 157 181 182 189 225 1 1 1 1 1 1 1\nThere is no one time difference that stands out as the most common. Instead, the time differences between her data points span a large range, with the majority being about 8-11 secs apart. I suspect that this means that Sandra’s watch is set to sample every 10 seconds or so. In addition, there are a lot more longer pauses seen, sometimes up to 4 minutes. Whether this is a result of random fluctuations due to the higher sampling rate, or is the result of pausing the watch at certain times, I do not know. I am most definitely not suggesting there is anything wrong with this, I just think that the better approach is to leave it running the whole time, and it is Mimi who is doing this and not Sandra.\nNote also that this means that Sandra’s data therefore has an order of magnitude fewer data points than Mimi’s does, since Mimi’s data is less smoothed out. This can be seen if we calculate the number of data points for each run and then show the distribution for Mimi and Sandra separately (note that each of Mimi’s runs is actually only half of her distance for the day):\nplot_dat \u0026lt;- list() for (n in c(\u0026quot;MimiAnderson\u0026quot;, \u0026quot;SandraVi\u0026quot;)) { plot_dat[[n]] \u0026lt;- data.frame(Name = n, nPoints = sapply(all_gpx[[n]], FUN = nrow)) } plot_dat \u0026lt;- do.call(rbind.data.frame, plot_dat) ggplot(aes(x = nPoints, fill = Name), data = plot_dat) + geom_density(alpha = 0.25) + ggtitle(\u0026quot;Number of Data Points Per Run\u0026quot;) + xlab(\u0026quot;Number of Data Points\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + theme(axis.title = element_text(size = 10), plot.title = element_text(size = 15, face = \u0026quot;bold\u0026quot;)) I’m not sure whether the people on LetsRun have been working on the assumption that both data sets were using the same parameters, but it is pretty clear to me that the sampling rate at least is different between the two runners. There may also be differences in the accuracy - perhaps the crews could confirm one way or another. Whilst the overall approach taken by Sandra is clearly the better of the two, the 1 sec sampling rate used by Mimi is the better option for the Strava data.\n 6.3 Dodgy fluctuations in cadence data One issue that has been raised is the fact that in most of Mimi’s runs, we occasionally see severe fluctuations in the cadence, which spikes up above 200 at times. This is absolutely true, which can be seen when we plot the cadence values over time. The following function will plot the raw cadence values against the cumulative time (in secs) from the start of each run:\nplot_cadence_over_time \u0026lt;- function (n, smooth = FALSE) { plot_dat \u0026lt;- list() for (r in names(all_gpx[[n]])) { if (smooth) { ## Smooths the data if requested - see below cad \u0026lt;- runmed(all_gpx[[n]][[r]][[\u0026quot;extensions.cadence\u0026quot;]], 11) } else { cad \u0026lt;- all_gpx[[n]][[r]][[\u0026quot;extensions.cadence\u0026quot;]] } plot_dat[[r]] \u0026lt;- data.frame(Run = r, Time = cumsum(all_gpx[[n]][[r]][[\u0026quot;time.diff\u0026quot;]]), Cadence = cad) } plot_dat \u0026lt;- do.call(rbind.data.frame, plot_dat) ggplot(aes(x = Time, y = Cadence, color = Run), data = plot_dat) + geom_point() + facet_grid(Run ~ .) + ggtitle(paste(n, \u0026quot;Cadence Over Time\u0026quot;)) + xlab(\u0026quot;Time (sec)\u0026quot;) + ylab(\u0026quot;Cadence (spm)\u0026quot;) + theme(axis.title = element_text(size = 10), plot.title = element_text(size = 15, face = \u0026quot;bold\u0026quot;)) + ylim(0,500) # Limits the plot to a maximum of 500 which will exclude a small number of outliers for Mimi } Then we can look at Mimi’s raw cadence over time:\nplot_cadence_over_time(\u0026quot;MimiAnderson\u0026quot;) ## Warning: Removed 120 rows containing missing values (geom_point). So there is no denying that these very high cadence values do exist in Mimi’s data. If, however, we look at Sandra’s data we do not see as many of these fluctuations. However, fluctuations are indeed still present, particularly for the shorter day on 2nd October, although they are nowhere near as high as those of Mimi (250 rather than 500):\nplot_cadence_over_time(\u0026quot;SandraVi\u0026quot;) Note that here we can make out the lunch breaks in Sandra’s data, whereas Mimi has her runs split into morning and afternoon.\nHowever, as discussed above, Mimi’s data is much deeper than Sandra’s. Sandra’s data has already undergone some smoothing, so it is likely that these blips are cancelled out by smoothing over a 10 second interval. Indeed, if we smooth Mimi’s data using a running median over an 11 sec window (which replaces the data points with a running average of the data point with the 5 data points either side) to approximate the 10 sec capture, we indeed see a much smoother distribution with these extreme values reduced to be more in keeping with what we see for Sandra.\nplot_cadence_over_time(\u0026quot;MimiAnderson\u0026quot;, smooth = TRUE) ## Warning: Removed 85 rows containing missing values (geom_point). It would appear that these very high fluctuations are a result of the increased sampling rate, although I do note that I do not see these sorts of fluctuations in my own 1 sec capture data:\nplot_cadence_over_time(\u0026quot;Mine\u0026quot;) This may simply be due to the fact that I am not running through so many different areas, many of which may have different GPS signals that affect the capture. Another possibility is that the accuracy of our watches is set to different modes. Mine is set to “Best”, but I have no idea what Mimi’s is set to. One idea that I had was to ask her to set one of her watches to 10 sec capture and upload both in parallel at the end of one of her runs. However, unfortunately it seems that this is no more an option. I am going to hunt through to find some longer runs in my personal data to see if this crops up in any of my more remote jaunts, but for now I don’t have an answer.\n 6.4 Mimi running in the 185-195 steps per minute range: Continuing with the cadence data, another issue that has cropped up several times is the fact that Mimi regularly runs in the 185-195 spm. Indeed, if we look at the distribution of the cadence in a histogram rather than looking at it over time, this certainly seems to be the case.\nThe following function will plot the above data as a series of overlaid density plots (note that I am smoothing the density estimates here slightly to make the overall distribution clearer and less spiky for the samples with fewer data points):\nplot_cadence_density \u0026lt;- function (n) { plot_dat \u0026lt;- list() for (r in names(all_gpx[[n]])) { if (r == \u0026quot;MimiAnderson_170828_preUSA\u0026quot;) next # Skip the pre-transcon run plot_dat[[r]] \u0026lt;- data.frame(Run = r, Cadence = all_gpx[[n]][[r]][[\u0026quot;extensions.cadence\u0026quot;]]) } plot_dat \u0026lt;- do.call(rbind.data.frame, plot_dat) vwalk \u0026lt;- Mode(plot_dat[[\u0026quot;Cadence\u0026quot;]][plot_dat[[\u0026quot;Cadence\u0026quot;]] \u0026gt; 100 \u0026amp; plot_dat[[\u0026quot;Cadence\u0026quot;]] \u0026lt; 150]) ## Walking vrun \u0026lt;- Mode(plot_dat[[\u0026quot;Cadence\u0026quot;]][plot_dat[[\u0026quot;Cadence\u0026quot;]] \u0026gt; 150 \u0026amp; plot_dat[[\u0026quot;Cadence\u0026quot;]] \u0026lt; 200]) ## Running ggplot(aes(x = Cadence, color = Run), data = plot_dat) + geom_density(alpha = 0.25, adjust = 3) + xlim(0,300) + ggtitle(paste(n, \u0026quot;Cadence Distribution\u0026quot;)) + xlab(\u0026quot;Cadence (spm)\u0026quot;) + ylab (\u0026quot;Density\u0026quot;) + theme(axis.title = element_text(size = 10), plot.title = element_text(size = 15, face = \u0026quot;bold\u0026quot;)) + geom_vline(xintercept = vwalk) + geom_vline(xintercept = vrun) + annotate(\u0026quot;text\u0026quot;, x = vwalk, y = 0.05, angle = 90, label = paste(vwalk, \u0026quot;spm\u0026quot;), vjust = 1.2, size = 10) + annotate(\u0026quot;text\u0026quot;, x = vrun, y = 0.05, angle = 90, label = paste(vrun, \u0026quot;spm\u0026quot;), vjust = 1.2, size = 10) } I am going annotate the peaks of these plots using a very basic method of taking the modal value (the one that occurs the most) over the entire data set for the walking and running distributions (very roughly defined, but as long as the modal value lies ion the range it should give the “correct” answer) . To do this, however, I need a function to calculate the mode since one does not exist in base R (for some odd reason):\nMode \u0026lt;- function(v) { uniqv \u0026lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } So let’s see the distribution for Mimi:\nplot_cadence_density(\u0026quot;MimiAnderson\u0026quot;) ## Warning: Removed 244 rows containing non-finite values (stat_density). So Mimi runs with a cadence of around 134 spm for running and 182 spm for running. Now let’s look at Sandra’s:\nplot_cadence_density(\u0026quot;SandraVi\u0026quot;) Sandra has generally lower cadence of 120 spm for walking, and 170 spm for running. She also appears from this to walk a lot less than Mimi, who seems to do a fairly even split between running and walking in general. In addition, the variation of Mimi’s running cadence is much higher than Sandra’s, so it appears that Sandra tends to run at a relatively constant cadence with a small amount of walking, whereas Mimi’s is much more variable and seems to be split in a 50:50 run/walk. Together with the longer differences between successive time-points, this may indicate that Sandra’s watch is set to pause automatically below a certain speed.\nI also decided to look at my own data to see what that looked like:\nplot_cadence_density(\u0026quot;Mine\u0026quot;) I also run with a fairly high cadence (just lower than Mimi’s but not dissimilar), and see more variation than Sandra. Now obviously I am not running across a continent in these runs - I am usually running with a belligerent dog who insists on stopping to sniff every bloody tree on the way. But it is not too dissimilar, and I see the distribution spreads out over 200 for some of the readings just like with Mimi. I’m an okay runner - probably not particularly good compared to many of the posters on LetsRun, but I do okay at shorter stuff and longer stuff. But it’s just a hobby for me, so I’m perfectly happy to self-associate as a hobby jogger. I don’t really know much about cadence, so I’m not sure if averaging 180+ is high or not? If nobody had suggested this was “garbage” and unbelievable, I would just assume that Mimi had a higher than normal cadence, similar to my own. I am a forefoot runner, and I think that Mimi is as well, and I believe that higher cadence tends to go hand in hand, but I am happy to bow to the experience of people more knowledgeable than myself in the matter.\nTo get an idea of the level of variation in the data (specifically the running cadence), let’s look at some aspects of that main distribution (excluding the outliers):\nfor (n in c(\u0026quot;MimiAnderson\u0026quot;, \u0026quot;SandraVi\u0026quot;, \u0026quot;Mine\u0026quot;)) { cad \u0026lt;- unlist(lapply(all_gpx[[n]], FUN = function (x) x[[\u0026quot;extensions.cadence\u0026quot;]])) cad \u0026lt;- cad[cad \u0026gt; 150 \u0026amp; cad \u0026lt; 250] cat(sprintf(\u0026quot;%12s: mode = %d\\n\u0026quot;, n, Mode(cad))) cat(sprintf(\u0026quot;%12s: mean = %.2f\\n\u0026quot;, n, mean(cad))) cat(sprintf(\u0026quot;%12s: median = %d\\n\u0026quot;, n, median(cad))) cat(sprintf(\u0026quot;%12s: SD = %.2f\\n\u0026quot;, n, sd(cad))) cat(sprintf(\u0026quot;%12s: SEM = %.3f\\n\u0026quot;, n, sd(cad)/sqrt(length(cad)))) cat(\u0026quot;\\n\u0026quot;) } MimiAnderson: mode = 182 MimiAnderson: mean = 181.56 MimiAnderson: median = 182 MimiAnderson: SD = 11.07 MimiAnderson: SEM = 0.026\nSandraVi: mode = 170 SandraVi: mean = 171.13 SandraVi: median = 170 SandraVi: SD = 4.54 SandraVi: SEM = 0.029 Mine: mode = 178 Mine: mean = 177.96 Mine: median = 178 Mine: SD = 7.53 Mine: SEM = 0.042 So clearly the standard deviation (SD) is much higher for Mimi’s data, but the standard error of the mean (SEM) is actually pretty comparable. SD and SEM, whilst both estimates of variability, tell you different things. The standard deviation is simply a measure of how different each individual data point is from the mean. It is descriptive of the data at hand. The SEM on the other hand is a measure of how far the mean of your sample is likely to be from the true population mean (under the assumption that each run is a random sampling of cadence values given Mimi’s true “normal” cadence). As your sample size increases, you more closely estimate the true mean of the population. This tells us that there is high variability in the sampling of cadence values for Mimi, but the precision is comparable with Sandra’s. This suggests nothing of whether the mean itself is actually believable of course, it is just worth noting the benefits of the increased sampling in these data.\nSo my overall feeling is that, whilst high, this was just the natural running gait of Mimi. Given recent events, this entire post ended up being highly expedited so that something was out there to provide a counter point to the accusations that have been made about data forgery and cheating, so in a rushed effort I looked around for some video of Mimi running to get an idea of her natural cadence. I found this video of her running at the end of her 7-day treadmill record. For the 18 seconds between 0:20 (when she begins to run properly) and 0:38 (when the camera pans away) I count 27-28 swings of her left hand/steps with her right foot, which would equate to a cadence of 180-187. Similarly, for the 16 seconds between 0:59 and 1:15, I count 24-25 swings/steps , which also equates to a cadence of 180-187. I’m not saying this is definitive proof, but this is at least evidence of her running with cadence similar to her average cadence across the USA, even at the end of 7 days on a treadmill. Adrenelin and a “sprint finish” mentality may play a role in achieving this as well of course. I would like to see more evidence of her running, and hopefully we will see some of that from the film crew that was with Mimi.\nSo here I have shown that, yes Mimi runs with a higher cadence than Sandra, but there is evidence that this is simply her natural gait. As to the fact that she regularly runs in the 185-195 range; well yes she does, but so do I. And I am no elite, particularly for these particular runs which are fairly perambulatory if I am honest. I can assure you that I have not doctored these data to be this mediocre. You can look through and even work out the points where my dog stopped to piss up a tree if you like. It’s not proof, but it is evidence.\nI also wanted to look a bit into how the cadence actually corresponds to the speed at which the women are running. Below is a distribution plot showing the pace at each time point for each of the data sets considered here. Notice that I am excluding the data points where the runners are not moving to avoid divide by 0 errors in the pace calculation:\nall_cor_dat \u0026lt;- list() for (n in c(\u0026quot;MimiAnderson\u0026quot;, \u0026quot;SandraVi\u0026quot;, \u0026quot;Mine\u0026quot;, \u0026quot;Fake\u0026quot;)) { cor_dat \u0026lt;- list() for (r in names(all_gpx[[n]])) { cor_dat[[r]] \u0026lt;- all_gpx[[n]][[r]][,c(\u0026quot;time.diff\u0026quot;, \u0026quot;dist.travelled\u0026quot;, \u0026quot;extensions.cadence\u0026quot;)] cor_dat[[r]][[\u0026quot;Run\u0026quot;]] \u0026lt;- r cor_dat[[r]][[\u0026quot;Pace\u0026quot;]] \u0026lt;- (cor_dat[[r]][[\u0026quot;time.diff\u0026quot;]]/60)/(cor_dat[[r]][[\u0026quot;dist.travelled\u0026quot;]]) } cor_dat \u0026lt;- do.call(rbind.data.frame, cor_dat) cor_dat[[\u0026quot;Name\u0026quot;]] \u0026lt;- n cor_dat \u0026lt;- subset(cor_dat, dist.travelled != 0) all_cor_dat[[n]] \u0026lt;- cor_dat } all_cor_dat \u0026lt;- do.call(rbind.data.frame, all_cor_dat) ggplot(aes(x = Pace, color = Name), data = all_cor_dat) + geom_density(alpha = 0.25) + xlim(0, 20) + xlab(\u0026quot;Pace (min/mile)\u0026quot;) + ylab(\u0026quot;Density\u0026quot;) + ggtitle(\u0026quot;Pace Comparison Between Data Sets\u0026quot;) + theme(axis.title = element_text(size = 10), plot.title = element_text(size = 15, face = \u0026quot;bold\u0026quot;)) ## Warning: Removed 53290 rows containing non-finite values (stat_density). From this, we can see that there is a big difference in how the women are approaching the race. As noted before, Mimi runs in a fairly even 50:50 split of running and walking. This graph confirms that with a fairly even split between faster running of about 8.5 mins/mile and slower walking of about 11 mins/mile. Sandra on the other hand appears to be very steady in her approach, moving consistently at a 170 spm cadence run of about 11.5 mins/mile. This was earlier in the run and no doubt changed over time as Sandra began to close in on Mimi over the past week. My runs are predominantly spent jogging at around 8 mins/mile (with the occasional downhill thrown in for fun). The distribution for the fake data however does not follow the same type of distribution as the other runs (with a clearly delineated multimodal distribution for run/walk/sprint segments), and again stands out when compared with the ostensibly genuine data.\n 6.5 Spoofed data So now I am getting to the nitty gritty of this post. The main accusation that I am attempting to quash is that of doctoring of the data. I have no answers regarding other perceived issues with the run, but the doctoring accusation I believe is a step too far. I have never denied that it would be possible to spoof the data. Of course it would. They are raw text files containing numbers - nothing more impressive than that. I did however think that spoofing it through Movescount would be very difficult, but it seems that I was wrong about that. It’s not simple to do, but it is doable with a little bit of know-how.\nHowever, I do believe that it would be impossible to generate spoofed data that did not stand out as such when compared with genuine data. Faking data is notoriously hard. That’s not to say that people don’t do it all of the time, and sometimes it takes a while to pick up on. But I think that creating data out of thin air that also matched with what is going on with the tracker (I appreciate people have issues with the tracker, but I’m not getting into that), what is going on with reports from the crew, matched with environmental effects and the terrain that she was running over, what will ultimately come out from the film crew, and importantly what is self consistent, would be near impossible to manage. The cadence and times would have to make sense given the position, terrain and environmental effects into account. LetsRun user Scam_Watcheroo developed a tool to spoof the data, but he had the benefit of being able to track the things that might give the data away in advance. Mimi would have had to develop her method (or more accurately get somebody else to develop the method) blind, with no idea what sort of things might show it up as being faked. Sounds incredibly risky to me. So in thus section, I wanted to look at a few things to see if the different data sets stand up to scrutiny.\nI am only touching the surface here, and I am looking into some more in depth methods to run statistical tests over the entire data set so far to check that the data are consistent and show the patterns one would expect. My hope in advance was that doing this would highlight the faked data as such. So to start with, I am not looking at consistency between the data sets, I am merely looking at the raw cadence data to look at a few potential things that might highlight anything that looked incongruous.\nFirst of all I went back to simply looking at the time differences between the data points. For my data and Mimi’s data, they are almost all 1s differences, but there is the occasional blip (presumably when it is not able to update with the satellite straight away) leading to a few counts of around 2-7 secs. The spoofed data has none of these:\nlapply(all_gpx[[\u0026quot;Fake\u0026quot;]], FUN = function (x) table(x[[\u0026quot;time.diff\u0026quot;]])) $Fake_171001_afternoon\n0 1  25895 25894\n$Fake_171002_afternoon\n0 1  26206 26205\n$Fake_171002_morning\n0 1  29092 29091\n$Fake_171007_afternoon\n0 1 1 19626  $Fake_171007_morning\n0 1 1 27499  The most recent ones (7th October) which were I think generated from scratch are ALL 1s differences, whilst the 2nd October ones (which were generated based on fiddling with Mimi’s uploads) were split half and half between 0s and 1s (a 0.5s sampling rate perhaps?). Either way, they do not have these little blips – the faked data appear to be too perfect. Being able to account for this and other such data imperfections heuristically (especially without knowing ahead of time that one would need to) would be bloody difficult and very very risky in my opinion.\nI am also looking at how well the spoofed data stand up to scrutiny using some other methods. One obvious test would be to see whether the cadence data obey Benford’s Law, which shows that the first digits (and indeed second digits, third digits, etc.) have a unique logarithmic distribution such that smaller numbers are more likely than bigger numbers. Notice here that I am looking at half of the cadence, since the actual reported data in the .gpx file was doubled to give the cadence in spm. However, in this case the first digits are somewhat constrained, since the cadence is typically in a very narrow range resulting in a huge proportion of 8s and 9s:\nn \u0026lt;- \u0026quot;Mine\u0026quot; cad_dat \u0026lt;- list() for (r in names(all_gpx[[n]])) { cad_dat[[r]] \u0026lt;- all_gpx[[n]][[r]][,c(\u0026quot;name\u0026quot;, \u0026quot;time\u0026quot;, \u0026quot;extensions.cadence\u0026quot;)] cad_dat[[r]][[\u0026quot;Run\u0026quot;]] \u0026lt;- r } cad_dat \u0026lt;- do.call(rbind.data.frame, cad_dat) cad_bentest \u0026lt;- benford(cad_dat[[\u0026quot;extensions.cadence\u0026quot;]]/2, number.of.digits = 1) plot(cad_bentest) What about if we look at the second digit:\nn \u0026lt;- \u0026quot;Mine\u0026quot; cad_dat \u0026lt;- list() for (r in names(all_gpx[[n]])) { cad_dat[[r]] \u0026lt;- all_gpx[[n]][[r]][,c(\u0026quot;name\u0026quot;, \u0026quot;time\u0026quot;, \u0026quot;extensions.cadence\u0026quot;)] cad_dat[[r]][[\u0026quot;Run\u0026quot;]] \u0026lt;- r } cad_dat \u0026lt;- do.call(rbind.data.frame, cad_dat) cad_bentest \u0026lt;- benford(as.numeric(substr(cad_dat[[\u0026quot;extensions.cadence\u0026quot;]]/2, 2, 2)), number.of.digits = 1) plot(cad_bentest) This is now approaching a more standardised distribution. It does not appear to follow Benford’s Law, but instead these data appear to be somewhat uniformly distributed. One idea that I have looked at is whether the trailing (and thus least significant) digits of the cadence data follow any particular distribution. Perhaps one might imagine that they should follow some distribution, such as the more uniform distribution seen above. Again remember here that I am plotting the raw cadence data, which is half of the cadence values reported in the distribution plots:\n## Look at the final digit of the cadence data all_digit_counts \u0026lt;- list() all_digit_percent \u0026lt;- list() for (n in names(all_gpx)) { all_digit \u0026lt;- matrix(0, ncol = 10, nrow = length(all_gpx[[n]]), dimnames = list(names(all_gpx[[n]]), as.character(0:9))) for (r in names(all_gpx[[n]])) { digit \u0026lt;- gsub(\u0026quot;^\\\\d*(\\\\d)$\u0026quot;, \u0026quot;\\\\1\u0026quot;, all_gpx[[n]][[r]][[\u0026quot;extensions.cadence\u0026quot;]]/2) all_digit[r, ] \u0026lt;- table(digit)[as.character(0:9)] } all_digit_counts[[n]] \u0026lt;- all_digit all_digit_percent[[n]] \u0026lt;- 100*all_digit/rowSums(all_digit) } ## Plot heatmap hm_dat \u0026lt;- do.call(rbind.data.frame, all_digit_percent) rownames(hm_dat) \u0026lt;- gsub(\u0026quot;^.*\\\\.\u0026quot;, \u0026quot;\u0026quot;, rownames(hm_dat)) pheatmap(hm_dat, cluster_rows = FALSE, cluster_cols = FALSE, main = \u0026quot;Cadence - trailing digit percentage\u0026quot;)#, display.numbers = FALSE, annotation_legend = TRUE, cluster_cols = TRUE, show_colnames = TRUE, show_rownames = FALSE) This figure shows the distribution of the percentage of trailing digits seen amongst the different data sets – Mimi’s, Sandra’s, the spoofed data, and some of my own runs. The colour of the heatmap indicates the percentage of times that digit is seen in the final position, with blue being less often and red being more often. In general:\n Mine seem to be quite uniformly distributed (mainly blue) Mimi’s are pretty uniform (except for the pre-USA run that I put in as well) with the exception of a regular depletion of 2s and 5s Sandra’s seem to show a depletion of 1s and 9s, and enrichment of several digits in most runs (but nothing consistent) The fake data however seem very consistent in their prevalence of 6s and 7s (and to a lesser extent 4s and 8s).  There is not really enough data here to identify a pattern, but from what is here the spoofed data stands out with a distribution that is different from that seen with my own data. Mimi’s is actually the most alike to data that I know to be real, although the depletion of certain digits is quite odd. But then the same is true for Sandra’s data as well to a greater degree (albeit different numbers). I plan to look into this in more detail using more data, including all of Sandra’s and Mimi’s runs, and a lot more genuine data taken from Strava as a base line to see if this distribution holds.\nObviously none of this “proves” these data are not fabricated. That is impossible. I do however thus far see no sufficient evidence to suggest that these data are not real (for both runners, although Sandra was never on “trial” here). And really I have only scratched the surface on how to test the validity of these data.\n  7 Conclusion I am very saddened about what has happened to Mimi on this journey. There are questions that I hope will be answered regarding certain aspects of the run, and I’m sure that these by themselves are enough to convince some people of wrong-doings. But I truly do not believe that Mimi has set out to flim flam, bamboozle or otherwise beffudle people into believing that she ran across America when she didn’t. I wrote this post before Mimi announced her intentions to stop, but the damage that she has done to herself must surely be evidence that she is doing it. And if you accept that the Strava data are genuine, there is no way to deny what she has done. Perhaps what I have introduced here will help a little to bring more people to that way of thinking, but others likely need more convincing. I will continue to try to provide reasoned explanations for some of the remaining inconsistencies where I can.\nSo my feeling is that there is a zebra hunt going on here. What is more likely; that a 55 year old grandma is running across the country at world record pace, or that she has convinced several people to go on a month-long trip to fake a run, and in the process developed an incredibly sophisticated piece of software (which accounts for specific nuances) to spoof the data (even though she is clearly doing something out there as she is losing weight and suffering exactly as one might expect for somebody running across America)? I’m going with the world record grandma. I do not think that Mimi is a witch.\nThere are likely many questions outstanding which I have not addressed here. This is a fairly rudimentary piece of work compared to the amount of time and effort that others have put into looking into this. I am interested to look into Scam_Watcheroo's blog post about this to see what other issues he addresses. I would also like to look at how Mimi’s performance changed over time, and in particular how it changed following the LetsRun forum taking off and her ultimate switch from Race Drone to the Garmin tracker. In addition, something that I have not considered is whether or not the data are modified in the move from MovesCount to Strava. Although the overal trends would not change drastically, the raw data themselves (and therefore the digit distributions) might. This is probably worth considering in due time.\nI genuinely hope that this is useful to some of you in addressing some of the concerns. Nothing I can do can change people’s opinions on what happened in the first 2 weeks, but I hope that I can at least start to alleviate fears that there is any duplicity in the Strava data. It is all academic now following Mimi’s recent announcement that she will be pulling out from the event, but hopefully this should also help to assure people of the validity of Sandra’s run as well (although her regular updating and constant tracking have allayed any such fears already). All I can do now is wish Sandra good luck in getting the record (she looks to be on excellent form), and wish Mimi a very speedy recovery. It is sad that it had to go like this.\n 8 Summary  There are aspects of the spoofed data that make it stand out when compared to Mimi’s and Sandra’s (and my own) data I just do not think that it would be possible to create a forged data set that stands up to intense scrutiny - this is fairly basic scrutiny and it stands out Mimi is using 1s capture mode on constant capture for her runs, with the very occasional 3 or 4 sec delay Sandra is using 10s capture and has longer pauses in data retrieval of several minutes at a time (auto-pause?) Mimi’s data sets are therefore an order of magnitude denser than Sandra’s Mimi’s cadence blips of 200+ spm are likely just random c*ck ups in data capture - they disappear if you smooth out to a 10s capture rate (I was trying to contact her crew to ask her to set a second watch to 10s capture for one of her runs to confirm this, but unfortunately it was too late) You probably don’t see them for Sandra because they get averaged out Mimi is running with a high cadence but the average seems to fit with previous evidence (albeit very limited and definitely open to scrutiny) of her running gait (evidence from the film crew videos in the future will also help if/when released) Mimi is often running in 185-195 range, but then so do I - granted I am not running across a continent on a day by day basis, but I am also nowhere near elite Mimi’s cadence is about 10 spm quicker than Sandra’s for both walking and running Mimi seems to have a fairly even split of walking and running, whilst Sandra seems to consistently run but at a slower overall pace   9 Session Info sessionInfo()  ","date":1539907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539907200,"objectID":"95f15fbd97eddecea555aa8c38389cf9","permalink":"/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/","publishdate":"2018-10-19T00:00:00Z","relpermalink":"/post/2018-10-19-strava-data-mining-assessing-mimi-andersons-world-record-run/","section":"post","summary":"Analysis of data from Strava to assess the validity of data for a World Record Run across the USA","tags":["R","Data Science","ultra","ultrarunning","GWR","Mimi","Anderson","Guinness","World","Record","USA","transcon"],"title":"Strava Data Mining: Assessing Mimi Anderson's World Record Run Across the USA","type":"post"},{"authors":["Larrieu D","Virè E","Sam Robson","Breusegem S","Kouzarides T","Jackson S"],"categories":null,"content":"","date":1530576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530576000,"objectID":"b87d01fe5112a84ef3f8dfa1deecfc70","permalink":"/publication/2018_inhibition_of_the_acetyltransferase_nat10_normalizes_progeric_and_aging_cells_by_rebalancing_the_transportin-1_nuclear_import_pathway/","publishdate":"2018-07-03T00:00:00Z","relpermalink":"/publication/2018_inhibition_of_the_acetyltransferase_nat10_normalizes_progeric_and_aging_cells_by_rebalancing_the_transportin-1_nuclear_import_pathway/","section":"publication","summary":"Hutchinson-Gilford progeria syndrome (HGPS) is an incurable premature aging disease. Identifying deregulated biological processes in HGPS might thus help define novel therapeutic strategies. Fibroblasts from HGPS patients display defects in nucleocytoplasmic shuttling of the GTP-bound form of the small GTPase Ran (RanGTP), which leads to abnormal transport of proteins into the nucleus. We report that microtubule stabilization in HGPS cells sequestered the nonclassical nuclear import protein Transportin-1 (TNPO1) in the cytoplasm, thus affecting the nuclear localization of its cargo, including the nuclear pore protein NUP153. Consequently, nuclear Ran, nuclear anchorage of the nucleoporin TPR, and chromatin organization were disrupted, deregulating gene expression and inducing senescence. Inhibiting N-acetyltransferase 10 (NAT10) ameliorated HGPS phenotypes by rebalancing the nuclear to cytoplasmic ratio of TNPO1. This restored nuclear pore complex integrity and nuclear Ran localization, thereby correcting HGPS cellular phenotypes. We observed a similar mechanism in cells from healthy aged individuals. This study identifies a nuclear import pathway affected in aging and underscores the potential for NAT10 inhibition as a possible therapeutic strategy for HGPS and perhaps also for pathologies associated with normal aging.","tags":[""],"title":"Inhibition of the acetyltransferase NAT10 normalizes progeric and aging cells by rebalancing the Transportin-1 nuclear import pathway","type":"publication"},{"authors":null,"categories":["Blog","Data Science","Running"],"content":"  1 Note 2 Introduction 3 Natural Language Processing 4 Random Forest 5 Model Improvements 6 Logistic Regression 7 Discussion   1 Note This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.\n 2 Introduction This post is a continuation from my previous post, looking at various aspects of the posting habits of the Ultra Running Community (URC). This was originally intended to be just an additional section in that blog post, but it was getting a little unwieldy so I decided to split it off into its own post to stop it from getting too crazy. This post is probably a lot less interesting than the last post, as it is really looking at one central question; can I predict which group member is posting based on the content of the message? Spoiler alert, you can’t! The posters on this forum are apparently all sarcastic SOBs, so it is difficult to pick them apart. But it was quite interesting trying.\nSo as a bit of an experiment, I decided to play around with these data to see if the language that people use in their posts is specific enough to allow us to predict who has written something based on what the post says. This is a job for machine-learning, which is really a lot less grandiose than it sounds. Essentially what we are doing here is using data to train a model of some description that can be applied to a new set of data to make predictions. In this case, we are looking to fit a model that is able to classify posts into one of a number of groups, where each group represents a single user. As an example of a classification problem, think of the spam filter in your email client. This is essentially a model that has been trained to look at the email message content and determine whether it is spam or not (e.g. if it is full of words like viagra, Nigerian Prince, penis enlargement, make money today, etc. then it is clearly all kosher). This would be a 2-class classification problem.\nFor classification problems such as this, we require a training set on which to fit our model, and a validation set to determine the quality of the model. The validation set must be independent of the training set, as we want to test how the model will generalize to new data. The idea of cross validation is essentially to split your training data into a training set and a validation set such that the validation is independent of the model fitting (to avoid the effects of over-fitting in the training set). There are various ways to split your data in this way. For now I will simply randomly select a subset for training and a smaller subset for validation (the Holdout Method), but for true cross-validation this should then be repeated several times so that the average over several validation sets is used. For example, in k-fold cross validation you would randomly distribute the data into k equally sized subsets, and use exactly one of these as the validation set and k-1 as the training set. This is then repeated k times, each time using a different subset as the validation set.\nIt makes sense to restrict this analysis to the most active posters, and so I will limit the analysis to only users who have contributed 50 or more posts to the forum. This gives us 5,233 posts, from 48 users. I will randomly select 4,000 posts for the training set, and use the remainder for validation:\nposts50 \u0026lt;- URC %\u0026gt;% group_by(from_name) %\u0026gt;% ## Group by poster filter(n() \u0026gt;= 50) %\u0026gt;% ## Select only posters with \u0026gt;50 posts select(from_name, message) ## Keep poster name and message content set.seed(0) ## Set seed for random number generation for reproducibility ids \u0026lt;- sample(1:nrow(posts50), 4000) ## Randomly select 4000 train \u0026lt;- posts50[ids,] ## Keep random ids as training set test \u0026lt;- posts50[-ids,] ## Use remaining ids as validation  3 Natural Language Processing The model that we will be using is the Bag Of Words model, which is a natural language processing technique that aims to represent text based on the frequency of words within it. There are some things that we can do to reduce the vector space of available terms, such as removing capital letters and removing so called “stop words” (common words like “is”, “and”, “but”, “the”, etc.). We can also limit the analysis to only words that occur frequently in the text, although there is a possibility of missing specific terms used by only one or two individuals, say, that may help the predictiveness of the model.\nI will be using the text2vec package in R which is efficient at generating the required document-term matrix (DTM) for fitting our model. In particular, it generates unique tokens for each term rather than using the terms themselves, which reduces computational overheads. An iterative function can then be applied to generate the DTM. So let’s generate such an iterator over the term tokens:\nlibrary(text2vec) train_tokens \u0026lt;- train$message %\u0026gt;% iconv(\u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub = \u0026quot;\u0026quot;) %\u0026gt;% # Convert to ASCII format tolower %\u0026gt;% # Make lower case word_tokenizer # Break terms into tokens it_train \u0026lt;- itoken(train_tokens, ids = train$from_name, progressbar = FALSE) it_train ## \u0026lt;itoken\u0026gt; ## Inherits from: \u0026lt;iterator\u0026gt; ## Public: ## chunk_size: 400 ## clone: function (deep = FALSE) ## counter: 0 ## ids: Jean-François Tantin Gary Kiernan Richard Lendon Iain Ed ... ## initialize: function (iterable, ids = NULL, n_chunks = 10, progress_ = interactive(), ## is_complete: active binding ## iterable: list ## length: active binding ## nextElem: function () ## preprocessor: list ## progress: FALSE ## progressbar: NULL ## tokenizer: list Next we use this iterator to create a vocabulary DTM for fitting the model. To start with, I will use all of the words, but later we could look at filtering out stop words and less frequent terms:\nvocab \u0026lt;- create_vocabulary(it_train) vectorizer \u0026lt;- vocab_vectorizer(vocab) train_dtm \u0026lt;- create_dtm(it_train, vectorizer) dim(train_dtm) ## [1] 4000 13922 The result is a matrix with 4,000 rows (the number of messages in the training set) and 13,922 columns (the number of unique terms in the training set). So each message is now represented as a vector of counts for all possible terms in the search space. The hope now is that we will be able to fit a model that is able to discriminate different users based on their word usage. Unlikely, but hey let’s give it a shot.\n 4 Random Forest In this case, our dependent variable is the name of the user who posted the message which is a categorical variable. The independent variables are the counts for each of the 13,922 terms across the data set. I am going to start by using a random forest model, which is one of the more popular classification models available. A decision tree is a quite simple (although incredibly powerful) stepwise model that you can think of like a flow chart. The model fitting will create a series of nodes where your independent variables are used to discrimate between one choice and another, eventually leading to a certain prediction depending on the values of the variables in your model. A random forest essentially fits a whole load of these classification decision trees and outputs the modal (most common) class across all of them.\nOne benefit of using random forests over something like generalised linear models (see later) is that, since they rely on fairly independent tests at each stage in the tree, they are more robust to correlated variables in the model. With such a large set of term variables there is undoubtedly correlation between many of these terms, particularly as many of these variables are likely to be largely made of zeroes. Of course, this sparsity itself causes somewhat of a problem, and should be taken into account in the analysis. But for now I will ignore it and just hope that it isn’t a problem…\nTo begin with,let’s fit a simple random forest model and see how it looks:\nlibrary(\u0026quot;randomForest\u0026quot;) library(\u0026quot;caret\u0026quot;) rf_model \u0026lt;- randomForest(x = as.matrix(train_dtm), y = as.factor(rownames(train_dtm))) Note that I set the y parameter to be a factor so that it is clear that this is a classification model. Now we can test our model by seeing how it performs at predicting the user for our test data set. First we generate a similar DTM for the test data set. Note that we use the same vectorizer as we used for the training set:\ntest_tokens \u0026lt;- test$message %\u0026gt;% iconv(\u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub = \u0026quot;\u0026quot;) %\u0026gt;% # Convert to ASCII format tolower %\u0026gt;% # Make lower case word_tokenizer # Break terms into tokens it_test \u0026lt;- itoken(test_tokens, ids = test$from_name, progressbar = FALSE) test_dtm \u0026lt;- create_dtm(it_test, vectorizer) And then we use our model to predict the user for each of the posts in our test data set. To do this we use the predict() method for randomForest objects, and output the response class with the majority vote amongst all of the decision trees:\ntest_predict \u0026lt;- predict.train(rf_model, test_dtm, type = \u0026quot;response\u0026quot;) So, how did we do? Let’s see how many of these were correctly predicted:\ntable(test_predict == rownames(test_dtm)) ## ## FALSE TRUE ## 760 473 So this model predicts the poster only 38.4 % of the time, which isn’t particularly good.\n 5 Model Improvements So can we improve this? Yes, probably. The first thing that I can try is to be a little more clever in the way that I parameterise the data. So rather than simply counting words, I will instead use n-grams – combinations of n words that will be more sensitive to the types of phrases that different people typically use. Obviously increasing n in this case will also increase the memory and run time considerably, so there are limits to what we can feasibly do. Also, it is probably worth noting that removal of stop words is less likely to be the best way to go about this, as this will affect the structure of the n-grams. So this time let us leave the stop words in, but parameterise with 3-grams. I will also limit the count to those n-grams used at least 10 times:\nvocab \u0026lt;- create_vocabulary(it_train, ngram = c(1L, 3L)) ## use 1-, 2- and 3-grams vocab \u0026lt;- vocab %\u0026gt;% prune_vocabulary(term_count_min = 10) ## Only keep n-grams with count greater than 10 vectorizer \u0026lt;- vocab_vectorizer(vocab) dtm_train \u0026lt;- create_dtm(it_train, vectorizer) Note here that we used the notation 1L, 3L, which tells R to explicitly use integer values rather than numeric values. In many cases this has little to no effect, but in programming an integer variable will take up much less memory (4 bytes per element) than a double precision floating point number (8 bytes per element).\nAnother thing that we can do to improve the model fit is that we can attempt to normalise our DTM to account for the fact that different Facebook messages may be longer or shorter than others. Typically the “documents” in this case (the messages) are very small so I imagine this will have only a minimal effect. Here I will use the term frequency-inverse document frequency (TF-IDF) transformation. The idea here is to not only normalise the data, but also to scale the terms such that terms that are more common (i.e. those used regularly in all posts) are down-weighted, whilst those that are more specific to a small number of users (and will thus be more predictive) are up-weighted.\ntfidf \u0026lt;- TfIdf$new() train_dtm_tfidf \u0026lt;- fit_transform(train_dtm, tfidf) Finally there is some fine tunning that can be made to the model fitting procudure. Here we are dealing with a very sparse set of data, since most of the counts are zero in this matrix (not everybody uses every word or set of words). This can cause issues with the random forest model. In addition, there may be some imbalance in the classes (for instance as we saw above different individuals post more often than others).\nOne particular aspect to explore is that different selections for the parameters can have big effects on the quality of the model. The two main parameters for a random forest are the number of trees (ntree) and the number of features that are evaluated at each branch in the trees (mtry). The higher the better for the number of trees, although run-time can be a hindrance on this. For the second parameter, I have seen it suggested that the square root of the number of features is a good place to start, and this is the default for classification anyway. So let’s try increasing the number of trees, and running this on the TF-IDF transformed 3-gram data:\nrf_model_tfidf \u0026lt;- randomForest(x = as.matrix(train_dtm_tfidf), y = as.factor(rownames(train_dtm_tfidf)), ntree = 1000) One note to make here is that this is slllllllloooooooooowwwwwwwwww! This needed to be run overnight to finish. Using something like python is probably a better bet when running machine learning algorithms like this, and I will probably do another post later in the future to look at some alternative ways to do this.\nSo let’s take a look at whether or not this model is more effective at predicting the user:\ndtm_test \u0026lt;- create_dtm(it_test, vectorizer) test_dtm_tfidf \u0026lt;- fit_transform(test_dtm, tfidf) test_predict \u0026lt;- predict(rf_model_tfidf, as.matrix(test_dtm_tfidf), type = \u0026quot;response\u0026quot;) table(test_predict == rownames(test_dtm)) ## ## FALSE TRUE ## 751 482 Wow, so now we have improved the prediction to a whopping 39.1%. Hmm. An improvement of 0.7% was not quite as much as I was hoping for.\n 6 Logistic Regression Okay, so let’s try a different model to see if that has any effect. I am going to fit a logistic regression. Regression is simply an attempt to fit a linear approximation to a set of data that minimises the difference between the modeled value and the true value (the residuals). I will do a more thorough post on statistical modelling in the future, but for now think of regression models as being an attempt to fit a line of best fit between some variable \\(y\\) that you suspect is dependent on some other variables \\(x_1, x_2, ..., x_n\\). The idea then is to use this model to predict \\(y\\) based on new measurements of \\(x_1, x_2, ..., x_n\\). So here we are trying to fit a model that will provide us with an estimate of the user based on the words used in the post.\nHere I will use the glmnet package to fit the logistic regression. Logistic regression is a subset of Generalised Linear Models (GLM), which are an extension of ordinary linear regression allowing for errors that are not normally distributed through the use of a link function. Since we have multiple possible classes in the dependent variable, this will be a multinomial logistic regression:\nlibrary(\u0026quot;glmnet\u0026quot;) glm_model_tfidf \u0026lt;- cv.glmnet(x = train_dtm_tfidf, y = as.factor(train$from_name), family = \u0026#39;multinomial\u0026#39;, alpha = 1, type.measure = \u0026quot;deviance\u0026quot;, nfolds = 5, thresh = 1e-3, maxit = 1e3) This is an n-fold cross-validated GLM (hence cv.glmnet), which is a method of validation for the model that splits the data into n equally sized subsets, then uses n-1 subsets as training data and the remaining subset as the validation data to test the accuracy of the model. This is repeated n times, and the average is used. This is actually a better method than I have used in these data (selecting a test data set and running the model on the remaining subset) as every sample is used in the validation, which avoids over-fitting.\nThe family parameter gives the model family that defines the error model, which in turn determines the link function to be used. In this case we are using multinomial logistic regression, so the predicted response is a vector of probabilities between 0 and 1 – one for each potential response – all adding to 1. The link function, which defines the relationship between the linear predictor and the mean of the distribution function, is the logit function, which in the binary case gives the log odds of the prediction:\n\\[X\\beta = ln \\frac{\\mu}{(1-\\mu)}\\]\nThe alpha value will utilise an L1 regularisation of the data to account for the sparsity that we see in the data. The type.measure value determines the measurement to use to determine the cross validation, in this case the misclassification error. nfolds gives the value of k for the k-fold cross validation, thresh gives the threshold for the convergence of the coordinate descent loop, and maxit gives the maximum number of iterations to perform.\nSo let’s see if this is any better:\ntest_predict \u0026lt;- predict(glm_model_tfidf, as.matrix(test_dtm_tfidf), type = \u0026quot;response\u0026quot;) table(colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm)) ## ## FALSE TRUE ## 752 481 Nope. We still only see about 39% accurately assigned.\n 7 Discussion Okay, so it is possible (highly probable?) that I have made some mistakes in this analysis, and that I could vastly improve the creation of the DTM, but I think it is more propbable that these posts are simply not distinct enough to determine individuals writing styles. I guess in a group with such a narrow focus, it is inevitable that people will be posting very similar content to one another. There is after all only so many ways to ask “Suunto or Garmin”.\nLet’s examine why we are struggling to distinguish these posts in a little more detail. Below is a heatmap showing the probability for each of the 48 potential posters, predicted for all 1,233 of the posts in the validation data set. A heatmap is a kind of 3-dimensional plot, where colour is used to represent the third dimension. So the 48 potential posters are shown on the x-axis, the 1,233 posts are shown on the y-axis, and the magnitude of the estimated probability for user i based on post j is represented by a colour from red (0% probability) to white (100% probability). Note that here I have scaled the data using a square root so that smaller probabilities (which we expect to see) are more visible. The rows and columns are arranged such that more similar values are closer together.\nlibrary(\u0026quot;gplots\u0026quot;) heatmap.2(sqrt(test_predict[,,1]), trace = \u0026quot;none\u0026quot;, margins = c(10,0), labRow = FALSE) So the biggest problem here is that the vast majority of the posts are estimated as most likely coming from either Neil Bryant, Stef Schuermans or James Adams. And actually, the ones that it gets correct are almost all from one of these posters:\npie(sort(table(rownames(test_dtm)[colnames(test_predict)[apply(test_predict, MAR = 1, which.max)] == rownames(test_dtm)]))) I wonder whether these guys are skewing the model because of their, ahem, above average posting habits. But frankly at this stage I’m kind of bored, so I think that I will leave it there. Another time maybe. Ultimately I believe that these posts are simply too short and bereft of salient information to be useful for making predictions.\n ","date":1524528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524528000,"objectID":"f4f89b1ef08062ffc6b2b8dd362a7463","permalink":"/post/2018-04-24-how-predictable-are-ultrarunners/","publishdate":"2018-04-24T00:00:00Z","relpermalink":"/post/2018-04-24-how-predictable-are-ultrarunners/","section":"post","summary":"Further exploration of posts from the Ultra Running Community (URC) Facebook page, including using machine learning techniques including logistic regression and random forest to explore the predictibility ofposters based on the content of their posts","tags":["R","Facebook","ultrarunning","ultra","running","ultramarathon","marathon","Suunto","Garmin","dplyr","tidyr","ggplot2","natural language processing","NLP","machine learning","randomforest","glm","multinomial","logistic regression"],"title":"How Predictable Are Ultra Runners?","type":"post"},{"authors":null,"categories":["Blog","Data Science","Running"],"content":"  1 Note 2 Introduction 3 Rfacebook 3.1 Temporary Token 3.2 fbOAuth  4 Ultra Running Community 5 Likes, Comments and Shares 6 Top Contributors 7 When are people posting? 8 Most Popular Posts 9 How Often Do People Actually Talk About Ultras? 10 Suunto or Garmin? 11 Summing Up 12 Session Info   1 Note This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.\n 2 Introduction In my last post, I took a look at ways to pull data down from Twitter and analyse some specific trends. It was quite interesting for me to see how easy it is to access these data, and there is a huge amount to be gleened from these sorts of data. The idea of this post is to use a similar approach to pull data from the Ultra Running Community page on Facebook, and then to use these data to play around further with the dplyr, tidyr and ggplot2 R packages. Just for funsies, I’m also going to have a bit of a crack at some machine learning concepts. In particular, a question that seems to comes up pretty regularly is whether the best GPS watch for running is from Suunto or Garmin. I figured I could save us all some time and answer the question once and for all…\nJust a little aside here; I think that some people missed the point last time. I honestly don’t care much about these questions, they are just a jumping off point for me to practice some of the data analysis techniques that come up in my work. The best way to get better at something is to practice, so these posts are just a way of combining something I love with a more practical purpose. The idea of this blog is for me to practice these things until they become second nature. Of course in the process, I may just find something interesting along the way.\nProbably not though.\n 3 Rfacebook Following on from my experiences playing around with the Twitter API, I decided to have a look to see if there were similar programmatic ways to access Facebook data. This can be accomplished using the Rfacebook package in R, which is very similar to the TwitteR package that I used previously. This package accesses the Facebook Graph API Explorer, allowing access to a huge amount of data from the Facebook social graph.\nSo first of all, let’s install the Rfacebook package. We can install the stable version from the Comprehensive R Archive Network (CRAN):\ninstall.packages(\u0026quot;Rfacebook\u0026quot;) Or install the more up-to-date but less stable developmental version from Github:\nlibrary(\u0026quot;devtools\u0026quot;) install_github(\u0026quot;pablobarbera/Rfacebook/Rfacebook\u0026quot;) I am using the developmental version here. There are several additional packages that also need to be installed:\ninstall.packages(c(\u0026quot;httr\u0026quot;, \u0026quot;rjson\u0026quot;, \u0026quot;httpuv\u0026quot;)) As with TwitteR, access to the API is controlled through the use of API tokens. There are two ways of doing this - either by registering as a developer and generating an app as I did with TwitteR, or through the use of a temporary token which gives you access for a limited period of 2 hours. Let’s look at each of these in turn:\n3.1 Temporary Token To generate a temporary token, just go to the Graph API Explorer page and generate a new token by clicking on Get Token -\u0026gt; Get Access Token. You need to select the permissions that you want to grant access for, which will depend on what you are looking do:\nCreate Temporary Access Token\n I just granted permission to everything for this analysis. Once you have an access token, this can be used as the token parameter when using functions such as getUsers().\n 3.2 fbOAuth The above is the most simple method, but this access token will only last for 2 hours, at which point you will need to generate a new one. If you want a longer term solution, you can set up Open Authorization access in a similar way to for the TwitteR package. The downside is that you lose the ability to search friend networks unless your friends are also using the app that you generate - and I don’t want to inflict that on people just so that I can steal their identity analyse their networks.\nThis method is almost identical to the process used for generating the OAuth tokens in the TwitteR app, and a good description of how to do it can be found in this blog post.\nHowever, I am feeling pretty lazy today and so I will just use the temporary method.\n  4 Ultra Running Community With nearly 18,000 members, the Ultra Running Community Facebook page is a very active community of ultrarunners from around the world. Runners are able to ask questions, share blogs, and generally speak with like-minded individuals about everything from gear selection to how best to prevent chaffing when running. It’s been going since June 2012, so there are a fair few posts available to look through.\nSo let’s load in all of the posts from the URC page:\nlibrary(\u0026quot;Rfacebook\u0026quot;) token \u0026lt;- \u0026quot;XXXXXX\u0026quot; ## Insert your temporary token from Graph API Explorer URC \u0026lt;- getGroup(\u0026quot;259647654139161\u0026quot;, token, n=50000) This command will create an object of class data.frame containing the most recent 50,000 posts available in the Facebook Group with ID 259647654139161 (which is the internal ID for the Ultra Running Community page). The page was set up in June 2012 By Neil Bryant, and currently (as of 20th March 2017) contains a total of 24,836 posts. So this command will actually capture every single post.\nThe data.frame is somewhat of the workhorse of R, and looks to the user like a spreadsheet like you would expect to see in Excel. Behind the scene it is actually a list of lists, with each column representing a particular measurement or descriptive annotation of that particular datum. The ideal situation is to design your data frame such that every row is an individual measurement, and every column is some aspect relating to that measurement. This can sometimes go against the instinctual way that you might store data, but makes downstream analyses much simpler.\nAs an example, suppose that you were measuring something (blood glucose levels, weight, lung capacity, VO2 max, etc.) at three times of the day for 2 individuals. Your natural inclination may be to design your table in this way:\n  Sample Measurement 1 Measurement 2 Measurement 3    Sample1 0.3 0.4 0.3  Sample2 0.6 0.6 0.7    But actually the optimum way to represent this is to treat each measurement as a different row in your data table, and use a descriptive categorical variable to represent the repeated measurements:\n  Sample Measurement Replicate    Sample1 0.3 1  Sample1 0.4 2  Sample1 0.3 3  Sample2 0.6 1  Sample2 0.6 2  Sample2 0.7 3    You can then add additional information relating to each individual measurement, which can be factored into your model down the line.\nIn this case, we have a data frame where each row is a post on the URC feed, and each column gives you information on the post such as who wrote it, what the post says, when it was written, any links involved, and how many likes, comments and shares each post has. We can take a quick look at what the data.frame looks like by using the str() function. This will tell us a little about each column, such as the data format (character, numeric, logical, factor, etc.) and the first few entries in each column:\nstr(URC) ## \u0026#39;data.frame\u0026#39;: 24836 obs. of 11 variables: ## $ from_id : chr \u0026quot;10203232759527000\u0026quot; \u0026quot;10154582131480554\u0026quot; \u0026quot;10206266800967107\u0026quot; \u0026quot;10153499987966664\u0026quot; ... ## $ from_name : chr \u0026quot;Steph Wade\u0026quot; \u0026quot;Esther Bramley\u0026quot; \u0026quot;Tom Chapman\u0026quot; \u0026quot;Polat Dede\u0026quot; ... ## $ message : chr \u0026quot;Anyone who runs in Speedcross 3s tried Sportiva? I\u0026#39;ve had several pairs of Speedcross but thinking about tryin\u0026quot;| __truncated__ \u0026quot;Hi guys server pain in knee two weeks after 41miler. Ran 3 miles Tuesday no problem. Pain started at 5m and got\u0026quot;| __truncated__ \u0026quot;mega depressed; need advice. Running really well over xmas, since then, painful hip \u0026amp; groin, chasing itb, glute\u0026quot;| __truncated__ NA ... ## $ created_time : chr \u0026quot;2017-03-19T08:38:15+0000\u0026quot; \u0026quot;2017-03-18T12:19:26+0000\u0026quot; \u0026quot;2017-03-18T16:30:54+0000\u0026quot; \u0026quot;2017-03-19T08:42:38+0000\u0026quot; ... ## $ type : chr \u0026quot;status\u0026quot; \u0026quot;status\u0026quot; \u0026quot;status\u0026quot; \u0026quot;photo\u0026quot; ... ## $ link : chr NA NA NA \u0026quot;https://www.facebook.com/tahtaliruntosky/photos/a.659614340816057.1073741827.659609490816542/1145262965584523/?type=3\u0026quot; ... ## $ id : chr \u0026quot;259647654139161_1064067560363829\u0026quot; \u0026quot;259647654139161_1063418100428775\u0026quot; \u0026quot;259647654139161_1063562803747638\u0026quot; \u0026quot;259647654139161_1064068937030358\u0026quot; ... ## $ story : logi NA NA NA NA NA NA ... ## $ likes_count : num 0 0 0 0 0 2 2 58 7 1 ... ## $ comments_count: num 5 23 9 0 25 9 4 64 77 12 ... ## $ shares_count : num 0 1 0 0 0 0 0 0 3 0 ...  5 Likes, Comments and Shares The first step in any data analysis is to check that the data make sense. You’ve probably heard the old adage “garbage in; garbage out”, so data cleaning is an essential first step to ensure that we are not basing our conclusions on erroneous information from the beginning. There are far too many posts here to look at them all by hand, but there are a few things we can certainly have a look at to check that the values make sense.\nFor instance, let’s take a look at the number of likes, comments and shares. We would expect all of these values to be positive whole numbers, so this is something that is easy to check. To do this, I will be making use of the ggplot2 package, which allows for some incredibly powerful plotting in R. The idea is to define the plot in terms of aesthetics, where different elements of the plot (x and y values, colour, size, shape, etc.) can be mapped to elements of your data.\nIn this case I want to plot a distribution plot where the colour of the plot is mapped to whether we are looking at likes, comments or shares. To do this, I need to rearrange the data such that the likes_count, comments_count and shares_count columns are in a single column counts, with an additional column defining whether it is a like, a comment, or a share count (as described in the example above).\nI will use the tidyr, stringr and dplyr packages to rearrange the data:\nlibrary(\u0026quot;tidyr\u0026quot;) library(\u0026quot;dplyr\u0026quot;) library(\u0026quot;stringr\u0026quot;) like_comment_share \u0026lt;- URC %\u0026gt;% gather(count_type, count, likes_count:shares_count) %\u0026gt;% select(count_type, count) head(like_comment_share) ## count_type count ## 1 likes_count 0 ## 2 likes_count 0 ## 3 likes_count 0 ## 4 likes_count 0 ## 5 likes_count 0 ## 6 likes_count 2 tidyr, stringr and dplyr are incredibly powerful packages written by Hadley Wickham, which provide a simple to understand grammar to apply to the filtering and tweaking of data frames in R. In particular, these can be used to convert the data into the tidy format described above, allowing very simple and intuitive plotting with ggplot2. One particularly useful feature is the ability to use the %\u0026gt;% command to pipe the output to perform multiple data frame modifications.\nIn the above code, we pipe the raw data URC into the gather() function, which will take the three columns from likes_count through to shares_count and split them into two new columns: count_type which will be one of shares_count, likes_count and comments_count , and count which will take the value from the specified column. So essentially this produces a new data set with 3 times as many rows. This is then piped into select() which will select the relevant columns.\nFirst let’s just check that they are are all positive integers as expected:\nall(like_comment_share$count == as.integer(like_comment_share$count)) [1] TRUE\nall(like_comment_share$count \u0026gt;= 0) [1] TRUE\nAnnoyingly there is no easy way to check that a vector of numbers is made up of integers, so this line will check that the numbers do not change after converting to integers. Similarly, we use all() to check that all of the counts are greater than or equal to zero. They are as we would hope.\nThen we can use ggplot2 for the plotting:\nlibrary(\u0026quot;ggplot2\u0026quot;) ggplot(like_comment_share, aes(x = log10(count+1), col = count_type, fill = count_type)) + geom_density(alpha = 0.1) + labs(x = \u0026quot;Count (log10)\u0026quot;, y = \u0026quot;Density\u0026quot;) + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 20), legend.text = element_text(size = 18), legend.title = element_text(size = 24)) Here we use this output to plot a distribution plot showing the distribution of counts for the three different metrics – shares, comments and likes. We use the count as the x aesthetic, and count_type as both the col and fill aesthetics to colour them. The main function ggplot() will specify the aesthetics, and then we add additional elements to the plot by using the + command. Here we add the geom_density() element to plot the data in a density plot (the alpha value will make the colours transparent for overplotting), the labs() function will change the plot labels, and the theme() function let’s you change aspects of the figure text, such as the size.\nNote that here I have plotted the \\(log_{10}\\) of the counts, which reduces the effects of outliers. Also note that I have added 1 to the counts. This is because \\(log_{10}(0)\\) is undefined. The idea here is that a count of 1 will get a value of 0, a count of 10 gets a value of 1, a count of 100 gets a value of 2, etc.\nSo what does this tell us? Well not too much really. Not many people share posts from the page, but there aren’t too many that don’t get comments or likes. So it is a very active community. Posts tend to have more comments than likes, which makes sense because you can only like something once, but can comment as many times as you want. But the main thing that this shows is that these counts all seem to be in the right sort of expected range.\nOften exploratory plots like this can be useful to highlight problems in the raw data. One such example might be if a negative count existed in these data, which could happen due to input errors but quite clearly does not represent a valid count. As it happens, since these data are not manually curated, it is highly unlikely that such errors will be present, but you should never assume anything about any given data set.\n 6 Top Contributors Let’s take a look at the all-time most common contributors to the page, again using the ggplot2 package:\ntop_contributors \u0026lt;- URC %\u0026gt;% count(from_name) %\u0026gt;% top_n(50, n) %\u0026gt;% arrange(desc(n)) ggplot(top_contributors, aes(x = factor(from_name, levels = top_contributors$from_name), y = n, fill = n)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + scale_fill_gradient(low=\u0026quot;blue\u0026quot;, high=\u0026quot;red\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Posts\u0026quot;) + theme(axis.title = element_text(size = 18), axis.text.x = element_text(size = 12, angle = 45, hjust = 1), axis.text.y = element_text(size = 14), legend.position = \u0026quot;none\u0026quot;) Here I have first used dplyr to count up the number of posts per user and select the top 50 contributors, then used ggplot2 to plot a barplot showing the number of posts per person. I have used the scale_fill_gradient() element to colour the bars based on their height, such that those with the highest number of posts are coloured red, whilst those with the lowest are coloured blue.\nThe top contributor to the page is Neil Bryant (757 posts), who is the founder member so this makes sense. James Adams is the second biggest contributor (489 posts), and he has less of an excuse really.\nLet’s take a look at James’ posting habits:\nlibrary(\u0026quot;xts\u0026quot;) jamesadams \u0026lt;- URC %\u0026gt;% filter(from_name == \u0026quot;James Adams\u0026quot;) %\u0026gt;% mutate(created_time = as.POSIXct(created_time)) %\u0026gt;% count(created_time) jamesadams_xts \u0026lt;- xts(jamesadams$n, order.by = jamesadams$created_time) jamesadams_month \u0026lt;- apply.monthly(jamesadams_xts, FUN = sum) plot(jamesadams_month, ylab = \u0026quot;Number of Posts\u0026quot;, main = \u0026quot;\u0026quot;, cex.lab = 1.7, cex.axis = 1.4) Here I have used the package xts to deal with the POSIXct date format. In particular this will deal correctly with months with zero counts. James has been pretty active since summer 2013 (probably publicising his book), but his activity has been on the decline throughout 2016 – the price you pay when your family size doubles I guess.\n 7 When are people posting? Next we can break the posts down by the day on which they are posted:\nURC \u0026lt;- URC %\u0026gt;% mutate(dow = factor(weekdays(as.POSIXct(created_time)), labels = c(\u0026quot;Monday\u0026quot;, \u0026quot;Tuesday\u0026quot;, \u0026quot;Wednesday\u0026quot;, \u0026quot;Thursday\u0026quot;, \u0026quot;Friday\u0026quot;, \u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;))) post_day \u0026lt;- URC %\u0026gt;% count(dow) ggplot(post_day, aes(x = dow, y = n, fill = dow)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Posts\u0026quot;) + theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1), axis.text.y = element_text(size = 16), axis.title = element_text(size = 20), legend.position = \u0026quot;none\u0026quot;) Similarly to previously, here I have used dplyr to reduce the full data down to a table of counts of posts per day of the week, then plotted them using ggplot2. Surprisingly (to me anyway) there is no increase in activity during the weekend. I guess most of us are checking Facebook during working hours and busy running at the weekend…\nWednesdays are interestingly bereft of posts though for some reason. Could this be people checking URC at work on Monday and Tuesday through boredom, only to find themselves told off and having to catch up on work by Wedesday? Then by the time the weekend rolls around we’re all back liking away with impunity ready to go through the whole process again the next week.\nLet’s look at the same plot for the 1,000 most popular posts (based on likes):\npost_day \u0026lt;- URC %\u0026gt;% top_n(1000, likes_count) %\u0026gt;% count(dow) ggplot(post_day, aes(x = dow, y = n, fill = dow)) + geom_bar(stat = \u0026quot;identity\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Number of Posts\u0026quot;) + theme(axis.text.x = element_text(size = 20, angle = 45, hjust = 1), axis.text.y = element_text(size = 16), axis.title = element_text(size = 20), legend.position = \u0026quot;none\u0026quot;) So from this it is clear that if you want people to like your post, you should post on a Tuesday or a Thursday. Quite why people might be feeling so much more likely to click that all important like button on these dayas, I have no idea. But hey, stats don’t lie.\n 8 Most Popular Posts So looking at the popular posts above got me thinking about how best to actually define a “popular” post. Is it a post with a lot of likes, or a post that everybody is commenting on? Let’s take a look at the top 5 posts based on each criteria:\nprint.AsIs(URC %\u0026gt;% arrange(desc(likes_count)) %\u0026gt;% top_n(5, likes_count) %\u0026gt;% select(message))  message 1 Thinking how far I’ve come and getting a bit emotional. .3 year ago i was a size 20/22 and couldnt run to end of the street. Yesterday i ran 30 mile as a training run and wasn’t even aching afterwards. Now nearly 45 and a size 10 and never felt better. I love my life!!!! 2 I saw this picture couple years ago and I found it very inspiring so I thought I’d share it. is a 12-year-old childhood cancer survivor who loves to run with his dad. 3 Not sure if swear words are accepted. 088208820882 4 “What you doing up here?” said the sheep.Pike last night, not a soul to be seen… 5 0880\nprint.AsIs(URC %\u0026gt;% arrange(desc(comments_count)) %\u0026gt;% top_n(5, comments_count) %\u0026gt;% select(message))  message 1 For me claiming something you haven’t earned is not only immoral it’s fraudulent. And it’s a huge insult to all who’ve attempted the feat before you and legitimately fallen short. What do you guys think? 2 Anyone else do race to the stones and found it a rip off? I was not impressed with most things. were good such as medics and lots of water but who wants fudge bars and cadburies at aid stations. like a money making race to me, especially considering all the sponsorship they had. ’ve got lots of other rants about it but let’s hear anyone else’s thoughts first 3 New Game.am trying to convince some new ultra runners that you do not need to spend a load of money on kit to put one foot in front of the other a few more times. This is difficult given that half the posts in forums seems to be asking for recommendations or giving recommendations as to how one might waste money on kit.out of interest, what was the value of the kit you wore in your last ultra? Including nutrition. Obviously you will have to guess if you had them as a gift or can’t remember. Surely someone is going to have less than £100? 4 I had a small sabre rattling session last night with someone on this group. Nothing major by any stretch of the imagination - we just have opposing views on DNF. But it got me curios to what the opinions of others are on this subject. Is failing to finish something that you would risk your life to avoid? Is it something to fear? Is it something that will eventually happen to us all? Is it something that we can learn from? Etc,etc. Your thoughts please 5 i cannot wait to watch the EPSN footage - amazing stuff. It is a shame Robert has his doubters though.: was described as “trolling”, which was an over the top description (agree with the comments there)\nIt seems to me that the posts with more likes tend to be posts with a much more positive message than those with most comments. The top liked posts are those from people who have overcome some form of adversity (such as the top liked post with 1,369 likes from Mandy Norris who had awesomely run 30 miles after losing half her body weight), whilst the top commented posts tend to be more controversial posts (such as the top commented post with 287 comments about Mark Vaz’s fraudulent JOGLE “World Record”).\nLet’s take a look at how closely these two poularity measures are correlated in the Ultra Running Community:\nggplot(URC, aes(x = comments_count, y = likes_count)) + geom_point(shape = 19, alpha = 0.2, size = 5) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = TRUE) + geom_point(data = URC %\u0026gt;% top_n(5, comments_count), aes(x = comments_count, y = likes_count, color = \u0026quot;red\u0026quot;, size = 5)) + geom_point(data = URC %\u0026gt;% top_n(5, likes_count), aes(x = comments_count, y = likes_count, color = \u0026quot;blue\u0026quot;, size = 5)) + labs(x = \u0026quot;Number of Comments\u0026quot;, y = \u0026quot;Number of Likes\u0026quot;) + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 20), legend.position = \u0026quot;none\u0026quot;) Here we are plotting a correlation scatterplot between the number of likes and the number of comments for each post. I have a set an alpha value of 0.2 for the scatterplot so that the individual points are made more see-through. That way, overplotting can be seen by darker coloured plots. I have also added in a line of best fit generated by fitting a linear model (method= lm), together with an estimate of the standard error shown by the grey surrounding of the blue line (se = TRUE). Finally I have highlighted the top 5 commented posts in blue, and the top 5 liked posts in red.\nIt is pretty clear from this plot that there is virtually no correlation between the number of comments and the number of likes, particularly for those with more likes or comments. In general the posts with more likes do not have the most comments (and vice versa), suggesting that in general we like the nice posts, but comment on the ones that upset us. In fact, it looks as if Mandy’s post is the only exception, with both the highest number of likes but also a high number of comments (220).\nWe can calculate the correlation between these measures, which is a measure of the linear relationship between two variables. A value of 1 indicates that they are entirely dependent on one another, whilst a value of 0 indicates that the two are entirely independent of one another. A value of -1 indicates an inverse depdnedancy, such that an increase in one variable is associated with a similar decrease in the other variable. Given the difference in the scales between likes and comments, I will use the Spearman correlation, which looks at correlation between the ranks of the data and therefore ensures that each unit increment is 1 for both variables meaning that it is robust to outliers. The Spearman correlation between these two variables is 0.27, so there is virtually no correlation between likes and comments.\n 9 How Often Do People Actually Talk About Ultras? It seems recently that there is more talk of charlatans and frauds than races, and a lot of people have commented on the fact that there seems to be less and less actual discussion of ultras recently. So let’s see if this is the case, by tracking how often the term ultra is actually used over time:\nultraposts \u0026lt;- URC %\u0026gt;% filter(str_detect(message, \u0026quot;ultra\u0026quot;)) %\u0026gt;% mutate(created_time = as.POSIXct(created_time)) %\u0026gt;% count(created_time) ultraposts_xts \u0026lt;- xts(ultraposts$n, order.by = ultraposts$created_time) ultraposts_month \u0026lt;- apply.monthly(ultraposts_xts, FUN = sum) plot(ultraposts_month, ylab = \u0026quot;Number of Ultra Posts\u0026quot;, main = \u0026quot;\u0026quot;) Over the last year or so, the number of people in the group has risen dramatically, and yet it certainly seems that fewer people are actually discussing ultras these days. I guess read into that what you will – perhaps the feed is indeed dominated by Suunto vs Garmin questions?\nHell, let’s find out.\n 10 Suunto or Garmin? So let’s take a look at the real question that everybody cares about – which is more popular, Suunto or Garmin. All my cards on the table; I have a Suunto Peak Ambit 3, but if it helps I had to Google that because I really don’t keep up on these things. I’m really not a gear not, and prefer to make do. The only reason that I got this is that my previous watch died a death, and I like to use a watch for navigation. I didn’t pay for it – at that price I couldn’t bring myself to fork out the money. But it was a gift, and I am very pleased with it. It has a great battery life, and is pretty simple when loading data to my computer. Despite being a stats guy, I don’t normally focus much on my own data, but actually it has been interesting to see how slow I have become recently due to an ongoing injury. Perhaps it will help me to push myself in training onece it is sorted.\nBut as I understand it, the Garmin Fenix 3 does exactly the same stuff. Is one better than the other? I couldn’t possibly say. Many people have tried, but I suspect that it comes down to personal preference rather than there being some objective difference between the two.\nBut just for the hell of it, let’s see how often people talk about the two. I will be simply using fuzzy matching to look for any use of the terms “suunto” or “garmin” in the post. Fuzzy matching is able to spot slight misspellings, such as “Sunto” or “Garmin”, and is carried out using the base agrep() function in R:\nsuunto \u0026lt;- agrep(\u0026quot;suunto\u0026quot;, URC$message, ignore.case = TRUE) garmin \u0026lt;- agrep(\u0026quot;garmin\u0026quot;, URC$message, ignore.case = TRUE) pie(c(length(setdiff(suunto, garmin)), length(setdiff(garmin, suunto)), length(intersect(suunto, garmin))), labels = c(\u0026quot;Suunto\u0026quot;, \u0026quot;Garmin\u0026quot;, \u0026quot;Both\u0026quot;), cex = 2) So of the 24,836 posts on the URC page, 237 (0.95 %) mention Suunto, whilst 552 (2.22 %) mention Garmin. Only 77 (0.3 %) mention both, which I assume are the posts specifically asking which of the two is best. Given the way some people moan about how often this question comes up, these numbers are actually surprisingly small I think. But based on this it seems that Garmin is more popular, although it would be interesting to look at the actual responses on those “VS” posts to see what the outcome actually was in each case.\nHaving said that, there is nothing to suggest what these posts about Garmin’s are actually saying. They may be generally saying that they hate Garmins. So I am going to play around with a bit of sentiment analysis using the qdap package. Essentially this is a machine learning algorithm that has been trained to identify the sentiment underlying a post, with positive values representing a generally positive sentiment (and vice versa). So let’s break down the Garmin and Suunto posts to see how they stack up:\nlibrary(\u0026quot;qdap\u0026quot;) ## Convert to ASCII and get rid of upper case suunto_msg \u0026lt;- URC$message[suunto] %\u0026gt;% iconv(\u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub = \u0026quot;\u0026quot;) %\u0026gt;% tolower garmin_msg \u0026lt;- URC$message[garmin] %\u0026gt;% iconv(\u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub = \u0026quot;\u0026quot;) %\u0026gt;% tolower ## Calculate the sentiment polarity suunto_sentiment \u0026lt;- polarity(gsub(\u0026quot;[[:punct:]]\u0026quot;, \u0026quot;\u0026quot;, suunto_msg)) garmin_sentiment \u0026lt;- polarity(gsub(\u0026quot;[[:punct:]]\u0026quot;, \u0026quot;\u0026quot;, garmin_msg)) ## Plot in a stacked barplot sent_dat \u0026lt;- data.frame(Watch = c(rep(\u0026quot;Suunto\u0026quot;, length(suunto)), rep(\u0026quot;Garmin\u0026quot;, length(garmin))), Sentiment = c(suunto_sentiment$all$polarity, garmin_sentiment$all$polarity)) ggplot(sent_dat, aes(x = Sentiment, col = Watch, fill = Watch)) + geom_density(alpha = 0.1) + labs(x = \u0026quot;Sentiment Polarity\u0026quot;, y = \u0026quot;Density\u0026quot;) + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 20), legend.text = element_text(size = 18), legend.title = element_text(size = 24)) So a value of zero on this distribution plot suggests a neutral sentiment to the post, a positive number suggests a positive sentiment, and a negative number suggests a negative sentiment. While the majority of the posts seem to be fairly neutral in both cases, it certainly seems that the majority of the Garmin posts are positive whilst the Suunto posts are more neutral with many positive and negative posts.\nWe can actually put a number on this, for whether or not there is truly a statistically significant difference between the distribution of sentiment scores for the two watches. To do this, we will us a statistical test that checks how likely it is that we would see a difference of the magnitude seen here given that there is no difference between what people actually think of the watch. This is the so-called “null-hypothesis”, and essentially says that there is no difference, and any differences that we do see are purely random errors. We can test this hypothesis using one of a number of different tests, with the aim to see whether there is evidence that we can reject this null hypothesis, thus suggesting that there is indeed a true difference between the distributions. So we never really “prove” that there is a difference, but instead show that there is suitable evidence to disprove the null hypothesis.\nTo do this some test statistic is calculated and is tested to see if it is significantly different than what you would expect to see by chance. Typically this is assessed using a “p-value”, which is one of the most misunderstood measurements in statistics. This value represents the probability that you would see a test statistic at least as high as that measured purely by chance, even if both sets of data were drawn from the same distribution. So both the Garmin and Suunto scores are a tiny subset of all possible opinions of people in the world, the population distribution. Our two sample populations are either drawn from an overall population where there is no difference, or from two distinct populations for people who have a view one way or the other.\nIt is pretty clear from the above figure that these values are not normally distributed (a so-called “bell-curve” distribution), so we cannot use a simple t-test which basically tests the difference in the means between two distributions (after taking the variance into account). Instead we would be better off using a non-parametric test which does not require the data to be parameterised into some fixed probability density function. The Kolmogorov Smirnov test is one method that can be used, and works by looking at how the cummulative distribution functions of two distinct samples differ:\nks.test(subset(sent_dat, Watch == \u0026quot;Suunto\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]], subset(sent_dat, Watch == \u0026quot;Garmin\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]]) ## Warning in ks.test(subset(sent_dat, Watch == \u0026quot;Suunto\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]], ## subset(sent_dat, : p-value will be approximate in the presence of ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: subset(sent_dat, Watch == \u0026quot;Suunto\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]] and subset(sent_dat, Watch == \u0026quot;Garmin\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]] ## D = 0.093653, p-value = 0.1091 ## alternative hypothesis: two-sided Here we run a two-sided test, which simply means that we have no a priori reason to suspect one distribution to be greater than the other. We could do a one-sided test where the alternative hypothesis that we are testing is “A is greater than B”, rather than the two-sided test where we are testing the alternative hypothesis that “A is not equal to B”. \\(D\\) is the maximum distance between the empirical distribution functions, and \\(p\\) is the p-value. Typically, a threshold used to reject the null hypothesis is for \\(p\\) to be less than 0.05 (5 %), although it is fairly arbitrary. But in this case, we would conclude that there is not sufficient evidence to reject the null hypothesis.\nAs an alternative, we can instead use the Wilcoxon rank sum test (also called the Mann-Whitney U test). The idea is to rank all of the data, sum up the ranks from one of the samples, and use this to calculate the test statistic \\(U\\) (which takes into account the sample sizes). So if the distributions are pretty similar, the sum of the ranks will be similar for sample 1 and sample 2. If there is a big difference, one sample will have more higher ranked values than the other, resulting in a higher value for \\(U\\). Let’s take a look at this:\nwilcox.test(subset(sent_dat, Watch == \u0026quot;Suunto\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]], subset(sent_dat, Watch == \u0026quot;Garmin\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]]) ## ## Wilcoxon rank sum test with continuity correction ## ## data: subset(sent_dat, Watch == \u0026quot;Suunto\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]] and subset(sent_dat, Watch == \u0026quot;Garmin\u0026quot;)[[\u0026quot;Sentiment\u0026quot;]] ## W = 59157, p-value = 0.03066 ## alternative hypothesis: true location shift is not equal to 0 Note that the test statistic here is actually \\(W\\), but in this case \\(W\\) and \\(U\\) are equivalent. So this test would result in us rejecting the null hypothesis with the same threshold as above. So which one is correct? Well, this is a great example of why you should never trust statistics. Both of these are perfectly reasonable tests to perform in this case but give different results. Many people would simply choose the one that gives the lowest p-value, but this is pretty naughty and is often called “p-hacking”. At the end of the day, a p-value higher than 0.05 does not mean that there is not a true difference between the distributions, just that the current data does not provide enough evidence to reject the null hypothesis.\nSo my conclusion from this is that I made the wrong decision, and will from now on look upon my useless Suunto watch with hatred and resentment. I can only hope that this post will save anyone else from making such an awful mistake.\n 11 Summing Up It has been quite fun playing around with these data tonight, and I have had an opportunity to try out a few new techniques that I have wanted to play with for a while. As ever, there is loads more that can be gleaned from these data, but I should probably do something a little more productive right now. Like sleeping. I have actually done a while load more playing with machine learning algorithms of my own, but this post has already become a little too unruly so I will post this later as a separate post.\nBut in summary, people on the Ultra Running Community page spend far too much time posting during working hours, seem to be talking less and less about ultra running, and definitely prefer Garmins to Suuntos. So this has all been completely worth it.\n 12 Session Info sessionInfo() ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Sierra 10.12.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] qdap_2.3.2 RColorBrewer_1.1-2 qdapTools_1.3.3 ## [4] qdapRegex_0.7.2 qdapDictionaries_1.0.7 xts_0.11-2 ## [7] zoo_1.8-6 ggplot2_3.2.0 stringr_1.4.0 ## [10] dplyr_0.8.3 tidyr_1.0.0 randomForest_4.6-14 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.2 lattice_0.20-38 xlsxjars_0.6.1 ## [4] gtools_3.8.1 assertthat_0.2.1 zeallot_0.1.0 ## [7] digest_0.6.20 slam_0.1-45 R6_2.4.0 ## [10] plyr_1.8.4 chron_2.3-54 backports_1.1.4 ## [13] evaluate_0.14 blogdown_0.16 pillar_1.4.2 ## [16] rlang_0.4.0 lazyeval_0.2.2 data.table_1.12.2 ## [19] gdata_2.18.0 rmarkdown_1.14 gender_0.5.2 ## [22] labeling_0.3 igraph_1.2.4.1 RCurl_1.95-4.12 ## [25] munsell_0.5.0 compiler_3.6.1 xfun_0.8 ## [28] pkgconfig_2.0.2 htmltools_0.3.6 reports_0.1.4 ## [31] tidyselect_0.2.5 tibble_2.1.3 gridExtra_2.3 ## [34] bookdown_0.12 codetools_0.2-16 XML_3.98-1.20 ## [37] crayon_1.3.4 withr_2.1.2 bitops_1.0-6 ## [40] openNLP_0.2-6 grid_3.6.1 gtable_0.3.0 ## [43] lifecycle_0.1.0 magrittr_1.5 scales_1.0.0 ## [46] xlsx_0.6.1 stringi_1.4.3 reshape2_1.4.3 ## [49] NLP_0.2-0 openNLPdata_1.5.3-4 xml2_1.2.0 ## [52] venneuler_1.1-0 ellipsis_0.2.0.1 vctrs_0.2.0 ## [55] wordcloud_2.6 tools_3.6.1 glue_1.3.1 ## [58] purrr_0.3.3 plotrix_3.7-6 parallel_3.6.1 ## [61] yaml_2.2.0 tm_0.7-6 colorspace_1.4-1 ## [64] rJava_0.9-11 knitr_1.23  ","date":1524009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524009600,"objectID":"e1c3af067f3388d9d2b91f54eb0d76c6","permalink":"/post/2018-04-18-suunto-or-garmin/","publishdate":"2018-04-18T00:00:00Z","relpermalink":"/post/2018-04-18-suunto-or-garmin/","section":"post","summary":"Analysis of posts from the Ultra Running Community (URC) Facebook page looking at syntax usage, posting habits, and sentiment analysis using R.","tags":["R","Data Science","Facebook","ultrarunning","ultra","running","ultramarathon","marathon","Suunto","Garmin","dplyr","tidyr","ggplot2"],"title":"Suunto Or Garmin? The Age Old Question.","type":"post"},{"authors":null,"categories":["Blog","Data Science","Running"],"content":"  1 Note 2 Welcome! 3 Setting up the Twitter API 4 TwitteR 5 Ultramarathon, Ultra Marathon, or Ultra-Marathon? 6 Ultrarunning, Ultra Running or Ultra-Running? 7 Word Cloud 8 Final Word   1 Note This blog post was originally written in 2017 for a less formal personal blog with a focus on ultramarathon running, which is a hobby that I am quite passionate about. I have decided to include all of my data science related blog posts here to keep things centralised, but please excuse the more informal language used throughout.\n 2 Welcome! Well, welcome to my new blog I guess. I have had a bit of a hiatus from writing recently but I am trying to get back into the habit over on my running-related blog http://constantforwardmotion.blogspot.com. Over there I will be mainly moaning about my injury woes (and maybe talking about some of my more ridiculous races if I ever get back to being able to run again), but this blog is a little bit different. In the past I have played around with various bits of data for some quite interesting (I think anyway) posts (e.g. this post looking at the 2013 Centurion Running Thames Path 100 mile race). I am a data analyst by trade, and I am about to start a whole new stage of my career, working as a Senior Post Doc at the University of Portsmouth where I will be building my own Bioinformatcs lab. Scary stuff.\nAnyway, I decided to set this blog up as a more technical place to play around with various data analysis techniques, new algorithms, new packages, etc. Since it is something that I am pretty passionate about, there is likely to be a bit of a running theme throughout, but really I will be looking at data from a whole load of different sources. I often play around with “fun” challenges like those set by Project Euler and Kaggle, so I figured that it may be useful for me to put some of these out there in case my dumb mistakes when learning these things can help somebody else in the future. As the great philosopher Jake from Adventure Time, once said:\n Dude, sucking at something is the first step towards being sorta good at something\n So, here is the first post in what I hope will become a regular source of potentially interesting data wrangling. I just wanted to do something simple to start with, and one thing that I have always wanted to play with is the Twitter API for accessing the myriad completely valid and interesting opinions of the millions of Twitter users out there… Hopefully I will keep away from the shadier parts of the interwebz, but in all seriousness there is a huge amount of useful stuff floating around out there.\nSo quite why I picked this particular question as my first post I have no idea. With billions of opinions and social interactions available to me, I have chosen to answer the following rather inconsequential question:\n Is it an “Ultramarathon”, “Ultra Marathon” or “Ultra-Marathon”?\n My idea of a good time is running a really, really long way, usually for a whole day or sometimes even more. Any race over a marathon in distance is called an “ultramarathon” - i.e. it is beyond the marathon. Now, there is a huge amount of baggage attached to this, and some people seem to really get their panties in a bunch over the term. Does it “count” as an ultramarathon if I run back to the car after a marathon? Does it “count” as an ultramarathon if I walk most of it? Does it “count” as an ultramarathon if I run a marathon a day for a whole week? There’s a lot of questions about “counting”, but I’ve never been very good at counting personally (says the mathematician…). I actually really dislike the word myself as it smacks a little of elitism, and I prefer to just think of it as running. A 10K is a running race, a marathon is a running race, and a 100 miler is a running race. Let’s just leave it at that.\nAnyway, one thing that I have never seen definitively answered is what the correct nomenclature should actually be, and I find myself switching willy nilly between the three possible spellings as the winds change. I’ve probably used all three in this post. So I thought that I would let the people speak, and see what the general consensus is of the Twitterati. And let’s face it, no ultrarunner worth their salt would run without Tweeting about it. So let’s take a look at which term is used most often on Twitter and settle this mass debate that I am having with myself (chortle) once and for all.\n 3 Setting up the Twitter API Twitter uses OAuth as a way to control programmatic access to its information without requiring passwords. Essentially, Twitter grants you an access token which is used to grant you access to the client information via a web service without actually giving you direct access to the client machine. It’s pretty easy to set up. Once you have a Twitter account set up, go to https://apps.twitter.com and click on the “Create New App” button. This will bring up the following page:\nCreate a new app in Twitter\n I filled this in with details of this blog, and created a new app. Note that to avoid errors further down the line, you need to set the “Callback URL” field to http://127.0.0.1:1410 which will return to your localhost on port 1410 following authentication. Press “Create your Twitter application” to create your app. This will take you to a page with information about your new app, including a tab at the top of the page called “Keys and Access Tokens”. The “API Key” and “API Secret” can then be used to access the API.\nOne important change to make to the basic settings is to click on the “Permissions” tab and make sure that your app is set up to have permissions to “Read, Write and Access direct messages”. Be sure to regenerate your access tokens after making any changes.\n 4 TwitteR Since R is my go-to analysis package, I will be using the TwitteR package from Jeff Gentry to access the API. You can also access through scripting languages like perl and python, which I will likely explore in the future. You can install TwitteR from the Comprehensive R Archive Network by doing the following:\ninstall.packages(\u0026quot;twitteR\u0026quot;) Or alternatively you can install the less stable, but more up-to-date, development version from GitHub:\ninstall_github(\u0026quot;geoffjentry/twitteR\u0026quot;) This requires the devtools package to be installed as well.\nWe should now be set up, but actually I found that I also needed to install some additional packages so that OAuth credentials can be correctly captured in the browser-based authentication:\ninstall.packages(\u0026quot;httpuv\u0026quot;) install.packages(\u0026quot;httr\u0026quot;) Now we need to set up our authorisation (put your API key and secret in place of these placeholders):\nlibrary(\u0026quot;twitteR\u0026quot;) setup_twitter_oauth(\u0026quot;API key\u0026quot;, \u0026quot;API secret\u0026quot;) This will open up a browser session where you can authenticate your API app. After this, close the browser and you are ready to go!\n 5 Ultramarathon, Ultra Marathon, or Ultra-Marathon? So now we are all set up and we can take a look at how to access the API. The workhorse of the twitteR package is the searchTwitter() function. This can search for something like a hashtag or key word, and can use basic boolean logic such as AND (+) and OR (-). The API actually only allows you to access information from a short time in the past, so we can only get Tweets from the last week or so. So let’s get the most recent Tweets relating to ultrarunning and count how many Tweets over the last few days have used the three different terms:\nlibrary(\u0026quot;twitteR\u0026quot;) numtweets \u0026lt;- 500000 um_tweets_all \u0026lt;- searchTwitter(\u0026quot;ultramarathon|ultra marathon\u0026quot;, n = numtweets) um_tweets_trim \u0026lt;- strip_retweets(um_tweets_all, strip_manual=TRUE, strip_mt=TRUE) um_tweets_text \u0026lt;- sapply(um_tweets_trim, function (x) x$getText()) um_tweets_text \u0026lt;- um_tweets_text[grep(\u0026quot;ultramarathon|ultra marathon|ultra-marathon\u0026quot;, um_tweets_text, ignore.case = TRUE)] um_count \u0026lt;- NULL for (t in c(\u0026quot;ultramarathon\u0026quot;, \u0026quot;ultra-marathon\u0026quot;, \u0026quot;ultra marathon\u0026quot;)) { um_count[[t]] \u0026lt;- length(grep(t, um_tweets_text, ignore.case = TRUE)) } par(mar = c(0,0,0,0)) pie(um_count, col = c(\u0026quot;grey90\u0026quot;,\u0026quot;grey70\u0026quot;,\u0026quot;grey50\u0026quot;)) Let’s look at this code. After loading the twitteR package and specifying the number of Tweets to load, we access the Twitter API using searchTwitter and load in the 500,000 most recent Tweets with the terms “ultra”, “running”, or “ultrarunning” in them (there will be many fewer than this, but I want to capture every Tweet possible). This produces a list of 961 objects of class status, which is a specially defined reference class as a container for Twitter statuses. Next we strip out all of the retweets to leave 585 Tweets, then pull out the text from the list objects (sapply() applies the accessor function getText() to all elements of the list). Since I am only interested in three specific terms, I use regular expressions to look only at Tweets containing one of these terms (giving 316 Tweets), and then count how many Tweets contain each of the three specific terms. Finally I generate a pie chart of the results (yes, yes, I know - I hate pie charts as well).\nA couple of things here. First of all, there are a lot of retweets. Of the 961 Tweets originally analysed, only 585 remain after removing the retweets. This means that 39.13% of these Tweets were retweets. Man, we ultrarunners aren’t very original are we? Unfortunately this therefore drastically reduces the number of Tweets that I am analysing. Secondly, because of the way the pattern matching is done we end up with a lot of Tweets with “ultra” or “running” in them that don’t match any of the three specific terms that I am looking at here. Also, this counting may be double counting some Tweets if both versions are used in a single Tweet. But I can’t be bothered taking such stupidity into account right now! ;)\nSo with these caveats in place, it seems pretty clear that the correct term is most definitely “Ultra Marathon”. So there you go.\n 6 Ultrarunning, Ultra Running or Ultra-Running? Okay cool. So we know how to define the event. How about the act of running an ultra marathon? So let’s do the same again, this time looking at whether I should be saying “ultrarunning”, “ultra running”, or “ultra-running”. The code is practically identical, just using slightly different words in the regular expression:\nur_tweets_all \u0026lt;- searchTwitter(\u0026quot;ultrarunning|ultra running\u0026quot;, n = numtweets) ur_tweets_trim \u0026lt;- strip_retweets(ur_tweets_all, strip_manual=TRUE, strip_mt=TRUE) ur_tweets_text \u0026lt;- sapply(ur_tweets_trim, function (x) x$getText()) ur_tweets_text \u0026lt;- ur_tweets_text[grep(\u0026quot;ultrarunning|ultra running|ultra-running\u0026quot;, ur_tweets_text, ignore.case = TRUE)] ur_count \u0026lt;- NULL for (t in c(\u0026quot;ultrarunning\u0026quot;, \u0026quot;ultra-running\u0026quot;, \u0026quot;ultra running\u0026quot;)) { ur_count[[t]] \u0026lt;- length(grep(t, ur_tweets_text, ignore.case = TRUE)) } par(mar = c(0,0,0,0)) pie(ur_count, col = c(\u0026quot;grey90\u0026quot;,\u0026quot;grey70\u0026quot;,\u0026quot;grey50\u0026quot;)) There are a lot more Tweets relating to ultra “running” compared to ultra “marathon”, with 11,984 Tweets in the starting data set. However, again we lose a lot of Tweets through retweets leaving us with only 2,112 Tweets to play with. After trimming out Tweets that don’t follow the format that I am looking at here, we are left with only 179 – even less than in the last analysis.\nIn this case, it is less clear cut, and whilst the single word term “ultrarunning” is used most often, the two word “ultra running” is not far behind. Damn, I wanted a clearly defined outcome, but I guess I will let you off whichever one you choose to use. But god help anybody who chooses to hyphenate either term…\n 7 Word Cloud Since we have these Tweets available, let’s generate a word cloud to see what other terms are being talked about in relation to ultra marathons and ultrarunning. A word cloud takes some text and works out the most common words within it, then represents them in a cloud of words (funnily enough) with more common words being more prominent. Here we use the text mining package tm for identifying and processing unique words from these Tweets, and the wordcloud package for plotting them. The Tweets are loaded into a Corpus object, and various mappings are performed to remove irrelevant text like punctuation, as well as commonly used words in English like I, We, and, the, etc. Note that I have converted the encoding of all of these Tweets into UTF-8 encoding as I was having issues using the tolower() function when some Tweets contained non-UTF-8 characters. I have coloured the plot using the brewer.pal() function from the RColorBrewer package, which in this case generates a palette of 9 equally spaced colours ranging from Red to blue (via white). The parameters here will plot a maximum of 1,000 words, and will only consider a word if it is present more than 5 times. By not using a random order, the most prominent words are plotted at the center of the cloud:\nlibrary(\u0026quot;tm\u0026quot;) library(\u0026quot;wordcloud\u0026quot;) library(\u0026quot;SnowballC\u0026quot;) all_tweets \u0026lt;- c(ur_tweets_text, um_tweets_text) all_tweets \u0026lt;- iconv(all_tweets, \u0026quot;latin1\u0026quot;, \u0026quot;ASCII\u0026quot;, sub = \u0026quot;\u0026quot;) ## Convert encodings cloud_dat \u0026lt;- Corpus(VectorSource(all_tweets)) ## Create Corpus cloud_dat \u0026lt;- tm_map(cloud_dat, PlainTextDocument) ## Make plain text cloud_dat \u0026lt;- tm_map(cloud_dat, content_transformer(tolower)) ## Convert to lower case cloud_dat \u0026lt;- tm_map(cloud_dat, removePunctuation) ## Remove punctuation cloud_dat \u0026lt;- tm_map(cloud_dat, removeWords, stopwords(\u0026quot;english\u0026quot;)) ## Remove common English words par(mar = c(0,0,0,0)) wordcloud(cloud_dat, max.words = 1000, min.freq = 5, random.order = FALSE, colors = brewer.pal(9, \u0026quot;RdBu\u0026quot;)) Kind of what you would expect, with ultra, marathon, running, ultramarathon, and ultrarunning being most prominent. training is also pretty common, so it’s good to know that there is some of that going on between Tweets. I’m also pleased to see that tom is quite enriched, which I can only assume to be Tom from Bognor’s take-over of the internet. It’s also nice to see people talking about shorts, but come on people – it’s always shorts weather!\n 8 Final Word This has been quite useful for me to get the hang of using the Twitter API, and I hope it has been a little interesting or useful for some of you. I am hoping that this will kick off regular use of this blog, and I will try and update it more regularly along with my less technical running blog as I start using it to play with new toys in my work. And hey, at least now you know that they are called Ultra Marathons and that I love Ultrarunning. So we’ve all learned something today. And knowing is half the battle.\n ","date":1523750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523750400,"objectID":"9ccb6d79db4a85ec1e87038eb4a6156a","permalink":"/post/2018-04-15-ultrarunner-or-ultra-runner/","publishdate":"2018-04-15T00:00:00Z","relpermalink":"/post/2018-04-15-ultrarunner-or-ultra-runner/","section":"post","summary":"This post explores how to access tweets from Twitter using the Twitter API to see which term is used most often to desribe people partaking in ultra distance running.","tags":["R","Twitter","Data Science","Ultra","Marathon","Running"],"title":"Ultramarathon, Ultra Marathon or Ultra-Marathon?","type":"post"},{"authors":null,"categories":["Blog"],"content":"Welcome to the website of Dr Sam Robson, Lead Bioinformatician at the Centre for Enzyme Innovation at the University of Portsmouth. I set up this group having recently moved from the University of Cambridge, and hope to develop the group further over the coming years. I am going to aim to post regularly on this blog with posts relating to data analysis, experimental design, advances in sequencing technology, and other related matters that may be of interest to visitors.\nI have kicked things off with a tutorial for those of you that are interested in learning how to use the statistical programming language R. This language is incredibly useful for anybody looking to perform any kind of statistical analysis, and the Bioconductor packages offer countless extensions to the base functionality to allow you to work with data of any type that you might care to mention. If you are interested in data analysis, I thoroughly recommend checking it out, and if you find it useful or if you splot any issues please comment.\nIn addition, I have begun to build a list of commonly used bioinformatics tools, which may help other people to see which tools are currently available for a variety of different bioinformatics tasks. It is a work in progress, so please bear with me while I continue to populate it. I hope to keep this up to date as I discover new tools.\nIn the short time that I have been at the University of Portsmouth, I have had similar conversations with a number of researchers and so will focus on discussing some of these commonly occurring questions in my first few posts (e.g. sample size for experimental design, number of reads required for a sequencing experiment, where to sequence your data, differences between sequecning technologies, etc.). Some of these may then develop further into additional resources.\n","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"3a51d96db9e07a3192570fbe3ad0d79c","permalink":"/post/2018-04-01-welcome-to-the-university-of-portsmouth-bioinformatics-group-website/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/post/2018-04-01-welcome-to-the-university-of-portsmouth-bioinformatics-group-website/","section":"post","summary":"Welcome to the website of Dr Sam Robson, Lead Bioinformatician at the Centre for Enzyme Innovation at the University of Portsmouth.","tags":["Bioinformatics","University of Portsmouth","Welcome"],"title":"Welcome to the University of Portsmouth Bioinformatics Group Website","type":"post"},{"authors":["Barbieri I","Tzelepis K","Pandolfini L","Shi J","Millán-Zambrano G","Sam Robson","Aspris D","Migliori V","Bannister AJ","Han N","De Braekeleer E","Ponstingl H","Hendrick A","Vakoc CR","Vassiliou GS","Kouzarides T"],"categories":null,"content":"","date":1512604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512604800,"objectID":"314950648627ab731f9a2beca4570e8f","permalink":"/publication/2017_promoter-bound_mettl3_maintains_myeloid_leukaemia_by_m6a-dependent_translation_control/","publishdate":"2017-12-07T00:00:00Z","relpermalink":"/publication/2017_promoter-bound_mettl3_maintains_myeloid_leukaemia_by_m6a-dependent_translation_control/","section":"publication","summary":"N6-methyladenosine (m6A) is an abundant internal RNA modification in both coding and non-coding RNAs that is catalysed by the METTL3–METTL14 methyltransferase complex. However, the specific role of these enzymes in cancer is still largely unknown. Here we define a pathway that is specific for METTL3 and is implicated in the maintenance of a leukaemic state. We identify METTL3 as an essential gene for growth of acute myeloid leukaemia cells in two distinct genetic screens. Downregulation of METTL3 results in cell cycle arrest, differentiation of leukaemic cells and failure to establish leukaemia in immunodeficient mice. We show that METTL3, independently of METTL14, associates with chromatin and localizes to the transcriptional start sites of active genes. The vast majority of these genes have the CAATT-box binding protein CEBPZ present at the transcriptional start site, and this is required for recruitment of METTL3 to chromatin. Promoter bound METTL3 induces m6A modification within the coding region of the associated mRNA transcript, and enhances its translation by relieving ribosome stalling. We show that genes regulated by METTL3 in this way are necessary for acute myeloid leukaemia. Together, these data define METTL3 as a regulator of a chromatin based pathway that is necessary for maintenance of the leukaemic state and identify this enzyme as a potential therapeutic target for acute myeloid leukaemia.","tags":[""],"title":"Promoter-bound METTL3 maintains myeloid leukaemia by m6A-dependent translation control","type":"publication"},{"authors":null,"categories":["Bioinformatics"],"content":"  1 Commercial Sequencing Facilities 1.1 Theragen Etex 1.2 Source Bioscience 1.3 LGC Sequencing Service 1.4 Mr. DNA Lab 1.5 GATC Biotech 1.6 Genome Scan  2 Comparison Table 2.1 Platforms 2.2 Technologies 2.3 Services    On this page you will find a list of commercial and academic sequencing facilities that you may wish to consider when looking for sequencing solutions for your project. In cases where the facility have been used by a member of the faculty here at the University of Portsmouth, you will find links to the researchers with experience. I will attempt to collate some feedback from these researchers on this page over time, but if you have any further questions, please contact the researchers indicated directly.\n1 Commercial Sequencing Facilities 1.1 Theragen Etex A company in South Korea who offer a number of different sequencing services, including whole genome, exome, transcriptome, and epigenome sequencing technologies. They offer sequencing using Illumina HiSeq 4000/2500, Ion Torrent PGM and Ion Torrent Proton sequencers. Their services include whole genome sequencing, exome sequencing, RNA seq, epigenomics and de novo assembly. They also offer library preparation and bioinformatics support if required. They offer a guarentee on sequence on coverage, although this results from pooling of samples across multiple lanes which may cause batch difficulties in downstream analyses. They are very quick to respond to emails and in our experience seem very keen to provide us with good quality data. Typically they seem to offer very competitive rates for sequencing needs.\nProf. Darek Gorecki - RNA seq\n 1.2 Source Bioscience A company based in Nottingham who seem to offer a very comprehensive catalogue of sequencing services, including RNA seq, ChIP seq, whole genome sequencing, exome sequencing, targeted sequening and metagenomics. They offer DNA/RNA extraction, library preparation, and bioinformatics support if required. They offer Illumina MiSeq, Illumina HiSeq and Illumina NextSeq services. They are one of the leading providers of commercial sequencing in Europe. Data that we have recieved from them in the past has been of high quality, however we have had one case of lost data which is yet to be resolved in a satisfactory manner. Communication when requesting services and data generation are very fast, but communication when problems have arisen has been very poor.\nProf. Darek Gorecki - RNA seq\nDr. Maria Salta - 16S rDNA Sequencing\nProf. Matt Guille - RNA Seq\n 1.3 LGC Sequencing Service LGC (Laboratory of the Government Chemist) are a UK-based sequencing company with a long history in bioanalytics. They offer a range of services as well as next generation sequencing, including consultancy, product testing and forensic science. They are able to provide services including RNA seq, ChIP seq, metagenomics, microbial sequencing, and genotyping. They offer assistance with DNA/RNA extraction, library preparation, project design, and bioinformatics support.\nJoanne Preston - 16S rDNA Sequencing\nJoy Watts - Transcriptome Assembly\n 1.4 Mr. DNA Lab They offer sequencing using Illumina MiSeq, Illumina HiSeq, Pacific Biosciences Sequel (for longer read lengths), Ion Torrent S5 XL and Ion Torrent PGM machines. They offer support for many sequencing services, including whole genome sequencing, exome sequencing, de novo assembly, metagenomics, RNA seq and ChIP seq. They offer a guarentee on sequence on coverage, although this results from pooling of samples across multiple lanes which may cause batch difficulties in downstream analyses.\n 1.5 GATC Biotech GATC Biotech are a German-based commercial sequencing facility providing a comprehensive list of next generation sequencing services including whole genome sequencing, de novo sequencing, targeted sequencing, amplicon sequencing, metagenomics, RNA seq, ChIP seq and BS seq. They have expertise in Illumina MiSeq, Illumina HiSeq and PacBio RS II. Their INVIEW and NGSELECT packages provide a complete service, from DNA/RNA extraction, through library preparation and sequencing, to bioinformatics analysis.\n 1.6 Genome Scan Through their ServiceXS department, Genome Scan provide a comprehensive series of research services, including whole genome sequencing, exome sequencing, RNA seq, small RNA seq and bisulfite seq. They offer sequencing using Illumina HiSeq 2500/4000, Illumina NextSeq 500 and Pacific Bioscience RS II (for longer read lengths). They provide a full sequencing service, including library proeration and downstream bioinformatics support.\n  2 Comparison Table Below is a table that I have compiled based on the instruments available and the analyses that each facility is able to provide. Please note that I may have missed something - if I have made a mistake please let me know at samuel.robson@port.ac.uk. It is highly likely that any of these facilities would have the capacity to run whatever sequencing experiment you are hoping to conduct, but this table is based on the expertise highlighted on their websites:\n2.1 Platforms   Facility Theragen Source LGC Mr DNA GATC Genome Scan    HiSeq T T F T T T  MiSeq F T T T T F  NextSeq F T T F F T  PacBio F F F T T T  Ion Torrent T F F T F F     2.2 Technologies   Facility Theragen Source LGC Mr DNA GATC Genome Scan    De Novo T T F T T T  Transcriptomics T T T T T T  Epigenetics ? T T T T T  WGS T T T T T T  WES T T T T T T  Metagenomics F T T T T T  Epigenomics T T F ? T T     2.3 Services   Facility Theragen Source LGC Mr DNA GATC Genome Scan    DNA/RNA Extraction ? T T ? T ?  Library Prep T T T T T T  Bioinformatics T T T T T T      ","date":1508198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508198400,"objectID":"97b0cdee33abe5dd9f0092d301648967","permalink":"/resources/sequencingfacilities/","publishdate":"2017-10-17T00:00:00Z","relpermalink":"/resources/sequencingfacilities/","section":"resources","summary":"A list of commercial and academic sequencing facilities used by members of the faculty","tags":["Bioinformatics","Sequencing","NGS"],"title":"Sequencing Facilities","type":"resources"},{"authors":["Sam Robson"],"categories":[],"content":"Built in Portsmouth in 1510, the Mary Rose was the flagship of the fleet of Henry VIII, and had a long and lustrious career until it capsized north of the Isle of Wight in the Battle of the Solent in 1545. Fewer than 35 of the roughly 400 crew members escaped. The wreck was discovered in 1971, and following years of excavation activities the hull was raised from the seabed in 1982 and transported to its home port of Portsmouth. Since then, years of archaeological work has been carried out on the hull itself and on the nearly 20,000 artefacts that have so far been discovered.\nWorking together with Dr Garry Scarlett at the University of Portsmouth, and the Mary Rose Museum, we will be looking to understand more about the members of the crew by working on genotyping of the ancient DNA from the skeletons of crew members raised from the wreckage. Using whole genome sequencing techniques, we hope to develop an understanding of phenotypic characteristics, disease traits and geographic information of crew members of one of the most famous ship wrecks in history.\n","date":1506643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506643200,"objectID":"63604a9044173a5fa77a2bda9be86918","permalink":"/project/2017-09-29-genotyping-of-crew-members-of-the-mary-rose/","publishdate":"2017-09-29T00:00:00Z","relpermalink":"/project/2017-09-29-genotyping-of-crew-members-of-the-mary-rose/","section":"project","summary":"Genotyping of ancient DNA from crew members from the Mary Rose to identfy phenotypic traits and disease traits","tags":["Heritage","NGS","Genomics","Whole Genome"],"title":"Genotyping of Ancient DNA from Crew Members from the Mary Rose","type":"project"},{"authors":null,"categories":["Bioinformatics"],"content":"  1 Sequence Alignment 1.1 ClustalW 1.2 SeaView 1.3 PAGAN 1.4 NCBI BLAST 1.5 HMMer 1.6 FASTA 1.7 MUSCLE  2 Motif and Domain Analysis 2.1 MEME Suite 2.1.1 MEME 2.1.2 DREME 2.1.3 MAST 2.1.4 TOMTOM 2.1.5 GOMO 2.1.6 MEME-CHIP  2.2 HOMER Suite  3 Domain Analysis 3.1 InterProScan 3.2 Pfam 3.3 SMART 3.4 Data Repositories 3.5 Gene Expression Omnibus (GEO) 3.6 ArrayExpress 3.7 Sequence Read Archive (SRA)  4 Genome Mapping 4.1 BWA 4.2 Bowtie 4.3 MAQ 4.4 TopHat 4.5 STAR 4.6 NovoAlign 4.7 SHRiMP 4.8 ELAND  5 De Novo Assembly 5.1 SOAP 5.2 ABySS 5.3 Trinity  6 Read Quantification 6.1 HTseq 6.2 Kallisto  7 Gene Ontology (GO) and Network Analysis 7.1 DAVID 7.2 GSEA 7.3 GO 7.4 GREAT 7.5 PANTHER 7.6 blast2GO  8 Quality Control 8.1 FastQC 8.2 MultiQC  9 Peak Calling 9.1 FindPeaks 9.2 MACS 9.3 HPeak 9.4 PeakSeq 9.5 QuEST 9.6 HOMER 9.7 PeakRanger  10 Sequence Manipulation 10.1 BEDTools 10.2 SAMtools  11 Adapter Trimming 11.1 Cutadapt 11.2 Trimmomatic 11.3 Trim Galore 11.4 Reaper  12 Pathway Analysis 12.1 Ingenuity 12.2 KEGG 12.3 Cytoscape 12.4 STRING  13 Genome Browsers 13.1 University of California, Santa Cruz Genome Browser (UCSC) 13.2 Ensembl Genome Browser 13.3 Integrative Genomics Viewer (IGV)    1 Sequence Alignment 1.1 ClustalW Useful for aligning a small number of sequences, such as homologs from multiple species. Not so useful for multiple alignment of a large amount of sequences from ChIP seq for instance.\n 1.2 SeaView Graphical visualisation of multiple sequence alignment. Can perform alignment of sequences, or can be used to visualise the results of alignments from other algorithms. Can be manually adjusted which can be useful when incorporating prior knowledge into the alignment.\n 1.3 PAGAN A phylogeny-aware general purpose aligner which can be more useful for clustering when there are likely to be large gaps in some sequences compared to others (for instance if comparing gene sequences from different species with non-homologous exons).\n 1.4 NCBI BLAST Suite of tools for searching a protein or nucleotide database to find potential matches or homologs, or for aligning multiple sequences. This tool is particularly useful when you have a query sequence and want to find either known homologs amongst different species, or to find additional proteins with similar structural domains.\n 1.5 HMMer Similar to BLAST, HMMer uses hidden Markov models to improve the accuracy and ability to detect remote homologs by using a stronger underlying mathematical model. This suite of tools can be used to search for homologs in protein and transcript databases in a similar way to BLAST.\n 1.6 FASTA FASTA is the original sequence similarity search tool, and can be used to search for a query protein amongst known protein or ranscript databases such as UniProt. It uses a heuristic approach, which has been improved over the years, but BLAST is probably a better approach to use in general.\n 1.7 MUSCLE MUSCLE is a multiple sequence comparison tool similar to ClustalW that may be slightly faster with more accurate results. It can be used to perform gapped alignment of a set of sequences in any supported format (e.g. fasta).\n  2 Motif and Domain Analysis 2.1 MEME Suite The MEME suite is a series of tools for motif analysis and consists of the following tools:\n2.1.1 MEME De-Novo discovery of motifs from a series of related protein or transcript sequences. This method is more designed for comparison of sequences from, say, a handful of transcription factor sites and is not so well designed for large number of sequences (e.g. from ChIP seq experiments). This tool is better for the identification of wider motifs. For narrower motifs use DREME.\n 2.1.2 DREME Like MEME above, but better for the discovery of narrower motifs.\n 2.1.3 MAST MAST is a motif alignment and search tool for the identification of known (or identified) motifs amongst some database of sequences. This can be either a protein or transcript database, or for instance a known motif can be searched within a set of sequences such as from a ChIP seq experiment. A match score is calculated by comparing the position weight matrix of the motif with each sequence.\n 2.1.4 TOMTOM TOMTOM is a way to compare de novo motifs to a database of known motifs contained within the MEME database. A list of matching motifs is returned, so this can be used to see if for instance your motif matches the known binding motif of a transcription factor within the database. One of the databases that is queried is the JASPAR database (see resources) which contains a large well-maintained and curated set of known TF binding sites.\n 2.1.5 GOMO GOMO takes an input motif and scores all genes based on the binding affinity of the motif at their upstream promoter region. It then performs a gene ontology analysis to see if this motif is associated with the promoter regions of genes involved in any particular function.\n 2.1.6 MEME-CHIP MEME-CHIP is a combined motif discovery tool for ChIP seq results. It relies on the fact that the mid-100bp region of the supplied sequences represent the likely binding location of the peak, performs motif discovery across a large number of sequences, and compares against known motif databases.\n  2.2 HOMER Suite The HOMER (Hypergeometric Optimization of Motif EnRichment) suite of tools is ostensibly a package (like MEME) for de novo identification of enriched motifs from genomic data, but has expanded considerably to provide resources for many aspects of NGS analysis. It has tools for analysis of ChIP-Seq, RNA-Seq, GRO-Seq, Hi-C and BS-Seq data. For instance, for ChIP-seq it has some very useful functions for annotation of peaks (annotatePeaks.pl), motif analysis on called peaks (findMotifsGenome.pl), and differential peak analysis (getDifferentialPeaksReplicates.pl).\n  3 Domain Analysis 3.1 InterProScan Interpro is a database of protein families and known structural domains from the EBI. InterProScan allows users to scan a query sequence for known structural domains or associated protein families, comparing against proteins in the UniProt Knowledge Database.\n 3.2 Pfam Pfam is a database from the EBI containing identified protein famillies based on multiple sequence alignments and HMM analysis of known sequences. The Pfam website can be used to view protein families, or view the protein domain structure of known proteins, or alternative can be used to search a query sequence for known domains.\n 3.3 SMART SMART is a well-maintained modular database (based on SwissProt, TReMBL and the like) of annotated and functional protein domains. Domain structure of known proteins as well as identification of known domains within novel protein sequences can be achieved using this tool.\n 3.4 Data Repositories  3.5 Gene Expression Omnibus (GEO) GEO is the NCBI repository of high throughput data, including sequencing and microarray data. All data maintained within this database is MIAME-compliant, and is synched with projects in ArrayExpress as well. Most experiments contain both raw data (raw chip data for microarrays, or raw read files for sequencing), as well as processed data files (mapped data, called peaks, normalised microarray results, etc).\n 3.6 ArrayExpress ArrayExpress is the EBI-maintained repository for high-throughput data. As with GEO, it contains data for microarray experiments and sequencing experiments of all kinds, all of which are MIAME-compliant. Most experiments contain both raw data (raw chip data for microarrays, or raw read files for sequencing), as well as processed data files (mapped data, called peaks, normalised microarray results, etc).\n 3.7 Sequence Read Archive (SRA) The SRA is probably the single most useful source for the storage of sequence level data in the world. GEO and ArrayEpxress are linked to the SRA, and this is where the raw and mapped data for most sequencing experiments are stored. These data are stored in a proprietary XML-style format and the SRA Toolkit applications can be used to dump the files into one of a number of useful formats (e.g. fastq-dump and sam-dump).\n  4 Genome Mapping 4.1 BWA The Burrows-Wheeler Aligner is one of the most widely used mapping algorithms available, and is particularly useful for mapping low-divergent sequences against a large reference genome. Each sequence is mapped with a mapping quality score, which takes into account gapped alignments and mismatches, and this can be used to define accurate mapping. The output SAM files also contain the location of mismatches in CIGAR format. The benefits of using this algorithm are its speed and accuracy. One negative is that if a sequence maps to multiple locations, only one mapped loci is returned (obviously with a low mapping score). If you want to see all mapped locations (e.g. if you are interested in regions of low complexity) then Bowtie may be better.\n 4.2 Bowtie Bowtie is one of the most commonly used mapping algorithms, and uses indexing using Burrows-Wheeler to produce fast and memory efficient alignments. Bowtie can be used to provide multiple mapping coordinates for reads from regions of low complexity and repetitive sequences in the genome. Bowtie is well maintained (with version 2 currently in circulation).\n 4.3 MAQ MAQ is an older aligner that does not seem to be maintained any longer. It has been superseded by BWA and Bowtie and should probably not be used any longer.\n 4.4 TopHat TopHat is a splice-junction aware mapper for the alignment of RNA Seq data to the genome. It is based on the Bowtie aligner above, but analyses mapped data to identify splice junctions between exons. Alternatively a list of known exon splice junctions can be provided which can be used to guide the alignment across splice junctions. This is one of the most commonly used tools for the mapping of RNA seq data.\n 4.5 STAR Like TopHat, STAR is a splice aware aligner that is used to map RNA Seq reads to the genome. However, STAR uses a much faster algorithm than TopHat and tends to produce slightly better results.\n 4.6 NovoAlign Novoalign is a licensed mapping software tool (although a free version is available). For multi-mapping reads, you can choose to return all, none, or just one randomly selected mapping in your output.\n 4.7 SHRiMP SHRiMP is another older free mapping algorithm which no longer has active support from the authors. It is based on seeding together with the Smith-Waterman algorithm, but is surprisingly fast.\n 4.8 ELAND ELAND is the Illumina pipeline software that comes attached to each of the Illumina sequencing machines. It is their proprietary software and is not easy to get hold of, although as indirect customers we could probably obtain a copy if necessary.\n  5 De Novo Assembly 5.1 SOAP SOAP is a suite of tools for analysis of short oligonucleotide sequences, including for de novo assembly, alignment against a target genome, splice mapping for RNA seq data, and SNP and indel calling for resequencing experiments. The website has not been updated since 2010, so this may not be maintained any more and may be an older tool that is not of any use any more.\n 5.2 ABySS ABySS is a de novo assembler that is paired end aware and well suited for short reads. It is well maintained, with the latest version (1.5.2) out in July.\n 5.3 Trinity Trinity is a complete package for assembly of transcripts from Illumina RNA Seq data, developed at the Broad Institute ant the Hebrew University of Jerusalem. There are three main stages to the assembly: 1) The Inchworm program performs the read assembly, generating either full length transcripts, or unique portions for alternatively spliced transcripts; 2) The Chrysalis program then clusters these unique contigs into “gene” clusters and constructs de Bruijn graphs for each cluster based on member assignment; 3) The Butterfly program then deconvolutes these graphs and generates the full length transcripts for the alternatively spliced transcripts and identiying paralogous genes. It can be used in conjunction with the Trinotate package, which uses a number of standard programs for annoattion of the resulting transcripts.\n  6 Read Quantification 6.1 HTseq HTseq is a python based tool for assessing the quantification of reads amongst specific genomic elements (typically used to count reads that map to individual genes in RNA Seq data analysis). It relies on mapped data using one of the (likely splice-aware) mapping elgorithms mentioned above. In particular, it allows you to specify how best to deal with reads that overlap multiple exons (e.g. due to multiple splicing isoforms, or due to reads mapping to genes that overlap on opposite strands).\n 6.2 Kallisto Kallisto is an alignment-free read quantification algorithm, used to quantify the abundance of transcripts from (say) RNA-Seq data. It uses an index of the target transcriptome and uses a pseudoalignment method which can quantify 30 million human reads in less than 3 minutes. This has been shown to be comparable to other assembly-based quantification methods (e.g. Tophat2, STAR), and actually is robust to read-level errors and so often out-performs these, whilst also being orders of magnitude faster.\n  7 Gene Ontology (GO) and Network Analysis 7.1 DAVID DAVID is an online tool for performing gene ontology analysis of a set of target genes. As well as comparing gene lists against gene ontology classes, it is also able to compare against many other annotated gene lists, including pathways from KEGG and Panther, disease genes, literature searches, etc. It is a fantastic resource for looking for enriched functions within a given gene list.\n 7.2 GSEA GSEA is the Broad Institute’s visualisation tool for looking for enrichment of functional annotations in a given gene set. You provide the tool with a molecular profile data set (e.g. from RNA seq or microarray) which allows you to rank all genes in the genome based on some measure of expression (or enrichment above a control). This list is compared against a database of gene sets (including curated sets such as gene ontology and disease data sets, as well as any sets that may be of interest specifically for your current experiment , such as results from a different experiment), the genes are ranked based on the profile data, and are scored based on whether or not they are present in the database. The idea is that if there is an enrichment of genes of a certain function in your data set, then you will see this as an enrichment in the graph of the score towards the end of the genes (it will peak at the start, and then tail off for instance). If however there is no enrichment (the genes in the current database are spread across the entire continuum of expression scores) then no such peak will be seen.\n 7.3 GO The Gene Ontology consortium maintains a well maintained and fully curated database of gene ontologies for the vast majority of identified genes amongst a whole host of organisms. Gene ontology classes are subdivided into 3 groups: biological process, molecular function, and cellular process. The gene ontology classes also follow a hierarchical structure, with increased fidelity as you look at more fine-grained annotations. The GO website allows you to search for enrichment of certain GO terms within genes in a dataset that you input, such as from a microarray experiment.\n 7.4 GREAT GREAT is a software tool for predicting cis-regulatory regions for a given set of genomic regions. Essentially it takes a set of genomic loci and analyses the annotation of any nearby genes to see if the specified regions are close by to genes of a specific function. For instance, you might have identified a set of ncRNAs that are specifically involved in cis-regualtory activity of target genes of a specific class.\n 7.5 PANTHER The PANTHER (Protein ANalysis THrough Evolutionary Relationships) database is another database of gene/protein family and subfamily functional classes, that actually uses the GO terms from the gene ontology database. It uses evolutionarily conserved elements of novel genes/proteins to identify functions based on similar families present in the database. It is ultimately used in a similar way to the GO database.\n 7.6 blast2GO blast2GO is a bioinformatics package for the identification of GO classes and other functional information from novel nucleotide level data (e.g. from de novo sequencing). It utilises the BLAST algorithm to identify genes with similar sequence structure and known functional annotation that can be inferred across to the novel sequence.\n  8 Quality Control 8.1 FastQC FastQC from the Babraham Institute is a very easy to use tool for quality control of fastq files, such as those used in sequencing. This can be a fantastic first pass analysis for sequencing data to check that no obvious problems exist with the sequencing data prior to mapping. A variety of figures are generated, including the base-level base calling scores across each read which can show if there are any systematic regions that require trimming (e.g. the 3’ end of the reads where quality can fade), the per-base sequence composition which can help to identify the presence of contaminants (such as adapter sequences), and the levels of sequence duplication which might indicate significant PCR bias. It is usually a very good idea to run FastQC on the sequencing files as a matter of course prior to mapping to see what specific issues you should expect to encounter in the processing.\n 8.2 MultiQC MultiQC is a tool that I have recently started to use which is able to create interactive reports aggregating the results from various QC programs into one simple to navigate web page. This provides a single report for each sample that can easily be perused to identify issues with the data. It is set up to deal with output from many common analysis tools, including cutadapt, fastQC, Trimmomatic, BWA, Tophat, Bowtie, STAR, etc.\n  9 Peak Calling 9.1 FindPeaks FindPeaks is part of the Vancouver short read analysis package, and is a peak calling algorithm designed to identify regions of enrichment above control and background levels, in a set of sequencing data (e.g. ChIP seq). It works well at identifying tighter peaks but is not well designed for the identification of extended enriched regions (e.g. H3K36me3).\n 9.2 MACS MACS is a model-based peak-calling algorithm that uses the difference in the ChIP fragments from the positive and negative strands to pinpoint the peak center. A poisson model is used to model the background, and enriched regions above the background or above a control sample are estimated. MACS is useful for tight and medium width peaks, but is not so good at identifying wider peaks such as for gene-wide histone marks.\n 9.3 HPeak HPeak uses hidden Markov models to identify regions of the genome with significantly more mapped reads than expected by chance. It was released in 2010 and version 2 is now out, although I do not think that it is maintained and updated any more.\n 9.4 PeakSeq PeakSeq is another recent peak calling algorithm that identifies local thresholds for peak calling based on a Poisson approximation. The ChIPseq data are first normalised to a control sample, and then enriched regions are identified based on this threshold.\n 9.5 QuEST Quantitative enrichment of sequence tags (QuEST) is an older peak calling algorithm that works in a similar way to MACS. It is most suited for identifying clear tight peaks in ChIP seq data.\n 9.6 HOMER The HOMER suite of tools is very useful for performing various aspects of ChIPseq analyses, such as annotation of called peaks and motif finding. Their peak calling tool is designed specifically for ChIP style sequencing data, and relies on defining a constant peak length for all peaks. This is designed to keep things sinple for the motif analysis tool, but means that this algorithm is not well designed for identifying longer\n 9.7 PeakRanger PeakRanger is a versatile peak calling algorithm that is useful for both narrow and broader peaks in ChIP seq data.\n  10 Sequence Manipulation 10.1 BEDTools Bedtools is a suite of Unix tools for dealing with files of bed, bedgraph, wig, etc. They can be used to do everything from intersecting, merging, counting, complementing and shuffling genomic interval files, as well as converting between different file formats such as BAM, BED, GFF/GTF and VCF.\n 10.2 SAMtools SamtTools is a suite of Unix tools for dealing with SAM and BAM formatted files following mapping of genome wide sequencing data. This includes reading, writing, editing, indexing and viewing of these files, as well as filtering based on different fields in the file (mapping quality etc.).\n  11 Adapter Trimming 11.1 Cutadapt Cutadapt is a tool allowing the trimming of adapter sequences from the 3’ and 5’ of called reads from sequencing studies. This can be used to trim off known sequences as well as trimming reads to a specified length. This method is not useful for paired end reads as it does not maintain synchronisation between the mate pair files.\n 11.2 Trimmomatic Trimmomatic is another tool for trimming sequences such as adapter sequences from called reads, that also allows you to crop reads to remove poor quality bases and maintains synchronisation between mate pair files.\n 11.3 Trim Galore Trim galore is a wrapper script for cutadapt that allows users to maintain synchronisation between mate pair files.\n 11.4 Reaper Reaper is part of the Kraken package from EMBL/EBI, and is a fast and memory efficient method for demultiplexing, trimming and filtering short read sequencing data. It can be used for a variety of purposes, including barcode filtering, quality checking, and the llike.\n  12 Pathway Analysis 12.1 Ingenuity Ingenuity pathway analysis is a platform for the analysis of pathways within sequencing or microarray results based on comparisons with the Ingenuity knowledge base. In particular, the pathway analysis tool provides an interactive approach to the analysis of complex ’omics data, by using known interactions between genes contained within the knowledgebase to generate connections between genes from your analyses.\n 12.2 KEGG Like the gene ontology database, the Kyoto encylopedia of genes and genomes is a database resource of various aspects of biological systems such as pathways, functional hierarchies, protein modules, reactome associations, etc. KEGG pathway maps provide interaction information between genes in the given pathway, and expression data can be overlaid to understand the interaction effects in your data.\n 12.3 Cytoscape Cytoscape is an open source software platform for the visualisation of complex networks, such as those resulting from interaction datasets, gene ontology classes, etc. It can be used to look for potential interaction networks by combining data from multiple genome-wide experiments.\n 12.4 STRING The STRING database provides protein-protein interaction networks identified using a variety of different methods (computer predictions, text mining from published reports and other databases, high throughput experiments, conserved co-expression, etc). This includes both direct interactions and functional interactions. This database can be very useful to understand protein interaction networks and to identify hub genes in gene expression results.\n  13 Genome Browsers 13.1 University of California, Santa Cruz Genome Browser (UCSC) One of the most well-known genome browsers available. Describing everything that this browser can do is beyond the scope of this brief outline, but the UCSC browser allows you to represent genomic data from various sources in parallel tracks on the internet browser allowing you to identify correlations between different factors throughout the genome. You can upload your own data in a variety of formats (e.g. mapped RNA seq data in wiggle format, raw read data in BAM format, called peaks from ChIP Seq data in BED format, etc.), and can also access a huge amount of data available from UCSC’s own servers (e.g. instances of the vast ENCODE data). This is a great way of visualising and exploring your data and should be explored in more detail to see what you can do.\n 13.2 Ensembl Genome Browser The Ensembl Genome Browser is an alternative web based genome browser to the UCSC browser offered by Ensembl. It is important to be aware that, whislt both UCSC and Ensembl use the same assemblies, there are many key differences that make them fairly incompatible. One important difference is that chromosomes are labelled differently (e.g. chr1, chr2, chr3 for UCSC but 1, 2, 3, for Ensembl). Similarly the genome builds are named differently. It generally works out simpler to pick one of these specifications and stick to it. Additional data (your own files or puclished data such as that from ENCODE) can be added using the track hubs feature, which allows you to specify where to find these data and how to represent them in your browser session.\n 13.3 Integrative Genomics Viewer (IGV) The IGV browser from the Broad Institute is an alternative to the UCSC browser. Unlike the UCSC browser, which accesses a centrally located database through a web broswer, the IGV runs locally and so does not suffer from the lag and slow-down often seen with the UCSC browser (particularly during high traffic times of the day). The IGV browser has some very nice personalisation and formatting options not present in UCSC (e.g. being able to easily resize all of your tracks). On first looks, it is a little more bare bones than UCSC due to the lack of data avaialble at your fingertips (as with UCSC), but these annotated published data sets can be added using well annotated track hubs.\n  ","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"c31da05378d24e51963757f6454a38ec","permalink":"/resources/bioinformaticstools/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/resources/bioinformaticstools/","section":"resources","summary":"A list of commonly used tools for bioinformatics analyses","tags":["Bioinformatics","How To","Tools"],"title":"Bioinformatics Tools","type":"resources"},{"authors":null,"categories":["How To","Tutorials"],"content":"  1 Introduction 2 Installing R 3 Basics of R 3.1 Introduction 3.2 Data Classes 3.3 Vectors 3.4 Lists 3.5 Matrices 3.6 Functions 3.7 Printing  4 Installing Packages 5 Data Frames 6 Reading and Writing Data 7 Control Sequences 7.1 IF ELSE 7.2 FOR 7.3 WHILE 7.4 Loop Control  8 Writing Functions in R 9 Some Simple Statistics 10 Plotting With R 10.1 Scatterplots 10.2 Histograms 10.3 Quantile-Quantile Plots 10.4 Line Plots 10.5 Density Plots 10.6 Boxplots 10.7 Bar Plots and Pie Charts 10.8 Graphical Control 10.9 Subplots 10.10 Saving Figures  11 Example Analysis 11.1 Introduction 11.2 Load Data 11.3 Calculate Fold Change 11.4 Compare Data    1 Introduction This tutorial is a basic introduction to R that was originally written for biologists to get a basic understanding of how R functions. R is a software package that is a free to use open-source version of the S programming language. It is designed mainly for running statistical analyses and is very powerful in this regard. Follow through the tutorial and run the example commands by typing them into the command line as you go to see what happens. Don’t be afraid to play around with things as you go – it’s the best way to find out what certain functions do.\nYou will notice that I have added comments to some of the code using the # comment character. Everything to the right of this character is ignored by R. This can be used to add comments to your code, for instance to explain what a particular code chunk does. You can NEVER have too many comments!\n 2 Installing R First of all, you will need to download and install R. The R website can be found at r-project.org. R is updated quite regularly – there is an updated release roughly every 6 months, with various developmental versions released between the official versions. The functions in R are actively maintained to ensure that they run as they should, and new functionality is added all of the time.\nThe current version is 3.3.2. To download it, go to the Comprehensive R Archive Network (cran.r-project.org). There are ready-made binaries available for MAC, windows, and most Linux distributions, so follow the links and download as instructed. You can also download the source code in a tarball, and can compile and install it using make.\nIt is also worth taking a look at the Integrated Development Environment RStudio, which is a great open-source interface for R.\n 3 Basics of R 3.1 Introduction Open the R environment. This is a command line version allowing you to see the results of the commands that you enter as you run them.\nThe command line is shown by the \u0026gt; character. Simply type your command here and press return to see the results. If your command is not complete, then the command line character will change to a + to indicate that more input is required, for instance a missing parenthesis:\nprint (\u0026quot;Hello World!\u0026quot; ## Error: \u0026lt;text\u0026gt;:2:0: unexpected end of input ## 1: print (\u0026quot;Hello World!\u0026quot; ## ^ R stores “variables” using names made up of characters and numbers. A variable, as the name suggests, is a data “object” that can take any value that you want, and can be changed.\nThe variable name can be anything that you like, although it must begin with a character. Whilst it is perfectly acceptable to use simple variable names such as x, y, i, I recommend using a more descriptive name (e.g. patient_height instead of x). There are lots of different variable naming conventions to choose from (e.g. see here), but once you have chosen one try and stick to it.\nTo assign a value to the variable, use the \u0026lt;- command (less-than symbol followed by minus symbol). You can also use the = symbol, but this has other uses (for instance using == to test for equality) so I prefer to use the \u0026lt;- command:\nx \u0026lt;- 3 x # Returns the value stored in \u0026#39;x\u0026#39; - currently 3 ## [1] 3 x \u0026lt;- 5 x # Returns the value stored in \u0026#39;x\u0026#39; - now 5 ## [1] 5 Simple arithmetic can be performed using the standard arithmetic operators (+, -, *, /), as well as the exponent operator (^). There is a level of precedence to these functions – the exponent will be calculated first, followed by multiplication and division, followed by plus and minus. For this reason, you must be careful that your arithmetic is doing what you expect it to do. You can get around this by encapsulating subsets of the sum in parentheses, which will be calculated from the inside out:\n1+2*3  ## [1] 7 (1 + 2) * 3  ## [1] 9 1 + (2 * 3)  ## [1] 7 Personally I think that you can NEVER have too many parentheses – they ensure that your equations are doing what they should, and they can help improve the readability of things making it easier to see what a calculation is trying to achieve.\nAnother operator that you may not have seen before is the “modulo” operator (%%), which gives you the remainder left after dividing by the number:\n6%%2 # 6 is divisible by 2 exactly three times ## [1] 0 6%%4 # 6 is divisible by 4 one time with a remainder of 2 ## [1] 2 You can also use other variables in these assignments:\nx \u0026lt;- 1 y \u0026lt;- x y ## [1] 1 z \u0026lt;- x + y z ## [1] 2  3.2 Data Classes Variables can take many forms, or “classes”. The most common are “numeric” (which you can do numerical calculations on), character (can contain letters, numbers, symbols etc., but cannot run numerical calculations), and logical (TRUE or FALSE). The speech marks character \u0026quot; is used to show that the class of y is “character”. You can also use the apostrophe '. There is a difference between these, but for now this is not important. You can check the class of a variable by using the class() function:\nx \u0026lt;- 12345 class(x) ## [1] \u0026quot;numeric\u0026quot; y \u0026lt;- \u0026quot;12345\u0026quot; class(y) ## [1] \u0026quot;character\u0026quot; Addition is a well-defined operation on numerical objects, but is not defined on character class objects. Attempting to use a function which has not been defined for the object in question will throw an error:\nx + 1 # x is numeric, so addition is well defined ## [1] 12346 y + 1 # y is a character, so addition is not defined - produces an error ## Error in y + 1: non-numeric argument to binary operator To see which objects are currently present in the R environment, use the ls() command. To remove a particular object, use the rm() command. BE CAREFUL – once you have removed an object, it is gone forever!\nx \u0026lt;- 5 ls () ## [1] \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; rm(x) ls () ## [1] \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; rm(list=ls()) # Removes all objects in the current R session ls () ## character(0) You can also change the class of a variable by assigning to the class() function:\nx \u0026lt;- \u0026quot;12345\u0026quot; x+1 ## Error in x + 1: non-numeric argument to binary operator class(x) ## [1] \u0026quot;character\u0026quot; class(x) \u0026lt;- \u0026quot;numeric\u0026quot; class(x) ## [1] \u0026quot;numeric\u0026quot; x+1 ## [1] 12346 The other important data class is “logical”, which is simply a binary TRUE or FALSE value. There are certain operators that are used to compare two variables. The obvious ones are “is less than” (\u0026lt;), “is greater than” (\u0026gt;), “is equal to”\u0026quot; (==). You can also combine these to see “is less than or equal to” (\u0026lt;=) or “is greater than or equal to” (\u0026gt;=). If the statement is true, then it will return the output “TRUE”. Otherwise it will return “FALSE”:\nx \u0026lt;- 2 y \u0026lt;- 3 x \u0026lt;= y ## [1] TRUE x \u0026gt;= y ## [1] FALSE You can also combine these logical tests to ask complex questions by using the “AND” (\u0026amp;\u0026amp;) or the “OR” (||) operators. You can also negate the output of a logical test by using the “NOT” (!) operator. This lets you test for very specific events in your data. Again, I recommend using parentheses to break up your tests to ensure that the tests occur in the order which you expect:\nx \u0026lt;- 3 y \u0026lt;- 7 z \u0026lt;- 6 (x \u0026lt;= 3 \u0026amp;\u0026amp; y \u0026gt;= 8) \u0026amp;\u0026amp; z == 6  ## [1] FALSE (x \u0026lt;= 3 \u0026amp;\u0026amp; y \u0026gt;= 8) || z == 6  ## [1] TRUE One important set of functions are the log and exponential functions. The exponential function is the function \\(e^x\\), such that \\(e^x\\) is its own derivative (\\(\\frac{d}{dx} e^x = e^x\\)). The value e is the constant 2.718281828…, which is the limit \\(\\lim_{n \\to \\infty} (1+\\frac{1}{n})^n\\). It is a very important value in mathematics (hence why it has its own constant). Logarithms are the inverse of exponents, with natural log being log base \\(e\\). Here are some examples:\nlog (8) ## Natural logarithm - base e ## [1] 2.079442 log2 (8) ## Log base 2 ## [1] 3 exp (1) ## e ## [1] 2.718282 exp (5) ## e^5 ## [1] 148.4132 log(exp(8)) ## log and exponential cancel out - base e ## [1] 8 exp(log(8)) ## log and exponential cancel out - base e ## [1] 8 2^(log2(8)) ## log and exponential cancel out - base 2 ## [1] 8 log2 (2^8) ## log and exponential cancel out - base 2 ## [1] 8  3.3 Vectors Single values are all well and good, but R has a number of ways to store multiple values in a single data structure. The simplest one of these is as a “vector” – simply a list of values of the same class. You create a vector by using the c() (concatenate) function:\nmy_vector \u0026lt;- c(1,2,3,4,5) my_vector ## [1] 1 2 3 4 5 This is a very useful way of storing linked data together. You can access the individual elements of the vector by using square brackets ([) to take a subset of the data. The elements in the vector are numbered from 1 upwards, so to take the first and last values we do the following:\nmy_vector \u0026lt;- c(10,20,30,40,50) my_vector[1] ## [1] 10 my_vector[5] ## [1] 50 my_vector[6] ## [1] NA As you can see, a value NA (Not Applicable) is returned if you try to take an element that does not exist. The subset can be as long as you like, as long as it’s not longer than the full set:\nmy_vector \u0026lt;- c(10,20,30,40,50) my_vector[1:4] ## [1] 10 20 30 40 Here, the : in the brackets simply means to take all of the numbers from 1 through to 4, so this returns the first 4 elements of the vector. For instance, this is a simple way to take the numbers from 1 to 20:\n1:20 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 To drop elements from an array, you use the minus symbol:\nmy_vector \u0026lt;- c(10,20,30,40,50) my_vector[-1]  ## [1] 20 30 40 50 my_vector[-length(my_vector)]  ## [1] 10 20 30 40 my_vector[-c(1,3,5)] ## [1] 20 40 Another way to generate a regular sequence in R is to use the seq() command. You supply the start number and the end number, and then either supply the parameter by to define the regular interval between values, or the parameter length to specify the total number of values to return between the start and end value:\nseq(from = 1, to = 20, by = 1) # Returns the same as 1:20 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 seq(from = 1, to = 20, by = 2) # Just the even numbers between 1 and 20  ## [1] 1 3 5 7 9 11 13 15 17 19 seq(from = 1, to = 20, length = 10) # Slightly different to above ## [1] 1.000000 3.111111 5.222222 7.333333 9.444444 11.555556 13.666667 ## [8] 15.777778 17.888889 20.000000 You can also use the rep() function to give a vector of the specified length containing repeated values:\nrep(10, times = 5) # Returns vector containing five copies of the number 10  ## [1] 10 10 10 10 10 rep(c(1,2,3), each = 5) # Returns five 1s, then five 2s, then five 3s ## [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 One of the most powerful features of R is the fact that arithmetic can be conducted on entire vectors rather than having to loop through all values in the vector. Vectorisation of calculations in this way can give huge improvements in performance. For instance, if you sum two vectors (of equal size), the result will be a vector where the ith entry is the sum of the ith entries from the input vectors:\nx \u0026lt;- c(2,3,2,4,5) y \u0026lt;- c(4,1,1,2,3) x+y ## [1] 6 4 3 6 8 x*y ## [1] 8 3 2 8 15  3.4 Lists Another data structure that is very useful is the “list”. A list contains a number of things in a similar way to the vector, but the things that it contains can all be completely different classes. They can even be vectors and other lists (a list of lists):\nmy_list \u0026lt;- list(12345, \u0026quot;12345\u0026quot;, c(1,2,3,4,5)) my_list ## [[1]] ## [1] 12345 ## ## [[2]] ## [1] \u0026quot;12345\u0026quot; ## ## [[3]] ## [1] 1 2 3 4 5 To subset a list, the syntax is slightly different and you use double square brackets:\nmy_list \u0026lt;- list(12345, \u0026quot;12345\u0026quot;, c(1,2,3,4,5)) my_list[[1]] ## [1] 12345 my_list[[3]] ## [1] 1 2 3 4 5 If your list contains lists or vectors, you can subset these as well by using multiple sets of square brackets:\nmy_list \u0026lt;- list(12345, \u0026quot;12345\u0026quot;, c(1,2,3,4,5)) my_list[[3]][5] ## [1] 5 Can you see the difference between subsetting using [[ and using [?\nmy_list \u0026lt;- list(12345, \u0026quot;12345\u0026quot;, c(1,2,3,4,5)) my_list[[3]] ## Returns a vector ## [1] 1 2 3 4 5 my_list[3] ## Returns a list ## [[1]] ## [1] 1 2 3 4 5 my_list[3][5] ## Not defined! ## [[1]] ## NULL You can give names to the values in a vector or in a list by using the names() function to make it easier to follow what the values are:\nmy_vector \u0026lt;- c(1:5) names(my_vector) \u0026lt;- c(\u0026quot;length\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;weight\u0026quot;, \u0026quot;age\u0026quot;) You can use these names instead of the reference number to subset lists and vectors:\nmy_vector \u0026lt;- c(1:5) names(my_vector) \u0026lt;- c(\u0026quot;length\u0026quot;, \u0026quot;width\u0026quot;, \u0026quot;height\u0026quot;, \u0026quot;weight\u0026quot;, \u0026quot;age\u0026quot;) my_vector[\u0026quot;age\u0026quot;] ## age ## 5 The number of values in a vector or list can be found by using the length() function:\nmy_vector \u0026lt;- 1:5 length(my_vector) ## [1] 5 We can also sort the data simply using the sort() function. If we want to get the indeces of the sorted vector (for instance to order a second vector based on the values in the first), we can use the order() function:\n## Some values and their corresponding names my_vals \u0026lt;- c( 0.2, 1.7, 0.5, 3.4, 2.7 ) my_names \u0026lt;- c(\u0026quot;val1\u0026quot;, \u0026quot;val2\u0026quot;, \u0026quot;val3\u0026quot;, \u0026quot;val4\u0026quot;, \u0026quot;val5\u0026quot;) ## Sort the data my_sorted \u0026lt;- sort(my_vals) ## Returns the values in sorted order my_order \u0026lt;- order(my_vals) ## Returns the indeces of the sorted values ## What is the difference between the two? my_sorted  ## [1] 0.2 0.5 1.7 2.7 3.4 my_order ## [1] 1 3 2 5 4 ## Get the sorted value names sort(my_names) ## This won\u0026#39;t work as this will order names alphabetically  ## [1] \u0026quot;val1\u0026quot; \u0026quot;val2\u0026quot; \u0026quot;val3\u0026quot; \u0026quot;val4\u0026quot; \u0026quot;val5\u0026quot; my_names[my_order] ## This gives us the order based on the values themselves ## [1] \u0026quot;val1\u0026quot; \u0026quot;val3\u0026quot; \u0026quot;val2\u0026quot; \u0026quot;val5\u0026quot; \u0026quot;val4\u0026quot; By default the sort functions sort from lowest to highest. You can sort in decreasing by order by using the decreasing parameter:\nsort(my_vals , decreasing = TRUE) ## [1] 3.4 2.7 1.7 0.5 0.2  3.5 Matrices Another data format is a “matrix”\u0026quot; (also known as an “array” in R). This is simply a table of values, and can be thought of as a multidimensional vector. To access specific values in the matrix, you again use the square bracket accessor function, but this time must specify both the row (first value) and column (second value):\nmy_matrix \u0026lt;- matrix(1:20, nrow = 5, ncol = 4) my_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 my_matrix[3,4] \u0026lt;- 99999 my_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 99999 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 By default, the values are added to the matrix in a column-wise fashion (from top to bottom for column 1, then the same for column 2, etc.). To fill the matrix in a row-wise fashion, use the byrow parameter:\nmy_matrix \u0026lt;- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE) You can also use the square bracket accessor function to extract subsets of the matrix:\nmy_matrix \u0026lt;- matrix(1:20, nrow = 5, ncol = 4) my_matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 sub_matrix \u0026lt;- my_matrix[1:2, 3:4] sub_matrix ## [,1] [,2] ## [1,] 11 16 ## [2,] 12 17 The cbind() (column bind) and rbind() (row bind) functions can also be used to concatenate vectors together by row or by column to give a matrix:\ncbind(c(1,2,3), c(4,5,6), c(7,8,9))  ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 rbind(c(1,2,3), c(4,5,6), c(7,8,9))  ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 You can change the names of both the rows and the columns by using the rownames() and colnames() functions:\nmy_matrix \u0026lt;- matrix(1:20, nrow = 5, ncol = 4) rownames(my_matrix) \u0026lt;- c(\u0026quot;row1\u0026quot;, \u0026quot;row2\u0026quot;, \u0026quot;row3\u0026quot;, \u0026quot;row4\u0026quot;, \u0026quot;row5\u0026quot;) colnames(my_matrix) \u0026lt;- c(\u0026quot;col1\u0026quot;, \u0026quot;col2\u0026quot;, \u0026quot;col3\u0026quot;, \u0026quot;col4\u0026quot;) my_matrix ## col1 col2 col3 col4 ## row1 1 6 11 16 ## row2 2 7 12 17 ## row3 3 8 13 18 ## row4 4 9 14 19 ## row5 5 10 15 20 The dimensions of the matrix can be found by using the dim() function, which gives the number of rows (first value) and the number of columns (second value) of the matrix. You can access the number of rows or columns directly by using the nrows() or ncols() functions respectively:\nmy_matrix \u0026lt;- matrix(1:20, nrow = 5, ncol = 4) dim(my_matrix) ## [1] 5 4 nrow(my_matrix) ## [1] 5 ncol(my_matrix) ## [1] 4  3.6 Functions R also uses functions (also known as methods, subroutines, and procedures) which simply take in one or more values, do something to them, and return a result. A simple example is the sum() function, which takes in two or more values in the form of a vector, and returns the sum of all of the values:\nmy_vector \u0026lt;- 1:5 sum(my_vector)  ## [1] 15 In this example, the sum() function takes only one variable (in this case a numeric vector). Sometimes functions take more than one variable (also known as “arguments”). These are named values that must be specified for the function to run. For example, the cor() function returns the correlation between two vectors. This requires several variables to be supplied – two vectors, x and y, of equal length – and you can also supply a number of additional arguments to control how the function works, including the method argument, which lets you specify which method to use to calculate the correlation:\nsample1 \u0026lt;- c(0.9, 1.2, 8.9, -0.3, 6.4) sample2 \u0026lt;- c(0.6, 1.3, 9.0, -0.5, 6.2) cor(sample1, sample2 , method = \u0026quot;pearson\u0026quot;) ## [1] 0.9991263 cor(sample1, sample2 , method = \u0026quot;spearman\u0026quot;) ## [1] 1 Note that we gave a name to the third argument (“method”), but not the first two. If you do not name arguments, they will be taken and assigned to the arguments in the order in which they are input. The first two arguments required by the function are x and y – the two vectors to compare. So there is no problem with not naming these (although you could, if you wanted to, say x=sample1, y=sample2). Any arguments not submitted will use their default value. For instance, the Pearson correlation is the default for method, so you could get this by simply typing:\npearson_cor \u0026lt;- cor(sample1 , sample2) However, there is another argument for cor(), use, for which we are happy to use the default value before we get to method. We therefore need to name method to make sure that “pearson”\u0026quot; is not assigned to the use argument in the function. It is always safer to name the arguments if you are unsure of the order. You can check the arguments using the args() function:\nargs(cor) ## function (x, y = NULL, use = \u0026quot;everything\u0026quot;, method = c(\u0026quot;pearson\u0026quot;, ## \u0026quot;kendall\u0026quot;, \u0026quot;spearman\u0026quot;)) ## NULL If you want to find out what a function does, there is a lot of very helpful documentation available in R. To see the documentation for a specific function, use the help() function. If you want to try and find a function, you can search using a keyword by using the help.search() function:\nhelp(cor) ?cor # Alternative for help() help.search(\u0026quot;correlation\u0026quot;) ??correlation # Alternative for help.search()  3.7 Printing print() can be used to print whatever is stored in the variable the function is called on. Print is what is known as an “overloaded”\u0026quot; function, which means that there are many functions named print(), each written to deal with a variable of a different class. The correct one is used based on the variable that you supply. So calling print() on a numeric variable will print the value stored in the variable. Calling it on a vector prints all of the values stored in the vector. Calling it on a list will print the contents of the list split into an easily identifiable way. There are also many more classes in R for which print is defined, but there are too many to describe here:\nx \u0026lt;- 1 print (x) ## [1] 1 y \u0026lt;- 1:5 print (y) ## [1] 1 2 3 4 5 z \u0026lt;- list(val1 = 1:5, val2 = 6:10) print (z) ## $val1 ## [1] 1 2 3 4 5 ## ## $val2 ## [1] 6 7 8 9 10 If you notice, using print() is the default when you just call the variable itself:\nz \u0026lt;- list(val1 = 1:5, val2 = 6:10) print (z) ## $val1 ## [1] 1 2 3 4 5 ## ## $val2 ## [1] 6 7 8 9 10 z ## $val1 ## [1] 1 2 3 4 5 ## ## $val2 ## [1] 6 7 8 9 10 cat() is similar to print in that the results of calling it are that text is printed to the console. The main use for cat() is to conCATenate two or more variables together and instantly print them to the console. The additional argument sep specifies the character to use to separate the different variables:\ncat(\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;, sep = \u0026quot; \u0026quot;)  ## hello world x \u0026lt;- 1:5 y \u0026lt;- \u0026quot;bottles of beer\u0026quot; cat(x, y, sep = \u0026quot;\\t\u0026quot;) ## 1 2 3 4 5 bottles of beer \\t is a special printing character that you can use with the cat() function that prints a tab character. Another similar special character that you may need to use is \\n which prints a new line.\nAnother similar function is the paste() function, which also concatenates multiple values together. The differences between this and cat() are that the results of paste() can be saved to a different variable which requires a call to print() to see the results, and paste() can be used to concatenate individual elements of a vector by using the additional collapse argument:\nprint(paste(\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;, sep = \u0026quot;\\t\u0026quot;)) ## [1] \u0026quot;hello\\tworld\u0026quot; print(paste(\u0026quot;sample\u0026quot;, 1:5, sep=\u0026quot;_\u0026quot;)) # Returns a vector of values ## [1] \u0026quot;sample_1\u0026quot; \u0026quot;sample_2\u0026quot; \u0026quot;sample_3\u0026quot; \u0026quot;sample_4\u0026quot; \u0026quot;sample_5\u0026quot; print(paste(\u0026quot;sample\u0026quot;, 1:5, sep=\u0026quot;_\u0026quot;, collapse=\u0026quot;\\n\u0026quot;)) # Prints values separated by new lines ## [1] \u0026quot;sample_1\\nsample_2\\nsample_3\\nsample_4\\nsample_5\u0026quot; Do you notice the difference between print() and cat()? While print() prints the \\t character as is, cat() prints the actual tab space. This is a process known as “interpolation”. In many programming languages, using double quotes in strings results in special characters being interpolated, whilst single quotes will print as is. However, in R the two can be used relatively interchangeably.\nThere are also other characters, such as ', \u0026quot;, / and \\, which may require “escaping” with a backslash to avoid R interpreting the character in a different context. For instance, if you have a string containing an apostrophe within a string defined using apostrophes, the string will be interpreted as terminating earlier, and the code will not do what you expect:\ncat(\u0026#39;It\u0026#39;s very annoying when this happens...\u0026#39;) ## Error: \u0026lt;text\u0026gt;:1:9: unexpected symbol ## 1: cat(\u0026#39;It\u0026#39;s ## ^ Here, the string submitted to cat() is actual “It” rather than the intended “It’s very annoying when this happens…”. The function will not know what to do about the remainder of the string, so an error will occur. However, by escaping the apostrophe, the string will be interpreted correctly:\ncat(\u0026#39;It\\\u0026#39;s easily fixed though!\u0026#39;) ## It\u0026#39;s easily fixed though! Another alternative is to use double apostrophes as the delimiter, which will avoid the single apostrophe being misinterpreted:\ncat(\u0026quot;It\u0026#39;s easily fixed though!\u0026quot;) ## It\u0026#39;s easily fixed though! One function that gives you slightly more control over the formatting of your data is the sprintf() function. This function allows you to specify things like the width in which to print each variable, which is useful for arranging output in a table format (note that you need to use cat() to actual print to the screen):\ncat(sprintf(\u0026quot;%10s\\t%5s\\n\u0026quot;, \u0026quot;Hello\u0026quot;, \u0026quot;World\u0026quot;), sprintf(\u0026quot;%10s\\t%5s\\n\u0026quot;, \u0026quot;Helloooooo\u0026quot;, \u0026quot;World\u0026quot;)) ## Hello World ## Helloooooo World The sprintf() function takes as input a string telling R how you want your inputs to be formatted, followed by a list of the inputs. Within the formatting string, placeholders of the form %10s are replaced by the given inputs, with the first being replaced by the first argument in the list, and so on (so the number of additional arguments to sprintf must match the number of placeholders). The number in the placeholder defines the width to allocate for printing that argument (positive is right aligned, negative is left aligned), decimal numbers in the placeholder define precision of floating point numbers, and the letter defines the type of argument to print (e.g. s for string, i for integer, f for fixed point decimal, e for exponential decimal). Note that special characters are interpolated by cat() as before. Here are some examples:\ncat(sprintf(\u0026quot;%20s\\n\u0026quot;, \u0026quot;Hello\u0026quot;))  ## Hello cat(sprintf(\u0026quot;%-20s\\n\u0026quot;, \u0026quot;Hello\u0026quot;))  ## Hello cat(sprintf(\u0026quot;%10i\\n\u0026quot;, 12345))  ## 12345 cat(sprintf(\u0026quot;%10f\\n\u0026quot;, 12.345))  ## 12.345000 cat(sprintf(\u0026quot;%10e\\n\u0026quot;, 12.345))  ## 1.234500e+01   4 Installing Packages The main R package contains a large number of commonly used functions. There are also additional functions available in other “packages” that you can get hold of from the Comprehensive R Archive Network, or CRAN. To load in a package, first download and install the package from CRAN using the install.packages() function (if it is not already downloaded), and then use the “library” command to make the libraries available to your current R session:\n?xtable  ## No documentation for \u0026#39;xtable\u0026#39; in specified packages and libraries: ## you could try \u0026#39;??xtable\u0026#39; install.packages(\u0026quot;xtable\u0026quot;) ## Error in contrib.url(repos, \u0026quot;source\u0026quot;): trying to use CRAN without setting a mirror library(\u0026quot;xtable\u0026quot;) ?xtable The function xtable() is not available in the R environment until you have loaded the package. Only the most commonly used functions are made available in the R environment by default (for example the package “stats” is loaded by default, which contains all commonly used statistical fuctions). There are also a number of commonly used packages that are part of the R installation, but which are not automatically loaded when you start a new R session. There are also thousands of additional packages available, some written by users, which can perform most of the things that you would ever want to do. Chances are, if you want to do something it’s already available from somewhere. Don’t re-invent the wheel if you can help it.\nSince R is so useful for analysing biological data, the bioconductor project was set up to bring together packages used for the analysis of high-throughput data (it started with microarrays, but now there are packages available for analysis of sequencing data). Bioconductor packages can be downloaded from bioconductor.org. However, there is also a simple way to install bioconductor packages directly from within R:\nsource(\u0026quot;http://bioconductor.org/biocLite.R\u0026quot;) # Load the biocLite() script biocLite() # Installs the basic packages required to use bioconductor biocLite(\u0026quot;DESeq\u0026quot;) # Installs a specific bioconductor package  5 Data Frames Data frames are the most powerful data types in R. They look similar to matrices, but the data structure is actually more similar to a list of vectors (all of the same length). The simplest way to think of them is as being similar to spreadsheets in Excel.\nYou can create data frames either in a similar way to how you create a list, or also by converting a matrix object:\ndata.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12))  ## val1 val2 val3 val4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) ## V1 V2 V3 V4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 Notice how, in the second data frame, no column names are specified so R sets the defaults as V1, V2, V3, etc. Whilst data frames do have row names, it is the column names that are the most important. As with lists, these can be changed by using the names() command:\nmy_df \u0026lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) names(my_df) \u0026lt;- c(\u0026quot;val1\u0026quot;, \u0026quot;val2\u0026quot;, \u0026quot;val3\u0026quot;, \u0026quot;val4\u0026quot;) my_df ## val1 val2 val3 val4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 You access the elements of a data frame either using single square bracket notation in the same way as for a matrix, or you can access the individual columns using double square bracket notation in the same way as for lists. You can also access the individual columns by using the special $ operator which is specifically used for data frames:\nmy_df \u0026lt;- as.data.frame(matrix(1:12, nrow = 3, ncol = 4)) names(my_df) \u0026lt;- c(\u0026quot;val1\u0026quot;, \u0026quot;val2\u0026quot;, \u0026quot;val3\u0026quot;, \u0026quot;val4\u0026quot;) my_df ## val1 val2 val3 val4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 sub_df \u0026lt;- my_df[1:2, 3:4] sub_df ## val3 val4 ## 1 7 10 ## 2 8 11 val1 \u0026lt;- my_df[[1]] val1 ## [1] 1 2 3 val2 \u0026lt;- my_df[[\u0026quot;val2\u0026quot;]] val2 ## [1] 4 5 6 val3 \u0026lt;- my_df$val3 val3 ## [1] 7 8 9 The beauty of data frames is that the data frame columns can be dealt with as if they were individual variables. For this reason, the column names must be suitable variable names (i.e. alphanumeric and not starting with a number) and must be unique. If you attach a data frame, you can access the columns as if they were variables:\nmy_df \u0026lt;- data.frame(val1 = c(1:3), val2 = c(4:6), val3 = c(7:9), val4 = c(10:12)) attach(my_df) ## The following objects are masked _by_ .GlobalEnv: ## ## val1, val2, val3 my_df ## val1 val2 val3 val4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 val1 + 1000  ## [1] 1001 1002 1003 detach(my_df) As a slight aside, I dislike using the attach/detach approach to accessing elements of a data frame, as it can make it difficult when reading through your code to tell which variable is being accessed. For instance, if you have a variable named myname, and a data frame with a column myname, then using df$myname in your code makes it much clearer where you are accessing your data from than simply using myname.\nNotice that to make changes to the data frame itself, we need to use the $ accessor function (or double square brackets), otherwise a new variable val1 will be created. Data frames should be set up in such a way that every row represents an independent observation, and the columns represent the independent variables that you may be interested in. For instance, if you have taken a measurement of say the weight of each sample in triplicate, you would not represent the data like this:\n  SampleName Weight1 Weight2 Weight3    Sample1 67.1 67.3 66.8  Sample2 80.3 79.8 79.5    But instead you would ensure that the two independent variables (weight and replicate number) were in their own columns:\n  SampleName Replicate Weight    Sample1 1 67.1  Sample1 2 67.3  Sample1 3 66.8  Sample2 1 80.3  Sample2 2 79.8  Sample2 3 79.5    Now all of the weights are in a single column that can be analysed.\nSubsetting a data frame is also very powerful. The subset command allows you to look for the rows of a data frame that fit certain criteria. For instance, to pull out the genes that show more than 2-fold expression and a p-value less than 0.05, you would do the following:\ngene_exp \u0026lt;- data.frame(geneName = paste(\u0026quot;gene\u0026quot;, 1:10, sep = \u0026quot;\u0026quot;), foldChange = rnorm(10, mean = 2, sd = 1), pVal = rnorm(10, mean = 0.05, sd = 0.05)) signif_genes \u0026lt;- subset(gene_exp, foldChange \u0026gt; 2 \u0026amp; pVal \u0026lt;= 0.05) signif_genes ## geneName foldChange pVal ## 1 gene1 2.234659 0.038236225 ## 3 gene3 2.713056 0.047058009 ## 5 gene5 2.982536 0.003146303 Notice here that we use a single \u0026amp; rather than the double \u0026amp;\u0026amp; that we used earlier. This is because we are doing a vector-based logical test (that is performing the test on each element of the vector to get a vector of logical values at the end). It is very easy to forget this and accidentally use the \u0026amp;\u0026amp;, which will not give you what you want:\nfake_signif_genes \u0026lt;- subset(gene_exp, foldChange \u0026gt; 2 \u0026amp;\u0026amp; pVal \u0026lt;= 0.05) fake_signif_genes ## geneName foldChange pVal ## 1 gene1 2.2346589 0.038236225 ## 2 gene2 1.1171569 0.010953464 ## 3 gene3 2.7130557 0.047058009 ## 4 gene4 1.9352067 0.046603706 ## 5 gene5 2.9825355 0.003146303 ## 6 gene6 2.2315724 0.138186853 ## 7 gene7 1.6141956 0.093650234 ## 8 gene8 1.8556861 0.032846692 ## 9 gene9 0.7700426 0.063940521 ## 10 gene10 1.3124471 0.081082591 Another form of data that comes in very handy, particularly with data frames, is the “factor”. Factors are a way of dealing with categorical data, and simply encode the possible levels with numberic dummy values 0, 1, 2, etc. (which are used in modelling procedures such as ANOVA):\nmy_vector \u0026lt;- c(\u0026quot;apples\u0026quot;, \u0026quot;pears\u0026quot;, \u0026quot;apples\u0026quot;, \u0026quot;oranges\u0026quot;, \u0026quot;pears\u0026quot;) my_vector ## [1] \u0026quot;apples\u0026quot; \u0026quot;pears\u0026quot; \u0026quot;apples\u0026quot; \u0026quot;oranges\u0026quot; \u0026quot;pears\u0026quot; my_factor \u0026lt;- as.factor(my_vector) my_factor ## [1] apples pears apples oranges pears ## Levels: apples oranges pears levels(my_factor) ## [1] \u0026quot;apples\u0026quot; \u0026quot;oranges\u0026quot; \u0026quot;pears\u0026quot; Since data frames can hold data of different classes within its columns (a data frame is essentially a glorified list), it is very important to ensure that each column is assigned the correct class so that R functions that you use later do the correct thing with the data. For instance, R will automatically convert character entries to factors with all possible values as the factor levels. You can quickly see the class of all of your columns by using the str() function:\nstr(gene_exp) ## \u0026#39;data.frame\u0026#39;: 10 obs. of 3 variables: ## $ geneName : Factor w/ 10 levels \u0026quot;gene1\u0026quot;,\u0026quot;gene10\u0026quot;,..: 1 3 4 5 6 7 8 9 10 2 ## $ foldChange: num 2.23 1.12 2.71 1.94 2.98 ... ## $ pVal : num 0.03824 0.01095 0.04706 0.0466 0.00315 ... Whilst factors are incredibly useful in modelling and plotting, they may not necessarily be exactly what you want. For instance, in this case the column geneName has been converted into a factor, with levels gene1, …, gene10. If we try and add in a new gene, gene11, this will not work as all entries of a factor must be one of the specified levels:\ngene_exp_plus \u0026lt;- rbind(gene_exp, c(\u0026quot;gene11\u0026quot;, 1.789, 0.0034)) ## Warning in `[\u0026lt;-.factor`(`*tmp*`, ri, value = \u0026quot;gene11\u0026quot;): invalid factor ## level, NA generated gene_exp_plus ## geneName foldChange pVal ## 1 gene1 2.23465888817809 0.0382362251121874 ## 2 gene2 1.11715691372872 0.0109534636149735 ## 3 gene3 2.71305571811009 0.0470580093671668 ## 4 gene4 1.93520669112966 0.0466037061884548 ## 5 gene5 2.98253553079485 0.00314630349135952 ## 6 gene6 2.23157241340974 0.138186853004448 ## 7 gene7 1.61419556341739 0.093650234491183 ## 8 gene8 1.85568611921072 0.0328466921261732 ## 9 gene9 0.770042566261392 0.0639405210499306 ## 10 gene10 1.31244706165258 0.0810825912438105 ## 11 \u0026lt;NA\u0026gt; 1.789 0.0034 Instead we would be better off treating geneName as a character vector, since we are unlikely to treat it as a categorical variable in later model fitting analyses.\n 6 Reading and Writing Data Reading and writing data in R is quite simple, and is most easily done by using pure text files. Functions exist for reading other formats as well (e.g. Excel tables), but for now we will concentrate on raw text. There are some very basic example files available from here.\nUnless you give the complete path for a file, R will look in it’s current working directory for any files that you want to load in. By default, R will use your system’s home directory, but you can set this by using the setwd() function. You can check that the correct working directory is set by using the getwd() function:\nsetwd(\u0026quot;/path/to/mydir/\u0026quot;) getwd () If you have a list of data in a file (e.g. a list of gene names separated by new lines), then the simplest method to use is scan(). You must tell scan() where to find the data file (either the full path, or a relative path from the current working directory), as well as the format that the data should be read in as (generally either “character” or “numeric”):\nmy_file \u0026lt;- \u0026quot;gene_list.txt\u0026quot; gene_list \u0026lt;- scan(my_file, what = \u0026quot;character\u0026quot;, sep = \u0026quot;\\n\u0026quot;) gene_list ## [1] \u0026quot;gene1\u0026quot; \u0026quot;gene2\u0026quot; \u0026quot;gene3\u0026quot; \u0026quot;gene4\u0026quot; \u0026quot;gene5\u0026quot; \u0026quot;gene6\u0026quot; \u0026quot;gene7\u0026quot; ## [8] \u0026quot;gene8\u0026quot; \u0026quot;gene9\u0026quot; \u0026quot;gene10\u0026quot; \u0026quot;gene11\u0026quot; \u0026quot;gene12\u0026quot; \u0026quot;gene13\u0026quot; \u0026quot;gene14\u0026quot; ## [15] \u0026quot;gene15\u0026quot; \u0026quot;gene16\u0026quot; \u0026quot;gene17\u0026quot; \u0026quot;gene18\u0026quot; \u0026quot;gene19\u0026quot; \u0026quot;gene20\u0026quot; \u0026quot;gene21\u0026quot; ## [22] \u0026quot;gene22\u0026quot; \u0026quot;gene23\u0026quot; \u0026quot;gene24\u0026quot; \u0026quot;gene25\u0026quot; \u0026quot;gene26\u0026quot; \u0026quot;gene27\u0026quot; \u0026quot;gene28\u0026quot; ## [29] \u0026quot;gene29\u0026quot; \u0026quot;gene30\u0026quot; \u0026quot;gene31\u0026quot; \u0026quot;gene32\u0026quot; \u0026quot;gene33\u0026quot; \u0026quot;gene34\u0026quot; \u0026quot;gene35\u0026quot; ## [36] \u0026quot;gene36\u0026quot; \u0026quot;gene37\u0026quot; \u0026quot;gene38\u0026quot; \u0026quot;gene39\u0026quot; \u0026quot;gene40\u0026quot; \u0026quot;gene41\u0026quot; \u0026quot;gene42\u0026quot; ## [43] \u0026quot;gene43\u0026quot; \u0026quot;gene44\u0026quot; \u0026quot;gene45\u0026quot; \u0026quot;gene46\u0026quot; \u0026quot;gene47\u0026quot; \u0026quot;gene48\u0026quot; \u0026quot;gene49\u0026quot; ## [50] \u0026quot;gene50\u0026quot; For tables (for instance tab-delimited files saved from Excel), the easiest way is to use the read.table() function. This works by using scan() to read in each line from the table, then splitting the line by the specified delimiter. It is easier (or at least you are less prone to mistakes) to read such files when there are no empty cells, so try to fill empty data with a missing data character, such as NA (the default):\nstr(read.table(\u0026quot;sample_annotation.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;)) ## \u0026#39;data.frame\u0026#39;: 6 obs. of 4 variables: ## $ SampleName: Factor w/ 6 levels \u0026quot;sample1\u0026quot;,\u0026quot;sample2\u0026quot;,..: 1 2 3 4 5 6 ## $ Treatment : Factor w/ 2 levels \u0026quot;Control\u0026quot;,\u0026quot;Drug\u0026quot;: 1 1 1 2 2 2 ## $ Replicate : int 1 2 3 1 2 3 ## $ CellType : Factor w/ 1 level \u0026quot;HeLa\u0026quot;: 1 1 1 1 1 1 There are lots of additional arguments to the read.table() function; header is a boolean value that says whether or not the first row should be used to name the columns of the data frame, sep gives the delimiter between column entries (e.g. \\t for tab-delimited files, or , for comma-separated files), skip tells R to skip the first n rows of the input, and nrow tells R to only load the first n rows that it sees:\nstr(read.table(\u0026quot;sample_annotation.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, nrow = 2)) ## \u0026#39;data.frame\u0026#39;: 2 obs. of 4 variables: ## $ SampleName: Factor w/ 2 levels \u0026quot;sample1\u0026quot;,\u0026quot;sample2\u0026quot;: 1 2 ## $ Treatment : Factor w/ 1 level \u0026quot;Control\u0026quot;: 1 1 ## $ Replicate : int 1 2 ## $ CellType : Factor w/ 1 level \u0026quot;HeLa\u0026quot;: 1 1 str(read.table(\u0026quot;sample_annotation.txt\u0026quot;, header = FALSE, sep = \u0026quot;\\t\u0026quot;)) ## \u0026#39;data.frame\u0026#39;: 7 obs. of 4 variables: ## $ V1: Factor w/ 7 levels \u0026quot;sample1\u0026quot;,\u0026quot;sample2\u0026quot;,..: 7 1 2 3 4 5 6 ## $ V2: Factor w/ 3 levels \u0026quot;Control\u0026quot;,\u0026quot;Drug\u0026quot;,..: 3 1 1 1 2 2 2 ## $ V3: Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;Replicate\u0026quot;: 4 1 2 3 1 2 3 ## $ V4: Factor w/ 2 levels \u0026quot;CellType\u0026quot;,\u0026quot;HeLa\u0026quot;: 1 2 2 2 2 2 2 Notice that when the header is not used, the numeric column Replicate is now interpreted in the same way as the character columns, because now the first entry is non-numeric. By default, read.table() converts character columns into factors, which can be avoided by setting the stringsAsFactors argument to FALSE:\nstr(read.table(\u0026quot;sample_annotation.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE)) ## \u0026#39;data.frame\u0026#39;: 6 obs. of 4 variables: ## $ SampleName: chr \u0026quot;sample1\u0026quot; \u0026quot;sample2\u0026quot; \u0026quot;sample3\u0026quot; \u0026quot;sample4\u0026quot; ... ## $ Treatment : chr \u0026quot;Control\u0026quot; \u0026quot;Control\u0026quot; \u0026quot;Control\u0026quot; \u0026quot;Drug\u0026quot; ... ## $ Replicate : int 1 2 3 1 2 3 ## $ CellType : chr \u0026quot;HeLa\u0026quot; \u0026quot;HeLa\u0026quot; \u0026quot;HeLa\u0026quot; \u0026quot;HeLa\u0026quot; ...  7 Control Sequences One of the most useful things to be able to do with computers is to repeat the same command multiple times without having to do it by hand each time. For this, control sequences can be used to give you close control over the progress of your program.\n7.1 IF ELSE The first control sequence to look at is the “if else” command, which acts as a switch to run one of a selection of possible commands given a switch that you specify. For instance, you may want to do something different depending on whether a value is odd or even:\nmy_val \u0026lt;- 3 if (my_val%%2 == 0) { # If it is even (exactly divisible by 2) cat (\u0026quot;Value is even\\n\u0026quot;) } else { # Otherwise it must be odd cat (\u0026quot;Value is odd\\n\u0026quot;) } ## Value is odd Here, the expression in the parentheses following “if” is evaluated, and if it evaluates to TRUE then the block of code contained within the following curly braces is evaluated. Otherwise, the block of code following the “else” statement is evaluated. You can add additional tests by using the “else if” statement:\nmy_val \u0026lt;- 27 if (my_val%%2 == 0) { cat(\u0026quot;Value is divisible by 2\\n\u0026quot;) } else if (my_val%%3 == 0) { cat(\u0026quot;Value is divisible by 3\\n\u0026quot;) } else if (my_val%%4 == 0) { ... } else if (my_val%%n == 0) { cat(\u0026quot;Value is divisible by n\\n\u0026quot;) } else { cat(\u0026quot;Value is not divisible by 1:n\\n\u0026quot;) } Each switch is followed by a block of code surrounded by curly braces, and the conditional statements are evaluated until one evaluates to TRUE, at which point R avaluates this code bloack then exits. If none of them evaluate to TRUE, then the default code block following “else” is evaluated instead. If no “else” block is present, then the default is to just do nothing. These blocks can be as complicated as you like, and you can have “if else” statements within the blocks to create a hierarchical structure. Note that this ifelse block will ony return the smallest factor of myval.\n 7.2 FOR Another control structure is the “for loop”, which will conduct the code in the block multiple times for a variety of values that you specify at the start. For instance, here is a simple countdown script:\nfor (i in 10:1) { cat(i, \u0026quot;...\\n\u0026quot;, sep = \u0026quot;\u0026quot;) if (i == 1) { cat(\u0026quot;Blastoff!\\n\u0026quot;) } } ## 10... ## 9... ## 8... ## 7... ## 6... ## 5... ## 4... ## 3... ## 2... ## 1... ## Blastoff! So the index i is taken from the set of numbers (10, 9, …, 1), starting at the first value 10, and each time prints out the number followed by a newline. Then an if statement checks to see if we have reached the final number, at which point it is time for blast off! At this point, it returns to the start of the block, updates the number to the second value 9, and repeats. It does this until there are no more values to use.\nAs a small aside, this is slightly inefficient. Evaluation of the if statement is conducted every single time the loop is traversed (10 times in this example). It will only ever be true at the end of the loop, so we could always take this out of the loop and evaluate the final printout after the loop is finished and save ourselves 10 calculations. Whilst the difference here is negligible, thinking of things like this may save you time in the future:\nfor (i in 10:1) { cat(i, \u0026quot;...\\n\u0026quot;, sep = \u0026quot;\u0026quot;) } cat(\u0026quot;Blastoff!\\n\u0026quot;) ## 10... ## 9... ## 8... ## 7... ## 6... ## 5... ## 4... ## 3... ## 2... ## 1... ## Blastoff!  7.3 WHILE The final main control structure is the “while loop”. This is similar to the “for loop”, and will continue to evaluate the code chunk as long as the specified expression evaluates to TRUE:\ni \u0026lt;- 10 while (i \u0026gt; 0) { cat(i, \u0026quot;...\\n\u0026quot;, sep = \u0026quot;\u0026quot;) i \u0026lt;- i - 1 } cat(\u0026quot;Blastoff!\\n\u0026quot;) ## 10... ## 9... ## 8... ## 7... ## 6... ## 5... ## 4... ## 3... ## 2... ## 1... ## Blastoff! This does exactly the same as the “for loop” above. In general, either can be used for a given purpose, but there are times when one would be more “elegant” than the other. For instance, here the for loop is better as you do not need to manually subtract 1 from the index each time.\nHowever, if you did not know how many iterations were required before finding what you are looking for (for instance searching through a number of files), a “while loop” may be more suitable.\nHOWEVER: Be aware that it is possible to get caught up in an “infinite loop”. This happens if the conditional statement never evaluates to FALSE. If this happens, press either ESCAPE or press the CONROL key and the letter c (CTRL+c) to quit out of the current function (CMD+c) for Mac). For instance, if we forget to decrement the index, i will always be 10 and will therefore never be less than 0. This loop will therefore run forever:\ni \u0026lt;- 10 while (i \u0026gt; 0) { cat(i, \u0026quot;...\\n\u0026quot;, sep = \u0026quot;\u0026quot;) } cat(\u0026quot;Blastoff!\\n\u0026quot;)  7.4 Loop Control You can leave control loops early by using flow control constructs. next skips out of the current loop and moves onto the next in the sequence:\nfor (i in 1:10) { if (i == 5) { next } cat (i, \u0026quot;\\n\u0026quot;, sep = \u0026quot;\u0026quot;) } cat(\u0026quot;Finished loop\\n\u0026quot;) ## 1 ## 2 ## 3 ## 4 ## 6 ## 7 ## 8 ## 9 ## 10 ## Finished loop break will leave the loop entirely, and will return to the function after the last curly brace in the code chunk:\nfor (i in 1:10) { if (i == 5) { break } cat (i, \u0026quot;\\n\u0026quot;, sep = \u0026quot;\u0026quot;) } cat(\u0026quot;Finished loop\\n\u0026quot;) ## 1 ## 2 ## 3 ## 4 ## Finished loop   8 Writing Functions in R There are many functions available in R, and chances are if you want to do something somebody has already written the function to do it. It is best to not re-invent the wheel if possible (or at least it is more efficient – sometimes it is good to reinvent the wheel to understand how it works), but very often you will want to create your own functions to save replicating code.\nA function takes in one or more variables, does something with them, and returns something (e.g. a value or a plot). For instance, calculating the mean of a number of values is simply a case of adding them together and dividing by the number of values. Let’s write a function to do this and check that it matches the mean() function in R:\nmy_mean \u0026lt;- function (x) { # Here, x is a numeric vector nvals \u0026lt;- length(x) valsum \u0026lt;- sum(x) return (valsum/nvals) } my_vals \u0026lt;- c(3,5,6,3,4,3,4,7) my_mean(my_vals)  ## [1] 4.375 mean(my_vals) ## [1] 4.375 So, as with the loops earlier, the function is contained within a block of curly braces. A numeric vector is given to the function, the mean is calculated, and this value is returned to the user using the return() function. This value can be captured into a variable of your choosing in the same way as with any function.\nYou can also add further arguments to the function call. If you want an argument to have a default value, you can specify this in the function declaration. This is the value that will be used if no argument value is specified. Any arguments that do not have a default value must be specified when calling the function, or an error will be thrown:\nfoo \u0026lt;- function(x, arg) { return(paste(x, arg, sep = \u0026quot; \u0026quot;)) } foo (\u0026quot;Hello\u0026quot;)  ## Error in paste(x, arg, sep = \u0026quot; \u0026quot;): argument \u0026quot;arg\u0026quot; is missing, with no default Now let’s try and add a default value for arg:\nfoo \u0026lt;- function(x, arg = \u0026quot;World!\u0026quot;) { return(paste(x, arg, sep = \u0026quot; \u0026quot;)) } foo (\u0026quot;Hello\u0026quot;)  ## [1] \u0026quot;Hello World!\u0026quot; This is a good point to mention an idea known as “scope”. After running the following function, have a look at the value valsum calculated within the function:\nmy_mean \u0026lt;- function (x) { # Here, x is a numeric vector nvals \u0026lt;- length(x) valsum \u0026lt;- sum(x) return (valsum/nvals) } my_vals \u0026lt;- c(3,5,6,3,4,3,4,7) my_mean(my_vals) ## [1] 4.375 print(valsum)  ## Error in print(valsum): object \u0026#39;valsum\u0026#39; not found So what went wrong? The error message says that R cannot find the object valsum. So where is it? The “scope” of an object is the environment where it can be found. Up until now, we have been using what are known as “global variables”. That is we have created all of our objects within the “global environment”, which is the top level where R searches for objects. These objects are available at all times.\nHowever, when we call a function, a new environment, or “scope”, is created, and all variables created within the function become “local variables” that can only be accessed from within the function itself. As soon as we leave the function, these local variables are deleted. If you think about it, this makes sense – otherwise, every time we called a function, memory would fill up with a whole load of temporary objects. Also, how many functions do you think create an object called x? Pretty much all of them (it’s generally the name of the first argument, as in my example). If we created an object x, then ran a couple of functions, and then went to use x again, chances are it would no longer be what we thought it was.\nSo, the function itself is completely self-contained. A copy of the input variable is stored in a new local variable called x, something is done to this object (possibly creating additional objects along the way), something is returned, and then all of these objects in the scope of the function are removed, and we move back into the global environment.\nFunctions are incredibly useful when we want to repeat the same set of actions on multiple sets of data. The “apply”\u0026quot; set of functions are very useful for running a single function multiple times on input data.\napply() works on a matrix or data frame, and applies the function named by the argument FUN across either the rows or the columns of the table, as specified with the MAR (margin) argument (MAR=1 for rows, MAR=2 for columns). For instance, suppose that you had a matrix of expression values from a microarray, where each row was a different gene, and each column is the signal from a different probe on the array. We may want to calculate the mean value across these probes for each gene:\nprobe_file \u0026lt;- \u0026quot;probe_values.txt\u0026quot; probe_dat \u0026lt;- read.table(probe_file, header = TRUE, sep = \u0026quot;\\t\u0026quot;) probe_means \u0026lt;- apply(probe_dat[, -1], MAR = 1, FUN = mean) probe_means ## [1] 21.2 29.8 85.8 63.6 25.6 44.8 70.8 88.4 47.6 35.8 90.2 57.8 24.2 23.2 ## [15] 25.0 57.6 83.0 62.8 33.8 28.4 13.2 58.4 24.6 28.2 47.2 6.4 94.6 14.2 ## [29] 39.6 53.4 80.2 47.8 9.8 58.8 59.8 0.4 63.8 33.0 22.4 53.8 37.8 68.8 ## [43] 99.6 97.6 5.0 59.8 95.4 -0.2 1.4 52.2 Additionally, apply can be used to apply a function to all values by using MAR=c(1,2) to run across rows and columns:\nprobe_file \u0026lt;- \u0026quot;probe_values.txt\u0026quot; probe_dat \u0026lt;- read.table(probe_file, header = TRUE, sep = \u0026quot;\\t\u0026quot;) probe_dat_log \u0026lt;- apply(probe_dat[, -1], MAR = c(1,2), FUN = exp) probe_dat_log ## Probe1 Probe2 Probe3 Probe4 Probe5 ## [1,] 4.424134e+05 7.896296e+13 2.648912e+10 2.980958e+03 3.931334e+12 ## [2,] 3.931334e+12 1.907347e+21 8.886111e+06 5.987414e+04 1.285160e+19 ## [3,] 2.235247e+37 5.052394e+31 1.811239e+41 1.220403e+39 8.223013e+36 ## [4,] 3.404276e+29 4.607187e+28 5.685720e+24 1.373383e+32 1.041376e+23 ## [5,] 3.584913e+09 1.068647e+13 1.484132e+02 7.896296e+13 8.659340e+16 ## [6,] 3.493427e+19 2.581313e+20 1.142007e+26 3.185593e+16 5.834617e+14 ## [7,] 2.758513e+33 1.545539e+25 6.837671e+30 1.373383e+32 1.373383e+32 ## [8,] 2.451246e+40 1.373383e+32 4.375039e+48 3.025077e+36 2.038281e+34 ## [9,] 3.185593e+16 1.907347e+21 1.041376e+23 1.545539e+25 2.353853e+17 ## [10,] 2.146436e+14 6.565997e+07 1.285160e+19 2.581313e+20 1.171914e+16 ## [11,] 2.235247e+37 6.076030e+37 5.399228e+44 6.837671e+30 1.467662e+45 ## [12,] 1.142007e+26 8.438357e+26 4.201210e+25 7.016736e+20 1.142007e+26 ## [13,] 1.784823e+08 1.784823e+08 4.311232e+15 5.987414e+04 4.311232e+15 ## [14,] 9.744803e+09 1.627548e+05 9.744803e+09 2.353853e+17 6.565997e+07 ## [15,] 1.957296e+11 7.200490e+10 4.851652e+08 2.146436e+14 1.318816e+09 ## [16,] 2.091659e+24 3.493427e+19 1.858672e+31 1.014800e+33 8.659340e+16 ## [17,] 2.038281e+34 2.451246e+40 4.923458e+41 5.540622e+34 1.252363e+29 ## [18,] 9.253782e+29 3.831008e+22 4.093997e+35 1.545539e+25 1.041376e+23 ## [19,] 4.311232e+15 1.907347e+21 1.957296e+11 2.146436e+14 7.200490e+10 ## [20,] 1.446257e+12 1.318816e+09 5.834617e+14 5.320482e+11 7.896296e+13 ## [21,] 8.103084e+03 2.648912e+10 3.584913e+09 5.459815e+01 1.096633e+03 ## [22,] 6.235149e+27 5.685720e+24 7.694785e+23 2.515439e+30 9.496119e+19 ## [23,] 3.584913e+09 6.398435e+17 2.415495e+07 1.318816e+09 3.584913e+09 ## [24,] 3.584913e+09 5.987414e+04 4.311232e+15 8.659340e+16 2.146436e+14 ## [25,] 1.409349e+22 8.659340e+16 1.285160e+19 4.201210e+25 4.727839e+18 ## [26,] 1.096633e+03 7.389056e+00 1.957296e+11 1.096633e+03 4.539993e-05 ## [27,] 1.811239e+41 1.112864e+36 4.093997e+35 7.307060e+43 4.375039e+48 ## [28,] 1.627548e+05 3.931334e+12 1.353353e-01 1.627548e+05 4.851652e+08 ## [29,] 1.739275e+18 1.285160e+19 1.907347e+21 8.659340e+16 2.648912e+10 ## [30,] 7.016736e+20 4.201210e+25 2.091659e+24 1.739275e+18 8.438357e+26 ## [31,] 4.093997e+35 1.858672e+31 6.663176e+40 4.093997e+35 6.837671e+30 ## [32,] 1.409349e+22 2.830753e+23 3.831008e+22 8.659340e+16 4.727839e+18 ## [33,] 2.415495e+07 1.484132e+02 2.648912e+10 1.000000e+00 2.008554e+01 ## [34,] 6.837671e+30 1.409349e+22 1.041376e+23 1.694889e+28 2.830753e+23 ## [35,] 1.252363e+29 1.041376e+23 6.837671e+30 3.104298e+26 2.581313e+20 ## [36,] 5.459815e+01 3.059023e-07 4.424134e+05 8.315287e-07 1.202604e+06 ## [37,] 9.253782e+29 1.142007e+26 4.093997e+35 1.041376e+23 7.694785e+23 ## [38,] 1.068647e+13 3.185593e+16 5.320482e+11 1.586013e+15 1.586013e+15 ## [39,] 2.415495e+07 9.744803e+09 1.586013e+15 3.931334e+12 2.980958e+03 ## [40,] 7.694785e+23 6.398435e+17 5.685720e+24 1.041376e+23 2.293783e+27 ## [41,] 5.834617e+14 1.068647e+13 4.727839e+18 1.446257e+12 2.830753e+23 ## [42,] 9.253782e+29 1.858672e+31 5.540622e+34 7.694785e+23 3.404276e+29 ## [43,] 1.338335e+42 6.493134e+50 1.084464e+46 1.220403e+39 1.651636e+38 ## [44,] 1.811239e+41 4.093997e+35 3.989520e+45 1.986265e+44 1.467662e+45 ## [45,] 5.459815e+01 5.459815e+01 6.565997e+07 6.144212e-06 5.987414e+04 ## [46,] 4.607187e+28 4.201210e+25 7.694785e+23 3.831008e+22 1.252363e+29 ## [47,] 4.489613e+38 2.178204e+47 2.758513e+33 3.989520e+45 1.338335e+42 ## [48,] 7.389056e+00 3.678794e-01 1.831564e-02 1.125352e-07 6.565997e+07 ## [49,] 5.459815e+01 1.234098e-04 6.565997e+07 9.118820e-04 2.718282e+00 ## [50,] 9.496119e+19 7.694785e+23 1.142007e+26 6.398435e+17 4.201210e+25 The same results can be generated by using a for loop to loop over all entries, but this is much slower.\nlapply() (“list apply”) is similar but runs over a list of values, and returns the output as a list of values. In this example, the mean is calculated for a number of vectors, but these vectors can be different sizes (unlike for a matrix or data frame):\nmy_list \u0026lt;- list(val1 = c(2,4,2,3,4,3,4), val2 = c(1,2), val3 = c(10,2,5,9)) lapply(my_list, FUN = mean) ## $val1 ## [1] 3.142857 ## ## $val2 ## [1] 1.5 ## ## $val3 ## [1] 6.5 However, since the output is a list, the output could also be a list of vectors:\nmy_list \u0026lt;- list(val1 = c(2,4,2,3,4,3,4), val2 = c(1,2), val3 = c(10,2,5,9)) lapply(my_list, FUN = sort) ## $val1 ## [1] 2 2 3 3 4 4 4 ## ## $val2 ## [1] 1 2 ## ## $val3 ## [1] 2 5 9 10 sapply() (“simple apply”) is similar to lapply(), but returns the results as a vector rather than a list. This is a better method to use when returning a single value for each list entry:\nmy_list \u0026lt;- list(val1 = c(2,4,2,3,4,3,4), val2 = c(1,2), val3 = c(10,2,5,9)) sapply(my_list, FUN = mean) ## val1 val2 val3 ## 3.142857 1.500000 6.500000 mapply() (“multivariate apply”) is very useful for vectorization, and works by applying the function FUN to the first elements of each object, then to the second element, and so on. The following example will replicate the number n n-times for numbers 1 to 5. This could also be done using loops, but loops do not scale as well as vectorised functions such as this:\nmapply(rep, 1:5, 1:5) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 2 ## ## [[3]] ## [1] 3 3 3 ## ## [[4]] ## [1] 4 4 4 4 ## ## [[5]] ## [1] 5 5 5 5 5 tapply() is a little more complicated, but essentially applies a function after breaking data up based on some index variable. It is useful for calculating summary statistics of different groups of data, and uses a factor parameter INDEX to define the groups over which to apply the function FUN. So in the following code, tapply will apply the function mean() on the values of Expression for the two different treatment classes defined in the INDEX variable:\nmy_dat \u0026lt;- data.frame(Treatment = c(\u0026quot;Control\u0026quot;, \u0026quot;Control\u0026quot;, \u0026quot;Control\u0026quot;, \u0026quot;Treated\u0026quot;, \u0026quot;Treated\u0026quot;, \u0026quot;Treated\u0026quot;), Expression = c(13, 17, 9, 28, 37, 34)) tapply(my_dat$Expression, INDEX = my_dat$Treatment, FUN = mean) ## Control Treated ## 13 33  9 Some Simple Statistics R is mainly designed for easy computation of statistics and there are many in-built functions and additional libraries that allow you to carry out most tasks. Most simple statistics can be easily calculated using in-built functions. The following example creates two vectors of 100 random values sampled from a normal distribution with mean 0 and standard deviation 1, then calculates various basic summary statistics:\nx \u0026lt;- sort(rnorm(100, mean = 0, sd = 1)) min(x)  ## [1] -2.336472 max(x)  ## [1] 2.939707 mean(x)  ## [1] 0.07717416 median(x) ## [1] 0.02808852 The minimum and maximum values are the smallest and largest values respectively. The mean is what most people would think of when you asked for the average, and is calculated by summing the values and dividing by the total number of values. The median is another way of looking at the average, and is essentially the middle value (50^th^ percentile). Other percentiles can be calculated, which can give you an idea of where the majority of your data lie:\nquantile(x, probs = 0.25)  ## 25% ## -0.6394031 quantile(x, probs = 0.75)  ## 75% ## 0.6994683 quantile(x, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% ## -2.33647209 -1.31947437 -0.87549325 -0.55566797 -0.15779236 0.02808852 ## 60% 70% 80% 90% 100% ## 0.27175879 0.56818018 1.05753511 1.62287076 2.93970745 The summary() function will calculate many of these basic statistics for you:\nsummary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.33647 -0.63940 0.02809 0.07717 0.69947 2.93971 The variance is the average of the squared distances from the mean, and is a measure of how spread out the data are from the average. The standard deviation is simply the square root of this value \\(var(x) = sd(x)^2\\):\nsd(x) ## [1] 1.07332 var(x) ## [1] 1.152016 all.equal(sd(x)^2, var(x)) ## [1] TRUE sum((x-mean(x))^2)/(length(x)-1) ## [1] 1.152016 The covariance is a measure of how much two sets of data vary together:\ny \u0026lt;- sort(rnorm(100, mean = 0, sd = 1)) var(y) ## [1] 0.9048439 cov(x, y) ## [1] 1.006295 The covariance is related to the correlation between two data sets, which is a number between -1 and 1 indicating the level of dependance between the two variables. A value of 1 indicates perfect correlation, so that as one value increases so does the other. A value of -1 indicates perfect anti-correlation, so that as one value increases the other decreases. A value of 0 indicates that the two values change independently of one another:\ncor(x, y) ## [1] 0.9856189 cov(x, y)/(sd(x) * sd(y))  ## [1] 0.9856189 This value is known as the Pearson correlation. An alternative method for calculating the correlation between two sets of values is to use the Spearman correlation, which is essentially the same as the Pearson correlation but is calculated on the ranks of the data rather than the values themselves. In this way, each value increases by only one unit at a time, meaning that the correlation score is more robust to the presence of outliers:\ncor(x, y, method = \u0026quot;spearman\u0026quot;) ## [1] 1 So these values are pretty highly dependent on one another – not surprising considering that they are both drawn randomly from the same distribution. We can calculate the line of best fit between the two vectors by using linear regression, which searches for the best straight line model \\(y = a + bx\\) that minimises the squared distances between the line (estimated values) and the observed data points:\nmy_lin_mod \u0026lt;- lm(y ~ x) summary(my_lin_mod) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.75519 -0.06913 0.03729 0.10091 0.29603 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.26517 0.01620 -16.37 \u0026lt;2e-16 *** ## x 0.87351 0.01513 57.74 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1616 on 98 degrees of freedom ## Multiple R-squared: 0.9714, Adjusted R-squared: 0.9712 ## F-statistic: 3334 on 1 and 98 DF, p-value: \u0026lt; 2.2e-16 Explaining this output is beyond the scope of this short tutorial, but the coefficient estimates give us the values for a (-0.2651651) and b (0.8735073) in the linear model. The p-value tells us how significant these estimates are. In statistical terms, we are testing the null hypothesis that the coefficient is actually equal to zero (i.e. there is not an association between x and y). The p-value gives the probability of detecting a coefficient at least as large as the one that we calculated in our model given that the null hypothesis is actually true. If this probability is low enough, we can safely reject the null hypothesis and say that this variable is statistically significant. Often a value of 0.05 (5%) is used as the cutoff for rejection of the null hypothesis.\nHypothesis testing is a large part of statistics. The t-test is a commonly used test for comparing the means of two sets of data. In simple terms we are looking to see if they are significantly different (e.g. does the expression of a particular gene change significantly following treatment with a drug). In statistical terms, we are testing to see if the change that we see in the means is greater than we would expect by chance alone.\nt.test(x, y)  ## ## Welch Two Sample t-test ## ## data: x and y ## t = 1.917, df = 195.18, p-value = 0.0567 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.007920033 0.557774135 ## sample estimates: ## mean of x mean of y ## 0.07717416 -0.19775289 Since both x and y are drawn from the same distribution, the test shows there is no evidence that there is a difference between the mean. Let’s try again with a different data set, drawn from a different distribution:\nz \u0026lt;- rnorm(100, mean = 10, sd = 1) t.test(x, z) ## ## Welch Two Sample t-test ## ## data: x and z ## t = -67.773, df = 197.06, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.239515 -9.660461 ## sample estimates: ## mean of x mean of y ## 0.07717416 10.02716244 This time, the p-value is much less than 0.05, so we can make the claim that the mean of z is significantly different from that of x. By default, the t.test() function is 2-sided, meaning that it does not distinguish between whether or not the difference in the means is an increase or a decrease in z. We can specify the alternative parameter to define the alternative hypothesis that we want to test:\nt.test(x, z, alternative = \u0026quot;less\u0026quot;) ## Tests if mean(x) \u0026lt; mean(z)  ## ## Welch Two Sample t-test ## ## data: x and z ## t = -67.773, df = 197.06, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -9.707361 ## sample estimates: ## mean of x mean of y ## 0.07717416 10.02716244 t.test(x, z, alternative = \u0026quot;greater\u0026quot;) ## Tests if mean(x) \u0026gt; mean(z) ## ## Welch Two Sample t-test ## ## data: x and z ## t = -67.773, df = 197.06, p-value = 1 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -10.19262 Inf ## sample estimates: ## mean of x mean of y ## 0.07717416 10.02716244 This tells us that the difference in the means between x and z is less than 0, or that the mean of z is greater than that of x (as we expect).\n 10 Plotting With R One of the most useful functions of R is the ability to plot publication-quality figures simply and easily. The vast number of tools available to users for plotting figures is beyond the scope of this tutorial, but I will mention a few of the most commonly used plotting functions to allow you to have a quick look at your data. These functions are all part of the base plotting package, but I also recommend looking into the ggplot2() package for an incredibly intuative appraoch to plotting data.\n10.1 Scatterplots Scatterplots are probably the simplest plot that we can look at. Here we take two sets of values and plot one against the other to see how they correlate. This means that the two data sets are paired, such that the first element of each data set represents one event, the second represents another, and so on. For instance, for every student in a class, we may have scores from tests taken at the start and at the end of the year, and we want to compare them against one another to see how they compare. Here is how to plot a simple scatterplot:\nplot(x, y, pch = 19, ## Plot each point as a filled circle col = \u0026quot;red\u0026quot;, ## Colour each point red xlab = \u0026quot;This is x\u0026quot;, ## Add a label to the x-axis ylab = \u0026quot;This is y\u0026quot;, ## Add a label to the y-axis main = \u0026quot;This is y vs. x\u0026quot;, ## Add a main title to the plot cex.main = 1.4, ## Change the size of the title cex.lab = 1.2, ## Change the size of the axis labels cex.axis = 1.1 ## Change the size of the axis values ) There are lots of additional plotting arguments that can be set in the plot() command. These are just a few. These arguments will typically work for any plotting function that you may want to use.\nplot() is the standard plotting function, and works differently depending on the type of data on which it is called. Most of the following plots use this function in some way, even though it may not be obvious.\nHere we have coloured all of our points a single colour by using the col = \u0026quot;red\u0026quot; argument. However, we can assign colours to each point separately by supplying a vector of colours that is the same length as x and y. This means that we can set colours based on the data themselves:\nmy_cols \u0026lt;- rep(\u0026quot;black\u0026quot;, length(x)) my_cols[x \u0026gt; 1 \u0026amp; y \u0026gt; 1] \u0026lt;- \u0026quot;red\u0026quot; my_cols[x \u0026gt; 1 \u0026amp; y \u0026lt; -1] \u0026lt;- \u0026quot;green\u0026quot; my_cols[x \u0026lt; 0 \u0026amp; y \u0026gt; 0] \u0026lt;- \u0026quot;blue\u0026quot; plot(x, y, col = my_cols, pch = 19) Since this plot is useful for observing the level of correlation between two data sets, it may be useful to add a couple of lines in to the plot to help us determine if there is a trend indicating that x is well correlated with y. First of all we will add lines in through the origin, and then we will add in a dotted line along the x = y line (since, if the two datasets were exactly correlated, the points would lie on this line). To do this, we use the abline() function. This plots a straight line in one of three ways. We can either specify a horizontal line by specifying the h argument, or we can specify a vertical line by using the v argument, or we can specify a straight line in the format \\(y = a + bx\\) (where a is the intercept term and b is the gradient term):\nplot(x, y, ylim = c(-3,3), xlim = c(-3,3)) abline(h = 0) abline(v = 0) abline(a = 0, b = 1, lty = 2) ## lty gives the line type - in this case dotted Notice that abline() does not create a new plot, but instead adds to the plot that we already have. This is because it does not call the plot.new() function, which would otherwise create a new plotting region.\nWe may be particularly interested in how the line of best fit looks as compared to the \\(x = y\\) line, as this will show us if there is a general trend in the data or not. To do this we can use a linear model to predict a and b from the data:\nplot(x, y, ylim = c(-3,3), xlim = c(-3,3)) my_lin_model \u0026lt;- lm(y ~ x) abline(my_lin_model, lty = 2, col = \u0026quot;red\u0026quot;) If you want to explicitly pull out a and b, use the coef() function to get the coefficients:\ncoef(my_lin_model)[1] ## Get the intercept from the coefficients of the model  ## (Intercept) ## -0.2651651 coef(my_lin_model)[2] ## Get the gradient from the coefficients of the model ## x ## 0.8735073  10.2 Histograms Now let’s look at the distribution of the data. A histogram is useful for this. Here we count up the number of values that fall into discrete bins. The size of the bins (or the number of bins) can be specified by using the breaks argument:\nx \u0026lt;- rnorm (1000) par(mfrow=c(1,2)) hist(x) ## Shows a nice bell shape curve about mean 0 hist(x, breaks = 200) ## More fine-grained  10.3 Quantile-Quantile Plots Quantile-quantile plots are a particular type of scatterplot that are used to see if two data sets are drawn from the same distribution. To do this, it plots the quantiles of each data set against each other. That is it plots the 0th percentile of data set A (the minimum value) against the 0th percentile of data set B, the 50th percentiles (the medians) against each other, the 100th percentiles (the maximum values) against each other, etc. Simply, it sorts both data sets, and makes them both the same length by estimating any missing values, then plots a scatterplot of the sorted data. If the two data sets are drawn from the same distribution, this plot should follow the \\(x = y\\) identity line at all but the most extreme point.\nHere is a QQ plot for two data sets drawn from the same normal distribution:\nx1 \u0026lt;- rnorm(100, mean = 0, sd = 1) x2 \u0026lt;- rnorm(1000, mean = 0, sd = 1) qqplot(x1, x2) abline(a = 0, b = 1, lty = 2) And here is a QQ plot for two data sets drawn from different normal distributions:\nx1 \u0026lt;- rnorm(100, mean = 0, sd = 1) x2 \u0026lt;- rnorm(1000, mean = 1, sd = 3) qqplot(x1, x2) abline(a = 0, b = 1, lty = 2)  10.4 Line Plots Scatterplots are useful for generating correlation plots for pairs of data. Another form of data is a set of values along a continuum, for instance we may have the read count along the length of the genome. For this, a scatterplot may not be the most sensible way of viewing these data. Instead, a line plot may be a more fitting way of viewing the data. To do this we simply specify the type argument to be line (or simply l).\nOne thing to be careful of with data such as this is that you must make sure that the data are ordered from left to right (or right to left) on the x axis so that connecting the points makes sense on the continuum. For instance, the following plot is not terribly useful:\nx = c(2,4,5,3,1,7,9,8,6,10) y = c(4,2,5,4,10,6,6,5,6,9) plot(x = x, y = y, type = \u0026#39;l\u0026#39;) But if we order the data from left to right then it will be a lot more useful:\nplot(x = x[order(x)], y = y[order(x)], type = \u0026#39;l\u0026#39;)  You can also plot both points and lines by setting the type argument to both (or b):\nplot(x = x[order(x)], y = y[order(x)], type = \u0026#39;b\u0026#39;)  10.5 Density Plots We can use a line plot like this to plot the density of the data, which gives us a similar plot to the histogram. The benefit of this type of plot over a histogram is that you can overlay the distribution of multiple data sets. The density() function is a kernal density estimator function that basically calculates the density of the data within each bin such that the total area under the resulting curve is 1. This makes these plots useful for comparing data sets of different sizes as they are essentially normalised. We can add a legend to this plot to make it clear which line represents which sample. Again, this does not call plot.new() so will appear on top of the current plot:\n## Create 2 random normal distributions about 5 and 10 respectively x1 \u0026lt;- rnorm(100, mean = 5, sd = 1) x2 \u0026lt;- rnorm(1000, mean = 10, sd = 1) ## Calculate the density of each x1dens \u0026lt;- density(x1) x2dens \u0026lt;- density(x2) ## Set up a plotting region explicitly plot.new() plot.window(xlim = c(0,15), ylim = c(0,0.5)) range ## function (..., na.rm = FALSE) .Primitive(\u0026quot;range\u0026quot;) title(xlab = \u0026quot;Value\u0026quot;, ylab = \u0026quot;Density\u0026quot;, main = \u0026quot;Density Plot\u0026quot;) axis(1) axis(2) ## Add the data (notice that these do not call plot.new() so will add onto the current figure lines(x1dens , col = \u0026quot;red\u0026quot;) lines(x2dens , col = \u0026quot;blue\u0026quot;) ## Add a legend legend(\u0026quot;topleft\u0026quot;, legend = c(\u0026quot;Mean = 5\u0026quot;, \u0026quot;Mean = 10\u0026quot;), col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), lty = 1)  10.6 Boxplots Another way to compare the distribution of two (or more) data sets is by using a boxplot. A boxplot shows the overal distribution by plotting a box bounded by the first and third quartiles, with the median highlighted. This shows where the majority of the data lie. Additional values are plotted as whiskers coming out from the main box:\nboxplot(x1, x2, names = c(\u0026quot;Mean = 5\u0026quot;, \u0026quot;Mean = 10\u0026quot;), ylab = \u0026quot;Value\u0026quot;) boxplot() can also take the data in the form of a data frame, which is useful for instance if you want to compare the distribution of expression values over all genes for a number of different samples. This will automatically label the boxes with the column names from the data frame:\nmy_data \u0026lt;- data.frame(Sample1 = rnorm(100), Sample2 = rnorm(100), Sample3 = rnorm(100), Sample4 = rnorm(100), Sample5 = rnorm(100)) boxplot(my_data)  10.7 Bar Plots and Pie Charts Now let’s say that we have a data set that shows the number of called peaks from a ChIPseq data set that fall into distinct genomic features (exons, introns, promoters and intergenic regions). One way to look at how the peaks fall would be to look at a pie graph:\nmy_peak_nums \u0026lt;- c(\u0026quot;exon\u0026quot; = 1400, \u0026quot;intron\u0026quot; = 900, \u0026quot;promoter\u0026quot; = 200, \u0026quot;intergenic\u0026quot; = 150) pie(my_peak_nums) This figure shows that the majority of the peaks fall into exons. However, pie graphs are typically discouraged by statisticians, because your eyes can often misjudge estimates of the area taken up by each feature. A better way of looking at data such as this would be in the form of a barplot:\nbarplot(my_peak_nums, ylab = \u0026quot;Number of Peaks in Feature\u0026quot;, main = \u0026quot;Peaks in Gene Features\u0026quot;) Now let’s suppose that we had data showing the number of peaks in different genomic features for multiple samples. We could plot multiple pie charts:\nmy_peak_nums \u0026lt;- data.frame(GeneFeature = c(\u0026quot;exon\u0026quot;, \u0026quot;intron\u0026quot;, \u0026quot;promoter\u0026quot;, \u0026quot;intergenic\u0026quot;), Sample1 = c( 1400, 900, 200, 150 ), Sample2 = c( 2400, 1000, 230,250 ), Sample3 = c( 40,30, 5,7 ) ) par(mfrow = c(1,3)) pie(my_peak_nums[[2]], main = \u0026quot;Sample1\u0026quot;, labels = my_peak_nums[[1]]) pie(my_peak_nums[[3]], main = \u0026quot;Sample2\u0026quot;, labels = my_peak_nums[[1]]) pie(my_peak_nums[[4]], main = \u0026quot;Sample3\u0026quot;, labels = my_peak_nums[[1]]) par(mfrow = c(1,1)) ## Reset the plotting region However comparing across multiple pie charts is very difficult. Instead, a single barplot will work better. Note that here the number of peaks is different for each sample, so it makes more sense to convert the data into a format whereby the bar height represents the percentage of peaks within a particular feature:\n## Convert to percentages so that the samples are comparable my_peak_percent \u0026lt;- my_peak_nums[, 2:4] for (i in 1:3) { my_peak_percent[[i]] \u0026lt;- 100*my_peak_percent[[i]]/sum(my_peak_percent[[i]]) } ## Convert to a matrix to satisfy requirements for barplot() my_peak_percent \u0026lt;- as.matrix(my_peak_percent) ## Plot the bar plot barplot(my_peak_percent , ylab = \u0026quot;Percentage of Peaks in Feature\u0026quot;, main = \u0026quot;Peaks in Gene Features\u0026quot;, legend.text = my_peak_nums[[\u0026quot;GeneFeature\u0026quot;]]) Notice that the default way that barplot() works is to plot the bars in a single stack for each sample. This is fine for comparing the exons, but trying to compare the other classes is much harder. A better way to plot these data would be to plot the bars side by side for each sample:\nbarplot(my_peak_percent , ylab = \u0026quot;Percentage of Peaks in Feature\u0026quot;, main = \u0026quot;Peaks in Gene Features\u0026quot;, legend.text = my_peak_nums[[\u0026quot;GeneFeature\u0026quot;]], beside = TRUE)  10.8 Graphical Control That covers the majority of the basic plotting functions that you may want to use. You can change the standard plotting arguments by using the par() command:\npar(mar = c(5,10,0,3)) ## Sets the figure margins (in \u0026#39;number of lines\u0026#39;) - b,l,t,r par(las = 1) ## Changes axis labels to always be horizontal par(tcl = -0.2) ## Change the size of the axis ticks plot(x = rnorm(100), y = rnorm(100))  10.9 Subplots By default, the graphics device will plot a single figure only. There are several ways to create subfigures within this region. The first is to set the mfrow argument in par(). This will split the graphics region into equally sized subplots:\npar(mfrow = c(3, 2)) ## Creates a figure region with 3 rows and 2 columns for (i in 1:6) { plot(x = rnorm(100), y = rnorm(100)) } However, if you want more control over your plotting, you can use the layout() function which allows you to specify the size and layout of the subplots. This function takes a matrix specifying where in the grid of subplots each plot should be drawn to. So the first call to plot() will put its figure in the grid regions labelled 1, the scond call will put its figure anywhere that there is a 2, etc. Anywhere that you do not want a figure should have a 0. The heights and widths arguments allow you to specify the size of each grid region. You can check what the resulting figure layout will look like by using layout.show(n), where n is the number of subplots in your figure. With a bit of work, you can get some very good layouts:\nmy_layout \u0026lt;- matrix(c(1,1,1,1,2,2,3,4,2,2,3,4,0,0,3,4,0,0,5,5), nrow = 5, ncol = 4, byrow = TRUE) layout(my_layout, widths = c(10,10,2,2), heights = c(1,5,5,5,2)) my_layout ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 2 2 3 4 ## [3,] 2 2 3 4 ## [4,] 0 0 3 4 ## [5,] 0 0 5 5 layout.show(5) ## Can you see how this matrix leads to this layout?   10.10 Saving Figures By default, figures are generated in a seperate window from R. However, you can save the figure to an external file by using one of the functions png(), pdf(), jpeg(), etc. These functions open a new “device”, which R can use to plot to. After the figure has been plotted, the device must be turned off again using the dev.off() function. There are many arguments that can be used for these functions. In general, these define the dimensions and resolution of the resulting figure. It can be difficult to get these right, so play around to see how they affect things:\npng(\u0026quot;figures/test_figure.png\u0026quot;, height = 10, width = 10, unit = \u0026quot;in\u0026quot;, res = 300) plot(1:10, 1:10, type = \u0026quot;l\u0026quot;, main = \u0026quot;My Test Figure\u0026quot;, xlab = \u0026quot;x axis\u0026quot;, ylab = \u0026quot;y axis\u0026quot;) dev.off()   11 Example Analysis 11.1 Introduction This is just a simple example analysis to give you an idea of the sort of things that we can do with R. Suppose that we have two experiments, each looking at the effects on gene expression of using a particular drug (“Drug A” and “Drug B”). For each experiment we have two samples; one showing the gene expression when treated with the drug, and the other showing the gene expression when treated with some control agent. Obviously in a real experiment, we would have many replicates, but here we have \\(n=1\\). We want to do the following:\nFor each drug, we want to get the fold change for each gene For each drug, we want to identify the genes that are significantly changed when using the drug We want to compare the results for Drug A with those from Drug B to find genes that are affected similarly by both drugs We want to plot the correlation between the fold change values for the two drugs to see how similar they are  For this, we will need four files. These files are in a tab-delimited text format. They are tables of values where each row is separated by a new line, and each column is separated by a tab character (\\t). These files can be created by and read into Excel for ease of use. To avoid errors when reading in files from text, it is good practice to ensure that there are no missing cells in your data. Instead try to get into the habit of using some “missing”\u0026quot; character (e.g. NA).\n  File Name Description    experiment1_control.txt Expression levels for ctrl in expt 1  experiment1_drug.txt Expression levels for drug A in expt 1  experiment2_control.txt Expression levels for control in expt 2  experiment2_drug.txt Expression levels for drug A in expt 2     11.2 Load Data First let’s load in the data:\nexpt1_ctrl \u0026lt;- read.table(\u0026quot;experiment1_control.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) expt1_drug \u0026lt;- read.table(\u0026quot;experiment1_drug.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) expt2_ctrl \u0026lt;- read.table(\u0026quot;experiment2_control.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) expt2_drug \u0026lt;- read.table(\u0026quot;experiment2_drug.txt\u0026quot;, header = TRUE, sep = \u0026quot;\\t\u0026quot;, stringsAsFactors = FALSE) Use head() to look at the data. Each of these files contains two columns; the gene name and some value that represents the expression level for that gene (assume that these values have been calculated after pre-processing, normalisation, etc.).\nIn all of these cases, the list of gene names is identical, and in the same order which means that we could compare row 1 from the control-treated file with row 2 from the drug-treated file to get all of the comparisons. However, in a real data set you will not know for sure that the gene names match so I recommend merging the files together into a single data frame to ensure that all analyses are conducted on a gene by gene basis on the correct values.\nWe therefore create a single data frame for both experiments using the merge() command:\nexpt1 \u0026lt;- merge(expt1_ctrl, expt1_drug, by = \u0026quot;GeneName\u0026quot;) ## The \u0026#39;by\u0026#39; variable tells merge which column to merge names(expt1)[2] \u0026lt;- \u0026quot;Control\u0026quot; names(expt1)[3] \u0026lt;- \u0026quot;Drug\u0026quot; expt2 \u0026lt;- merge(expt2_ctrl, expt2_drug, by = \u0026quot;GeneName\u0026quot;) names(expt2)[2] \u0026lt;- \u0026quot;Control\u0026quot; names(expt2)[3] \u0026lt;- \u0026quot;Drug\u0026quot;  11.3 Calculate Fold Change Now we calculate the fold change for each gene by dividing the drug-treated expression by the control expression. To avoid divide by zero errors, we can set a minimum expression value. This will also ensure that we are only looking at expression changes between significant expression values. Since we want to do the same thing to both the experiment 1 and the experiment 2 data sets, it makes sense to write a single function to use for both:\nget_fold_change \u0026lt;- function (x, min_expression = 10) { ctrl_val \u0026lt;- as.numeric(x[\u0026quot;Control\u0026quot;]) drug_val \u0026lt;- as.numeric(x[\u0026quot;Drug\u0026quot;]) ctrl_val \u0026lt;- ifelse(ctrl_val \u0026lt;= min_expression, min_expression, ctrl_val) drug_val \u0026lt;- ifelse(drug_val \u0026lt;= min_expression, min_expression, drug_val) return(drug_val/ctrl_val) } expt1[[\u0026quot;FoldChange\u0026quot;]] \u0026lt;- apply(expt1, MAR = 1, FUN = get_fold_change) expt2[[\u0026quot;FoldChange\u0026quot;]] \u0026lt;- apply(expt2, MAR = 1, FUN = get_fold_change)  11.4 Compare Data Now let’s find the genes that are upregulated and downregulated in each experiment. Due to the lack of replicates, we do not have any estimate for the variance of these genes, so we will have to make do with using a threshold on the fold change:\nfold_change_threshold \u0026lt;- 1.5 expt1_up \u0026lt;- subset(expt1, FoldChange \u0026gt;= fold_change_threshold)[[\u0026quot;GeneName\u0026quot;]] expt1_down \u0026lt;- subset(expt1, FoldChange \u0026lt;= 1/fold_change_threshold)[[\u0026quot;GeneName\u0026quot;]] expt2_up \u0026lt;- subset(expt2, FoldChange \u0026gt;= fold_change_threshold)[[\u0026quot;GeneName\u0026quot;]] expt2_down \u0026lt;- subset(expt2, FoldChange \u0026lt;= 1/fold_change_threshold)[[\u0026quot;GeneName\u0026quot;]] cat(\u0026quot;Upregulated in Experiment 1:\u0026quot;, paste(expt1_up, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Upregulated in Experiment 1: ## gene12 ## gene8 cat(\u0026quot;Downregulated in Experiment 1:\u0026quot;, paste(expt1_down, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Downregulated in Experiment 1: ## gene32 ## gene46 cat(\u0026quot;Upregulated in Experiment 2:\u0026quot;, paste(expt2_up, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Upregulated in Experiment 2: ## gene18 ## gene50 ## gene8 cat(\u0026quot;Downregulated in Experiment 2:\u0026quot;, paste(expt2_down, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Downregulated in Experiment 2: ## gene22 ## gene43 So we now have the genes that change when each of the drugs is used. But now we want to compare the two drugs together. First, let’s see if there are any genes similarly affected by both drugs. We can do this using the intersect() function which gives the intersect of two lists:\ncommon_up \u0026lt;- intersect(expt1_up, expt2_up) common_down \u0026lt;- intersect(expt1_down, expt2_down) cat(\u0026quot;Upregulated in Experiment 1 and Experiment 2:\u0026quot;, paste(common_up, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Upregulated in Experiment 1 and Experiment 2: ## gene8 cat(\u0026quot;Downregulated in Experiment 1 and Experiment 2:\u0026quot;, paste(common_down, collapse = \u0026quot;\\n\u0026quot;), sep = \u0026quot;\\n\u0026quot;) ## Downregulated in Experiment 1 and Experiment 2: So we can see that only one gene is similarly affected by both drugs (“gene8”). Now let’s plot a figure to see how the fold change differs between the two drugs:\nfold_change_data \u0026lt;- merge(expt1[, c(\u0026quot;GeneName\u0026quot;, \u0026quot;FoldChange\u0026quot;)], expt2[, c(\u0026quot;GeneName\u0026quot;, \u0026quot;FoldChange\u0026quot;)], by = \u0026quot;GeneName\u0026quot;) names(fold_change_data)[2] \u0026lt;- \u0026quot;Experiment1\u0026quot; names(fold_change_data)[3] \u0026lt;- \u0026quot;Experiment2\u0026quot; plot(x = log2(fold_change_data[[\u0026quot;Experiment1\u0026quot;]]), y = log2(fold_change_data[[\u0026quot;Experiment2\u0026quot;]]), pch = 19, xlab = \u0026quot;log2(Experiment1 Fold Change)\u0026quot;, ylab = \u0026quot;log2(Experiment2 Fold Change)\u0026quot;, main = \u0026quot;Experiment1 Fold Change vs Experiment2 Fold Change\u0026quot;, cex.lab = 1.3, cex.axis = 1.2, cex.main = 1.4, xlim = c(-2,2), ylim = c(-2,2) ) abline(h = 0) abline(v = 0) abline(a = 0, b = 1, lty = 2) This figure shows that the effect on the gene expression is actually quite different for the two drugs. We can also see this by looking at the correlation between the two experiments:\ncor(x = fold_change_data[[\u0026quot;Experiment1\u0026quot;]], y = fold_change_data[[\u0026quot;Experiment2\u0026quot;]], method = \u0026quot;pearson\u0026quot;) ## [1] 0.08381614 cor(x = fold_change_data[[\u0026quot;Experiment1\u0026quot;]], y = fold_change_data[[\u0026quot;Experiment2\u0026quot;]], method = \u0026quot;spearman\u0026quot;) ## [1] -0.02618115   ","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"a8280562ed4eae29bb73b167e84d00a7","permalink":"/resources/rtutorial/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/resources/rtutorial/","section":"resources","summary":"A tutorial for the use of the statistical programming language R, with a Bioinformatics leaning","tags":["R","How To","Tutorials"],"title":"How To Use R","type":"resources"},{"authors":["Li M","Amaral P","Cheung P","Bergmann J","Kinoshita M","Kalkan T","Ralser M","Sam Robson","Meyenn F","Paramor M","Yang F","Chen C","Nichols J","Spector D","Kouzarides T","He L","Smith A"],"categories":null,"content":"","date":1503014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503014400,"objectID":"9ea31af3384dab0dbb3687827e9a5bb2","permalink":"/publication/2017_a_lncrna_fine_tunes_the_dynamics_of_a_cell_state_transition_involving_lin28_let-7_and_de_novo_dna_methylation/","publishdate":"2017-08-18T00:00:00Z","relpermalink":"/publication/2017_a_lncrna_fine_tunes_the_dynamics_of_a_cell_state_transition_involving_lin28_let-7_and_de_novo_dna_methylation/","section":"publication","summary":"Execution of pluripotency requires progression from the naïve status represented by mouse embryonic stem cells (ESCs) to a state capacitated for lineage specification. This transition is coordinated at multiple levels. Non-coding RNAs may contribute to this regulatory orchestra. We identified a rodent-specific long non-coding RNA (lncRNA) linc1281, hereafter Ephemeron (Eprn), that modulates the dynamics of exit from naïve pluripotency. Eprn deletion delays the extinction of ESC identity, an effect associated with perduring Nanog expression. In the absence of Eprn, Lin28a expression is reduced which results in persistence of let-7 microRNAs, and the up-regulation of de novo methyltransferases Dnmt3a/b is delayed. Dnmt3a/b deletion retards ES cell transition, correlating with delayed Nanog promoter methylation and phenocopying loss of Eprn or Lin28a. The connection from lncRNA to miRNA and DNA methylation facilitates the acute extinction of naïve pluripotency, a pre-requisite for rapid progression from preimplantation epiblast to gastrulation in rodents. Eprn illustrates how lncRNAs may introduce species-specific network modulations.","tags":[""],"title":"A lncRNA fine tunes the dynamics of a cell state transition involving Lin28, let-7 and de novo DNA methylation","type":"publication"},{"authors":["Sam Robson"],"categories":[],"content":"Marine biofilms, or \u0026ldquo;biofouling\u0026rdquo;, are communities of micro-organisms bound together on a surface by an extracellular matrix composed of extracellular polymeric substances (EPS). Biofilms consist predominantly of phytoplankton (particularly diatoms which are photosynthesising algae) and bacteria, which bind to surfaces in the marine environment to form complex 3 dimensional surface structures.\nThe formation of these biofilms can affect all types of surfaces in the marine environment, and substrates include such man-made structures as the hulls of ships, nets used by fishing trawlers, pipelines used to transport oil and gas, and boat propellors. Biofilm formation on ships can have a costly impact due to increased drag - for example the increased cost of fuel has been estimated at around $56 million per year for DDG-51 naval ships.\nFor this reason, testing novel antifouling substrates that are environmentally benign is of great importance to many marine-based industries. Working together with Dr. Maria Salta we have been analysing the changes in bacterial communities on different man-made surfaces using next-generation sequencing. This project combines microbiological analysis with both 16S rDNA amplicon sequencing (for populationa analysis) and whole transcriptome sequencing using RNA seq (for gene expression analysis).\n","date":1502928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559779200,"objectID":"1e4238030bc5529f1aa2644b01d02c2a","permalink":"/project/2017-08-17-anti-fouling-marine-biofilms/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/project/2017-08-17-anti-fouling-marine-biofilms/","section":"project","summary":"Metatranscriptomic analysis of marine biofilm composition on commercially available and novel anti-fouling substrates","tags":["Marine Biology","Microbiology","NGS","Transcriptomics"],"title":"Effects of Anti-Fouling Coatings on Marine Biofilms","type":"project"},{"authors":["Sam Robson"],"categories":[],"content":"Exposure to radiation can have deleterious effects on gene expression regulation, particularly due to increased DNA damage and deregulation of DNA damage checkpoint pathways. Working together with Dr. Adelaide Lerebours, we are performing differential expression analyses to understand the effects of radiation on 3-spined stickleback under laboratory conditions, and in particular attempting to characterise the differences between normal and abnormal offspring of exposed parents to understand how mutations are passed down through the lineage. In addition, we are looking at the effects of radiation exposure in the field, by looking at the effects of radiation exposure on lakes around the Chernobyl nuclear power station which suffered a catastrophic meltdown in 1986. The effects of this are still being felt today, and this project will allow us to understand the negative impact that this disaster has had on aquatic life in the area.\n","date":1502928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502928000,"objectID":"adcbaadb4e1d496d4a188c926b1bafd5","permalink":"/project/2017-08-17-effect-of-radiation-exposure-on-fish/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/project/2017-08-17-effect-of-radiation-exposure-on-fish/","section":"project","summary":"Identification of differentially regulated gene pathways as a result of radiation exposure","tags":["Marine Biology","Environmental Biology","NGS","Transcriptomics"],"title":"Effects of Radiation Exposure in the Environment","type":"project"},{"authors":["Sam Robson"],"categories":[],"content":"Duchenne muscular dystrophy (DMD) is an X-linked recessive form of muscular dystrophy typically presenting in boys at around the age of four. DMD is caused by mutations in the DMD gene, which is the largest gene in the genome with 79 exons encoding a range of dystrophin proteins. These proteins are expressed in skeletal and cardiac muscles and also in brain and other non-muscle tissues, where they have specific roles. Their absence due to DMD mutations causes specific abnormalities such as progressive muscle weakness and wasting and cognitive deficits. Muscle weakness begins in the upper legs and pelvis but quickly spreads until boys are unable to walk and eventually cannot breathe unaided. As yet, no cure exists for DMD, despite it being the most common form of muscular dystrophy.\nTogether with Professor Darek Gorecki, we are using RNA seq analysis to identify the changes in expression associated with a point mutation in the Dmd gene in a dystrophic mouse model. By identifying the pathways most impacted by the dystrophic phenotype, we hope to identify novel areas for future therapeutic intervention.\n","date":1502928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502928000,"objectID":"6a60bdf29286663b7b107cae40ffd6d9","permalink":"/project/2017-08-17-expression-profiling-of-dmd/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/project/2017-08-17-expression-profiling-of-dmd/","section":"project","summary":"Differential gene expression analysis in a model of Duchenne Muscular Dystrophy","tags":["Disease","NGS","Pharmacology","Transcriptomics"],"title":"Gene Expression Profiling of Duchenne Muscular Dystrophy","type":"project"},{"authors":["Sam Robson"],"categories":[],"content":"Together with Professor Matt Guille and Dr. Fiona Myers, we are examining development in the model species Xenopus laevis. We are using a wide range of techniques, including ChIP-seq to study epigenetic changes and to observe transcription factor binding throughout development, RNA-seq to study changes in gene expression, structural analysis of DNA tertiary structures, and Nanopore sequencing using the Oxford Nanopore MinION system to look at whole genome changes.\n","date":1502928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502928000,"objectID":"169b73f7c2265e5f1619d746a364d91d","permalink":"/project/2017-08-17-role-of-variant-histones-on-xenopus-development/","publishdate":"2017-08-17T00:00:00Z","relpermalink":"/project/2017-08-17-role-of-variant-histones-on-xenopus-development/","section":"project","summary":"Analysis of development in _Xenopus laevis_ using whole genome analysis","tags":["NGS","Epigenetics and Development","Transcriptomics","Genomics"],"title":"Xenopus Development Project","type":"project"},{"authors":["Sayou C","Millán-Zambrano G","Santos-Rosa H","Petfalski E","Sam Robson","Houseley J","Kouzarides T","Tollervey D"],"categories":null,"content":"","date":1493078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493078400,"objectID":"73735a7c46e21a4823b71f1951e7f03c","permalink":"/publication/2017_rna-binding-by-the-histone-methyltransferases-set1-and-set2/","publishdate":"2017-04-25T00:00:00Z","relpermalink":"/publication/2017_rna-binding-by-the-histone-methyltransferases-set1-and-set2/","section":"publication","summary":"Histone methylation at H3K4 and H3K36 is commonly associated with genes actively transcribed by RNA polymerase II (RNAPII) and is catalyzed by yeast Set1 and Set2, respectively. Here we report that both methyltransferases can be UV-crosslinked to RNA in vivo. High-throughput sequencing of the bound RNAs revealed strong Set1 enrichment near the transcription start site, whereas Set2 was distributed along pre-mRNAs. A subset of transcripts showed notably high enrichment for Set1 or Set2 binding relative to RNAPII, suggesting functional post-transcriptional interactions. In particular, Set1 was strongly bound to the SET1 mRNA, Ty1 retrotransposons, and non-coding RNAs from the rDNA intergenic spacers, consistent with its previously reported silencing roles. Set1 lacking RRM2 showed reduced in vivo crosslinking to RNA and reduced chromatin occupancy. In addition, levels of H3K4 tri-methylation were decreased whereas di-methylation was increased. We conclude that RNA binding by Set1 contributes to both chromatin association and methyltransferase activity.","tags":[""],"title":"RNA binding by the histone methyltransferases Set1 and Set2","type":"publication"},{"authors":["Bamborough P","Chung CW","Demont EH","Furze RC","Bannister AJ","Che KH","Diallo H","Douault C","Grandi P","Kouzarides T","Michon AM","Mitchell DJ","Prinjha RK","Rau C","Sam Robson","Sheppard RJ","Upton R","Watson RJ"],"categories":null,"content":"","date":1473638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473638400,"objectID":"bbc658e68fda627fc852e99a901b8766","permalink":"/publication/2016_a-chemical-probe-for-the-atad2-bromodomain/","publishdate":"2016-09-12T00:00:00Z","relpermalink":"/publication/2016_a-chemical-probe-for-the-atad2-bromodomain/","section":"publication","summary":"ATAD2 is a cancer-associated protein whose bromodomain has been described as among the least druggable of that target class. Starting from a potent lead, permeability and selectivity were improved through a dual approach: 1) using CF2 as a sulfone bio-isostere to exploit the unique properties of fluorine, and 2) using 1,3-interactions to control the conformation of a piperidine ring. This resulted in the first reported low-nanomolar, selective and cell permeable chemical probe for ATAD2.","tags":[""],"title":"A Chemical Probe for the ATAD2 Bromodomain","type":"publication"},{"authors":["Natalie H. Theodoulou","Paul Bamborough","Andrew J. Bannister","Isabelle Becher","Rino A. Bit","Ka Hing Che","Chun-wa Chung","Antje Dittmann","Gerard Drewes","David H. Drewry","Laurie Gordon","Paola Grandi","Melanie Leveridge","Matthew Lindon","Anne-Marie Michon","Judit Molnar","Sam Robson","Nicholas C. O. Tomkinson","Tony Kouzarides","Rab K. Prinjha","and Philip G. Humphreys"],"categories":null,"content":"","date":1456358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456358400,"objectID":"168571548d9a5ba419a5038315dd003e","permalink":"/publication/2016_discovery_of_i-brd9_a_selective_cell_active_chemical_probe_for_bromodomain_containing_protein_9_inhibition/","publishdate":"2016-02-25T00:00:00Z","relpermalink":"/publication/2016_discovery_of_i-brd9_a_selective_cell_active_chemical_probe_for_bromodomain_containing_protein_9_inhibition/","section":"publication","summary":"Acetylation of histone lysine residues is one of the most well-studied post-translational modifications of chromatin, selectively recognized by bromodomain “reader” modules. Inhibitors of the bromodomain and extra terminal domain (BET) family of bromodomains have shown profound anticancer and anti-inflammatory properties, generating much interest in targeting other bromodomain-containing proteins for disease treatment. Herein, we report the discovery of I-BRD9, the first selective cellular chemical probe for bromodomain-containing protein 9 (BRD9). I-BRD9 was identified through structure-based design, leading to greater than 700-fold selectivity over the BET family and 200-fold over the highly homologous bromodomain-containing protein 7 (BRD7). I-BRD9 was used to identify genes regulated by BRD9 in Kasumi-1 cells involved in oncology and immune response pathways and to the best of our knowledge, represents the first selective tool compound available to elucidate the cellular phenotype of BRD9 bromodomain inhibition.","tags":[""],"title":"Discovery of I-BRD9, a selective cell active chemical probe for bromodomain containing protein 9 inhibition","type":"publication"},{"authors":["Picaud S","Fedorov O","Thanasopoulou A","Leonards K","Jones K","Meier J","Olzscha H","Monteiro O","Martin S","Philpott M","Tumber A","Filippakopoulos P","Yapp C","Wells C","Che K","Bannister A","Sam Robson","Kumar U","Parr N","Lee K","Lugo D","Jeffrey P","Taylor S","Vecellio M","Bountra C","Brennan P","O'Mahony A","Velichko S","Müller S","Hay D","Daniels D","Urh M","La Thangue N","Kouzarides T","Prinjha R","Schwaller J","Knapp S"],"categories":null,"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"692370bf1d4605036b7e6bb977ecb084","permalink":"/publication/2015_generation_of_a_selective_small_molecule_inhibitor_of_the_cbp_p300_bromodomain_for_leukemia_therapy/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/2015_generation_of_a_selective_small_molecule_inhibitor_of_the_cbp_p300_bromodomain_for_leukemia_therapy/","section":"publication","summary":"The histone acetyltransferases CBP/p300 are involved in recurrent leukemia-associated chromosomal translocations and are key regulators of cell growth. Therefore, efforts to generate inhibitors of CBP/p300 are of clinical value. We developed a specific and potent acetyl-lysine competitive protein–protein interaction inhibitor, I-CBP112, that targets the CBP/p300 bromodomains. Exposure of human and mouse leukemic cell lines to I-CBP112 resulted in substantially impaired colony formation and induced cellular differentiation without significant cytotoxicity. I-CBP112 significantly reduced the leukemia-initiating potential of MLL-AF9+ acute myeloid leukemia cells in a dose-dependent manner in vitro and in vivo. Interestingly, I-CBP112 increased the cytotoxic activity of BET bromodomain inhibitor JQ1 as well as doxorubicin. Collectively, we report the development and preclinical evaluation of a novel, potent inhibitor targeting CBP/p300 bromodomains that impairs aberrant self-renewal of leukemic cells. The synergistic effects of I-CBP112 and current standard therapy (doxorubicin) as well as emerging treatment strategies (BET inhibition) provide new opportunities for combinatorial treatment of leukemia and potentially other cancers.","tags":[""],"title":"Generation of a selective small molecule inhibitor of the CBP/p300 bromodomain for leukemia therapy","type":"publication"},{"authors":["Viré E","Curtis C","Davalos V","Git A","Sam Robson","Villanueva A","Vidal A","Barbieri I","Aparicio S","Esteller M","Caldas C","Kouzarides T"],"categories":null,"content":"","date":1394064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394064000,"objectID":"2930d175b3c66d4f4e4f6ea3d5e2d347","permalink":"/publication/2014_the_breast_cancer_oncogene_emsy_represses_transcription_of_antimetastatic_microrna_mir-31/","publishdate":"2014-03-06T00:00:00Z","relpermalink":"/publication/2014_the_breast_cancer_oncogene_emsy_represses_transcription_of_antimetastatic_microrna_mir-31/","section":"publication","summary":"Amplification of the EMSY gene in sporadic breast and ovarian cancers is a poor prognostic indicator. Although EMSY has been linked to transcriptional silencing, its mechanism of action is unknown. Here, we report that EMSY acts as an oncogene, causing the transformation of cells in vitro and potentiating tumor formation and metastatic features in vivo. We identify an inverse correlation between EMSY amplification and miR-31 expression, an antimetastatic microRNA, in the METABRIC cohort of human breast samples. Re-expression of miR-31 profoundly reduced cell migration, invasion, and colony-formation abilities of cells overexpressing EMSY or haboring EMSY amplification. We show that EMSY is recruited to the miR-31 promoter by the DNA binding factor ETS-1, and it represses miR-31 transcription by delivering the H3K4me3 demethylase JARID1b/PLU-1/KDM5B. Altogether, these results suggest a pathway underlying the role of EMSY in breast cancer and uncover potential diagnostic and therapeutic targets in sporadic breast cancer.","tags":[""],"title":"The breast cancer oncogene EMSY represses transcription of antimetastatic microRNA miR-31","type":"publication"},{"authors":["Dawson M","Gudgin E","Horton S","Giotopoulos G","Meduri E","Sam Robson","Cannizzero E","Osaki H","Wiese M","Putwain S","Fong C","Grove C","Craig J","Dittmann A","Lugo D","Jeffrey P","Drewes G","Lee K","Bullinger L","Prinjha R","Kouzarides T","Vassiliou G","Huntly P"],"categories":null,"content":"","date":1391212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391212800,"objectID":"2d7f2cd59a32eb278ce8a1e96b7e080e","permalink":"/publication/2014_recurrent_mutations_including_nmp1c_activates_a_brd4_dependent_core_transcriptional_program_in_acute_myeloid_leukemia/","publishdate":"2014-02-01T00:00:00Z","relpermalink":"/publication/2014_recurrent_mutations_including_nmp1c_activates_a_brd4_dependent_core_transcriptional_program_in_acute_myeloid_leukemia/","section":"publication","summary":"Recent evidence suggests that inhibition of bromodomain and extra-terminal (BET) epigenetic readers may have clinical utility against acute myeloid leukemia (AML). Here we validate this hypothesis, demonstrating the efficacy of the BET inhibitor I-BET151 across a variety of AML subtypes driven by disparate mutations. We demonstrate that a common ‘core’ transcriptional program, which is HOX gene independent, is downregulated in AML and underlies sensitivity to I-BET treatment. This program is enriched for genes that contain ‘super-enhancers’, recently described regulatory elements postulated to control key oncogenic driver genes. Moreover, our program can independently classify AML patients into distinct cytogenetic and molecular subgroups, suggesting that it contains biomarkers of sensitivity and response. We focus AML with mutations of the Nucleophosmin gene (NPM1) and show evidence to suggest that wild-type NPM1 has an inhibitory influence on BRD4 that is relieved upon NPM1c mutation and cytosplasmic dislocation. This leads to the upregulation of the core transcriptional program facilitating leukemia development. This program is abrogated by I-BET therapy and by nuclear restoration of NPM1. Finally, we demonstrate the efficacy of I-BET151 in a unique murine model and in primary patient samples of NPM1c AML. Taken together, our data support the use of BET inhibitors in clinical trials in AML.","tags":[""],"title":"Recurrent mutations, including NPM1c, activate a BRD4-dependent core transcriptional program in acute myeloid leukemia","type":"publication"},{"authors":["Tessarz P","Santos-Rosa H","Sam Robson","Sylvestersen K","Nielson C","Kouzarides T"],"categories":null,"content":"","date":1390435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390435200,"objectID":"b378efd88d9281d6edaff4780595e325","permalink":"/publication/2014_glutamine_methylation_in_histone_h2a_is_an_rna-polymerase-i-dedicated_modification/","publishdate":"2014-01-23T00:00:00Z","relpermalink":"/publication/2014_glutamine_methylation_in_histone_h2a_is_an_rna-polymerase-i-dedicated_modification/","section":"publication","summary":"Nucleosomes are decorated with numerous post-translational modifications capable of influencing many DNA processes. Here we describe a new class of histone modification, methylation of glutamine, occurring on yeast histone H2A at position 105 (Q105) and human H2A at Q104. We identify Nop1 as the methyltransferase in yeast and demonstrate that fibrillarin is the orthologue enzyme in human cells. Glutamine methylation of H2A is restricted to the nucleolus. Global analysis in yeast, using an H2AQ105me-specific antibody, shows that this modification is exclusively enriched over the 35S ribosomal DNA transcriptional unit. We show that the Q105 residue is part of the binding site for the histone chaperone FACT (facilitator of chromatin transcription) complex. Methylation of Q105 or its substitution to alanine disrupts binding to FACT in vitro. A yeast strain mutated at Q105 shows reduced histone incorporation and increased transcription at the ribosomal DNA locus. These features are phenocopied by mutations in FACT complex components. Together these data identify glutamine methylation of H2A as the first histone epigenetic mark dedicated to a specific RNA polymerase and define its function as a regulator of FACT interaction with nucleosomes.","tags":[""],"title":"Glutamine methylation in histone H2A is an RNA-polymerase-I-dedicated modification","type":"publication"},{"authors":["Wyspianska B","Bannister A","Barbieri I","Nangalia J","Godfrey A","Calero-Nieto F","Sam Robson","Rioja I","Li J","Wiese M","Cannizzaro E","Dawson M","Huntly B","Prinjha R","Green A","Gottgens B","Kouzarides T"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"d7799f80cdd7242c4ce1a2c8a614ffd1","permalink":"/publication/2014_bet_protein_inhibition_shows_efficacy_against_jak2v617f-driven_neoplasms/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/2014_bet_protein_inhibition_shows_efficacy_against_jak2v617f-driven_neoplasms/","section":"publication","summary":"Small molecule inhibition of the BET family of proteins, which bind acetylated lysines within histones, has been shown to have a marked therapeutic benefit in pre-clinical models of mixed lineage leukemia (MLL) fusion protein-driven leukemias. Here, we report that I-BET151, a highly specific BET family bromodomain inhibitor, leads to growth inhibition in a human erythroleukemic (HEL) cell line as well as in erythroid precursors isolated from polycythemia vera patients. One of the genes most highly downregulated by I-BET151 was LMO2, an important oncogenic regulator of hematopoietic stem cell development and erythropoiesis. We previously reported that LMO2 transcription is dependent upon Janus kinase 2 (JAK2) kinase activity in HEL cells. Here, we show that the transcriptional changes induced by a JAK2 inhibitor (TG101209) and I-BET151 in HEL cells are significantly over-lapping, suggesting a common pathway of action. We generated JAK2 inhibitor resistant HEL cells and showed that these retain sensitivity to I-BET151. These data highlight I-BET151 as a potential alternative treatment against myeloproliferative neoplasms driven by constitutively active JAK2 kinase.","tags":[""],"title":"BET protein inhibition shows efficacy against JAK2V617F-driven neoplasms","type":"publication"},{"authors":["Castelo-Branco G","Amaral P","Engström P","Sam Robson","Marques S","Bertone P","Kouzarides T"],"categories":null,"content":"","date":1379376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379376000,"objectID":"eae8afbf2c89ec6955b5c8cf476546ff","permalink":"/publication/2013_the_non-coding_snrna_7sk_controls_transcriptional_termination_poising_and_bidirectionality_in_embryonic_stem_cells/","publishdate":"2013-09-17T00:00:00Z","relpermalink":"/publication/2013_the_non-coding_snrna_7sk_controls_transcriptional_termination_poising_and_bidirectionality_in_embryonic_stem_cells/","section":"publication","summary":"Pluripotency is characterized by a unique transcriptional state, in which lineage-specification genes are poised for transcription upon exposure to appropriate stimuli, via a bivalency mechanism involving the simultaneous presence of activating and repressive methylation marks at promoter-associated histones. Recent evidence suggests that other mechanisms, such as RNA polymerase II pausing, might be operational in this process, but their regulation remains poorly understood. Here we identify the non-coding snRNA 7SK as a multifaceted regulator of transcription in embryonic stem cells. We find that 7SK represses a specific cohort of transcriptionally poised genes with bivalent or activating chromatin marks in these cells, suggesting a novel poising mechanism independent of Polycomb activity. Genome-wide analysis shows that 7SK also prevents transcription downstream of polyadenylation sites at several active genes, indicating that 7SK is required for normal transcriptional termination or control of 3′-UTR length. In addition, 7SK suppresses divergent upstream antisense transcription at more than 2,600 loci, including many that encode divergent long non-coding RNAs, a finding that implicates the 7SK snRNA in the control of transcriptional bidirectionality. Our study indicates that a single non-coding RNA, the snRNA 7SK, is a gatekeeper of transcriptional termination and bidirectional transcription in embryonic stem cells and mediates transcriptional poising through a mechanism independent of chromatin bivalency.","tags":[""],"title":"The non-coding snRNA 7SK controls transcriptional termination, poising, and bidirectionality in embryonic stem cells","type":"publication"},{"authors":["Xhemalce B","Sam Robson","Kouzarides T"],"categories":null,"content":"","date":1350000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1350000000,"objectID":"ebde144cd5258ee85d5e572e113917c8","permalink":"/publication/2012_human_rna_methyltransferase_bcdin3d_regulates_microrna_processing/","publishdate":"2012-10-12T00:00:00Z","relpermalink":"/publication/2012_human_rna_methyltransferase_bcdin3d_regulates_microrna_processing/","section":"publication","summary":"MicroRNAs (miRNAs) regulate key biological processes and their aberrant expression may lead to cancer. The primary transcript of canonical miRNAs is sequentially cleaved by the RNase III enzymes, Drosha and Dicer, which generate 5′ monophosphate ends that are important for subsequent miRNA functions. In particular, the recognition of the 5′ monophosphate of pre-miRNAs by Dicer is important for precise and effective biogenesis of miRNAs. Here, we identify a RNA-methyltransferase, BCDIN3D, that O-methylates this 5′ monophosphate and negatively regulates miRNA maturation. Specifically, we show that BCDIN3D phospho-dimethylates pre-miR-145 both in vitro and in vivo and that phospho-dimethylated pre-miR-145 displays reduced processing by Dicer in vitro. Consistently, BCDIN3D depletion leads to lower pre-miR-145 and concomitantly increased mature miR-145 levels in breast cancer cells, which suppresses their tumorigenic phenotypes. Together, our results uncover a miRNA methylation pathway potentially involved in cancer that antagonizes the Dicer-dependent processing of miR-145 as well as other miRNAs.","tags":[""],"title":"Human RNA methyltransferase BCDIN3D regulates microRNA processing","type":"publication"},{"authors":["Dawson M","Foster S","Bannister A","Sam Robson","Hannah R","Wang X","Vhemalce B","Wood A","Green A","Göttgens B","Kouzarides T"],"categories":null,"content":"","date":1348704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1348704000,"objectID":"d2d0687c362484c4cafcc71907a136cc","permalink":"/publication/2012_three_distinct_patterns_of_histone_h3y41ph_phosphorylation_mark_active_genes/","publishdate":"2012-09-27T00:00:00Z","relpermalink":"/publication/2012_three_distinct_patterns_of_histone_h3y41ph_phosphorylation_mark_active_genes/","section":"publication","summary":"The JAK2 tyrosine kinase is a critical mediator of cytokine-induced signaling. It plays a role in the nucleus, where it regulates transcription by phosphorylating histone H3 at tyrosine 41 (H3Y41ph). We used chromatin immunoprecipitation coupled to massively parallel DNA sequencing (ChIP-seq) to define the genome-wide pattern of H3Y41ph in human erythroid leukemia cells. Our results indicate that H3Y41ph is located at three distinct sites: (1) at a subset of active promoters, where it overlaps with H3K4me3, (2) at distal cis-regulatory elements, where it coincides with the binding of STAT5, and (3) throughout the transcribed regions of active, tissue-specific hematopoietic genes. Together, these data extend our understanding of this conserved and essential signaling pathway and provide insight into the mechanisms by which extracellular stimuli may lead to the coordinated regulation of transcription.","tags":[""],"title":"Three distinct patterns of histone H3Y41 phosphorylation mark active genes","type":"publication"},{"authors":["Sam Robson","Ward L","Brown H","Turner H","Hunter E","Pelengaris S","Khan M"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"20776d147e4ecc08e1f6a1b9ba5c0402","permalink":"/publication/2011_deciphering_c-myc-regulated_genes_in_two_distinct_tissues/","publishdate":"2011-12-01T00:00:00Z","relpermalink":"/publication/2011_deciphering_c-myc-regulated_genes_in_two_distinct_tissues/","section":"publication","summary":"The transcription factor MYC is a critical regulator of diverse cellular processes, including both replication and apoptosis. Differences in MYC-regulated gene expression responsible for such opposing outcomes in vivo remain obscure. To address this we have examined time-dependent changes in global gene expression in two transgenic mouse models in which MYC activation, in either skin suprabasal keratinocytes or pancreatic islet β-cells, promotes tissue expansion or involution, respectively. Consistent with observed phenotypes, expression of cell cycle genes is increased in both models (albeit enriched in β-cells), as are those involved in cell growth and metabolism, while expression of genes involved in cell differentiation is down-regulated. However, in β-cells, which unlike suprabasal keratinocytes undergo prominent apoptosis from 24 hours, there is up-regulation of genes associated with DNA-damage response and intrinsic apoptotic pathways, including Atr, Arf, Bax and Cycs. In striking contrast, this is not the case for suprabasal keratinocytes, where pro-apoptotic genes such as Noxa are down-regulated and key anti-apoptotic pathways (such as Igf1-Akt) and those promoting angiogenesis are up-regulated. Moreover, dramatic up-regulation of steroid hormone-regulated Kallikrein serine protease family members in suprabasal keratinocytes alone could further enhance local Igf1 actions, such as through proteolysis of Igf1 binding proteins. Activation of MYC causes cell growth, loss of differentiation and cell cycle entry in both β-cells and suprabasal keratinocytes in vivo. Apoptosis, which is confined to β-cells, may involve a combination of a DNA-damage response and downstream activation of pro-apoptotic signalling pathways, including Cdc2a and p19Arf/p53, and downstream targets. Conversely, avoidance of apoptosis in suprabasal keratinocytes may result primarily from the activation of key anti-apoptotic signalling pathways, particularly Igf1-Akt, and induction of an angiogenic response, though intrinsic resistance to induction of p19Arf by MYC in suprabasal keratinocytes may contribute.","tags":[""],"title":"Deciphering c-MYC-regulated genes in two distinct tissues","type":"publication"},{"authors":["Dawson MA","Prinjha RK","Dittmann A","Giotopoulos G","Bantscheff M","Chan WI","Sam Robson","Chung CW","Hopf C","Savitski MM","Huthmacher C","Gudgin E","Lugo D","Beinke S","Chapman TD","Roberts EJ","Soden PE","Auger KR","Mirguet O","Doehner K","Delwel R","Burnett AK","Jeffrey P","Drewes G","Lee K","Huntly BJ","Kouzarides T"],"categories":null,"content":"","date":1317513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1317513600,"objectID":"6a0d5666d19d41605cef1941c081ffa5","permalink":"/publication/2011_inhibition_of_bet_recruitment_to_chromatin_as_an_effective_treatment_for_mll-fusion_leaukemia/","publishdate":"2011-10-02T00:00:00Z","relpermalink":"/publication/2011_inhibition_of_bet_recruitment_to_chromatin_as_an_effective_treatment_for_mll-fusion_leaukemia/","section":"publication","summary":"Recurrent chromosomal translocations involving the mixed lineage leukaemia (MLL) gene initiate aggressive forms of leukaemia, which are often refractory to conventional therapies. Many MLL-fusion partners are members of the super elongation complex (SEC), a critical regulator of transcriptional elongation, suggesting that aberrant control of this process has an important role in leukaemia induction. Here we use a global proteomic strategy to demonstrate that MLL fusions, as part of SEC2, 3 and the polymerase-associated factor complex (PAFc), are associated with the BET family of acetyl-lysine recognizing, chromatin ‘adaptor’ proteins. These data provided the basis for therapeutic intervention in MLL-fusion leukaemia, via the displacement of the BET family of proteins from chromatin. We show that a novel small molecule inhibitor of the BET family, GSK1210151A (I-BET151), has profound efficacy against human and murine MLL-fusion leukaemic cell lines, through the induction of early cell cycle arrest and apoptosis. I-BET151 treatment in two human leukaemia cell lines with different MLL fusions alters the expression of a common set of genes whose function may account for these phenotypic changes. The mode of action of I-BET151 is, at least in part, due to the inhibition of transcription at key genes (BCL2, C-MYC and CDK6) through the displacement of BRD3/4, PAFc and SEC components from chromatin. In vivo studies indicate that I-BET151 has significant therapeutic value, providing survival benefit in two distinct mouse models of murine MLL–AF9 and human MLL–AF4 leukaemia. Finally, the efficacy of I-BET151 against human leukaemia stem cells is demonstrated, providing further evidence of its potent therapeutic potential. These findings establish the displacement of BET proteins from chromatin as a promising epigenetic therapy for these aggressive leukaemias.","tags":[""],"title":"Inhibition of BET recruitment to chromatin as an effective treatment for MLL-fusion leukaemia","type":"publication"},{"authors":["Bartke T","Vermeulen M","Xhemalce B","Sam Robson","Kouzarides T"],"categories":null,"content":"","date":1288310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288310400,"objectID":"f475e0a83456a4dcb73011d8f91ddf48","permalink":"/publication/2010_nucleosome-interacting_proteins_regulated_by_dna_and_histone_methylation/","publishdate":"2010-10-29T00:00:00Z","relpermalink":"/publication/2010_nucleosome-interacting_proteins_regulated_by_dna_and_histone_methylation/","section":"publication","summary":"Modifications on histones or on DNA recruit proteins that regulate chromatin function. Here, we use nucleosomes methylated on DNA and on histone H3 in an affinity assay, in conjunction with a SILAC-based proteomic analysis, to identify 'crosstalk' between these two distinct classes of modification. Our analysis reveals proteins whose binding to nucleosomes is regulated by methylation of CpGs, H3K4, H3K9, and H3K27 or a combination thereof. We identify the origin recognition complex (ORC), including LRWD1 as a subunit, to be a methylation-sensitive nucleosome interactor that is recruited cooperatively by DNA and histone methylation. Other interactors, such as the lysine demethylase Fbxl11/KDM2A, recognize nucleosomes methylated on histones, but their recruitment is disrupted by DNA methylation. These data establish SILAC nucleosome affinity purifications (SNAP) as a tool for studying the dynamics between different chromatin modifications and provide a modification binding 'profile' for proteins regulated by DNA and histone methylation.","tags":[""],"title":"Nucleosome-interacting proteins regulated by DNA and histone methylation","type":"publication"},{"authors":["Wellcome Trust Case Control Consortium","Craddock N","Hurles ME","Cardin N","Pearson RD","Plagnol V","Sam Robson","Vukcevic D","Barnes C","Conrad DF","Giannoulatou E","Holmes C","Marchini JL","Stirrups K","Tobin MD","Wain LV","Yau C","Aerts J","Ahmad T","Andrews TD","Arbury H","Attwood A","Auton A","Ball SG","Balmforth AJ","Barrett JC","Barroso I","Barton A","Bennett AJ","Bhaskar S","Blaszczyk K","Bowes J","Brand OJ","Braund PS","Bredin F","Breen G","Brown MJ","Bruce IN","Bull J","Burren OS","Burton J","Byrnes J","Caesar S","Clee CM","Coffey AJ","Connell JM","Cooper JD","Dominiczak AF","Downes K","Drummond HE","Dudakia D","Dunham A","Ebbs B","Eccles D","Edkins S","Edwards C","Elliot A","Emery P","Evans DM","Evans G","Eyre S","Farmer A","Ferrier IN","Feuk L","Fitzgerald T","Flynn E","Forbes A","Forty L","Franklyn JA","Freathy RM","Gibbs P","Gilbert P","Gokumen O","Gordon-Smith K","Gray E","Green E","Groves CJ","Grozeva D","Gwilliam R","Hall A","Hammond N","Hardy M","Harrison P","Hassanali N","Hebaishi H","Hines S","Hinks A","Hitman GA","Hocking L","Howard E","Howard P","Howson JM","Hughes D","Hunt S","Isaacs JD","Jain M","Jewell DP","Johnson T","Jolley JD","Jones IR","Jones LA","Kirov G","Langford CF","Lango-Allen H","Lathrop GM","Lee J","Lee KL","Lees C","Lewis K","Lindgren CM","Maisuria-Armer M","Maller J","Mansfield J","Martin P","Massey DC","McArdle WL","McGuffin P","McLay KE","Mentzer A","Mimmack ML","Morgan AE","Morris AP","Mowat C","Myers S","Newman W","Nimmo ER","O'Donovan MC","Onipinla A","Onyiah I","Ovington NR","Owen MJ","Palin K","Parnell K","Pernet D","Perry JR","Phillips A","Pinto D","Prescott NJ","Prokopenko I","Quail MA","Rafelt S","Rayner NW","Redon R","Reid DM","Renwick","Ring SM","Robertson N","Russell E","St Clair D","Sambrook JG","Sanderson JD","Schuilenburg H","Scott CE","Scott R","Seal S","Shaw-Hawkins S","Shields BM","Simmonds MJ","Smyth DJ","Somaskantharajah E","Spanova K","Steer S","Stephens J","Stevens HE","Stone MA","Su Z","Symmons DP","Thompson JR","Thomson W","Travers ME","Turnbull C","Valsesia A","Walker M","Walker NM","Wallace C","Warren-Perry M","Watkins NA","Webster J","Weedon MN","Wilson AG","Woodburn M","Wordsworth BP","Young AH","Zeggini E","Carter NP","Frayling TM","Lee C","McVean G","Munroe PB","Palotie A","Sawcer SJ","Scherer SW","Strachan DP","Tyler-Smith C","Brown MA","Burton PR","Caulfield MJ","Compston A","Farrall M","Gough SC","Hall AS","Hattersley AT","Hill AV","Mathew CG","Pembrey M","Satsangi J","Stratton MR","Worthington J","Deloukas P","Duncanson A","Kwiatkowski DP","McCarthy MI","Ouwehand W","Parkes M","Rahman N","Todd JA","Samani NJ","Donnelly P"],"categories":null,"content":"","date":1270080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1270080000,"objectID":"8649e7b015064ff60d6114f27ad8ac0f","permalink":"/publication/2010_genome-wide_association_study_of_cnvs_in_16000_cases_of_eight_common_diseases_and_3000_shared_controls/","publishdate":"2010-04-01T00:00:00Z","relpermalink":"/publication/2010_genome-wide_association_study_of_cnvs_in_16000_cases_of_eight_common_diseases_and_3000_shared_controls/","section":"publication","summary":"Copy number variants (CNVs) account for a major proportion of human genetic polymorphism and have been predicted to have an important role in genetic susceptibility to common disease. To address this we undertook a large, direct genome-wide study of association between CNVs and eight common human diseases. Using a purpose-designed array we typed approximately 19,000 individuals into distinct copy-number classes at 3,432 polymorphic CNVs, including an estimated approximately 50% of all common CNVs larger than 500 base pairs. We identified several biological artefacts that lead to false-positive associations, including systematic CNV differences between DNAs derived from blood and cell lines. Association testing and follow-up replication analyses confirmed three loci where CNVs were associated with disease-IRGM for Crohn's disease, HLA for Crohn's disease, rheumatoid arthritis and type 1 diabetes, and TSPAN8 for type 2 diabetes-although in each case the locus had previously been identified in single nucleotide polymorphism (SNP)-based studies, reflecting our observation that most common CNVs that are well-typed on our array are well tagged by SNPs and so have been indirectly explored through SNP studies. We conclude that common CNVs that can be typed on existing platforms are unlikely to contribute greatly to the genetic basis of common human diseases.","tags":[""],"title":"Genome-wide association study of CNVs in 16,000 cases of eight common diseases and 3,000 shared controls","type":"publication"},{"authors":["Conrad D","Pinto D","Redon R","Feuk L","Gokcumen O","Zhang Y","Aerts J","Andrews D","Barnes C","Campbell P","Fitzgerald T","Hu M","Hwa C","Kristiansson K","Macarthur D","Macdonald J","Onyiah N","Wing Chun Pang A","Sam Robson","Stirrups K","Valsesia A","Walter K","Wei J","Tyler-Smith C","Carter N","Lee C","Scherer S","Hurles M"],"categories":null,"content":"","date":1270080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1270080000,"objectID":"40db7918363aee1847f20cf5dafee88a","permalink":"/publication/2010_origins_and_functional_impact_of_copy_number_variation_in_the_human_genome/","publishdate":"2010-04-01T00:00:00Z","relpermalink":"/publication/2010_origins_and_functional_impact_of_copy_number_variation_in_the_human_genome/","section":"publication","summary":"Structural variations of DNA greater than 1 kilobase in size account for most bases that vary among human genomes, but are still relatively under-ascertained. Here we use tiling oligonucleotide microarrays, comprising 42 million probes, to generate a comprehensive map of 11,700 copy number variations (CNVs) greater than 443 base pairs, of which most (8,599) have been validated independently. For 4,978 of these CNVs, we generated reference genotypes from 450 individuals of European, African or East Asian ancestry. The predominant mutational mechanisms differ among CNV size classes. Retrotransposition has duplicated and inserted some coding and non-coding DNA segments randomly around the genome. Furthermore, by correlation with known trait-associated single nucleotide polymorphisms (SNPs), we identified 30 loci with CNVs that are candidates for influencing disease susceptibility. Despite this, having assessed the completeness of our map and the patterns of linkage disequilibrium between CNVs and SNPs, we conclude that, for complex traits, the heritability void left by genome-wide association studies will not be accounted for by common CNVs.","tags":[""],"title":"Origins and functional impact of copy number variation in the human genome","type":"publication"},{"authors":null,"categories":null,"content":" Twenty Seconds Curriculum Vitae in LaTex \nCurricula Vitae - Résumés A curriculum vitae, otherwise known as a CV or résumé, is a document used by individuals to communicate their work history, education and skill set. This is a style template for your curriculum written in LaTex. The main goal of this template is to provide a curriculum that is able to survive to the résumés screening of \u0026ldquo;twenty seconds\u0026rdquo;.\nThe author assumes no responsibility for the topicality, correctness, completeness or quality of the information provided and for the obtained résumés.\nThis is designed for computer scientists but there is no limitation to use it for résumés in other disciplines.\nIf you like this curriculum, please don\u0026rsquo;t forget to leave a star, to help the development and improvement. The basic idea is KISS - Keep It Simple, Stupid. In a nutshell \u0026ldquo;It is vain to do with more what can be done with fewer\u0026rdquo; \u0026ndash; Occam\u0026rsquo;s razor \u0026ndash;\n This template has been designed to create a \u0026ldquo;one-page\u0026rdquo; résumé is therefore not suitable to create curriculum of more than one-page.\n Please do not try to create curriculum more than one-page.\n  How to describe your experiences? There are many theories about the résumé screening process of \u0026ldquo;Big\u0026rdquo; companies. Resume screeners and the interviewers look in your résumé for:\n Are you smart? Can you code (act for what you apply)?  Anyway according to the guidelines of this template you should use a really simple form to describe each items in your résumé:\nAccomplished \u0026lt;X\u0026gt; by implementing \u0026lt;Y\u0026gt; which led to \u0026lt;Z\u0026gt;  Here\u0026rsquo;s an examples:\nReduced object rendering time by 75% by applying Floyd's algorithm, leading to a 10% reduction in system boot time.  \u0026ndash; Cracking the Coding Interview, Book, Gayle Laakmann Mcdowell \u0026ndash;\nToy Résumé Build This guide walks you to build your résumé.\nBuild requirements:\n LaTex installation.  additionals packages:\n ClearSans, fontenc tikz xcolor textpos ragged2e etoolbox ifmtarg ifthen pgffor marvosym parskip    Build through GNU Make command Clean your project résumé.\nmake clean  Build your project résumé.\nmake all  \u0026ndash; Alternately you can build through your favorite LaTex editor. \u0026ndash;\nEnvironment style and list of commands The style is divided in two parts. The former is the left side bar: that contains personal information, profile picture, and information about your professional skills. The second part is the body that should be contains details about your academic studies, professional experiences and all the information that you want (remember the KISS principle).\nProfile environment These are the command to set up the profile information.\n Set up the image profile.\n\\profilepic{paht_name}  Set up your name.\n\\cvname{your name}  Set up your job profile.\n\\cvjobtitle{your job title}  Set up your date of birth.\n\\cvdate{date}  Set up your address.\n\\cvaddress{address}  Set up your telephone number.\n\\cvnumberphone{phone number}  Set up your email.\n\\cvmail{email address}  Set up your personal home page.\n\\cvsite{home page address}  Set up a brief description of you.\n\\about{brief description}  Set up the skills with chart style. Each skill must is a couple {name/value}, where the value is a floating point value between 0 and 6. This is an agreement for the graphics issues, the 0 correspond to a Fundamental awareness while 6 to a Expert awareness level.\n\\skills{{name skill1/5.8},{name skill2/4}}  Set up the skills with text style.\n\\skillstext{{name skill1/5.8},{name skill2/4}}   To create the profile use the command:\n\\makeprofile  Body environment The body document part is composed by sections. In the sections you can put two kinds of list items.\nThe first (Twenty items environment) intends a list of detailed information with four part: Data \u0026ndash; Title \u0026ndash; Place \u0026ndash; Description.\nThe second (Twenty items short environment) intends a fewer informationinformation (you can customize this list more easily): Data \u0026ndash; Description.\nSections  Set up a new section in the body part.\n\\section{sction name}   Twenty items environment \\begin{twenty} \\twentyitem {year} {title} {place} {description} \\end{twenty}  Twenty items short environment \\begin{twentyshort} \\twentyitemshort {year} {description} \\end{twentyshort}  Other commands There other two fun command: \\icon and \\round; that enables to wrap the text in oval shape.\n\t\\icon{text} \\round{text, color}  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6bcff10fd4ed43417cbdcf0da7e4fb23","permalink":"/cv/samrobson_cv_academic/readme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/samrobson_cv_academic/readme/","section":"cv","summary":"Twenty Seconds Curriculum Vitae in LaTex \nCurricula Vitae - Résumés A curriculum vitae, otherwise known as a CV or résumé, is a document used by individuals to communicate their work history, education and skill set. This is a style template for your curriculum written in LaTex. The main goal of this template is to provide a curriculum that is able to survive to the résumés screening of \u0026ldquo;twenty seconds\u0026rdquo;.","tags":null,"title":"","type":"cv"}]