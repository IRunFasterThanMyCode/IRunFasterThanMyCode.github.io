load("URC_posts.Rdat")
library("dplyr")
posts50 <- URC                 %>%
group_by(from_name) %>%    ## Group by poster
filter(n() >= 50)   %>%    ## Select only posters with >50 posts
select(from_name, message) ## Keep poster name and message content
set.seed(0) ## Set seed for random number generation for reproducibility
ids   <- sample(1:nrow(posts50), 4000) ## Randomly select 4000
train <- posts50[ids,]  ## Keep random ids as training set
test  <- posts50[-ids,] ## Use remaining ids as validation
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE, eval = TRUE, results = "hold", fig.height = 10, fig.width = 10, dpi = 300)
options(scipen = 999)
load("URC_posts.Rdat")
library("dplyr")
library(text2vec)
library(text2vec)
train_tokens <- train$message                      %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_train <- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train
vocab      <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
train_dtm  <- create_dtm(it_train, vectorizer)
dim(train_dtm)
set.seed(0)
load("URC_posts.Rdat")
library("dplyr")
posts50 <- URC                 %>%
group_by(from_name) %>%    ## Group by poster
filter(n() >= 50)   %>%    ## Select only posters with >50 posts
select(from_name, message) ## Keep poster name and message content
set.seed(0) ## Set seed for random number generation for reproducibility
ids   <- sample(1:nrow(posts50), 4000) ## Randomly select 4000
train <- posts50[ids,]  ## Keep random ids as training set
test  <- posts50[-ids,] ## Use remaining ids as validation
library(text2vec)
train_tokens <- train$message                      %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_train <- itoken(train_tokens, ids = train$from_name, progressbar = FALSE)
it_train
vocab      <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
train_dtm  <- create_dtm(it_train, vectorizer)
dim(train_dtm)
library("randomForest")
load("rf_model.Rdat")
test_tokens <- test$message                       %>%
iconv("latin1", "ASCII", sub = "") %>% # Convert to ASCII format
tolower                            %>% # Make lower case
word_tokenizer                         # Break terms into tokens
it_test     <- itoken(test_tokens, ids = test$from_name, progressbar = FALSE)
test_dtm    <- create_dtm(it_test, vectorizer)
test_predict <- predict.train(rf_model, test_dtm, type = "response")
library("caret")
test_predict <- predict.train(rf_model, test_dtm, type = "response")
load("rf_model.Rdat")
test_predict
test_predict <- predict.train(rf_model, test_dtm, type = "response")
sessioninfo()
session.info()
sessionInfo()
sessionInfo()
